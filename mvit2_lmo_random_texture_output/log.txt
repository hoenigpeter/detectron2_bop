[09/20 10:54:42] detectron2 INFO: Rank of current process: 0. World size: 1
[09/20 10:54:42] detectron2 INFO: Environment info:
----------------------  -------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.10 (default, May 26 2023, 14:05:08) [GCC 9.4.0]
numpy                   1.23.5
detectron2              0.6 @/home/hoenig/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 9.4
CUDA compiler           CUDA 11.6
detectron2 arch flags   8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          525.125.06
CUDA_HOME               /usr/local/cuda
Pillow                  9.3.0
torchvision             0.13.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221122
iopath                  0.1.9
cv2                     4.3.0
----------------------  -------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/20 10:54:42] detectron2 INFO: Command line arguments: Namespace(config_file='', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[09/20 10:54:42] d2.config.lazy WARNING: The config contains objects that cannot serialize to a valid yaml. ./mvit2_lmo_random_texture_output/config.yaml is human-readable but cannot be loaded.
[09/20 10:54:42] d2.config.lazy WARNING: Config is saved using cloudpickle at ./mvit2_lmo_random_texture_output/config.yaml.pkl.
[09/20 10:54:42] detectron2 INFO: Full config saved to ./mvit2_lmo_random_texture_output/config.yaml
[09/20 10:54:42] d2.utils.env INFO: Using a generated random seed 45365869
[09/20 10:54:43] detectron2 INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): MViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      )
      (blocks): ModuleList(
        (0): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=96, out_features=192, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (2): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (3): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=192, out_features=384, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (4): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (6): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.107)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.120)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.133)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.147)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (12): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.160)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (13): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.173)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (14): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.187)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=384, out_features=768, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (15): MultiScaleBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (scale2_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (scale3_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (scale4_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (scale5_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): CascadeROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): ModuleList(
      (0): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (1): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (2): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
    )
    (box_predictor): ModuleList(
      (0): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=9, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (1): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=9, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (2): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=9, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 8, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[09/20 10:54:49] d2.data.datasets.coco INFO: Loading datasets/BOP_DATASETS/lmo_random_texture_all/lmo_random_texture_all_annotations_train.json takes 4.21 seconds.
[09/20 10:54:49] d2.data.datasets.coco INFO: Loaded 50000 images in COCO format from datasets/BOP_DATASETS/lmo_random_texture_all/lmo_random_texture_all_annotations_train.json
[09/20 10:54:50] d2.data.build INFO: Removed 1 images with no usable annotations. 49999 images left.
[09/20 10:54:51] d2.data.build INFO: Distribution of instances among all 8 categories:
[36m|  category  | #instances   | category   | #instances   | category   | #instances   |
|:----------:|:-------------|:-----------|:-------------|:-----------|:-------------|
|     1      | 42661        | 2          | 46350        | 3          | 44932        |
|     4      | 46815        | 5          | 43326        | 6          | 44723        |
|     7      | 45088        | 8          | 44727        |            |              |
|   total    | 358622       |            |              |            |              |[0m
[09/20 10:54:51] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: []
[09/20 10:54:51] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[09/20 10:54:51] d2.data.common INFO: Serializing 49999 elements to byte tensors and concatenating them all ...
[09/20 10:54:52] d2.data.common INFO: Serialized dataset takes 158.09 MiB
[09/20 10:54:53] fvcore.common.checkpoint INFO: [Checkpointer] Loading from detectron2://ImageNetPretrained/mvitv2/MViTv2_S_in1k.pyth ...
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv1.norm.bias in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv1.norm.weight in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv2.norm.bias in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv2.norm.weight in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv3.norm.bias in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv3.norm.weight in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv4.norm.bias in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv4.norm.weight in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv1.norm.bias in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv1.norm.weight in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv2.norm.bias in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv2.norm.weight in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv3.norm.bias in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv3.norm.weight in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv4.norm.bias in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv4.norm.weight in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv1.norm.bias in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv1.norm.weight in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv2.norm.bias in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv2.norm.weight in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv3.norm.bias in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv3.norm.weight in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv4.norm.bias in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv4.norm.weight in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn1.norm.bias in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn1.norm.weight in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn2.norm.bias in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn2.norm.weight in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn3.norm.bias in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn3.norm.weight in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn4.norm.bias in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn4.norm.weight in model is torch.Size([256]).
[09/20 10:54:53] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/20 10:54:53] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone.bottom_up:
| Names in Model      | Names in Checkpoint                                                                                                                                                                                    | Shapes                                                                                                                   |
|:--------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|
| blocks.0.attn.*     | blocks.0.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (96,) (96,96) (288,) (288,96) (111,96) (111,96)     |
| blocks.0.mlp.fc1.*  | blocks.0.mlp.fc1.{bias,weight}                                                                                                                                                                         | (384,) (384,96)                                                                                                          |
| blocks.0.mlp.fc2.*  | blocks.0.mlp.fc2.{bias,weight}                                                                                                                                                                         | (96,) (96,384)                                                                                                           |
| blocks.0.norm1.*    | blocks.0.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.0.norm2.*    | blocks.0.norm2.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.attn.*     | blocks.1.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,96) (55,96) (55,96)    |
| blocks.1.mlp.fc1.*  | blocks.1.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.1.mlp.fc2.*  | blocks.1.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.1.norm1.*    | blocks.1.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.norm2.*    | blocks.1.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.1.proj.*     | blocks.1.proj.{bias,weight}                                                                                                                                                                            | (192,) (192,96)                                                                                                          |
| blocks.10.attn.*    | blocks.10.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.10.mlp.fc1.* | blocks.10.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.10.mlp.fc2.* | blocks.10.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.10.norm1.*   | blocks.10.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.10.norm2.*   | blocks.10.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.attn.*    | blocks.11.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.11.mlp.fc1.* | blocks.11.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.11.mlp.fc2.* | blocks.11.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.11.norm1.*   | blocks.11.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.norm2.*   | blocks.11.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.attn.*    | blocks.12.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.12.mlp.fc1.* | blocks.12.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.12.mlp.fc2.* | blocks.12.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.12.norm1.*   | blocks.12.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.norm2.*   | blocks.12.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.attn.*    | blocks.13.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.13.mlp.fc1.* | blocks.13.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.13.mlp.fc2.* | blocks.13.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.13.norm1.*   | blocks.13.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.norm2.*   | blocks.13.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.attn.*    | blocks.14.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,384) (27,96) (27,96) |
| blocks.14.mlp.fc1.* | blocks.14.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.14.mlp.fc2.* | blocks.14.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.14.norm1.*   | blocks.14.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.norm2.*   | blocks.14.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.14.proj.*    | blocks.14.proj.{bias,weight}                                                                                                                                                                           | (768,) (768,384)                                                                                                         |
| blocks.15.attn.*    | blocks.15.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,768) (13,96) (13,96) |
| blocks.15.mlp.fc1.* | blocks.15.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.15.mlp.fc2.* | blocks.15.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.15.norm1.*   | blocks.15.norm1.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.15.norm2.*   | blocks.15.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.2.attn.*     | blocks.2.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,192) (55,96) (55,96)   |
| blocks.2.mlp.fc1.*  | blocks.2.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.2.mlp.fc2.*  | blocks.2.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.2.norm1.*    | blocks.2.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.2.norm2.*    | blocks.2.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.3.attn.*     | blocks.3.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,192) (55,96) (55,96) |
| blocks.3.mlp.fc1.*  | blocks.3.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.3.mlp.fc2.*  | blocks.3.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.3.norm1.*    | blocks.3.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.3.norm2.*    | blocks.3.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.3.proj.*     | blocks.3.proj.{bias,weight}                                                                                                                                                                            | (384,) (384,192)                                                                                                         |
| blocks.4.attn.*     | blocks.4.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.4.mlp.fc1.*  | blocks.4.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.4.mlp.fc2.*  | blocks.4.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.4.norm1.*    | blocks.4.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.4.norm2.*    | blocks.4.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.5.attn.*     | blocks.5.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.5.mlp.fc1.*  | blocks.5.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.5.mlp.fc2.*  | blocks.5.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.5.norm1.*    | blocks.5.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.5.norm2.*    | blocks.5.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.6.attn.*     | blocks.6.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.6.mlp.fc1.*  | blocks.6.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.6.mlp.fc2.*  | blocks.6.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.6.norm1.*    | blocks.6.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.6.norm2.*    | blocks.6.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.attn.*     | blocks.7.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.7.mlp.fc1.*  | blocks.7.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.7.mlp.fc2.*  | blocks.7.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.7.norm1.*    | blocks.7.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.norm2.*    | blocks.7.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.attn.*     | blocks.8.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.8.mlp.fc1.*  | blocks.8.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.8.mlp.fc2.*  | blocks.8.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.8.norm1.*    | blocks.8.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.norm2.*    | blocks.8.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.attn.*     | blocks.9.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.9.mlp.fc1.*  | blocks.9.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.9.mlp.fc2.*  | blocks.9.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.9.norm1.*    | blocks.9.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.norm2.*    | blocks.9.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| patch_embed.proj.*  | patch_embed.proj.{bias,weight}                                                                                                                                                                         | (96,) (96,3,7,7)                                                                                                         |
[09/20 10:54:54] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.bottom_up.scale2_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale3_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale4_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale5_norm.{bias, weight}[0m
[34mbackbone.fpn_lateral2.{bias, weight}[0m
[34mbackbone.fpn_lateral3.{bias, weight}[0m
[34mbackbone.fpn_lateral4.{bias, weight}[0m
[34mbackbone.fpn_lateral5.{bias, weight}[0m
[34mbackbone.fpn_output2.{bias, weight}[0m
[34mbackbone.fpn_output3.{bias, weight}[0m
[34mbackbone.fpn_output4.{bias, weight}[0m
[34mbackbone.fpn_output5.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv0.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv1.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mroi_heads.box_head.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv1.weight[0m
[34mroi_heads.box_head.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv2.weight[0m
[34mroi_heads.box_head.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv3.weight[0m
[34mroi_heads.box_head.0.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv4.weight[0m
[34mroi_heads.box_head.0.fc1.{bias, weight}[0m
[34mroi_heads.box_head.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv1.weight[0m
[34mroi_heads.box_head.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv2.weight[0m
[34mroi_heads.box_head.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv3.weight[0m
[34mroi_heads.box_head.1.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv4.weight[0m
[34mroi_heads.box_head.1.fc1.{bias, weight}[0m
[34mroi_heads.box_head.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv1.weight[0m
[34mroi_heads.box_head.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv2.weight[0m
[34mroi_heads.box_head.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv3.weight[0m
[34mroi_heads.box_head.2.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv4.weight[0m
[34mroi_heads.box_head.2.fc1.{bias, weight}[0m
[34mroi_heads.box_predictor.0.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.0.cls_score.{bias, weight}[0m
[34mroi_heads.box_predictor.1.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.1.cls_score.{bias, weight}[0m
[34mroi_heads.box_predictor.2.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.2.cls_score.{bias, weight}[0m
[34mroi_heads.mask_head.deconv.{bias, weight}[0m
[34mroi_heads.mask_head.mask_fcn1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn1.weight[0m
[34mroi_heads.mask_head.mask_fcn2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn2.weight[0m
[34mroi_heads.mask_head.mask_fcn3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn3.weight[0m
[34mroi_heads.mask_head.mask_fcn4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn4.weight[0m
[34mroi_heads.mask_head.predictor.{bias, weight}[0m
[09/20 10:54:54] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mhead.projection.{bias, weight}[0m
  [35mnorm.{bias, weight}[0m
[09/20 10:54:54] d2.engine.train_loop INFO: Starting training from iteration 0
[09/20 10:55:12] d2.utils.events INFO:  eta: 1 day, 3:12:08  iter: 19  total_loss: 6.994  loss_cls_stage0: 1.895  loss_box_reg_stage0: 0.02558  loss_cls_stage1: 1.809  loss_box_reg_stage1: 0.008925  loss_cls_stage2: 1.825  loss_box_reg_stage2: 0.003899  loss_mask: 0.6932  loss_rpn_cls: 0.6901  loss_rpn_loc: 0.05759  time: 0.7703  data_time: 0.0108  lr: 6.7198e-06  max_mem: 6702M
[09/20 10:55:13] d2.engine.hooks INFO: Overall training speed: 18 iterations in 0:00:14 (0.8131 s / it)
[09/20 10:55:13] d2.engine.hooks INFO: Total training time: 0:00:14 (0:00:00 on hooks)
[09/20 10:55:13] d2.utils.events INFO:  eta: 1 day, 3:12:07  iter: 20  total_loss: 6.994  loss_cls_stage0: 1.895  loss_box_reg_stage0: 0.02558  loss_cls_stage1: 1.809  loss_box_reg_stage1: 0.008925  loss_cls_stage2: 1.825  loss_box_reg_stage2: 0.003899  loss_mask: 0.6932  loss_rpn_cls: 0.6901  loss_rpn_loc: 0.05759  time: 0.7703  data_time: 0.0108  lr: 6.7198e-06  max_mem: 6702M
