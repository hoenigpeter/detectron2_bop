[07/28 15:53:56] detectron2 INFO: Rank of current process: 0. World size: 1
[07/28 15:53:57] detectron2 INFO: Environment info:
----------------------  -------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.10 (default, May 26 2023, 14:05:08) [GCC 9.4.0]
numpy                   1.23.5
detectron2              0.6 @/home/hoenig/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 9.4
CUDA compiler           CUDA 11.6
detectron2 arch flags   8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          525.125.06
CUDA_HOME               /usr/local/cuda
Pillow                  9.3.0
torchvision             0.13.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221122
iopath                  0.1.9
cv2                     4.7.0
----------------------  -------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[07/28 15:53:57] detectron2 INFO: Command line arguments: Namespace(config_file='projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_b_3x.py', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[07/28 15:53:57] detectron2 INFO: Contents of args.config_file=projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_b_3x.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcascade_mask_rcnn_mvitv2_t_3x[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15moptimizer[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mlr_multiplier[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtrain[39m


[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mdepth[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mlast_block_indexes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m([39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m4[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m20[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m23[39m[38;5;15m)[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mdrop_path_rate[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.4[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15minit_checkpoint[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://ImageNetPretrained/mvitv2/MViTv2_B_in1k.pyth[39m[38;5;186m"[39m

[07/28 15:53:57] d2.config.lazy WARNING: The config contains objects that cannot serialize to a valid yaml. ./output/config.yaml is human-readable but cannot be loaded.
[07/28 15:53:57] d2.config.lazy WARNING: Config is saved using cloudpickle at ./output/config.yaml.pkl.
[07/28 15:53:57] detectron2 INFO: Full config saved to ./output/config.yaml
[07/28 15:53:57] d2.utils.env INFO: Using a generated random seed 60605440
[07/28 15:53:58] detectron2 INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): MViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      )
      (blocks): ModuleList(
        (0): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.017)
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (2): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.035)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=96, out_features=192, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (3): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.052)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (4): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.070)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=192, out_features=384, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (6): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.104)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.122)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.139)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.157)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.174)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.191)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (12): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.209)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (13): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.226)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (14): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.243)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (15): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.261)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (16): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.278)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (17): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.296)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (18): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.313)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (19): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.330)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (20): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.348)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (21): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.365)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=384, out_features=768, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (22): MultiScaleBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.383)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (23): MultiScaleBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.400)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (scale2_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (scale3_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (scale4_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (scale5_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): CascadeROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): ModuleList(
      (0): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (1): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (2): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
    )
    (box_predictor): ModuleList(
      (0): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=81, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (1): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=81, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (2): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=81, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[07/28 15:54:17] detectron2 INFO: Rank of current process: 0. World size: 1
[07/28 15:54:18] detectron2 INFO: Environment info:
----------------------  -------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.10 (default, May 26 2023, 14:05:08) [GCC 9.4.0]
numpy                   1.23.5
detectron2              0.6 @/home/hoenig/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 9.4
CUDA compiler           CUDA 11.6
detectron2 arch flags   8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          525.125.06
CUDA_HOME               /usr/local/cuda
Pillow                  9.3.0
torchvision             0.13.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221122
iopath                  0.1.9
cv2                     4.7.0
----------------------  -------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[07/28 15:54:18] detectron2 INFO: Command line arguments: Namespace(config_file='projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_b_3x.py', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[07/28 15:54:18] detectron2 INFO: Contents of args.config_file=projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_b_3x.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcascade_mask_rcnn_mvitv2_t_3x[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15moptimizer[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mlr_multiplier[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtrain[39m


[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mdepth[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mlast_block_indexes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m([39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m4[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m20[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m23[39m[38;5;15m)[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mdrop_path_rate[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.4[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15minit_checkpoint[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://ImageNetPretrained/mvitv2/MViTv2_B_in1k.pyth[39m[38;5;186m"[39m

[07/28 15:54:18] d2.config.lazy WARNING: The config contains objects that cannot serialize to a valid yaml. ./output/config.yaml is human-readable but cannot be loaded.
[07/28 15:54:18] d2.config.lazy WARNING: Config is saved using cloudpickle at ./output/config.yaml.pkl.
[07/28 15:54:18] detectron2 INFO: Full config saved to ./output/config.yaml
[07/28 15:54:18] d2.utils.env INFO: Using a generated random seed 21319881
[07/28 15:54:19] detectron2 INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): MViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      )
      (blocks): ModuleList(
        (0): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.017)
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (2): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.035)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=96, out_features=192, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (3): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.052)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (4): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.070)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=192, out_features=384, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (6): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.104)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.122)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.139)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.157)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.174)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.191)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (12): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.209)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (13): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.226)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (14): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.243)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (15): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.261)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (16): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.278)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (17): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.296)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (18): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.313)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (19): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.330)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (20): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.348)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (21): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.365)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=384, out_features=768, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (22): MultiScaleBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.383)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (23): MultiScaleBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.400)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (scale2_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (scale3_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (scale4_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (scale5_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): CascadeROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): ModuleList(
      (0): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (1): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (2): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
    )
    (box_predictor): ModuleList(
      (0): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=81, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (1): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=81, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (2): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=81, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[07/28 15:54:34] d2.data.datasets.coco INFO: Loading datasets/tless/tless_annotations_train.json takes 13.01 seconds.
[07/28 15:54:34] d2.data.datasets.coco INFO: Loaded 50000 images in COCO format from datasets/tless/tless_annotations_train.json
[07/28 15:54:38] d2.data.build INFO: Removed 0 images with no usable annotations. 50000 images left.
[07/28 15:54:39] d2.data.build INFO: Distribution of instances among all 30 categories:
[36m|  category  | #instances   | category   | #instances   | category   | #instances   |
|:----------:|:-------------|:-----------|:-------------|:-----------|:-------------|
|     1      | 19873        | 2          | 21790        | 3          | 21461        |
|     4      | 21610        | 5          | 23092        | 6          | 23830        |
|     7      | 26345        | 8          | 26271        | 9          | 22606        |
|     10     | 22289        | 11         | 22313        | 12         | 23290        |
|     13     | 20476        | 14         | 22314        | 15         | 22261        |
|     16     | 21595        | 17         | 24514        | 18         | 25429        |
|     19     | 23097        | 20         | 22733        | 21         | 23375        |
|     22     | 23035        | 23         | 24216        | 24         | 22070        |
|     25     | 23853        | 26         | 24387        | 27         | 25290        |
|     28     | 23366        | 29         | 24099        | 30         | 22921        |
|            |              |            |              |            |              |
|   total    | 693801       |            |              |            |              |[0m
[07/28 15:54:39] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [<detectron2.data.transforms.augmentation_impl.RandomApply object at 0x7f8dec0c05e0>, ResizeShortestEdge(short_edge_length=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], max_size=1333, sample_style='choice'), RandomFlip()]
[07/28 15:54:39] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[07/28 15:54:39] d2.data.common INFO: Serializing 50000 elements to byte tensors and concatenating them all ...
[07/28 15:54:42] d2.data.common INFO: Serialized dataset takes 389.07 MiB
[07/28 15:54:44] fvcore.common.checkpoint INFO: [Checkpointer] Loading from detectron2://ImageNetPretrained/mvitv2/MViTv2_B_in1k.pyth ...
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv1.norm.bias in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv1.norm.weight in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv2.norm.bias in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv2.norm.weight in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv3.norm.bias in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv3.norm.weight in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv4.norm.bias in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv4.norm.weight in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv1.norm.bias in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv1.norm.weight in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv2.norm.bias in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv2.norm.weight in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv3.norm.bias in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv3.norm.weight in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv4.norm.bias in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv4.norm.weight in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv1.norm.bias in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv1.norm.weight in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv2.norm.bias in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv2.norm.weight in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv3.norm.bias in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv3.norm.weight in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv4.norm.bias in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv4.norm.weight in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn1.norm.bias in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn1.norm.weight in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn2.norm.bias in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn2.norm.weight in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn3.norm.bias in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn3.norm.weight in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn4.norm.bias in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn4.norm.weight in model is torch.Size([256]).
[07/28 15:55:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:55:16] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone.bottom_up:
| Names in Model      | Names in Checkpoint                                                                                                                                                                                    | Shapes                                                                                                                   |
|:--------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|
| blocks.0.attn.*     | blocks.0.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (96,) (96,96) (288,) (288,96) (111,96) (111,96)     |
| blocks.0.mlp.fc1.*  | blocks.0.mlp.fc1.{bias,weight}                                                                                                                                                                         | (384,) (384,96)                                                                                                          |
| blocks.0.mlp.fc2.*  | blocks.0.mlp.fc2.{bias,weight}                                                                                                                                                                         | (96,) (96,384)                                                                                                           |
| blocks.0.norm1.*    | blocks.0.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.0.norm2.*    | blocks.0.norm2.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.attn.*     | blocks.1.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (96,) (96,96) (288,) (288,96) (111,96) (111,96)     |
| blocks.1.mlp.fc1.*  | blocks.1.mlp.fc1.{bias,weight}                                                                                                                                                                         | (384,) (384,96)                                                                                                          |
| blocks.1.mlp.fc2.*  | blocks.1.mlp.fc2.{bias,weight}                                                                                                                                                                         | (96,) (96,384)                                                                                                           |
| blocks.1.norm1.*    | blocks.1.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.norm2.*    | blocks.1.norm2.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.10.attn.*    | blocks.10.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.10.mlp.fc1.* | blocks.10.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.10.mlp.fc2.* | blocks.10.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.10.norm1.*   | blocks.10.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.10.norm2.*   | blocks.10.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.attn.*    | blocks.11.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.11.mlp.fc1.* | blocks.11.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.11.mlp.fc2.* | blocks.11.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.11.norm1.*   | blocks.11.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.norm2.*   | blocks.11.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.attn.*    | blocks.12.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.12.mlp.fc1.* | blocks.12.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.12.mlp.fc2.* | blocks.12.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.12.norm1.*   | blocks.12.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.norm2.*   | blocks.12.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.attn.*    | blocks.13.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.13.mlp.fc1.* | blocks.13.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.13.mlp.fc2.* | blocks.13.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.13.norm1.*   | blocks.13.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.norm2.*   | blocks.13.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.attn.*    | blocks.14.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.14.mlp.fc1.* | blocks.14.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.14.mlp.fc2.* | blocks.14.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.14.norm1.*   | blocks.14.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.norm2.*   | blocks.14.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.15.attn.*    | blocks.15.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.15.mlp.fc1.* | blocks.15.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.15.mlp.fc2.* | blocks.15.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.15.norm1.*   | blocks.15.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.15.norm2.*   | blocks.15.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.16.attn.*    | blocks.16.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.16.mlp.fc1.* | blocks.16.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.16.mlp.fc2.* | blocks.16.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.16.norm1.*   | blocks.16.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.16.norm2.*   | blocks.16.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.17.attn.*    | blocks.17.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.17.mlp.fc1.* | blocks.17.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.17.mlp.fc2.* | blocks.17.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.17.norm1.*   | blocks.17.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.17.norm2.*   | blocks.17.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.18.attn.*    | blocks.18.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.18.mlp.fc1.* | blocks.18.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.18.mlp.fc2.* | blocks.18.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.18.norm1.*   | blocks.18.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.18.norm2.*   | blocks.18.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.19.attn.*    | blocks.19.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.19.mlp.fc1.* | blocks.19.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.19.mlp.fc2.* | blocks.19.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.19.norm1.*   | blocks.19.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.19.norm2.*   | blocks.19.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.2.attn.*     | blocks.2.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,96) (55,96) (55,96)    |
| blocks.2.mlp.fc1.*  | blocks.2.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.2.mlp.fc2.*  | blocks.2.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.2.norm1.*    | blocks.2.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.2.norm2.*    | blocks.2.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.2.proj.*     | blocks.2.proj.{bias,weight}                                                                                                                                                                            | (192,) (192,96)                                                                                                          |
| blocks.20.attn.*    | blocks.20.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.20.mlp.fc1.* | blocks.20.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.20.mlp.fc2.* | blocks.20.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.20.norm1.*   | blocks.20.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.20.norm2.*   | blocks.20.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.21.attn.*    | blocks.21.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,384) (27,96) (27,96) |
| blocks.21.mlp.fc1.* | blocks.21.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.21.mlp.fc2.* | blocks.21.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.21.norm1.*   | blocks.21.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.21.norm2.*   | blocks.21.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.21.proj.*    | blocks.21.proj.{bias,weight}                                                                                                                                                                           | (768,) (768,384)                                                                                                         |
| blocks.22.attn.*    | blocks.22.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,768) (13,96) (13,96) |
| blocks.22.mlp.fc1.* | blocks.22.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.22.mlp.fc2.* | blocks.22.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.22.norm1.*   | blocks.22.norm1.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.22.norm2.*   | blocks.22.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.23.attn.*    | blocks.23.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,768) (13,96) (13,96) |
| blocks.23.mlp.fc1.* | blocks.23.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.23.mlp.fc2.* | blocks.23.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.23.norm1.*   | blocks.23.norm1.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.23.norm2.*   | blocks.23.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.3.attn.*     | blocks.3.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,192) (55,96) (55,96)   |
| blocks.3.mlp.fc1.*  | blocks.3.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.3.mlp.fc2.*  | blocks.3.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.3.norm1.*    | blocks.3.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.3.norm2.*    | blocks.3.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.4.attn.*     | blocks.4.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,192) (55,96) (55,96)   |
| blocks.4.mlp.fc1.*  | blocks.4.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.4.mlp.fc2.*  | blocks.4.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.4.norm1.*    | blocks.4.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.4.norm2.*    | blocks.4.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.5.attn.*     | blocks.5.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,192) (55,96) (55,96) |
| blocks.5.mlp.fc1.*  | blocks.5.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.5.mlp.fc2.*  | blocks.5.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.5.norm1.*    | blocks.5.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.5.norm2.*    | blocks.5.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.5.proj.*     | blocks.5.proj.{bias,weight}                                                                                                                                                                            | (384,) (384,192)                                                                                                         |
| blocks.6.attn.*     | blocks.6.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.6.mlp.fc1.*  | blocks.6.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.6.mlp.fc2.*  | blocks.6.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.6.norm1.*    | blocks.6.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.6.norm2.*    | blocks.6.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.attn.*     | blocks.7.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.7.mlp.fc1.*  | blocks.7.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.7.mlp.fc2.*  | blocks.7.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.7.norm1.*    | blocks.7.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.norm2.*    | blocks.7.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.attn.*     | blocks.8.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.8.mlp.fc1.*  | blocks.8.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.8.mlp.fc2.*  | blocks.8.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.8.norm1.*    | blocks.8.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.norm2.*    | blocks.8.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.attn.*     | blocks.9.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.9.mlp.fc1.*  | blocks.9.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.9.mlp.fc2.*  | blocks.9.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.9.norm1.*    | blocks.9.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.norm2.*    | blocks.9.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| patch_embed.proj.*  | patch_embed.proj.{bias,weight}                                                                                                                                                                         | (96,) (96,3,7,7)                                                                                                         |
[07/28 15:55:17] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.bottom_up.scale2_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale3_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale4_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale5_norm.{bias, weight}[0m
[34mbackbone.fpn_lateral2.{bias, weight}[0m
[34mbackbone.fpn_lateral3.{bias, weight}[0m
[34mbackbone.fpn_lateral4.{bias, weight}[0m
[34mbackbone.fpn_lateral5.{bias, weight}[0m
[34mbackbone.fpn_output2.{bias, weight}[0m
[34mbackbone.fpn_output3.{bias, weight}[0m
[34mbackbone.fpn_output4.{bias, weight}[0m
[34mbackbone.fpn_output5.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv0.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv1.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mroi_heads.box_head.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv1.weight[0m
[34mroi_heads.box_head.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv2.weight[0m
[34mroi_heads.box_head.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv3.weight[0m
[34mroi_heads.box_head.0.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv4.weight[0m
[34mroi_heads.box_head.0.fc1.{bias, weight}[0m
[34mroi_heads.box_head.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv1.weight[0m
[34mroi_heads.box_head.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv2.weight[0m
[34mroi_heads.box_head.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv3.weight[0m
[34mroi_heads.box_head.1.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv4.weight[0m
[34mroi_heads.box_head.1.fc1.{bias, weight}[0m
[34mroi_heads.box_head.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv1.weight[0m
[34mroi_heads.box_head.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv2.weight[0m
[34mroi_heads.box_head.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv3.weight[0m
[34mroi_heads.box_head.2.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv4.weight[0m
[34mroi_heads.box_head.2.fc1.{bias, weight}[0m
[34mroi_heads.box_predictor.0.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.0.cls_score.{bias, weight}[0m
[34mroi_heads.box_predictor.1.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.1.cls_score.{bias, weight}[0m
[34mroi_heads.box_predictor.2.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.2.cls_score.{bias, weight}[0m
[34mroi_heads.mask_head.deconv.{bias, weight}[0m
[34mroi_heads.mask_head.mask_fcn1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn1.weight[0m
[34mroi_heads.mask_head.mask_fcn2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn2.weight[0m
[34mroi_heads.mask_head.mask_fcn3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn3.weight[0m
[34mroi_heads.mask_head.mask_fcn4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn4.weight[0m
[34mroi_heads.mask_head.predictor.{bias, weight}[0m
[07/28 15:55:17] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mhead.projection.{bias, weight}[0m
  [35mnorm.{bias, weight}[0m
[07/28 15:55:17] d2.engine.train_loop INFO: Starting training from iteration 0
[07/28 15:55:20] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 422, in run_step
    loss_dict = self.model(data)
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 158, in forward
    features = self.backbone(images.tensor)
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/backbone/fpn.py", line 139, in forward
    bottom_up_features = self.bottom_up(x)
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/backbone/mvit.py", line 438, in forward
    x = blk(x)
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/backbone/mvit.py", line 259, in forward
    x_block = self.attn(x_norm)
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/backbone/mvit.py", line 144, in forward
    q, q_hw_pad = window_partition(q, self.q_win_size)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/backbone/utils.py", line 32, in window_partition
    x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h))
RuntimeError: CUDA out of memory. Tried to allocate 1.79 GiB (GPU 0; 23.67 GiB total capacity; 11.29 GiB already allocated; 817.69 MiB free; 11.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[07/28 15:55:20] d2.engine.hooks INFO: Total training time: 0:00:03 (0:00:00 on hooks)
[07/28 15:55:20] d2.utils.events INFO:  iter: 0    lr: N/A  max_mem: 12160M
[07/28 15:58:20] detectron2 INFO: Rank of current process: 0. World size: 1
[07/28 15:58:21] detectron2 INFO: Environment info:
----------------------  -------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.10 (default, May 26 2023, 14:05:08) [GCC 9.4.0]
numpy                   1.23.5
detectron2              0.6 @/home/hoenig/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 9.4
CUDA compiler           CUDA 11.6
detectron2 arch flags   8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          525.125.06
CUDA_HOME               /usr/local/cuda
Pillow                  9.3.0
torchvision             0.13.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221122
iopath                  0.1.9
cv2                     4.7.0
----------------------  -------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[07/28 15:58:21] detectron2 INFO: Command line arguments: Namespace(config_file='projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_b_3x.py', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[07/28 15:58:21] detectron2 INFO: Contents of args.config_file=projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_b_3x.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcascade_mask_rcnn_mvitv2_t_3x[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15moptimizer[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mlr_multiplier[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtrain[39m


[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mdepth[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mlast_block_indexes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m([39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m4[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m20[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m23[39m[38;5;15m)[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mdrop_path_rate[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.4[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15minit_checkpoint[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://ImageNetPretrained/mvitv2/MViTv2_B_in1k.pyth[39m[38;5;186m"[39m

[07/28 15:58:21] d2.config.lazy WARNING: The config contains objects that cannot serialize to a valid yaml. ./output/config.yaml is human-readable but cannot be loaded.
[07/28 15:58:21] d2.config.lazy WARNING: Config is saved using cloudpickle at ./output/config.yaml.pkl.
[07/28 15:58:21] detectron2 INFO: Full config saved to ./output/config.yaml
[07/28 15:58:21] d2.utils.env INFO: Using a generated random seed 24454063
[07/28 15:58:22] detectron2 INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): MViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      )
      (blocks): ModuleList(
        (0): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.017)
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (2): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.035)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=96, out_features=192, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (3): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.052)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (4): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.070)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=192, out_features=384, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (6): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.104)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.122)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.139)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.157)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.174)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.191)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (12): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.209)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (13): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.226)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (14): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.243)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (15): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.261)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (16): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.278)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (17): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.296)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (18): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.313)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (19): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.330)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (20): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.348)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (21): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.365)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=384, out_features=768, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (22): MultiScaleBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.383)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (23): MultiScaleBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.400)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (scale2_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (scale3_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (scale4_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (scale5_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): CascadeROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): ModuleList(
      (0): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (1): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (2): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
    )
    (box_predictor): ModuleList(
      (0): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=81, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (1): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=81, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (2): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=81, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[07/28 15:58:37] d2.data.datasets.coco INFO: Loading datasets/tless/tless_annotations_train.json takes 13.11 seconds.
[07/28 15:58:37] d2.data.datasets.coco INFO: Loaded 50000 images in COCO format from datasets/tless/tless_annotations_train.json
[07/28 15:58:41] d2.data.build INFO: Removed 0 images with no usable annotations. 50000 images left.
[07/28 15:58:42] d2.data.build INFO: Distribution of instances among all 30 categories:
[36m|  category  | #instances   | category   | #instances   | category   | #instances   |
|:----------:|:-------------|:-----------|:-------------|:-----------|:-------------|
|     1      | 19873        | 2          | 21790        | 3          | 21461        |
|     4      | 21610        | 5          | 23092        | 6          | 23830        |
|     7      | 26345        | 8          | 26271        | 9          | 22606        |
|     10     | 22289        | 11         | 22313        | 12         | 23290        |
|     13     | 20476        | 14         | 22314        | 15         | 22261        |
|     16     | 21595        | 17         | 24514        | 18         | 25429        |
|     19     | 23097        | 20         | 22733        | 21         | 23375        |
|     22     | 23035        | 23         | 24216        | 24         | 22070        |
|     25     | 23853        | 26         | 24387        | 27         | 25290        |
|     28     | 23366        | 29         | 24099        | 30         | 22921        |
|            |              |            |              |            |              |
|   total    | 693801       |            |              |            |              |[0m
[07/28 15:58:42] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [<detectron2.data.transforms.augmentation_impl.RandomApply object at 0x7fcf6c749c10>, ResizeShortestEdge(short_edge_length=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], max_size=1333, sample_style='choice'), RandomFlip()]
[07/28 15:58:42] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[07/28 15:58:42] d2.data.common INFO: Serializing 50000 elements to byte tensors and concatenating them all ...
[07/28 15:58:44] d2.data.common INFO: Serialized dataset takes 389.07 MiB
[07/28 15:58:46] fvcore.common.checkpoint INFO: [Checkpointer] Loading from detectron2://ImageNetPretrained/mvitv2/MViTv2_B_in1k.pyth ...
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv1.norm.bias in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv1.norm.weight in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv2.norm.bias in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv2.norm.weight in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv3.norm.bias in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv3.norm.weight in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv4.norm.bias in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv4.norm.weight in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv1.norm.bias in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv1.norm.weight in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv2.norm.bias in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv2.norm.weight in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv3.norm.bias in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv3.norm.weight in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv4.norm.bias in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv4.norm.weight in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv1.norm.bias in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv1.norm.weight in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv2.norm.bias in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv2.norm.weight in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv3.norm.bias in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv3.norm.weight in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv4.norm.bias in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv4.norm.weight in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn1.norm.bias in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn1.norm.weight in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn2.norm.bias in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn2.norm.weight in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn3.norm.bias in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn3.norm.weight in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn4.norm.bias in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn4.norm.weight in model is torch.Size([256]).
[07/28 15:58:47] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 15:58:47] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone.bottom_up:
| Names in Model      | Names in Checkpoint                                                                                                                                                                                    | Shapes                                                                                                                   |
|:--------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|
| blocks.0.attn.*     | blocks.0.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (96,) (96,96) (288,) (288,96) (111,96) (111,96)     |
| blocks.0.mlp.fc1.*  | blocks.0.mlp.fc1.{bias,weight}                                                                                                                                                                         | (384,) (384,96)                                                                                                          |
| blocks.0.mlp.fc2.*  | blocks.0.mlp.fc2.{bias,weight}                                                                                                                                                                         | (96,) (96,384)                                                                                                           |
| blocks.0.norm1.*    | blocks.0.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.0.norm2.*    | blocks.0.norm2.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.attn.*     | blocks.1.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (96,) (96,96) (288,) (288,96) (111,96) (111,96)     |
| blocks.1.mlp.fc1.*  | blocks.1.mlp.fc1.{bias,weight}                                                                                                                                                                         | (384,) (384,96)                                                                                                          |
| blocks.1.mlp.fc2.*  | blocks.1.mlp.fc2.{bias,weight}                                                                                                                                                                         | (96,) (96,384)                                                                                                           |
| blocks.1.norm1.*    | blocks.1.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.norm2.*    | blocks.1.norm2.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.10.attn.*    | blocks.10.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.10.mlp.fc1.* | blocks.10.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.10.mlp.fc2.* | blocks.10.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.10.norm1.*   | blocks.10.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.10.norm2.*   | blocks.10.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.attn.*    | blocks.11.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.11.mlp.fc1.* | blocks.11.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.11.mlp.fc2.* | blocks.11.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.11.norm1.*   | blocks.11.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.norm2.*   | blocks.11.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.attn.*    | blocks.12.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.12.mlp.fc1.* | blocks.12.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.12.mlp.fc2.* | blocks.12.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.12.norm1.*   | blocks.12.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.norm2.*   | blocks.12.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.attn.*    | blocks.13.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.13.mlp.fc1.* | blocks.13.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.13.mlp.fc2.* | blocks.13.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.13.norm1.*   | blocks.13.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.norm2.*   | blocks.13.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.attn.*    | blocks.14.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.14.mlp.fc1.* | blocks.14.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.14.mlp.fc2.* | blocks.14.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.14.norm1.*   | blocks.14.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.norm2.*   | blocks.14.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.15.attn.*    | blocks.15.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.15.mlp.fc1.* | blocks.15.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.15.mlp.fc2.* | blocks.15.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.15.norm1.*   | blocks.15.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.15.norm2.*   | blocks.15.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.16.attn.*    | blocks.16.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.16.mlp.fc1.* | blocks.16.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.16.mlp.fc2.* | blocks.16.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.16.norm1.*   | blocks.16.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.16.norm2.*   | blocks.16.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.17.attn.*    | blocks.17.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.17.mlp.fc1.* | blocks.17.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.17.mlp.fc2.* | blocks.17.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.17.norm1.*   | blocks.17.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.17.norm2.*   | blocks.17.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.18.attn.*    | blocks.18.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.18.mlp.fc1.* | blocks.18.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.18.mlp.fc2.* | blocks.18.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.18.norm1.*   | blocks.18.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.18.norm2.*   | blocks.18.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.19.attn.*    | blocks.19.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.19.mlp.fc1.* | blocks.19.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.19.mlp.fc2.* | blocks.19.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.19.norm1.*   | blocks.19.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.19.norm2.*   | blocks.19.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.2.attn.*     | blocks.2.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,96) (55,96) (55,96)    |
| blocks.2.mlp.fc1.*  | blocks.2.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.2.mlp.fc2.*  | blocks.2.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.2.norm1.*    | blocks.2.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.2.norm2.*    | blocks.2.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.2.proj.*     | blocks.2.proj.{bias,weight}                                                                                                                                                                            | (192,) (192,96)                                                                                                          |
| blocks.20.attn.*    | blocks.20.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.20.mlp.fc1.* | blocks.20.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.20.mlp.fc2.* | blocks.20.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.20.norm1.*   | blocks.20.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.20.norm2.*   | blocks.20.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.21.attn.*    | blocks.21.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,384) (27,96) (27,96) |
| blocks.21.mlp.fc1.* | blocks.21.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.21.mlp.fc2.* | blocks.21.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.21.norm1.*   | blocks.21.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.21.norm2.*   | blocks.21.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.21.proj.*    | blocks.21.proj.{bias,weight}                                                                                                                                                                           | (768,) (768,384)                                                                                                         |
| blocks.22.attn.*    | blocks.22.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,768) (13,96) (13,96) |
| blocks.22.mlp.fc1.* | blocks.22.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.22.mlp.fc2.* | blocks.22.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.22.norm1.*   | blocks.22.norm1.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.22.norm2.*   | blocks.22.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.23.attn.*    | blocks.23.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,768) (13,96) (13,96) |
| blocks.23.mlp.fc1.* | blocks.23.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.23.mlp.fc2.* | blocks.23.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.23.norm1.*   | blocks.23.norm1.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.23.norm2.*   | blocks.23.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.3.attn.*     | blocks.3.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,192) (55,96) (55,96)   |
| blocks.3.mlp.fc1.*  | blocks.3.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.3.mlp.fc2.*  | blocks.3.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.3.norm1.*    | blocks.3.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.3.norm2.*    | blocks.3.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.4.attn.*     | blocks.4.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,192) (55,96) (55,96)   |
| blocks.4.mlp.fc1.*  | blocks.4.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.4.mlp.fc2.*  | blocks.4.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.4.norm1.*    | blocks.4.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.4.norm2.*    | blocks.4.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.5.attn.*     | blocks.5.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,192) (55,96) (55,96) |
| blocks.5.mlp.fc1.*  | blocks.5.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.5.mlp.fc2.*  | blocks.5.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.5.norm1.*    | blocks.5.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.5.norm2.*    | blocks.5.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.5.proj.*     | blocks.5.proj.{bias,weight}                                                                                                                                                                            | (384,) (384,192)                                                                                                         |
| blocks.6.attn.*     | blocks.6.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.6.mlp.fc1.*  | blocks.6.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.6.mlp.fc2.*  | blocks.6.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.6.norm1.*    | blocks.6.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.6.norm2.*    | blocks.6.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.attn.*     | blocks.7.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.7.mlp.fc1.*  | blocks.7.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.7.mlp.fc2.*  | blocks.7.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.7.norm1.*    | blocks.7.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.norm2.*    | blocks.7.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.attn.*     | blocks.8.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.8.mlp.fc1.*  | blocks.8.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.8.mlp.fc2.*  | blocks.8.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.8.norm1.*    | blocks.8.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.norm2.*    | blocks.8.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.attn.*     | blocks.9.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.9.mlp.fc1.*  | blocks.9.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.9.mlp.fc2.*  | blocks.9.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.9.norm1.*    | blocks.9.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.norm2.*    | blocks.9.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| patch_embed.proj.*  | patch_embed.proj.{bias,weight}                                                                                                                                                                         | (96,) (96,3,7,7)                                                                                                         |
[07/28 15:58:48] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.bottom_up.scale2_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale3_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale4_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale5_norm.{bias, weight}[0m
[34mbackbone.fpn_lateral2.{bias, weight}[0m
[34mbackbone.fpn_lateral3.{bias, weight}[0m
[34mbackbone.fpn_lateral4.{bias, weight}[0m
[34mbackbone.fpn_lateral5.{bias, weight}[0m
[34mbackbone.fpn_output2.{bias, weight}[0m
[34mbackbone.fpn_output3.{bias, weight}[0m
[34mbackbone.fpn_output4.{bias, weight}[0m
[34mbackbone.fpn_output5.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv0.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv1.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mroi_heads.box_head.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv1.weight[0m
[34mroi_heads.box_head.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv2.weight[0m
[34mroi_heads.box_head.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv3.weight[0m
[34mroi_heads.box_head.0.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv4.weight[0m
[34mroi_heads.box_head.0.fc1.{bias, weight}[0m
[34mroi_heads.box_head.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv1.weight[0m
[34mroi_heads.box_head.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv2.weight[0m
[34mroi_heads.box_head.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv3.weight[0m
[34mroi_heads.box_head.1.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv4.weight[0m
[34mroi_heads.box_head.1.fc1.{bias, weight}[0m
[34mroi_heads.box_head.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv1.weight[0m
[34mroi_heads.box_head.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv2.weight[0m
[34mroi_heads.box_head.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv3.weight[0m
[34mroi_heads.box_head.2.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv4.weight[0m
[34mroi_heads.box_head.2.fc1.{bias, weight}[0m
[34mroi_heads.box_predictor.0.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.0.cls_score.{bias, weight}[0m
[34mroi_heads.box_predictor.1.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.1.cls_score.{bias, weight}[0m
[34mroi_heads.box_predictor.2.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.2.cls_score.{bias, weight}[0m
[34mroi_heads.mask_head.deconv.{bias, weight}[0m
[34mroi_heads.mask_head.mask_fcn1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn1.weight[0m
[34mroi_heads.mask_head.mask_fcn2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn2.weight[0m
[34mroi_heads.mask_head.mask_fcn3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn3.weight[0m
[34mroi_heads.mask_head.mask_fcn4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn4.weight[0m
[34mroi_heads.mask_head.predictor.{bias, weight}[0m
[07/28 15:58:48] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mhead.projection.{bias, weight}[0m
  [35mnorm.{bias, weight}[0m
[07/28 15:58:48] d2.engine.train_loop INFO: Starting training from iteration 0
[07/28 15:58:51] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 422, in run_step
    loss_dict = self.model(data)
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 158, in forward
    features = self.backbone(images.tensor)
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/backbone/fpn.py", line 139, in forward
    bottom_up_features = self.bottom_up(x)
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/backbone/mvit.py", line 438, in forward
    x = blk(x)
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/backbone/mvit.py", line 259, in forward
    x_block = self.attn(x_norm)
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/backbone/mvit.py", line 139, in forward
    k = attention_pool(k, self.pool_k, self.norm_k)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/backbone/mvit.py", line 31, in attention_pool
    x = norm(x)
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/normalization.py", line 189, in forward
    return F.layer_norm(
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 2503, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
RuntimeError: CUDA out of memory. Tried to allocate 142.00 MiB (GPU 0; 23.67 GiB total capacity; 11.96 GiB already allocated; 71.12 MiB free; 12.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[07/28 15:58:51] d2.engine.hooks INFO: Total training time: 0:00:03 (0:00:00 on hooks)
[07/28 15:58:51] d2.utils.events INFO:  iter: 0    lr: N/A  max_mem: 12246M
[07/28 16:12:30] detectron2 INFO: Rank of current process: 0. World size: 1
[07/28 16:12:31] detectron2 INFO: Environment info:
----------------------  -------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.10 (default, May 26 2023, 14:05:08) [GCC 9.4.0]
numpy                   1.23.5
detectron2              0.6 @/home/hoenig/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 9.4
CUDA compiler           CUDA 11.6
detectron2 arch flags   8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          525.125.06
CUDA_HOME               /usr/local/cuda
Pillow                  9.3.0
torchvision             0.13.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221122
iopath                  0.1.9
cv2                     4.7.0
----------------------  -------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[07/28 16:12:31] detectron2 INFO: Command line arguments: Namespace(config_file='projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_b_3x.py', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[07/28 16:12:31] detectron2 INFO: Contents of args.config_file=projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_b_3x.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcascade_mask_rcnn_mvitv2_t_3x[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15moptimizer[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mlr_multiplier[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtrain[39m


[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mdepth[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mlast_block_indexes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m([39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m4[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m20[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m23[39m[38;5;15m)[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mdrop_path_rate[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.4[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15minit_checkpoint[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://ImageNetPretrained/mvitv2/MViTv2_B_in1k.pyth[39m[38;5;186m"[39m

[07/28 16:12:31] detectron2 INFO: Full config saved to ./output/config.yaml
[07/28 16:12:31] d2.utils.env INFO: Using a generated random seed 34416338
[07/28 16:12:32] detectron2 INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): MViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      )
      (blocks): ModuleList(
        (0): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.017)
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (2): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.035)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=96, out_features=192, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (3): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.052)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (4): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.070)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=192, out_features=384, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (6): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.104)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.122)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.139)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.157)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.174)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.191)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (12): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.209)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (13): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.226)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (14): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.243)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (15): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.261)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (16): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.278)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (17): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.296)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (18): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.313)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (19): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.330)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (20): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.348)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (21): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.365)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=384, out_features=768, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (22): MultiScaleBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.383)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (23): MultiScaleBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.400)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (scale2_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (scale3_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (scale4_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (scale5_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
)
[07/28 16:12:45] d2.data.datasets.coco INFO: Loading datasets/tless/tless_annotations_train.json takes 12.19 seconds.
[07/28 16:12:45] d2.data.datasets.coco INFO: Loaded 50000 images in COCO format from datasets/tless/tless_annotations_train.json
[07/28 16:12:49] d2.data.build INFO: Removed 0 images with no usable annotations. 50000 images left.
[07/28 16:12:50] d2.data.build INFO: Distribution of instances among all 30 categories:
[36m|  category  | #instances   | category   | #instances   | category   | #instances   |
|:----------:|:-------------|:-----------|:-------------|:-----------|:-------------|
|     1      | 19873        | 2          | 21790        | 3          | 21461        |
|     4      | 21610        | 5          | 23092        | 6          | 23830        |
|     7      | 26345        | 8          | 26271        | 9          | 22606        |
|     10     | 22289        | 11         | 22313        | 12         | 23290        |
|     13     | 20476        | 14         | 22314        | 15         | 22261        |
|     16     | 21595        | 17         | 24514        | 18         | 25429        |
|     19     | 23097        | 20         | 22733        | 21         | 23375        |
|     22     | 23035        | 23         | 24216        | 24         | 22070        |
|     25     | 23853        | 26         | 24387        | 27         | 25290        |
|     28     | 23366        | 29         | 24099        | 30         | 22921        |
|            |              |            |              |            |              |
|   total    | 693801       |            |              |            |              |[0m
[07/28 16:12:50] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [<detectron2.data.transforms.augmentation_impl.RandomApply object at 0x7fc6258ecd00>, ResizeShortestEdge(short_edge_length=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], max_size=1333, sample_style='choice'), RandomFlip()]
[07/28 16:12:50] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[07/28 16:12:50] d2.data.common INFO: Serializing 50000 elements to byte tensors and concatenating them all ...
[07/28 16:12:51] d2.data.common INFO: Serialized dataset takes 389.07 MiB
[07/28 16:12:53] fvcore.common.checkpoint INFO: [Checkpointer] Loading from detectron2://ImageNetPretrained/mvitv2/MViTv2_B_in1k.pyth ...
[07/28 16:12:53] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone.bottom_up:
| Names in Model      | Names in Checkpoint                                                                                                                                                                                    | Shapes                                                                                                                   |
|:--------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|
| blocks.0.attn.*     | blocks.0.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (96,) (96,96) (288,) (288,96) (111,96) (111,96)     |
| blocks.0.mlp.fc1.*  | blocks.0.mlp.fc1.{bias,weight}                                                                                                                                                                         | (384,) (384,96)                                                                                                          |
| blocks.0.mlp.fc2.*  | blocks.0.mlp.fc2.{bias,weight}                                                                                                                                                                         | (96,) (96,384)                                                                                                           |
| blocks.0.norm1.*    | blocks.0.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.0.norm2.*    | blocks.0.norm2.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.attn.*     | blocks.1.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (96,) (96,96) (288,) (288,96) (111,96) (111,96)     |
| blocks.1.mlp.fc1.*  | blocks.1.mlp.fc1.{bias,weight}                                                                                                                                                                         | (384,) (384,96)                                                                                                          |
| blocks.1.mlp.fc2.*  | blocks.1.mlp.fc2.{bias,weight}                                                                                                                                                                         | (96,) (96,384)                                                                                                           |
| blocks.1.norm1.*    | blocks.1.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.norm2.*    | blocks.1.norm2.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.10.attn.*    | blocks.10.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.10.mlp.fc1.* | blocks.10.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.10.mlp.fc2.* | blocks.10.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.10.norm1.*   | blocks.10.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.10.norm2.*   | blocks.10.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.attn.*    | blocks.11.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.11.mlp.fc1.* | blocks.11.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.11.mlp.fc2.* | blocks.11.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.11.norm1.*   | blocks.11.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.norm2.*   | blocks.11.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.attn.*    | blocks.12.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.12.mlp.fc1.* | blocks.12.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.12.mlp.fc2.* | blocks.12.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.12.norm1.*   | blocks.12.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.norm2.*   | blocks.12.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.attn.*    | blocks.13.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.13.mlp.fc1.* | blocks.13.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.13.mlp.fc2.* | blocks.13.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.13.norm1.*   | blocks.13.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.norm2.*   | blocks.13.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.attn.*    | blocks.14.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.14.mlp.fc1.* | blocks.14.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.14.mlp.fc2.* | blocks.14.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.14.norm1.*   | blocks.14.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.norm2.*   | blocks.14.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.15.attn.*    | blocks.15.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.15.mlp.fc1.* | blocks.15.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.15.mlp.fc2.* | blocks.15.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.15.norm1.*   | blocks.15.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.15.norm2.*   | blocks.15.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.16.attn.*    | blocks.16.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.16.mlp.fc1.* | blocks.16.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.16.mlp.fc2.* | blocks.16.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.16.norm1.*   | blocks.16.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.16.norm2.*   | blocks.16.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.17.attn.*    | blocks.17.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.17.mlp.fc1.* | blocks.17.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.17.mlp.fc2.* | blocks.17.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.17.norm1.*   | blocks.17.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.17.norm2.*   | blocks.17.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.18.attn.*    | blocks.18.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.18.mlp.fc1.* | blocks.18.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.18.mlp.fc2.* | blocks.18.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.18.norm1.*   | blocks.18.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.18.norm2.*   | blocks.18.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.19.attn.*    | blocks.19.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.19.mlp.fc1.* | blocks.19.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.19.mlp.fc2.* | blocks.19.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.19.norm1.*   | blocks.19.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.19.norm2.*   | blocks.19.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.2.attn.*     | blocks.2.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,96) (55,96) (55,96)    |
| blocks.2.mlp.fc1.*  | blocks.2.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.2.mlp.fc2.*  | blocks.2.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.2.norm1.*    | blocks.2.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.2.norm2.*    | blocks.2.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.2.proj.*     | blocks.2.proj.{bias,weight}                                                                                                                                                                            | (192,) (192,96)                                                                                                          |
| blocks.20.attn.*    | blocks.20.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.20.mlp.fc1.* | blocks.20.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.20.mlp.fc2.* | blocks.20.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.20.norm1.*   | blocks.20.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.20.norm2.*   | blocks.20.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.21.attn.*    | blocks.21.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,384) (27,96) (27,96) |
| blocks.21.mlp.fc1.* | blocks.21.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.21.mlp.fc2.* | blocks.21.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.21.norm1.*   | blocks.21.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.21.norm2.*   | blocks.21.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.21.proj.*    | blocks.21.proj.{bias,weight}                                                                                                                                                                           | (768,) (768,384)                                                                                                         |
| blocks.22.attn.*    | blocks.22.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,768) (13,96) (13,96) |
| blocks.22.mlp.fc1.* | blocks.22.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.22.mlp.fc2.* | blocks.22.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.22.norm1.*   | blocks.22.norm1.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.22.norm2.*   | blocks.22.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.23.attn.*    | blocks.23.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,768) (13,96) (13,96) |
| blocks.23.mlp.fc1.* | blocks.23.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.23.mlp.fc2.* | blocks.23.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.23.norm1.*   | blocks.23.norm1.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.23.norm2.*   | blocks.23.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.3.attn.*     | blocks.3.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,192) (55,96) (55,96)   |
| blocks.3.mlp.fc1.*  | blocks.3.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.3.mlp.fc2.*  | blocks.3.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.3.norm1.*    | blocks.3.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.3.norm2.*    | blocks.3.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.4.attn.*     | blocks.4.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,192) (55,96) (55,96)   |
| blocks.4.mlp.fc1.*  | blocks.4.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.4.mlp.fc2.*  | blocks.4.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.4.norm1.*    | blocks.4.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.4.norm2.*    | blocks.4.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.5.attn.*     | blocks.5.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,192) (55,96) (55,96) |
| blocks.5.mlp.fc1.*  | blocks.5.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.5.mlp.fc2.*  | blocks.5.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.5.norm1.*    | blocks.5.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.5.norm2.*    | blocks.5.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.5.proj.*     | blocks.5.proj.{bias,weight}                                                                                                                                                                            | (384,) (384,192)                                                                                                         |
| blocks.6.attn.*     | blocks.6.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.6.mlp.fc1.*  | blocks.6.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.6.mlp.fc2.*  | blocks.6.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.6.norm1.*    | blocks.6.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.6.norm2.*    | blocks.6.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.attn.*     | blocks.7.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.7.mlp.fc1.*  | blocks.7.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.7.mlp.fc2.*  | blocks.7.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.7.norm1.*    | blocks.7.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.norm2.*    | blocks.7.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.attn.*     | blocks.8.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.8.mlp.fc1.*  | blocks.8.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.8.mlp.fc2.*  | blocks.8.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.8.norm1.*    | blocks.8.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.norm2.*    | blocks.8.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.attn.*     | blocks.9.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.9.mlp.fc1.*  | blocks.9.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.9.mlp.fc2.*  | blocks.9.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.9.norm1.*    | blocks.9.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.norm2.*    | blocks.9.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| patch_embed.proj.*  | patch_embed.proj.{bias,weight}                                                                                                                                                                         | (96,) (96,3,7,7)                                                                                                         |
[07/28 16:12:53] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.bottom_up.scale2_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale3_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale4_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale5_norm.{bias, weight}[0m
[34mbackbone.fpn_lateral2.{bias, weight}[0m
[34mbackbone.fpn_lateral3.{bias, weight}[0m
[34mbackbone.fpn_lateral4.{bias, weight}[0m
[34mbackbone.fpn_lateral5.{bias, weight}[0m
[34mbackbone.fpn_output2.{bias, weight}[0m
[34mbackbone.fpn_output3.{bias, weight}[0m
[34mbackbone.fpn_output4.{bias, weight}[0m
[34mbackbone.fpn_output5.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv0.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv1.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[07/28 16:12:53] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mhead.projection.{bias, weight}[0m
  [35mnorm.{bias, weight}[0m
[07/28 16:12:53] d2.engine.train_loop INFO: Starting training from iteration 0
[07/28 16:12:55] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 422, in run_step
    loss_dict = self.model(data)
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 167, in forward
    _, detector_losses = self.roi_heads(images, features, proposals, gt_instances)
TypeError: 'int' object is not callable
[07/28 16:12:55] d2.engine.hooks INFO: Total training time: 0:00:02 (0:00:00 on hooks)
[07/28 16:12:55] d2.utils.events INFO:  iter: 0    lr: N/A  max_mem: 16579M
[07/28 16:20:16] detectron2 INFO: Rank of current process: 0. World size: 1
[07/28 16:20:16] detectron2 INFO: Environment info:
----------------------  -------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.10 (default, May 26 2023, 14:05:08) [GCC 9.4.0]
numpy                   1.23.5
detectron2              0.6 @/home/hoenig/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 9.4
CUDA compiler           CUDA 11.6
detectron2 arch flags   8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          525.125.06
CUDA_HOME               /usr/local/cuda
Pillow                  9.3.0
torchvision             0.13.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221122
iopath                  0.1.9
cv2                     4.7.0
----------------------  -------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[07/28 16:20:16] detectron2 INFO: Command line arguments: Namespace(config_file='projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_s_3x.py', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[07/28 16:20:16] detectron2 INFO: Contents of args.config_file=projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_s_3x.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcascade_mask_rcnn_mvitv2_t_3x[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15moptimizer[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mlr_multiplier[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtrain[39m


[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mdepth[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mlast_block_indexes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m([39m[38;5;141m0[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m13[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m15[39m[38;5;15m)[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15minit_checkpoint[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://ImageNetPretrained/mvitv2/MViTv2_S_in1k.pyth[39m[38;5;186m"[39m

[07/28 16:20:16] detectron2 INFO: Full config saved to ./output/config.yaml
[07/28 16:20:16] d2.utils.env INFO: Using a generated random seed 19941683
[07/28 16:20:17] detectron2 INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): MViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      )
      (blocks): ModuleList(
        (0): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=96, out_features=192, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (2): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (3): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=192, out_features=384, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (4): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (6): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.107)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.120)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.133)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.147)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (12): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.160)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (13): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.173)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (14): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.187)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=384, out_features=768, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (15): MultiScaleBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (scale2_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (scale3_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (scale4_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (scale5_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
)
[07/28 16:20:30] d2.data.datasets.coco INFO: Loading datasets/tless/tless_annotations_train.json takes 12.09 seconds.
[07/28 16:20:30] d2.data.datasets.coco INFO: Loaded 50000 images in COCO format from datasets/tless/tless_annotations_train.json
[07/28 16:20:34] d2.data.build INFO: Removed 0 images with no usable annotations. 50000 images left.
[07/28 16:20:35] d2.data.build INFO: Distribution of instances among all 30 categories:
[36m|  category  | #instances   | category   | #instances   | category   | #instances   |
|:----------:|:-------------|:-----------|:-------------|:-----------|:-------------|
|     1      | 19873        | 2          | 21790        | 3          | 21461        |
|     4      | 21610        | 5          | 23092        | 6          | 23830        |
|     7      | 26345        | 8          | 26271        | 9          | 22606        |
|     10     | 22289        | 11         | 22313        | 12         | 23290        |
|     13     | 20476        | 14         | 22314        | 15         | 22261        |
|     16     | 21595        | 17         | 24514        | 18         | 25429        |
|     19     | 23097        | 20         | 22733        | 21         | 23375        |
|     22     | 23035        | 23         | 24216        | 24         | 22070        |
|     25     | 23853        | 26         | 24387        | 27         | 25290        |
|     28     | 23366        | 29         | 24099        | 30         | 22921        |
|            |              |            |              |            |              |
|   total    | 693801       |            |              |            |              |[0m
[07/28 16:20:35] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [<detectron2.data.transforms.augmentation_impl.RandomApply object at 0x7f45b6e15f40>, ResizeShortestEdge(short_edge_length=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], max_size=1333, sample_style='choice'), RandomFlip()]
[07/28 16:20:35] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[07/28 16:20:35] d2.data.common INFO: Serializing 50000 elements to byte tensors and concatenating them all ...
[07/28 16:20:36] d2.data.common INFO: Serialized dataset takes 389.07 MiB
[07/28 16:20:38] fvcore.common.checkpoint INFO: [Checkpointer] Loading from detectron2://ImageNetPretrained/mvitv2/MViTv2_S_in1k.pyth ...
[07/28 16:21:04] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone.bottom_up:
| Names in Model      | Names in Checkpoint                                                                                                                                                                                    | Shapes                                                                                                                   |
|:--------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|
| blocks.0.attn.*     | blocks.0.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (96,) (96,96) (288,) (288,96) (111,96) (111,96)     |
| blocks.0.mlp.fc1.*  | blocks.0.mlp.fc1.{bias,weight}                                                                                                                                                                         | (384,) (384,96)                                                                                                          |
| blocks.0.mlp.fc2.*  | blocks.0.mlp.fc2.{bias,weight}                                                                                                                                                                         | (96,) (96,384)                                                                                                           |
| blocks.0.norm1.*    | blocks.0.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.0.norm2.*    | blocks.0.norm2.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.attn.*     | blocks.1.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,96) (55,96) (55,96)    |
| blocks.1.mlp.fc1.*  | blocks.1.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.1.mlp.fc2.*  | blocks.1.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.1.norm1.*    | blocks.1.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.norm2.*    | blocks.1.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.1.proj.*     | blocks.1.proj.{bias,weight}                                                                                                                                                                            | (192,) (192,96)                                                                                                          |
| blocks.10.attn.*    | blocks.10.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.10.mlp.fc1.* | blocks.10.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.10.mlp.fc2.* | blocks.10.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.10.norm1.*   | blocks.10.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.10.norm2.*   | blocks.10.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.attn.*    | blocks.11.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.11.mlp.fc1.* | blocks.11.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.11.mlp.fc2.* | blocks.11.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.11.norm1.*   | blocks.11.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.norm2.*   | blocks.11.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.attn.*    | blocks.12.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.12.mlp.fc1.* | blocks.12.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.12.mlp.fc2.* | blocks.12.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.12.norm1.*   | blocks.12.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.norm2.*   | blocks.12.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.attn.*    | blocks.13.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.13.mlp.fc1.* | blocks.13.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.13.mlp.fc2.* | blocks.13.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.13.norm1.*   | blocks.13.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.norm2.*   | blocks.13.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.attn.*    | blocks.14.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,384) (27,96) (27,96) |
| blocks.14.mlp.fc1.* | blocks.14.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.14.mlp.fc2.* | blocks.14.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.14.norm1.*   | blocks.14.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.norm2.*   | blocks.14.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.14.proj.*    | blocks.14.proj.{bias,weight}                                                                                                                                                                           | (768,) (768,384)                                                                                                         |
| blocks.15.attn.*    | blocks.15.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,768) (13,96) (13,96) |
| blocks.15.mlp.fc1.* | blocks.15.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.15.mlp.fc2.* | blocks.15.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.15.norm1.*   | blocks.15.norm1.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.15.norm2.*   | blocks.15.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.2.attn.*     | blocks.2.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,192) (55,96) (55,96)   |
| blocks.2.mlp.fc1.*  | blocks.2.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.2.mlp.fc2.*  | blocks.2.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.2.norm1.*    | blocks.2.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.2.norm2.*    | blocks.2.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.3.attn.*     | blocks.3.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,192) (55,96) (55,96) |
| blocks.3.mlp.fc1.*  | blocks.3.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.3.mlp.fc2.*  | blocks.3.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.3.norm1.*    | blocks.3.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.3.norm2.*    | blocks.3.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.3.proj.*     | blocks.3.proj.{bias,weight}                                                                                                                                                                            | (384,) (384,192)                                                                                                         |
| blocks.4.attn.*     | blocks.4.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.4.mlp.fc1.*  | blocks.4.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.4.mlp.fc2.*  | blocks.4.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.4.norm1.*    | blocks.4.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.4.norm2.*    | blocks.4.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.5.attn.*     | blocks.5.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.5.mlp.fc1.*  | blocks.5.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.5.mlp.fc2.*  | blocks.5.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.5.norm1.*    | blocks.5.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.5.norm2.*    | blocks.5.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.6.attn.*     | blocks.6.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.6.mlp.fc1.*  | blocks.6.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.6.mlp.fc2.*  | blocks.6.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.6.norm1.*    | blocks.6.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.6.norm2.*    | blocks.6.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.attn.*     | blocks.7.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.7.mlp.fc1.*  | blocks.7.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.7.mlp.fc2.*  | blocks.7.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.7.norm1.*    | blocks.7.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.norm2.*    | blocks.7.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.attn.*     | blocks.8.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.8.mlp.fc1.*  | blocks.8.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.8.mlp.fc2.*  | blocks.8.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.8.norm1.*    | blocks.8.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.norm2.*    | blocks.8.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.attn.*     | blocks.9.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.9.mlp.fc1.*  | blocks.9.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.9.mlp.fc2.*  | blocks.9.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.9.norm1.*    | blocks.9.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.norm2.*    | blocks.9.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| patch_embed.proj.*  | patch_embed.proj.{bias,weight}                                                                                                                                                                         | (96,) (96,3,7,7)                                                                                                         |
[07/28 16:21:04] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.bottom_up.scale2_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale3_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale4_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale5_norm.{bias, weight}[0m
[34mbackbone.fpn_lateral2.{bias, weight}[0m
[34mbackbone.fpn_lateral3.{bias, weight}[0m
[34mbackbone.fpn_lateral4.{bias, weight}[0m
[34mbackbone.fpn_lateral5.{bias, weight}[0m
[34mbackbone.fpn_output2.{bias, weight}[0m
[34mbackbone.fpn_output3.{bias, weight}[0m
[34mbackbone.fpn_output4.{bias, weight}[0m
[34mbackbone.fpn_output5.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv0.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv1.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[07/28 16:21:04] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mhead.projection.{bias, weight}[0m
  [35mnorm.{bias, weight}[0m
[07/28 16:21:04] d2.engine.train_loop INFO: Starting training from iteration 0
[07/28 16:21:06] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 422, in run_step
    loss_dict = self.model(data)
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 167, in forward
    _, detector_losses = self.roi_heads(images, features, proposals, gt_instances)
TypeError: 'int' object is not callable
[07/28 16:21:06] d2.engine.hooks INFO: Total training time: 0:00:01 (0:00:00 on hooks)
[07/28 16:21:06] d2.utils.events INFO:  iter: 0    lr: N/A  max_mem: 10552M
[07/28 16:22:06] detectron2 INFO: Rank of current process: 0. World size: 1
[07/28 16:22:07] detectron2 INFO: Environment info:
----------------------  -------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.10 (default, May 26 2023, 14:05:08) [GCC 9.4.0]
numpy                   1.23.5
detectron2              0.6 @/home/hoenig/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 9.4
CUDA compiler           CUDA 11.6
detectron2 arch flags   8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          525.125.06
CUDA_HOME               /usr/local/cuda
Pillow                  9.3.0
torchvision             0.13.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221122
iopath                  0.1.9
cv2                     4.7.0
----------------------  -------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[07/28 16:22:07] detectron2 INFO: Command line arguments: Namespace(config_file='projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_s_3x.py', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[07/28 16:22:07] detectron2 INFO: Contents of args.config_file=projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_s_3x.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcascade_mask_rcnn_mvitv2_t_3x[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15moptimizer[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mlr_multiplier[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtrain[39m


[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mdepth[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mlast_block_indexes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m([39m[38;5;141m0[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m13[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m15[39m[38;5;15m)[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15minit_checkpoint[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://ImageNetPretrained/mvitv2/MViTv2_S_in1k.pyth[39m[38;5;186m"[39m

[07/28 16:22:07] d2.config.lazy WARNING: The config contains objects that cannot serialize to a valid yaml. ./output/config.yaml is human-readable but cannot be loaded.
[07/28 16:22:07] d2.config.lazy WARNING: Config is saved using cloudpickle at ./output/config.yaml.pkl.
[07/28 16:22:07] detectron2 INFO: Full config saved to ./output/config.yaml
[07/28 16:22:07] d2.utils.env INFO: Using a generated random seed 10128169
[07/28 16:22:07] detectron2 INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): MViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      )
      (blocks): ModuleList(
        (0): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=96, out_features=192, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (2): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (3): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=192, out_features=384, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (4): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (6): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.107)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.120)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.133)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.147)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (12): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.160)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (13): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.173)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (14): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.187)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=384, out_features=768, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (15): MultiScaleBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (scale2_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (scale3_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (scale4_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (scale5_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): CascadeROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): ModuleList(
      (0): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (1): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (2): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
    )
    (box_predictor): ModuleList(
      (0): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=81, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (1): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=81, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (2): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=81, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[07/28 16:22:22] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 13.39 seconds.
[07/28 16:22:22] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[07/28 16:22:27] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[07/28 16:22:30] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[07/28 16:22:30] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [<detectron2.data.transforms.augmentation_impl.RandomApply object at 0x7f20e6eed8e0>, ResizeShortestEdge(short_edge_length=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], max_size=1333, sample_style='choice'), RandomFlip()]
[07/28 16:22:30] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[07/28 16:22:30] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[07/28 16:22:32] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[07/28 16:22:35] fvcore.common.checkpoint INFO: [Checkpointer] Loading from detectron2://ImageNetPretrained/mvitv2/MViTv2_S_in1k.pyth ...
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv1.norm.bias in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv1.norm.weight in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv2.norm.bias in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv2.norm.weight in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv3.norm.bias in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv3.norm.weight in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv4.norm.bias in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv4.norm.weight in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv1.norm.bias in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv1.norm.weight in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv2.norm.bias in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv2.norm.weight in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv3.norm.bias in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv3.norm.weight in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv4.norm.bias in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv4.norm.weight in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv1.norm.bias in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv1.norm.weight in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv2.norm.bias in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv2.norm.weight in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv3.norm.bias in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv3.norm.weight in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv4.norm.bias in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv4.norm.weight in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn1.norm.bias in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn1.norm.weight in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn2.norm.bias in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn2.norm.weight in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn3.norm.bias in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn3.norm.weight in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn4.norm.bias in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn4.norm.weight in model is torch.Size([256]).
[07/28 16:22:35] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:22:35] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone.bottom_up:
| Names in Model      | Names in Checkpoint                                                                                                                                                                                    | Shapes                                                                                                                   |
|:--------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|
| blocks.0.attn.*     | blocks.0.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (96,) (96,96) (288,) (288,96) (111,96) (111,96)     |
| blocks.0.mlp.fc1.*  | blocks.0.mlp.fc1.{bias,weight}                                                                                                                                                                         | (384,) (384,96)                                                                                                          |
| blocks.0.mlp.fc2.*  | blocks.0.mlp.fc2.{bias,weight}                                                                                                                                                                         | (96,) (96,384)                                                                                                           |
| blocks.0.norm1.*    | blocks.0.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.0.norm2.*    | blocks.0.norm2.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.attn.*     | blocks.1.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,96) (55,96) (55,96)    |
| blocks.1.mlp.fc1.*  | blocks.1.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.1.mlp.fc2.*  | blocks.1.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.1.norm1.*    | blocks.1.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.norm2.*    | blocks.1.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.1.proj.*     | blocks.1.proj.{bias,weight}                                                                                                                                                                            | (192,) (192,96)                                                                                                          |
| blocks.10.attn.*    | blocks.10.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.10.mlp.fc1.* | blocks.10.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.10.mlp.fc2.* | blocks.10.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.10.norm1.*   | blocks.10.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.10.norm2.*   | blocks.10.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.attn.*    | blocks.11.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.11.mlp.fc1.* | blocks.11.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.11.mlp.fc2.* | blocks.11.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.11.norm1.*   | blocks.11.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.norm2.*   | blocks.11.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.attn.*    | blocks.12.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.12.mlp.fc1.* | blocks.12.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.12.mlp.fc2.* | blocks.12.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.12.norm1.*   | blocks.12.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.norm2.*   | blocks.12.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.attn.*    | blocks.13.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.13.mlp.fc1.* | blocks.13.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.13.mlp.fc2.* | blocks.13.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.13.norm1.*   | blocks.13.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.norm2.*   | blocks.13.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.attn.*    | blocks.14.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,384) (27,96) (27,96) |
| blocks.14.mlp.fc1.* | blocks.14.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.14.mlp.fc2.* | blocks.14.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.14.norm1.*   | blocks.14.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.norm2.*   | blocks.14.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.14.proj.*    | blocks.14.proj.{bias,weight}                                                                                                                                                                           | (768,) (768,384)                                                                                                         |
| blocks.15.attn.*    | blocks.15.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,768) (13,96) (13,96) |
| blocks.15.mlp.fc1.* | blocks.15.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.15.mlp.fc2.* | blocks.15.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.15.norm1.*   | blocks.15.norm1.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.15.norm2.*   | blocks.15.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.2.attn.*     | blocks.2.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,192) (55,96) (55,96)   |
| blocks.2.mlp.fc1.*  | blocks.2.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.2.mlp.fc2.*  | blocks.2.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.2.norm1.*    | blocks.2.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.2.norm2.*    | blocks.2.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.3.attn.*     | blocks.3.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,192) (55,96) (55,96) |
| blocks.3.mlp.fc1.*  | blocks.3.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.3.mlp.fc2.*  | blocks.3.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.3.norm1.*    | blocks.3.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.3.norm2.*    | blocks.3.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.3.proj.*     | blocks.3.proj.{bias,weight}                                                                                                                                                                            | (384,) (384,192)                                                                                                         |
| blocks.4.attn.*     | blocks.4.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.4.mlp.fc1.*  | blocks.4.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.4.mlp.fc2.*  | blocks.4.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.4.norm1.*    | blocks.4.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.4.norm2.*    | blocks.4.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.5.attn.*     | blocks.5.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.5.mlp.fc1.*  | blocks.5.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.5.mlp.fc2.*  | blocks.5.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.5.norm1.*    | blocks.5.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.5.norm2.*    | blocks.5.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.6.attn.*     | blocks.6.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.6.mlp.fc1.*  | blocks.6.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.6.mlp.fc2.*  | blocks.6.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.6.norm1.*    | blocks.6.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.6.norm2.*    | blocks.6.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.attn.*     | blocks.7.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.7.mlp.fc1.*  | blocks.7.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.7.mlp.fc2.*  | blocks.7.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.7.norm1.*    | blocks.7.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.norm2.*    | blocks.7.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.attn.*     | blocks.8.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.8.mlp.fc1.*  | blocks.8.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.8.mlp.fc2.*  | blocks.8.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.8.norm1.*    | blocks.8.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.norm2.*    | blocks.8.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.attn.*     | blocks.9.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.9.mlp.fc1.*  | blocks.9.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.9.mlp.fc2.*  | blocks.9.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.9.norm1.*    | blocks.9.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.norm2.*    | blocks.9.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| patch_embed.proj.*  | patch_embed.proj.{bias,weight}                                                                                                                                                                         | (96,) (96,3,7,7)                                                                                                         |
[07/28 16:22:35] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.bottom_up.scale2_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale3_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale4_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale5_norm.{bias, weight}[0m
[34mbackbone.fpn_lateral2.{bias, weight}[0m
[34mbackbone.fpn_lateral3.{bias, weight}[0m
[34mbackbone.fpn_lateral4.{bias, weight}[0m
[34mbackbone.fpn_lateral5.{bias, weight}[0m
[34mbackbone.fpn_output2.{bias, weight}[0m
[34mbackbone.fpn_output3.{bias, weight}[0m
[34mbackbone.fpn_output4.{bias, weight}[0m
[34mbackbone.fpn_output5.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv0.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv1.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mroi_heads.box_head.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv1.weight[0m
[34mroi_heads.box_head.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv2.weight[0m
[34mroi_heads.box_head.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv3.weight[0m
[34mroi_heads.box_head.0.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv4.weight[0m
[34mroi_heads.box_head.0.fc1.{bias, weight}[0m
[34mroi_heads.box_head.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv1.weight[0m
[34mroi_heads.box_head.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv2.weight[0m
[34mroi_heads.box_head.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv3.weight[0m
[34mroi_heads.box_head.1.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv4.weight[0m
[34mroi_heads.box_head.1.fc1.{bias, weight}[0m
[34mroi_heads.box_head.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv1.weight[0m
[34mroi_heads.box_head.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv2.weight[0m
[34mroi_heads.box_head.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv3.weight[0m
[34mroi_heads.box_head.2.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv4.weight[0m
[34mroi_heads.box_head.2.fc1.{bias, weight}[0m
[34mroi_heads.box_predictor.0.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.0.cls_score.{bias, weight}[0m
[34mroi_heads.box_predictor.1.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.1.cls_score.{bias, weight}[0m
[34mroi_heads.box_predictor.2.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.2.cls_score.{bias, weight}[0m
[34mroi_heads.mask_head.deconv.{bias, weight}[0m
[34mroi_heads.mask_head.mask_fcn1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn1.weight[0m
[34mroi_heads.mask_head.mask_fcn2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn2.weight[0m
[34mroi_heads.mask_head.mask_fcn3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn3.weight[0m
[34mroi_heads.mask_head.mask_fcn4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn4.weight[0m
[34mroi_heads.mask_head.predictor.{bias, weight}[0m
[07/28 16:22:35] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mhead.projection.{bias, weight}[0m
  [35mnorm.{bias, weight}[0m
[07/28 16:22:35] d2.engine.train_loop INFO: Starting training from iteration 0
[07/28 16:22:38] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 422, in run_step
    loss_dict = self.model(data)
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 158, in forward
    features = self.backbone(images.tensor)
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/backbone/fpn.py", line 139, in forward
    bottom_up_features = self.bottom_up(x)
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/backbone/mvit.py", line 438, in forward
    x = blk(x)
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/backbone/mvit.py", line 259, in forward
    x_block = self.attn(x_norm)
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/backbone/mvit.py", line 157, in forward
    attn = (q * self.scale) @ k.transpose(-2, -1)
RuntimeError: CUDA out of memory. Tried to allocate 1.08 GiB (GPU 0; 23.67 GiB total capacity; 18.61 GiB already allocated; 889.94 MiB free; 19.89 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[07/28 16:22:38] d2.engine.hooks INFO: Total training time: 0:00:02 (0:00:00 on hooks)
[07/28 16:22:38] d2.utils.events INFO:  iter: 0    lr: N/A  max_mem: 19054M
[07/28 16:23:13] detectron2 INFO: Rank of current process: 0. World size: 1
[07/28 16:23:14] detectron2 INFO: Environment info:
----------------------  -------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.10 (default, May 26 2023, 14:05:08) [GCC 9.4.0]
numpy                   1.23.5
detectron2              0.6 @/home/hoenig/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 9.4
CUDA compiler           CUDA 11.6
detectron2 arch flags   8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          525.125.06
CUDA_HOME               /usr/local/cuda
Pillow                  9.3.0
torchvision             0.13.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221122
iopath                  0.1.9
cv2                     4.7.0
----------------------  -------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[07/28 16:23:14] detectron2 INFO: Command line arguments: Namespace(config_file='projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_s_3x.py', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[07/28 16:23:14] detectron2 INFO: Contents of args.config_file=projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_s_3x.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcascade_mask_rcnn_mvitv2_t_3x[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15moptimizer[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mlr_multiplier[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtrain[39m


[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mdepth[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mlast_block_indexes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m([39m[38;5;141m0[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m13[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m15[39m[38;5;15m)[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15minit_checkpoint[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://ImageNetPretrained/mvitv2/MViTv2_S_in1k.pyth[39m[38;5;186m"[39m

[07/28 16:23:14] d2.config.lazy WARNING: The config contains objects that cannot serialize to a valid yaml. ./output/config.yaml is human-readable but cannot be loaded.
[07/28 16:23:14] d2.config.lazy WARNING: Config is saved using cloudpickle at ./output/config.yaml.pkl.
[07/28 16:23:14] detectron2 INFO: Full config saved to ./output/config.yaml
[07/28 16:23:14] d2.utils.env INFO: Using a generated random seed 17072129
[07/28 16:23:14] detectron2 INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): MViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      )
      (blocks): ModuleList(
        (0): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=96, out_features=192, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (2): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (3): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=192, out_features=384, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (4): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (6): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.107)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.120)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.133)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.147)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (12): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.160)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (13): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.173)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (14): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.187)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=384, out_features=768, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (15): MultiScaleBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (scale2_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (scale3_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (scale4_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (scale5_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): CascadeROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): ModuleList(
      (0): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (1): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (2): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
    )
    (box_predictor): ModuleList(
      (0): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=81, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (1): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=81, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (2): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=81, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[07/28 16:23:28] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 13.13 seconds.
[07/28 16:23:29] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[07/28 16:23:34] d2.data.build INFO: Removed 1021 images with no usable annotations. 117266 images left.
[07/28 16:23:36] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[07/28 16:23:36] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [<detectron2.data.transforms.augmentation_impl.RandomApply object at 0x7fe31e118760>, ResizeShortestEdge(short_edge_length=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], max_size=1333, sample_style='choice'), RandomFlip()]
[07/28 16:23:36] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[07/28 16:23:36] d2.data.common INFO: Serializing 117266 elements to byte tensors and concatenating them all ...
[07/28 16:23:39] d2.data.common INFO: Serialized dataset takes 451.21 MiB
[07/28 16:23:41] fvcore.common.checkpoint INFO: [Checkpointer] Loading from detectron2://ImageNetPretrained/mvitv2/MViTv2_S_in1k.pyth ...
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv1.norm.bias in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv1.norm.weight in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv2.norm.bias in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv2.norm.weight in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv3.norm.bias in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv3.norm.weight in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv4.norm.bias in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv4.norm.weight in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv1.norm.bias in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv1.norm.weight in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv2.norm.bias in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv2.norm.weight in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv3.norm.bias in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv3.norm.weight in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv4.norm.bias in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv4.norm.weight in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv1.norm.bias in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv1.norm.weight in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv2.norm.bias in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv2.norm.weight in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv3.norm.bias in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv3.norm.weight in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv4.norm.bias in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv4.norm.weight in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn1.norm.bias in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn1.norm.weight in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn2.norm.bias in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn2.norm.weight in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn3.norm.bias in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn3.norm.weight in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn4.norm.bias in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn4.norm.weight in model is torch.Size([256]).
[07/28 16:23:41] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:23:41] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone.bottom_up:
| Names in Model      | Names in Checkpoint                                                                                                                                                                                    | Shapes                                                                                                                   |
|:--------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|
| blocks.0.attn.*     | blocks.0.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (96,) (96,96) (288,) (288,96) (111,96) (111,96)     |
| blocks.0.mlp.fc1.*  | blocks.0.mlp.fc1.{bias,weight}                                                                                                                                                                         | (384,) (384,96)                                                                                                          |
| blocks.0.mlp.fc2.*  | blocks.0.mlp.fc2.{bias,weight}                                                                                                                                                                         | (96,) (96,384)                                                                                                           |
| blocks.0.norm1.*    | blocks.0.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.0.norm2.*    | blocks.0.norm2.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.attn.*     | blocks.1.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,96) (55,96) (55,96)    |
| blocks.1.mlp.fc1.*  | blocks.1.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.1.mlp.fc2.*  | blocks.1.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.1.norm1.*    | blocks.1.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.norm2.*    | blocks.1.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.1.proj.*     | blocks.1.proj.{bias,weight}                                                                                                                                                                            | (192,) (192,96)                                                                                                          |
| blocks.10.attn.*    | blocks.10.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.10.mlp.fc1.* | blocks.10.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.10.mlp.fc2.* | blocks.10.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.10.norm1.*   | blocks.10.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.10.norm2.*   | blocks.10.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.attn.*    | blocks.11.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.11.mlp.fc1.* | blocks.11.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.11.mlp.fc2.* | blocks.11.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.11.norm1.*   | blocks.11.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.norm2.*   | blocks.11.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.attn.*    | blocks.12.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.12.mlp.fc1.* | blocks.12.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.12.mlp.fc2.* | blocks.12.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.12.norm1.*   | blocks.12.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.norm2.*   | blocks.12.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.attn.*    | blocks.13.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.13.mlp.fc1.* | blocks.13.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.13.mlp.fc2.* | blocks.13.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.13.norm1.*   | blocks.13.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.norm2.*   | blocks.13.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.attn.*    | blocks.14.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,384) (27,96) (27,96) |
| blocks.14.mlp.fc1.* | blocks.14.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.14.mlp.fc2.* | blocks.14.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.14.norm1.*   | blocks.14.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.norm2.*   | blocks.14.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.14.proj.*    | blocks.14.proj.{bias,weight}                                                                                                                                                                           | (768,) (768,384)                                                                                                         |
| blocks.15.attn.*    | blocks.15.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,768) (13,96) (13,96) |
| blocks.15.mlp.fc1.* | blocks.15.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.15.mlp.fc2.* | blocks.15.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.15.norm1.*   | blocks.15.norm1.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.15.norm2.*   | blocks.15.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.2.attn.*     | blocks.2.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,192) (55,96) (55,96)   |
| blocks.2.mlp.fc1.*  | blocks.2.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.2.mlp.fc2.*  | blocks.2.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.2.norm1.*    | blocks.2.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.2.norm2.*    | blocks.2.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.3.attn.*     | blocks.3.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,192) (55,96) (55,96) |
| blocks.3.mlp.fc1.*  | blocks.3.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.3.mlp.fc2.*  | blocks.3.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.3.norm1.*    | blocks.3.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.3.norm2.*    | blocks.3.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.3.proj.*     | blocks.3.proj.{bias,weight}                                                                                                                                                                            | (384,) (384,192)                                                                                                         |
| blocks.4.attn.*     | blocks.4.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.4.mlp.fc1.*  | blocks.4.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.4.mlp.fc2.*  | blocks.4.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.4.norm1.*    | blocks.4.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.4.norm2.*    | blocks.4.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.5.attn.*     | blocks.5.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.5.mlp.fc1.*  | blocks.5.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.5.mlp.fc2.*  | blocks.5.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.5.norm1.*    | blocks.5.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.5.norm2.*    | blocks.5.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.6.attn.*     | blocks.6.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.6.mlp.fc1.*  | blocks.6.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.6.mlp.fc2.*  | blocks.6.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.6.norm1.*    | blocks.6.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.6.norm2.*    | blocks.6.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.attn.*     | blocks.7.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.7.mlp.fc1.*  | blocks.7.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.7.mlp.fc2.*  | blocks.7.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.7.norm1.*    | blocks.7.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.norm2.*    | blocks.7.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.attn.*     | blocks.8.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.8.mlp.fc1.*  | blocks.8.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.8.mlp.fc2.*  | blocks.8.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.8.norm1.*    | blocks.8.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.norm2.*    | blocks.8.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.attn.*     | blocks.9.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.9.mlp.fc1.*  | blocks.9.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.9.mlp.fc2.*  | blocks.9.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.9.norm1.*    | blocks.9.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.norm2.*    | blocks.9.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| patch_embed.proj.*  | patch_embed.proj.{bias,weight}                                                                                                                                                                         | (96,) (96,3,7,7)                                                                                                         |
[07/28 16:23:42] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.bottom_up.scale2_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale3_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale4_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale5_norm.{bias, weight}[0m
[34mbackbone.fpn_lateral2.{bias, weight}[0m
[34mbackbone.fpn_lateral3.{bias, weight}[0m
[34mbackbone.fpn_lateral4.{bias, weight}[0m
[34mbackbone.fpn_lateral5.{bias, weight}[0m
[34mbackbone.fpn_output2.{bias, weight}[0m
[34mbackbone.fpn_output3.{bias, weight}[0m
[34mbackbone.fpn_output4.{bias, weight}[0m
[34mbackbone.fpn_output5.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv0.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv1.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mroi_heads.box_head.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv1.weight[0m
[34mroi_heads.box_head.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv2.weight[0m
[34mroi_heads.box_head.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv3.weight[0m
[34mroi_heads.box_head.0.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv4.weight[0m
[34mroi_heads.box_head.0.fc1.{bias, weight}[0m
[34mroi_heads.box_head.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv1.weight[0m
[34mroi_heads.box_head.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv2.weight[0m
[34mroi_heads.box_head.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv3.weight[0m
[34mroi_heads.box_head.1.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv4.weight[0m
[34mroi_heads.box_head.1.fc1.{bias, weight}[0m
[34mroi_heads.box_head.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv1.weight[0m
[34mroi_heads.box_head.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv2.weight[0m
[34mroi_heads.box_head.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv3.weight[0m
[34mroi_heads.box_head.2.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv4.weight[0m
[34mroi_heads.box_head.2.fc1.{bias, weight}[0m
[34mroi_heads.box_predictor.0.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.0.cls_score.{bias, weight}[0m
[34mroi_heads.box_predictor.1.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.1.cls_score.{bias, weight}[0m
[34mroi_heads.box_predictor.2.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.2.cls_score.{bias, weight}[0m
[34mroi_heads.mask_head.deconv.{bias, weight}[0m
[34mroi_heads.mask_head.mask_fcn1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn1.weight[0m
[34mroi_heads.mask_head.mask_fcn2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn2.weight[0m
[34mroi_heads.mask_head.mask_fcn3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn3.weight[0m
[34mroi_heads.mask_head.mask_fcn4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn4.weight[0m
[34mroi_heads.mask_head.predictor.{bias, weight}[0m
[07/28 16:23:42] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mhead.projection.{bias, weight}[0m
  [35mnorm.{bias, weight}[0m
[07/28 16:23:42] d2.engine.train_loop INFO: Starting training from iteration 0
[07/28 16:23:55] d2.utils.events INFO:  eta: 10:39:42  iter: 19  total_loss: 12.78  loss_cls_stage0: 3.988  loss_box_reg_stage0: 0.01849  loss_cls_stage1: 3.79  loss_box_reg_stage1: 0.009039  loss_cls_stage2: 3.53  loss_box_reg_stage2: 0.00531  loss_mask: 0.6931  loss_rpn_cls: 0.6848  loss_rpn_loc: 0.1288  time: 0.5872  data_time: 0.0160  lr: 1.2308e-05  max_mem: 16339M
[07/28 16:24:08] d2.utils.events INFO:  eta: 11:06:31  iter: 39  total_loss: 2.894  loss_cls_stage0: 0.537  loss_box_reg_stage0: 0.1299  loss_cls_stage1: 0.3427  loss_box_reg_stage1: 0.07897  loss_cls_stage2: 0.2324  loss_box_reg_stage2: 0.02699  loss_mask: 0.6929  loss_rpn_cls: 0.5938  loss_rpn_loc: 0.1017  time: 0.5962  data_time: 0.0058  lr: 2.5095e-05  max_mem: 16339M
[07/28 16:24:20] d2.utils.events INFO:  eta: 11:16:36  iter: 59  total_loss: 2.192  loss_cls_stage0: 0.447  loss_box_reg_stage0: 0.1529  loss_cls_stage1: 0.2506  loss_box_reg_stage1: 0.09244  loss_cls_stage2: 0.1572  loss_box_reg_stage2: 0.03744  loss_mask: 0.6925  loss_rpn_cls: 0.3731  loss_rpn_loc: 0.1583  time: 0.6006  data_time: 0.0061  lr: 3.7882e-05  max_mem: 16339M
[07/28 16:24:32] d2.utils.events INFO:  eta: 11:24:50  iter: 79  total_loss: 2.082  loss_cls_stage0: 0.3885  loss_box_reg_stage0: 0.1709  loss_cls_stage1: 0.209  loss_box_reg_stage1: 0.09469  loss_cls_stage2: 0.1406  loss_box_reg_stage2: 0.03155  loss_mask: 0.6918  loss_rpn_cls: 0.2435  loss_rpn_loc: 0.09066  time: 0.6042  data_time: 0.0056  lr: 5.0669e-05  max_mem: 16339M
[07/28 16:24:44] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 422, in run_step
    loss_dict = self.model(data)
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 167, in forward
    _, detector_losses = self.roi_heads(images, features, proposals, gt_instances)
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/roi_heads/cascade_rcnn.py", line 145, in forward
    losses.update(self._forward_mask(features, proposals))
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 843, in _forward_mask
    features = self.mask_pooler(features, boxes)
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/poolers.py", line 261, in forward
    output.index_put_((inds,), pooler(x[level], pooler_fmt_boxes_level))
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/hoenig/.local/lib/python3.8/site-packages/torchvision/ops/roi_align.py", line 61, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/_ops.py", line 143, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: CUDA out of memory. Tried to allocate 334.00 MiB (GPU 0; 23.67 GiB total capacity; 19.30 GiB already allocated; 230.50 MiB free; 20.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[07/28 16:24:44] d2.engine.hooks INFO: Overall training speed: 96 iterations in 0:00:58 (0.6117 s / it)
[07/28 16:24:44] d2.engine.hooks INFO: Total training time: 0:00:58 (0:00:00 on hooks)
[07/28 16:24:44] d2.utils.events INFO:  eta: 11:25:45  iter: 98  total_loss: 2.356  loss_cls_stage0: 0.492  loss_box_reg_stage0: 0.2656  loss_cls_stage1: 0.2547  loss_box_reg_stage1: 0.141  loss_cls_stage2: 0.1602  loss_box_reg_stage2: 0.05005  loss_mask: 0.6907  loss_rpn_cls: 0.2217  loss_rpn_loc: 0.1012  time: 0.6083  data_time: 0.0056  lr: 6.2178e-05  max_mem: 19759M
[07/28 16:29:19] detectron2 INFO: Rank of current process: 0. World size: 1
[07/28 16:29:20] detectron2 INFO: Environment info:
----------------------  -------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.10 (default, May 26 2023, 14:05:08) [GCC 9.4.0]
numpy                   1.23.5
detectron2              0.6 @/home/hoenig/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 9.4
CUDA compiler           CUDA 11.6
detectron2 arch flags   8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          525.125.06
CUDA_HOME               /usr/local/cuda
Pillow                  9.3.0
torchvision             0.13.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221122
iopath                  0.1.9
cv2                     4.7.0
----------------------  -------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[07/28 16:29:20] detectron2 INFO: Command line arguments: Namespace(config_file='projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_s_3x.py', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[07/28 16:29:20] detectron2 INFO: Contents of args.config_file=projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_s_3x.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcascade_mask_rcnn_mvitv2_t_3x[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15moptimizer[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mlr_multiplier[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtrain[39m


[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mdepth[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mlast_block_indexes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m([39m[38;5;141m0[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m13[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m15[39m[38;5;15m)[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15minit_checkpoint[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://ImageNetPretrained/mvitv2/MViTv2_S_in1k.pyth[39m[38;5;186m"[39m

[07/28 16:29:20] d2.config.lazy WARNING: The config contains objects that cannot serialize to a valid yaml. ./output/config.yaml is human-readable but cannot be loaded.
[07/28 16:29:20] d2.config.lazy WARNING: Config is saved using cloudpickle at ./output/config.yaml.pkl.
[07/28 16:29:20] detectron2 INFO: Full config saved to ./output/config.yaml
[07/28 16:29:20] d2.utils.env INFO: Using a generated random seed 23421294
[07/28 16:29:21] detectron2 INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): MViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      )
      (blocks): ModuleList(
        (0): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=96, out_features=192, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (2): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (3): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=192, out_features=384, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (4): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (6): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.107)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.120)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.133)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.147)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (12): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.160)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (13): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.173)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (14): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.187)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=384, out_features=768, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (15): MultiScaleBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (scale2_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (scale3_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (scale4_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (scale5_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): CascadeROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): ModuleList(
      (0): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (1): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (2): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
    )
    (box_predictor): ModuleList(
      (0): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=31, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (1): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=31, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (2): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=31, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 30, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[07/28 16:29:33] d2.data.datasets.coco INFO: Loading datasets/tless/tless_annotations_train.json takes 11.75 seconds.
[07/28 16:29:34] d2.data.datasets.coco INFO: Loaded 50000 images in COCO format from datasets/tless/tless_annotations_train.json
[07/28 16:29:37] d2.data.build INFO: Removed 0 images with no usable annotations. 50000 images left.
[07/28 16:29:39] d2.data.build INFO: Distribution of instances among all 30 categories:
[36m|  category  | #instances   | category   | #instances   | category   | #instances   |
|:----------:|:-------------|:-----------|:-------------|:-----------|:-------------|
|     1      | 19873        | 2          | 21790        | 3          | 21461        |
|     4      | 21610        | 5          | 23092        | 6          | 23830        |
|     7      | 26345        | 8          | 26271        | 9          | 22606        |
|     10     | 22289        | 11         | 22313        | 12         | 23290        |
|     13     | 20476        | 14         | 22314        | 15         | 22261        |
|     16     | 21595        | 17         | 24514        | 18         | 25429        |
|     19     | 23097        | 20         | 22733        | 21         | 23375        |
|     22     | 23035        | 23         | 24216        | 24         | 22070        |
|     25     | 23853        | 26         | 24387        | 27         | 25290        |
|     28     | 23366        | 29         | 24099        | 30         | 22921        |
|            |              |            |              |            |              |
|   total    | 693801       |            |              |            |              |[0m
[07/28 16:29:39] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [<detectron2.data.transforms.augmentation_impl.RandomApply object at 0x7f9cf6cfafd0>, ResizeShortestEdge(short_edge_length=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], max_size=1333, sample_style='choice'), RandomFlip()]
[07/28 16:29:39] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[07/28 16:29:39] d2.data.common INFO: Serializing 50000 elements to byte tensors and concatenating them all ...
[07/28 16:29:40] d2.data.common INFO: Serialized dataset takes 389.07 MiB
[07/28 16:29:42] fvcore.common.checkpoint INFO: [Checkpointer] Loading from detectron2://ImageNetPretrained/mvitv2/MViTv2_S_in1k.pyth ...
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv1.norm.bias in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv1.norm.weight in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv2.norm.bias in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv2.norm.weight in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv3.norm.bias in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv3.norm.weight in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv4.norm.bias in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv4.norm.weight in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv1.norm.bias in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv1.norm.weight in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv2.norm.bias in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv2.norm.weight in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv3.norm.bias in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv3.norm.weight in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv4.norm.bias in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv4.norm.weight in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv1.norm.bias in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv1.norm.weight in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv2.norm.bias in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv2.norm.weight in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv3.norm.bias in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv3.norm.weight in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv4.norm.bias in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv4.norm.weight in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn1.norm.bias in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn1.norm.weight in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn2.norm.bias in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn2.norm.weight in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn3.norm.bias in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn3.norm.weight in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn4.norm.bias in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn4.norm.weight in model is torch.Size([256]).
[07/28 16:29:42] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[07/28 16:29:42] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone.bottom_up:
| Names in Model      | Names in Checkpoint                                                                                                                                                                                    | Shapes                                                                                                                   |
|:--------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|
| blocks.0.attn.*     | blocks.0.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (96,) (96,96) (288,) (288,96) (111,96) (111,96)     |
| blocks.0.mlp.fc1.*  | blocks.0.mlp.fc1.{bias,weight}                                                                                                                                                                         | (384,) (384,96)                                                                                                          |
| blocks.0.mlp.fc2.*  | blocks.0.mlp.fc2.{bias,weight}                                                                                                                                                                         | (96,) (96,384)                                                                                                           |
| blocks.0.norm1.*    | blocks.0.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.0.norm2.*    | blocks.0.norm2.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.attn.*     | blocks.1.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,96) (55,96) (55,96)    |
| blocks.1.mlp.fc1.*  | blocks.1.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.1.mlp.fc2.*  | blocks.1.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.1.norm1.*    | blocks.1.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.norm2.*    | blocks.1.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.1.proj.*     | blocks.1.proj.{bias,weight}                                                                                                                                                                            | (192,) (192,96)                                                                                                          |
| blocks.10.attn.*    | blocks.10.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.10.mlp.fc1.* | blocks.10.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.10.mlp.fc2.* | blocks.10.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.10.norm1.*   | blocks.10.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.10.norm2.*   | blocks.10.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.attn.*    | blocks.11.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.11.mlp.fc1.* | blocks.11.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.11.mlp.fc2.* | blocks.11.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.11.norm1.*   | blocks.11.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.norm2.*   | blocks.11.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.attn.*    | blocks.12.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.12.mlp.fc1.* | blocks.12.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.12.mlp.fc2.* | blocks.12.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.12.norm1.*   | blocks.12.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.norm2.*   | blocks.12.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.attn.*    | blocks.13.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.13.mlp.fc1.* | blocks.13.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.13.mlp.fc2.* | blocks.13.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.13.norm1.*   | blocks.13.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.norm2.*   | blocks.13.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.attn.*    | blocks.14.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,384) (27,96) (27,96) |
| blocks.14.mlp.fc1.* | blocks.14.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.14.mlp.fc2.* | blocks.14.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.14.norm1.*   | blocks.14.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.norm2.*   | blocks.14.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.14.proj.*    | blocks.14.proj.{bias,weight}                                                                                                                                                                           | (768,) (768,384)                                                                                                         |
| blocks.15.attn.*    | blocks.15.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,768) (13,96) (13,96) |
| blocks.15.mlp.fc1.* | blocks.15.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.15.mlp.fc2.* | blocks.15.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.15.norm1.*   | blocks.15.norm1.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.15.norm2.*   | blocks.15.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.2.attn.*     | blocks.2.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,192) (55,96) (55,96)   |
| blocks.2.mlp.fc1.*  | blocks.2.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.2.mlp.fc2.*  | blocks.2.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.2.norm1.*    | blocks.2.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.2.norm2.*    | blocks.2.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.3.attn.*     | blocks.3.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,192) (55,96) (55,96) |
| blocks.3.mlp.fc1.*  | blocks.3.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.3.mlp.fc2.*  | blocks.3.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.3.norm1.*    | blocks.3.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.3.norm2.*    | blocks.3.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.3.proj.*     | blocks.3.proj.{bias,weight}                                                                                                                                                                            | (384,) (384,192)                                                                                                         |
| blocks.4.attn.*     | blocks.4.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.4.mlp.fc1.*  | blocks.4.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.4.mlp.fc2.*  | blocks.4.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.4.norm1.*    | blocks.4.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.4.norm2.*    | blocks.4.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.5.attn.*     | blocks.5.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.5.mlp.fc1.*  | blocks.5.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.5.mlp.fc2.*  | blocks.5.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.5.norm1.*    | blocks.5.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.5.norm2.*    | blocks.5.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.6.attn.*     | blocks.6.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.6.mlp.fc1.*  | blocks.6.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.6.mlp.fc2.*  | blocks.6.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.6.norm1.*    | blocks.6.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.6.norm2.*    | blocks.6.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.attn.*     | blocks.7.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.7.mlp.fc1.*  | blocks.7.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.7.mlp.fc2.*  | blocks.7.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.7.norm1.*    | blocks.7.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.norm2.*    | blocks.7.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.attn.*     | blocks.8.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.8.mlp.fc1.*  | blocks.8.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.8.mlp.fc2.*  | blocks.8.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.8.norm1.*    | blocks.8.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.norm2.*    | blocks.8.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.attn.*     | blocks.9.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.9.mlp.fc1.*  | blocks.9.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.9.mlp.fc2.*  | blocks.9.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.9.norm1.*    | blocks.9.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.norm2.*    | blocks.9.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| patch_embed.proj.*  | patch_embed.proj.{bias,weight}                                                                                                                                                                         | (96,) (96,3,7,7)                                                                                                         |
[07/28 16:29:42] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.bottom_up.scale2_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale3_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale4_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale5_norm.{bias, weight}[0m
[34mbackbone.fpn_lateral2.{bias, weight}[0m
[34mbackbone.fpn_lateral3.{bias, weight}[0m
[34mbackbone.fpn_lateral4.{bias, weight}[0m
[34mbackbone.fpn_lateral5.{bias, weight}[0m
[34mbackbone.fpn_output2.{bias, weight}[0m
[34mbackbone.fpn_output3.{bias, weight}[0m
[34mbackbone.fpn_output4.{bias, weight}[0m
[34mbackbone.fpn_output5.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv0.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv1.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mroi_heads.box_head.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv1.weight[0m
[34mroi_heads.box_head.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv2.weight[0m
[34mroi_heads.box_head.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv3.weight[0m
[34mroi_heads.box_head.0.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv4.weight[0m
[34mroi_heads.box_head.0.fc1.{bias, weight}[0m
[34mroi_heads.box_head.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv1.weight[0m
[34mroi_heads.box_head.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv2.weight[0m
[34mroi_heads.box_head.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv3.weight[0m
[34mroi_heads.box_head.1.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv4.weight[0m
[34mroi_heads.box_head.1.fc1.{bias, weight}[0m
[34mroi_heads.box_head.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv1.weight[0m
[34mroi_heads.box_head.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv2.weight[0m
[34mroi_heads.box_head.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv3.weight[0m
[34mroi_heads.box_head.2.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv4.weight[0m
[34mroi_heads.box_head.2.fc1.{bias, weight}[0m
[34mroi_heads.box_predictor.0.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.0.cls_score.{bias, weight}[0m
[34mroi_heads.box_predictor.1.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.1.cls_score.{bias, weight}[0m
[34mroi_heads.box_predictor.2.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.2.cls_score.{bias, weight}[0m
[34mroi_heads.mask_head.deconv.{bias, weight}[0m
[34mroi_heads.mask_head.mask_fcn1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn1.weight[0m
[34mroi_heads.mask_head.mask_fcn2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn2.weight[0m
[34mroi_heads.mask_head.mask_fcn3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn3.weight[0m
[34mroi_heads.mask_head.mask_fcn4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn4.weight[0m
[34mroi_heads.mask_head.predictor.{bias, weight}[0m
[07/28 16:29:42] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mhead.projection.{bias, weight}[0m
  [35mnorm.{bias, weight}[0m
[07/28 16:29:42] d2.engine.train_loop INFO: Starting training from iteration 0
[07/28 16:29:55] d2.utils.events INFO:  eta: 10:48:20  iter: 19  total_loss: 10.24  loss_cls_stage0: 2.918  loss_box_reg_stage0: 0.005439  loss_cls_stage1: 2.81  loss_box_reg_stage1: 0.006559  loss_cls_stage2: 2.961  loss_box_reg_stage2: 0.006691  loss_mask: 0.6931  loss_rpn_cls: 0.6858  loss_rpn_loc: 0.1365  time: 0.5675  data_time: 0.0147  lr: 1.2308e-05  max_mem: 14270M
[07/28 16:30:08] d2.utils.events INFO:  eta: 11:06:12  iter: 39  total_loss: 4.263  loss_cls_stage0: 1.023  loss_box_reg_stage0: 0.4676  loss_cls_stage1: 0.5934  loss_box_reg_stage1: 0.279  loss_cls_stage2: 0.3494  loss_box_reg_stage2: 0.1047  loss_mask: 0.693  loss_rpn_cls: 0.5588  loss_rpn_loc: 0.1287  time: 0.6000  data_time: 0.0047  lr: 2.5095e-05  max_mem: 15381M
[07/28 16:30:20] d2.utils.events INFO:  eta: 11:12:54  iter: 59  total_loss: 3.795  loss_cls_stage0: 0.8482  loss_box_reg_stage0: 0.4695  loss_cls_stage1: 0.5353  loss_box_reg_stage1: 0.2939  loss_cls_stage2: 0.342  loss_box_reg_stage2: 0.109  loss_mask: 0.6925  loss_rpn_cls: 0.2626  loss_rpn_loc: 0.1304  time: 0.6062  data_time: 0.0051  lr: 3.7882e-05  max_mem: 15381M
[07/28 16:30:33] d2.utils.events INFO:  eta: 11:24:52  iter: 79  total_loss: 4.065  loss_cls_stage0: 0.9183  loss_box_reg_stage0: 0.5811  loss_cls_stage1: 0.5226  loss_box_reg_stage1: 0.3743  loss_cls_stage2: 0.2985  loss_box_reg_stage2: 0.1257  loss_mask: 0.6921  loss_rpn_cls: 0.2178  loss_rpn_loc: 0.1478  time: 0.6099  data_time: 0.0048  lr: 5.0669e-05  max_mem: 15381M
[07/28 16:30:47] d2.utils.events INFO:  eta: 11:49:03  iter: 99  total_loss: 4.391  loss_cls_stage0: 1.094  loss_box_reg_stage0: 0.7595  loss_cls_stage1: 0.5958  loss_box_reg_stage1: 0.4697  loss_cls_stage2: 0.3328  loss_box_reg_stage2: 0.1742  loss_mask: 0.6868  loss_rpn_cls: 0.1786  loss_rpn_loc: 0.1302  time: 0.6292  data_time: 0.0054  lr: 6.3457e-05  max_mem: 15381M
[07/28 16:31:00] d2.utils.events INFO:  eta: 11:51:59  iter: 119  total_loss: 5  loss_cls_stage0: 1.3  loss_box_reg_stage0: 0.9291  loss_cls_stage1: 0.6717  loss_box_reg_stage1: 0.5734  loss_cls_stage2: 0.328  loss_box_reg_stage2: 0.1853  loss_mask: 0.6667  loss_rpn_cls: 0.1471  loss_rpn_loc: 0.1581  time: 0.6324  data_time: 0.0051  lr: 7.6244e-05  max_mem: 15525M
[07/28 16:31:13] d2.utils.events INFO:  eta: 11:54:34  iter: 139  total_loss: 4.957  loss_cls_stage0: 1.309  loss_box_reg_stage0: 0.9558  loss_cls_stage1: 0.6745  loss_box_reg_stage1: 0.5801  loss_cls_stage2: 0.3395  loss_box_reg_stage2: 0.1935  loss_mask: 0.6292  loss_rpn_cls: 0.1073  loss_rpn_loc: 0.1167  time: 0.6347  data_time: 0.0052  lr: 8.9031e-05  max_mem: 15542M
[07/28 16:31:26] d2.utils.events INFO:  eta: 12:09:27  iter: 159  total_loss: 5.278  loss_cls_stage0: 1.296  loss_box_reg_stage0: 0.9447  loss_cls_stage1: 0.7912  loss_box_reg_stage1: 0.7537  loss_cls_stage2: 0.3987  loss_box_reg_stage2: 0.3101  loss_mask: 0.5826  loss_rpn_cls: 0.08563  loss_rpn_loc: 0.1117  time: 0.6394  data_time: 0.0058  lr: 0.00010182  max_mem: 17066M
[07/28 16:31:39] d2.utils.events INFO:  eta: 12:13:57  iter: 179  total_loss: 5.627  loss_cls_stage0: 1.24  loss_box_reg_stage0: 0.8803  loss_cls_stage1: 0.9028  loss_box_reg_stage1: 0.901  loss_cls_stage2: 0.4948  loss_box_reg_stage2: 0.4218  loss_mask: 0.5199  loss_rpn_cls: 0.0937  loss_rpn_loc: 0.1305  time: 0.6410  data_time: 0.0059  lr: 0.00011461  max_mem: 17066M
[07/28 16:31:53] d2.utils.events INFO:  eta: 12:13:01  iter: 199  total_loss: 6.096  loss_cls_stage0: 1.19  loss_box_reg_stage0: 0.8314  loss_cls_stage1: 1.012  loss_box_reg_stage1: 1.138  loss_cls_stage2: 0.6272  loss_box_reg_stage2: 0.6326  loss_mask: 0.4301  loss_rpn_cls: 0.08876  loss_rpn_loc: 0.1187  time: 0.6426  data_time: 0.0055  lr: 0.00012739  max_mem: 17842M
[07/28 16:32:05] d2.utils.events INFO:  eta: 12:08:48  iter: 219  total_loss: 6.361  loss_cls_stage0: 1.161  loss_box_reg_stage0: 0.8082  loss_cls_stage1: 1.063  loss_box_reg_stage1: 1.197  loss_cls_stage2: 0.738  loss_box_reg_stage2: 0.8613  loss_mask: 0.438  loss_rpn_cls: 0.07164  loss_rpn_loc: 0.0981  time: 0.6414  data_time: 0.0054  lr: 0.00014018  max_mem: 17842M
[07/28 16:32:18] d2.utils.events INFO:  eta: 12:05:35  iter: 239  total_loss: 6.433  loss_cls_stage0: 1.07  loss_box_reg_stage0: 0.7663  loss_cls_stage1: 1.059  loss_box_reg_stage1: 1.156  loss_cls_stage2: 0.792  loss_box_reg_stage2: 0.9784  loss_mask: 0.3901  loss_rpn_cls: 0.0701  loss_rpn_loc: 0.1092  time: 0.6400  data_time: 0.0058  lr: 0.00015297  max_mem: 17842M
[07/28 16:32:31] d2.utils.events INFO:  eta: 12:11:57  iter: 259  total_loss: 6.652  loss_cls_stage0: 1.081  loss_box_reg_stage0: 0.7297  loss_cls_stage1: 1.098  loss_box_reg_stage1: 1.229  loss_cls_stage2: 0.8723  loss_box_reg_stage2: 1.157  loss_mask: 0.374  loss_rpn_cls: 0.07363  loss_rpn_loc: 0.09235  time: 0.6426  data_time: 0.0057  lr: 0.00016  max_mem: 17842M
[07/28 16:32:44] d2.utils.events INFO:  eta: 12:10:37  iter: 279  total_loss: 6.642  loss_cls_stage0: 1.024  loss_box_reg_stage0: 0.711  loss_cls_stage1: 1.049  loss_box_reg_stage1: 1.209  loss_cls_stage2: 0.8725  loss_box_reg_stage2: 1.193  loss_mask: 0.3484  loss_rpn_cls: 0.07994  loss_rpn_loc: 0.1051  time: 0.6431  data_time: 0.0053  lr: 0.00016  max_mem: 17842M
[07/28 16:32:57] d2.utils.events INFO:  eta: 12:08:51  iter: 299  total_loss: 6.686  loss_cls_stage0: 1.007  loss_box_reg_stage0: 0.6845  loss_cls_stage1: 1.048  loss_box_reg_stage1: 1.229  loss_cls_stage2: 0.9163  loss_box_reg_stage2: 1.21  loss_mask: 0.3299  loss_rpn_cls: 0.08725  loss_rpn_loc: 0.1126  time: 0.6431  data_time: 0.0054  lr: 0.00016  max_mem: 17842M
[07/28 16:33:10] d2.utils.events INFO:  eta: 12:10:11  iter: 319  total_loss: 6.63  loss_cls_stage0: 0.953  loss_box_reg_stage0: 0.628  loss_cls_stage1: 1.05  loss_box_reg_stage1: 1.19  loss_cls_stage2: 0.9298  loss_box_reg_stage2: 1.319  loss_mask: 0.3119  loss_rpn_cls: 0.07162  loss_rpn_loc: 0.0959  time: 0.6432  data_time: 0.0050  lr: 0.00016  max_mem: 17842M
[07/28 16:33:23] d2.utils.events INFO:  eta: 12:09:58  iter: 339  total_loss: 6.694  loss_cls_stage0: 0.9172  loss_box_reg_stage0: 0.6157  loss_cls_stage1: 1.075  loss_box_reg_stage1: 1.239  loss_cls_stage2: 0.9893  loss_box_reg_stage2: 1.42  loss_mask: 0.2929  loss_rpn_cls: 0.04852  loss_rpn_loc: 0.08519  time: 0.6438  data_time: 0.0053  lr: 0.00016  max_mem: 17842M
[07/28 16:33:36] d2.utils.events INFO:  eta: 12:12:17  iter: 359  total_loss: 6.72  loss_cls_stage0: 0.9127  loss_box_reg_stage0: 0.5861  loss_cls_stage1: 1.067  loss_box_reg_stage1: 1.259  loss_cls_stage2: 0.9768  loss_box_reg_stage2: 1.484  loss_mask: 0.2943  loss_rpn_cls: 0.04568  loss_rpn_loc: 0.08682  time: 0.6448  data_time: 0.0057  lr: 0.00016  max_mem: 17842M
[07/28 16:33:50] d2.utils.events INFO:  eta: 12:11:35  iter: 379  total_loss: 6.375  loss_cls_stage0: 0.845  loss_box_reg_stage0: 0.5608  loss_cls_stage1: 0.9809  loss_box_reg_stage1: 1.123  loss_cls_stage2: 0.9246  loss_box_reg_stage2: 1.367  loss_mask: 0.253  loss_rpn_cls: 0.05717  loss_rpn_loc: 0.0915  time: 0.6458  data_time: 0.0051  lr: 0.00016  max_mem: 19672M
[07/28 16:34:03] d2.utils.events INFO:  eta: 12:11:26  iter: 399  total_loss: 6.043  loss_cls_stage0: 0.8277  loss_box_reg_stage0: 0.5885  loss_cls_stage1: 0.9073  loss_box_reg_stage1: 1.159  loss_cls_stage2: 0.8411  loss_box_reg_stage2: 1.343  loss_mask: 0.2695  loss_rpn_cls: 0.06754  loss_rpn_loc: 0.08609  time: 0.6461  data_time: 0.0053  lr: 0.00016  max_mem: 19672M
[07/28 16:34:16] d2.utils.events INFO:  eta: 12:12:28  iter: 419  total_loss: 6.392  loss_cls_stage0: 0.8681  loss_box_reg_stage0: 0.5609  loss_cls_stage1: 0.9914  loss_box_reg_stage1: 1.162  loss_cls_stage2: 0.909  loss_box_reg_stage2: 1.356  loss_mask: 0.2831  loss_rpn_cls: 0.06067  loss_rpn_loc: 0.08754  time: 0.6477  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 16:34:29] d2.utils.events INFO:  eta: 12:12:42  iter: 439  total_loss: 5.683  loss_cls_stage0: 0.7926  loss_box_reg_stage0: 0.5438  loss_cls_stage1: 0.8407  loss_box_reg_stage1: 1.052  loss_cls_stage2: 0.7723  loss_box_reg_stage2: 1.171  loss_mask: 0.2656  loss_rpn_cls: 0.06818  loss_rpn_loc: 0.09956  time: 0.6479  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 16:34:42] d2.utils.events INFO:  eta: 12:12:29  iter: 459  total_loss: 6.418  loss_cls_stage0: 0.7983  loss_box_reg_stage0: 0.5247  loss_cls_stage1: 0.974  loss_box_reg_stage1: 1.242  loss_cls_stage2: 0.889  loss_box_reg_stage2: 1.513  loss_mask: 0.2415  loss_rpn_cls: 0.04689  loss_rpn_loc: 0.08668  time: 0.6479  data_time: 0.0052  lr: 0.00016  max_mem: 19672M
[07/28 16:34:56] d2.utils.events INFO:  eta: 12:12:50  iter: 479  total_loss: 6.393  loss_cls_stage0: 0.785  loss_box_reg_stage0: 0.5412  loss_cls_stage1: 0.9639  loss_box_reg_stage1: 1.289  loss_cls_stage2: 0.8732  loss_box_reg_stage2: 1.497  loss_mask: 0.2394  loss_rpn_cls: 0.04529  loss_rpn_loc: 0.08415  time: 0.6482  data_time: 0.0051  lr: 0.00016  max_mem: 19672M
[07/28 16:35:09] d2.utils.events INFO:  eta: 12:12:37  iter: 499  total_loss: 5.93  loss_cls_stage0: 0.742  loss_box_reg_stage0: 0.5087  loss_cls_stage1: 0.8733  loss_box_reg_stage1: 1.181  loss_cls_stage2: 0.83  loss_box_reg_stage2: 1.357  loss_mask: 0.2164  loss_rpn_cls: 0.04755  loss_rpn_loc: 0.09442  time: 0.6485  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 16:35:22] d2.utils.events INFO:  eta: 12:11:36  iter: 519  total_loss: 5.736  loss_cls_stage0: 0.6999  loss_box_reg_stage0: 0.5282  loss_cls_stage1: 0.8142  loss_box_reg_stage1: 1.107  loss_cls_stage2: 0.7378  loss_box_reg_stage2: 1.178  loss_mask: 0.2626  loss_rpn_cls: 0.04959  loss_rpn_loc: 0.08565  time: 0.6484  data_time: 0.0052  lr: 0.00016  max_mem: 19672M
[07/28 16:35:34] d2.utils.events INFO:  eta: 12:10:34  iter: 539  total_loss: 5.812  loss_cls_stage0: 0.7004  loss_box_reg_stage0: 0.5048  loss_cls_stage1: 0.8528  loss_box_reg_stage1: 1.135  loss_cls_stage2: 0.8052  loss_box_reg_stage2: 1.401  loss_mask: 0.2278  loss_rpn_cls: 0.04944  loss_rpn_loc: 0.07868  time: 0.6481  data_time: 0.0052  lr: 0.00016  max_mem: 19672M
[07/28 16:35:47] d2.utils.events INFO:  eta: 12:10:06  iter: 559  total_loss: 5.441  loss_cls_stage0: 0.6974  loss_box_reg_stage0: 0.5414  loss_cls_stage1: 0.7839  loss_box_reg_stage1: 1.043  loss_cls_stage2: 0.74  loss_box_reg_stage2: 1.252  loss_mask: 0.2291  loss_rpn_cls: 0.04156  loss_rpn_loc: 0.07169  time: 0.6481  data_time: 0.0051  lr: 0.00016  max_mem: 19672M
[07/28 16:36:01] d2.utils.events INFO:  eta: 12:10:43  iter: 579  total_loss: 5.72  loss_cls_stage0: 0.7448  loss_box_reg_stage0: 0.5316  loss_cls_stage1: 0.8196  loss_box_reg_stage1: 1.124  loss_cls_stage2: 0.7496  loss_box_reg_stage2: 1.244  loss_mask: 0.2264  loss_rpn_cls: 0.03964  loss_rpn_loc: 0.08453  time: 0.6489  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 16:36:14] d2.utils.events INFO:  eta: 12:10:30  iter: 599  total_loss: 5.557  loss_cls_stage0: 0.6832  loss_box_reg_stage0: 0.527  loss_cls_stage1: 0.7696  loss_box_reg_stage1: 1.065  loss_cls_stage2: 0.7463  loss_box_reg_stage2: 1.224  loss_mask: 0.2292  loss_rpn_cls: 0.0566  loss_rpn_loc: 0.07931  time: 0.6487  data_time: 0.0050  lr: 0.00016  max_mem: 19672M
[07/28 16:36:26] d2.utils.events INFO:  eta: 12:10:00  iter: 619  total_loss: 5.398  loss_cls_stage0: 0.6756  loss_box_reg_stage0: 0.4804  loss_cls_stage1: 0.7343  loss_box_reg_stage1: 1.102  loss_cls_stage2: 0.6936  loss_box_reg_stage2: 1.268  loss_mask: 0.2071  loss_rpn_cls: 0.0472  loss_rpn_loc: 0.08715  time: 0.6480  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 16:36:39] d2.utils.events INFO:  eta: 12:08:55  iter: 639  total_loss: 5.441  loss_cls_stage0: 0.6963  loss_box_reg_stage0: 0.5026  loss_cls_stage1: 0.8095  loss_box_reg_stage1: 1.093  loss_cls_stage2: 0.7471  loss_box_reg_stage2: 1.338  loss_mask: 0.2186  loss_rpn_cls: 0.05814  loss_rpn_loc: 0.08578  time: 0.6475  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 16:36:52] d2.utils.events INFO:  eta: 12:08:48  iter: 659  total_loss: 5.561  loss_cls_stage0: 0.6747  loss_box_reg_stage0: 0.4859  loss_cls_stage1: 0.7822  loss_box_reg_stage1: 1.065  loss_cls_stage2: 0.7381  loss_box_reg_stage2: 1.302  loss_mask: 0.2235  loss_rpn_cls: 0.04206  loss_rpn_loc: 0.08624  time: 0.6477  data_time: 0.0052  lr: 0.00016  max_mem: 19672M
[07/28 16:37:05] d2.utils.events INFO:  eta: 12:08:29  iter: 679  total_loss: 5.301  loss_cls_stage0: 0.6381  loss_box_reg_stage0: 0.4861  loss_cls_stage1: 0.7289  loss_box_reg_stage1: 1.081  loss_cls_stage2: 0.7001  loss_box_reg_stage2: 1.252  loss_mask: 0.2008  loss_rpn_cls: 0.04264  loss_rpn_loc: 0.08534  time: 0.6472  data_time: 0.0054  lr: 0.00016  max_mem: 19672M
[07/28 16:37:18] d2.utils.events INFO:  eta: 12:08:10  iter: 699  total_loss: 5.172  loss_cls_stage0: 0.6353  loss_box_reg_stage0: 0.4813  loss_cls_stage1: 0.7192  loss_box_reg_stage1: 1.047  loss_cls_stage2: 0.6532  loss_box_reg_stage2: 1.222  loss_mask: 0.1888  loss_rpn_cls: 0.04376  loss_rpn_loc: 0.08752  time: 0.6471  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 16:37:30] d2.utils.events INFO:  eta: 12:07:24  iter: 719  total_loss: 5.287  loss_cls_stage0: 0.6279  loss_box_reg_stage0: 0.4532  loss_cls_stage1: 0.7292  loss_box_reg_stage1: 1.068  loss_cls_stage2: 0.7219  loss_box_reg_stage2: 1.377  loss_mask: 0.1709  loss_rpn_cls: 0.03867  loss_rpn_loc: 0.07308  time: 0.6467  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 16:37:44] d2.utils.events INFO:  eta: 12:07:44  iter: 739  total_loss: 5.458  loss_cls_stage0: 0.6605  loss_box_reg_stage0: 0.4874  loss_cls_stage1: 0.7478  loss_box_reg_stage1: 1.099  loss_cls_stage2: 0.6988  loss_box_reg_stage2: 1.338  loss_mask: 0.1976  loss_rpn_cls: 0.0397  loss_rpn_loc: 0.08362  time: 0.6474  data_time: 0.0061  lr: 0.00016  max_mem: 19672M
[07/28 16:37:57] d2.utils.events INFO:  eta: 12:07:27  iter: 759  total_loss: 5.523  loss_cls_stage0: 0.6646  loss_box_reg_stage0: 0.4827  loss_cls_stage1: 0.7761  loss_box_reg_stage1: 1.11  loss_cls_stage2: 0.7536  loss_box_reg_stage2: 1.362  loss_mask: 0.2049  loss_rpn_cls: 0.03836  loss_rpn_loc: 0.07286  time: 0.6473  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 16:38:10] d2.utils.events INFO:  eta: 12:07:23  iter: 779  total_loss: 5.294  loss_cls_stage0: 0.6394  loss_box_reg_stage0: 0.4871  loss_cls_stage1: 0.7333  loss_box_reg_stage1: 1.035  loss_cls_stage2: 0.6783  loss_box_reg_stage2: 1.269  loss_mask: 0.2107  loss_rpn_cls: 0.03938  loss_rpn_loc: 0.07214  time: 0.6476  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 16:38:23] d2.utils.events INFO:  eta: 12:07:29  iter: 799  total_loss: 5.12  loss_cls_stage0: 0.6029  loss_box_reg_stage0: 0.4722  loss_cls_stage1: 0.6542  loss_box_reg_stage1: 1.046  loss_cls_stage2: 0.634  loss_box_reg_stage2: 1.315  loss_mask: 0.1924  loss_rpn_cls: 0.04726  loss_rpn_loc: 0.07391  time: 0.6479  data_time: 0.0059  lr: 0.00016  max_mem: 19672M
[07/28 16:38:36] d2.utils.events INFO:  eta: 12:06:57  iter: 819  total_loss: 4.915  loss_cls_stage0: 0.5773  loss_box_reg_stage0: 0.4556  loss_cls_stage1: 0.6495  loss_box_reg_stage1: 0.9805  loss_cls_stage2: 0.6358  loss_box_reg_stage2: 1.179  loss_mask: 0.1852  loss_rpn_cls: 0.04926  loss_rpn_loc: 0.08296  time: 0.6475  data_time: 0.0053  lr: 0.00016  max_mem: 19672M
[07/28 16:38:49] d2.utils.events INFO:  eta: 12:06:44  iter: 839  total_loss: 4.827  loss_cls_stage0: 0.6045  loss_box_reg_stage0: 0.4444  loss_cls_stage1: 0.6407  loss_box_reg_stage1: 0.965  loss_cls_stage2: 0.615  loss_box_reg_stage2: 1.173  loss_mask: 0.1775  loss_rpn_cls: 0.04539  loss_rpn_loc: 0.08074  time: 0.6474  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 16:39:02] d2.utils.events INFO:  eta: 12:07:23  iter: 859  total_loss: 5.02  loss_cls_stage0: 0.5684  loss_box_reg_stage0: 0.438  loss_cls_stage1: 0.6607  loss_box_reg_stage1: 1.046  loss_cls_stage2: 0.6328  loss_box_reg_stage2: 1.311  loss_mask: 0.1695  loss_rpn_cls: 0.03163  loss_rpn_loc: 0.08014  time: 0.6483  data_time: 0.0061  lr: 0.00016  max_mem: 19672M
[07/28 16:39:15] d2.utils.events INFO:  eta: 12:06:30  iter: 879  total_loss: 4.631  loss_cls_stage0: 0.5489  loss_box_reg_stage0: 0.4386  loss_cls_stage1: 0.6481  loss_box_reg_stage1: 0.9266  loss_cls_stage2: 0.6098  loss_box_reg_stage2: 1.158  loss_mask: 0.1789  loss_rpn_cls: 0.03361  loss_rpn_loc: 0.07466  time: 0.6479  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 16:39:28] d2.utils.events INFO:  eta: 12:06:11  iter: 899  total_loss: 4.562  loss_cls_stage0: 0.5494  loss_box_reg_stage0: 0.4505  loss_cls_stage1: 0.5765  loss_box_reg_stage1: 0.9603  loss_cls_stage2: 0.5164  loss_box_reg_stage2: 1.15  loss_mask: 0.1671  loss_rpn_cls: 0.03884  loss_rpn_loc: 0.07785  time: 0.6477  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 16:39:40] d2.utils.events INFO:  eta: 12:05:34  iter: 919  total_loss: 4.375  loss_cls_stage0: 0.5367  loss_box_reg_stage0: 0.4433  loss_cls_stage1: 0.5591  loss_box_reg_stage1: 0.8835  loss_cls_stage2: 0.5447  loss_box_reg_stage2: 1.105  loss_mask: 0.1924  loss_rpn_cls: 0.04867  loss_rpn_loc: 0.07842  time: 0.6470  data_time: 0.0054  lr: 0.00016  max_mem: 19672M
[07/28 16:39:53] d2.utils.events INFO:  eta: 12:04:51  iter: 939  total_loss: 4.954  loss_cls_stage0: 0.6087  loss_box_reg_stage0: 0.4469  loss_cls_stage1: 0.6659  loss_box_reg_stage1: 0.9827  loss_cls_stage2: 0.629  loss_box_reg_stage2: 1.199  loss_mask: 0.1924  loss_rpn_cls: 0.03766  loss_rpn_loc: 0.06754  time: 0.6467  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 16:40:05] d2.utils.events INFO:  eta: 12:04:17  iter: 959  total_loss: 4.782  loss_cls_stage0: 0.6297  loss_box_reg_stage0: 0.5141  loss_cls_stage1: 0.669  loss_box_reg_stage1: 0.9556  loss_cls_stage2: 0.5894  loss_box_reg_stage2: 1.106  loss_mask: 0.187  loss_rpn_cls: 0.04648  loss_rpn_loc: 0.07982  time: 0.6464  data_time: 0.0053  lr: 0.00016  max_mem: 19672M
[07/28 16:40:19] d2.utils.events INFO:  eta: 12:04:44  iter: 979  total_loss: 5.094  loss_cls_stage0: 0.5862  loss_box_reg_stage0: 0.4548  loss_cls_stage1: 0.6777  loss_box_reg_stage1: 1.078  loss_cls_stage2: 0.6615  loss_box_reg_stage2: 1.265  loss_mask: 0.1894  loss_rpn_cls: 0.04228  loss_rpn_loc: 0.08409  time: 0.6467  data_time: 0.0054  lr: 0.00016  max_mem: 19672M
[07/28 16:40:32] d2.utils.events INFO:  eta: 12:04:12  iter: 999  total_loss: 4.544  loss_cls_stage0: 0.5533  loss_box_reg_stage0: 0.4027  loss_cls_stage1: 0.5987  loss_box_reg_stage1: 0.9436  loss_cls_stage2: 0.5819  loss_box_reg_stage2: 1.218  loss_mask: 0.1817  loss_rpn_cls: 0.03325  loss_rpn_loc: 0.06855  time: 0.6468  data_time: 0.0053  lr: 0.00016  max_mem: 19672M
[07/28 16:40:45] d2.utils.events INFO:  eta: 12:04:53  iter: 1019  total_loss: 4.648  loss_cls_stage0: 0.5784  loss_box_reg_stage0: 0.4406  loss_cls_stage1: 0.6444  loss_box_reg_stage1: 0.9587  loss_cls_stage2: 0.6213  loss_box_reg_stage2: 1.151  loss_mask: 0.1858  loss_rpn_cls: 0.0395  loss_rpn_loc: 0.06958  time: 0.6468  data_time: 0.0050  lr: 0.00016  max_mem: 19672M
[07/28 16:40:58] d2.utils.events INFO:  eta: 12:05:40  iter: 1039  total_loss: 4.725  loss_cls_stage0: 0.5363  loss_box_reg_stage0: 0.4231  loss_cls_stage1: 0.6093  loss_box_reg_stage1: 1.007  loss_cls_stage2: 0.5905  loss_box_reg_stage2: 1.24  loss_mask: 0.1685  loss_rpn_cls: 0.03456  loss_rpn_loc: 0.08947  time: 0.6470  data_time: 0.0051  lr: 0.00016  max_mem: 19672M
[07/28 16:41:10] d2.utils.events INFO:  eta: 12:05:29  iter: 1059  total_loss: 4.632  loss_cls_stage0: 0.5267  loss_box_reg_stage0: 0.416  loss_cls_stage1: 0.6069  loss_box_reg_stage1: 0.958  loss_cls_stage2: 0.5779  loss_box_reg_stage2: 1.187  loss_mask: 0.1645  loss_rpn_cls: 0.03883  loss_rpn_loc: 0.07348  time: 0.6466  data_time: 0.0050  lr: 0.00016  max_mem: 19672M
[07/28 16:41:24] d2.utils.events INFO:  eta: 12:06:25  iter: 1079  total_loss: 4.84  loss_cls_stage0: 0.5915  loss_box_reg_stage0: 0.4296  loss_cls_stage1: 0.6732  loss_box_reg_stage1: 1.043  loss_cls_stage2: 0.6642  loss_box_reg_stage2: 1.23  loss_mask: 0.1765  loss_rpn_cls: 0.02779  loss_rpn_loc: 0.07432  time: 0.6469  data_time: 0.0052  lr: 0.00016  max_mem: 19672M
[07/28 16:41:37] d2.utils.events INFO:  eta: 12:05:18  iter: 1099  total_loss: 5.188  loss_cls_stage0: 0.5708  loss_box_reg_stage0: 0.4267  loss_cls_stage1: 0.6821  loss_box_reg_stage1: 1.042  loss_cls_stage2: 0.6822  loss_box_reg_stage2: 1.289  loss_mask: 0.1761  loss_rpn_cls: 0.02645  loss_rpn_loc: 0.06033  time: 0.6472  data_time: 0.0060  lr: 0.00016  max_mem: 19672M
[07/28 16:41:50] d2.utils.events INFO:  eta: 12:05:03  iter: 1119  total_loss: 4.322  loss_cls_stage0: 0.4769  loss_box_reg_stage0: 0.4216  loss_cls_stage1: 0.5474  loss_box_reg_stage1: 0.9199  loss_cls_stage2: 0.5307  loss_box_reg_stage2: 1.096  loss_mask: 0.1715  loss_rpn_cls: 0.03193  loss_rpn_loc: 0.06986  time: 0.6474  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 16:42:03] d2.utils.events INFO:  eta: 12:04:36  iter: 1139  total_loss: 5.215  loss_cls_stage0: 0.6374  loss_box_reg_stage0: 0.4758  loss_cls_stage1: 0.6705  loss_box_reg_stage1: 1.065  loss_cls_stage2: 0.6468  loss_box_reg_stage2: 1.227  loss_mask: 0.1973  loss_rpn_cls: 0.0372  loss_rpn_loc: 0.06914  time: 0.6471  data_time: 0.0052  lr: 0.00016  max_mem: 19672M
[07/28 16:42:16] d2.utils.events INFO:  eta: 12:03:05  iter: 1159  total_loss: 4.867  loss_cls_stage0: 0.5788  loss_box_reg_stage0: 0.471  loss_cls_stage1: 0.6432  loss_box_reg_stage1: 1.003  loss_cls_stage2: 0.5992  loss_box_reg_stage2: 1.199  loss_mask: 0.1799  loss_rpn_cls: 0.03158  loss_rpn_loc: 0.06466  time: 0.6471  data_time: 0.0054  lr: 0.00016  max_mem: 19672M
[07/28 16:42:28] d2.utils.events INFO:  eta: 12:02:24  iter: 1179  total_loss: 4.641  loss_cls_stage0: 0.5132  loss_box_reg_stage0: 0.4006  loss_cls_stage1: 0.6213  loss_box_reg_stage1: 1  loss_cls_stage2: 0.6101  loss_box_reg_stage2: 1.258  loss_mask: 0.1486  loss_rpn_cls: 0.03557  loss_rpn_loc: 0.06211  time: 0.6466  data_time: 0.0051  lr: 0.00016  max_mem: 19672M
[07/28 16:42:41] d2.utils.events INFO:  eta: 12:02:06  iter: 1199  total_loss: 4.753  loss_cls_stage0: 0.5543  loss_box_reg_stage0: 0.4236  loss_cls_stage1: 0.6076  loss_box_reg_stage1: 0.9726  loss_cls_stage2: 0.5966  loss_box_reg_stage2: 1.278  loss_mask: 0.1624  loss_rpn_cls: 0.03247  loss_rpn_loc: 0.07133  time: 0.6466  data_time: 0.0054  lr: 0.00016  max_mem: 19672M
[07/28 16:42:54] d2.utils.events INFO:  eta: 12:02:26  iter: 1219  total_loss: 5.052  loss_cls_stage0: 0.5569  loss_box_reg_stage0: 0.4481  loss_cls_stage1: 0.6508  loss_box_reg_stage1: 1.057  loss_cls_stage2: 0.6393  loss_box_reg_stage2: 1.308  loss_mask: 0.1645  loss_rpn_cls: 0.03296  loss_rpn_loc: 0.0779  time: 0.6468  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 16:43:07] d2.utils.events INFO:  eta: 12:02:22  iter: 1239  total_loss: 4.208  loss_cls_stage0: 0.496  loss_box_reg_stage0: 0.4196  loss_cls_stage1: 0.5057  loss_box_reg_stage1: 0.8829  loss_cls_stage2: 0.515  loss_box_reg_stage2: 1.07  loss_mask: 0.1494  loss_rpn_cls: 0.04353  loss_rpn_loc: 0.07332  time: 0.6468  data_time: 0.0052  lr: 0.00016  max_mem: 19672M
[07/28 16:43:20] d2.utils.events INFO:  eta: 12:00:44  iter: 1259  total_loss: 4.683  loss_cls_stage0: 0.533  loss_box_reg_stage0: 0.4361  loss_cls_stage1: 0.5658  loss_box_reg_stage1: 0.9806  loss_cls_stage2: 0.5572  loss_box_reg_stage2: 1.162  loss_mask: 0.1636  loss_rpn_cls: 0.03962  loss_rpn_loc: 0.09339  time: 0.6465  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 16:43:33] d2.utils.events INFO:  eta: 12:01:02  iter: 1279  total_loss: 4.516  loss_cls_stage0: 0.5224  loss_box_reg_stage0: 0.4642  loss_cls_stage1: 0.5654  loss_box_reg_stage1: 0.9515  loss_cls_stage2: 0.5347  loss_box_reg_stage2: 1.108  loss_mask: 0.181  loss_rpn_cls: 0.03001  loss_rpn_loc: 0.07438  time: 0.6467  data_time: 0.0052  lr: 0.00016  max_mem: 19672M
[07/28 16:43:46] d2.utils.events INFO:  eta: 12:00:49  iter: 1299  total_loss: 4.623  loss_cls_stage0: 0.5414  loss_box_reg_stage0: 0.425  loss_cls_stage1: 0.6054  loss_box_reg_stage1: 0.9563  loss_cls_stage2: 0.5832  loss_box_reg_stage2: 1.209  loss_mask: 0.1914  loss_rpn_cls: 0.03016  loss_rpn_loc: 0.07225  time: 0.6464  data_time: 0.0052  lr: 0.00016  max_mem: 19672M
[07/28 16:43:59] d2.utils.events INFO:  eta: 12:00:20  iter: 1319  total_loss: 4.468  loss_cls_stage0: 0.5292  loss_box_reg_stage0: 0.4094  loss_cls_stage1: 0.6163  loss_box_reg_stage1: 0.9537  loss_cls_stage2: 0.5787  loss_box_reg_stage2: 1.145  loss_mask: 0.1517  loss_rpn_cls: 0.03121  loss_rpn_loc: 0.0709  time: 0.6464  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 16:44:11] d2.utils.events INFO:  eta: 12:00:07  iter: 1339  total_loss: 4.552  loss_cls_stage0: 0.5004  loss_box_reg_stage0: 0.431  loss_cls_stage1: 0.576  loss_box_reg_stage1: 0.986  loss_cls_stage2: 0.5433  loss_box_reg_stage2: 1.229  loss_mask: 0.1757  loss_rpn_cls: 0.03326  loss_rpn_loc: 0.08337  time: 0.6464  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 16:44:24] d2.utils.events INFO:  eta: 11:58:48  iter: 1359  total_loss: 4.445  loss_cls_stage0: 0.5038  loss_box_reg_stage0: 0.4223  loss_cls_stage1: 0.5252  loss_box_reg_stage1: 0.9357  loss_cls_stage2: 0.5072  loss_box_reg_stage2: 1.19  loss_mask: 0.1576  loss_rpn_cls: 0.03244  loss_rpn_loc: 0.08457  time: 0.6461  data_time: 0.0053  lr: 0.00016  max_mem: 19672M
[07/28 16:44:37] d2.utils.events INFO:  eta: 11:58:43  iter: 1379  total_loss: 4.618  loss_cls_stage0: 0.478  loss_box_reg_stage0: 0.4067  loss_cls_stage1: 0.5709  loss_box_reg_stage1: 0.9695  loss_cls_stage2: 0.5625  loss_box_reg_stage2: 1.249  loss_mask: 0.1526  loss_rpn_cls: 0.03  loss_rpn_loc: 0.08706  time: 0.6461  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 16:44:50] d2.utils.events INFO:  eta: 11:58:30  iter: 1399  total_loss: 4.877  loss_cls_stage0: 0.5397  loss_box_reg_stage0: 0.4583  loss_cls_stage1: 0.6098  loss_box_reg_stage1: 1.026  loss_cls_stage2: 0.5616  loss_box_reg_stage2: 1.192  loss_mask: 0.1629  loss_rpn_cls: 0.04274  loss_rpn_loc: 0.07498  time: 0.6461  data_time: 0.0059  lr: 0.00016  max_mem: 19672M
[07/28 16:45:03] d2.utils.events INFO:  eta: 11:57:48  iter: 1419  total_loss: 4.642  loss_cls_stage0: 0.5261  loss_box_reg_stage0: 0.4125  loss_cls_stage1: 0.6267  loss_box_reg_stage1: 0.9902  loss_cls_stage2: 0.6059  loss_box_reg_stage2: 1.272  loss_mask: 0.156  loss_rpn_cls: 0.02511  loss_rpn_loc: 0.0606  time: 0.6462  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 16:45:16] d2.utils.events INFO:  eta: 11:57:35  iter: 1439  total_loss: 4.54  loss_cls_stage0: 0.4902  loss_box_reg_stage0: 0.3976  loss_cls_stage1: 0.584  loss_box_reg_stage1: 0.9601  loss_cls_stage2: 0.548  loss_box_reg_stage2: 1.213  loss_mask: 0.1586  loss_rpn_cls: 0.02969  loss_rpn_loc: 0.06467  time: 0.6464  data_time: 0.0059  lr: 0.00016  max_mem: 19672M
[07/28 16:45:29] d2.utils.events INFO:  eta: 11:57:35  iter: 1459  total_loss: 4.267  loss_cls_stage0: 0.4523  loss_box_reg_stage0: 0.4037  loss_cls_stage1: 0.5328  loss_box_reg_stage1: 0.9095  loss_cls_stage2: 0.537  loss_box_reg_stage2: 1.175  loss_mask: 0.1456  loss_rpn_cls: 0.03416  loss_rpn_loc: 0.0666  time: 0.6466  data_time: 0.0064  lr: 0.00016  max_mem: 19672M
[07/28 16:45:42] d2.utils.events INFO:  eta: 11:57:05  iter: 1479  total_loss: 4.357  loss_cls_stage0: 0.5441  loss_box_reg_stage0: 0.418  loss_cls_stage1: 0.585  loss_box_reg_stage1: 0.8823  loss_cls_stage2: 0.544  loss_box_reg_stage2: 1.133  loss_mask: 0.1545  loss_rpn_cls: 0.04077  loss_rpn_loc: 0.07023  time: 0.6465  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 16:45:55] d2.utils.events INFO:  eta: 11:56:48  iter: 1499  total_loss: 4.353  loss_cls_stage0: 0.5029  loss_box_reg_stage0: 0.4214  loss_cls_stage1: 0.5504  loss_box_reg_stage1: 0.9367  loss_cls_stage2: 0.4995  loss_box_reg_stage2: 1.129  loss_mask: 0.1602  loss_rpn_cls: 0.03413  loss_rpn_loc: 0.06704  time: 0.6464  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 16:46:08] d2.utils.events INFO:  eta: 11:56:50  iter: 1519  total_loss: 4.374  loss_cls_stage0: 0.487  loss_box_reg_stage0: 0.4156  loss_cls_stage1: 0.5144  loss_box_reg_stage1: 0.9141  loss_cls_stage2: 0.5029  loss_box_reg_stage2: 1.1  loss_mask: 0.1521  loss_rpn_cls: 0.04245  loss_rpn_loc: 0.08295  time: 0.6465  data_time: 0.0059  lr: 0.00016  max_mem: 19672M
[07/28 16:46:21] d2.utils.events INFO:  eta: 11:56:46  iter: 1539  total_loss: 4.171  loss_cls_stage0: 0.489  loss_box_reg_stage0: 0.41  loss_cls_stage1: 0.5338  loss_box_reg_stage1: 0.9036  loss_cls_stage2: 0.4916  loss_box_reg_stage2: 1.054  loss_mask: 0.1468  loss_rpn_cls: 0.03719  loss_rpn_loc: 0.08541  time: 0.6464  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 16:46:34] d2.utils.events INFO:  eta: 11:56:20  iter: 1559  total_loss: 4.414  loss_cls_stage0: 0.4561  loss_box_reg_stage0: 0.3828  loss_cls_stage1: 0.5191  loss_box_reg_stage1: 0.9637  loss_cls_stage2: 0.4872  loss_box_reg_stage2: 1.247  loss_mask: 0.1497  loss_rpn_cls: 0.03562  loss_rpn_loc: 0.07419  time: 0.6462  data_time: 0.0052  lr: 0.00016  max_mem: 19672M
[07/28 16:46:47] d2.utils.events INFO:  eta: 11:56:07  iter: 1579  total_loss: 4.319  loss_cls_stage0: 0.4889  loss_box_reg_stage0: 0.4037  loss_cls_stage1: 0.5742  loss_box_reg_stage1: 0.9133  loss_cls_stage2: 0.5351  loss_box_reg_stage2: 1.151  loss_mask: 0.1628  loss_rpn_cls: 0.03712  loss_rpn_loc: 0.09381  time: 0.6463  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 16:47:00] d2.utils.events INFO:  eta: 11:55:42  iter: 1599  total_loss: 4.472  loss_cls_stage0: 0.4824  loss_box_reg_stage0: 0.3921  loss_cls_stage1: 0.5561  loss_box_reg_stage1: 0.9574  loss_cls_stage2: 0.5393  loss_box_reg_stage2: 1.294  loss_mask: 0.1412  loss_rpn_cls: 0.04078  loss_rpn_loc: 0.0686  time: 0.6464  data_time: 0.0054  lr: 0.00016  max_mem: 19672M
[07/28 16:47:12] d2.utils.events INFO:  eta: 11:55:02  iter: 1619  total_loss: 4.398  loss_cls_stage0: 0.5167  loss_box_reg_stage0: 0.4291  loss_cls_stage1: 0.5449  loss_box_reg_stage1: 0.9052  loss_cls_stage2: 0.5157  loss_box_reg_stage2: 1.118  loss_mask: 0.1716  loss_rpn_cls: 0.03413  loss_rpn_loc: 0.07325  time: 0.6462  data_time: 0.0053  lr: 0.00016  max_mem: 19672M
[07/28 16:47:25] d2.utils.events INFO:  eta: 11:55:07  iter: 1639  total_loss: 4.022  loss_cls_stage0: 0.4232  loss_box_reg_stage0: 0.3774  loss_cls_stage1: 0.5092  loss_box_reg_stage1: 0.8743  loss_cls_stage2: 0.5139  loss_box_reg_stage2: 1.179  loss_mask: 0.1484  loss_rpn_cls: 0.03163  loss_rpn_loc: 0.06349  time: 0.6460  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 16:47:38] d2.utils.events INFO:  eta: 11:55:04  iter: 1659  total_loss: 4.481  loss_cls_stage0: 0.508  loss_box_reg_stage0: 0.4132  loss_cls_stage1: 0.5721  loss_box_reg_stage1: 0.9651  loss_cls_stage2: 0.5583  loss_box_reg_stage2: 1.164  loss_mask: 0.1595  loss_rpn_cls: 0.02859  loss_rpn_loc: 0.05617  time: 0.6461  data_time: 0.0059  lr: 0.00016  max_mem: 19672M
[07/28 16:47:51] d2.utils.events INFO:  eta: 11:54:53  iter: 1679  total_loss: 4.262  loss_cls_stage0: 0.4696  loss_box_reg_stage0: 0.3766  loss_cls_stage1: 0.5356  loss_box_reg_stage1: 0.9101  loss_cls_stage2: 0.537  loss_box_reg_stage2: 1.19  loss_mask: 0.1321  loss_rpn_cls: 0.02922  loss_rpn_loc: 0.07098  time: 0.6461  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 16:48:04] d2.utils.events INFO:  eta: 11:54:35  iter: 1699  total_loss: 4.053  loss_cls_stage0: 0.4706  loss_box_reg_stage0: 0.3922  loss_cls_stage1: 0.5225  loss_box_reg_stage1: 0.8577  loss_cls_stage2: 0.4978  loss_box_reg_stage2: 1.028  loss_mask: 0.1625  loss_rpn_cls: 0.02941  loss_rpn_loc: 0.06702  time: 0.6460  data_time: 0.0051  lr: 0.00016  max_mem: 19672M
[07/28 16:48:17] d2.utils.events INFO:  eta: 11:54:24  iter: 1719  total_loss: 4.504  loss_cls_stage0: 0.5027  loss_box_reg_stage0: 0.4093  loss_cls_stage1: 0.5417  loss_box_reg_stage1: 0.9671  loss_cls_stage2: 0.5259  loss_box_reg_stage2: 1.215  loss_mask: 0.1722  loss_rpn_cls: 0.03978  loss_rpn_loc: 0.07286  time: 0.6459  data_time: 0.0059  lr: 0.00016  max_mem: 19672M
[07/28 16:48:29] d2.utils.events INFO:  eta: 11:53:38  iter: 1739  total_loss: 4.35  loss_cls_stage0: 0.4797  loss_box_reg_stage0: 0.3935  loss_cls_stage1: 0.5571  loss_box_reg_stage1: 0.9246  loss_cls_stage2: 0.5107  loss_box_reg_stage2: 1.209  loss_mask: 0.1506  loss_rpn_cls: 0.04138  loss_rpn_loc: 0.0684  time: 0.6458  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 16:48:43] d2.utils.events INFO:  eta: 11:53:58  iter: 1759  total_loss: 4.891  loss_cls_stage0: 0.4914  loss_box_reg_stage0: 0.4042  loss_cls_stage1: 0.6264  loss_box_reg_stage1: 1.067  loss_cls_stage2: 0.6197  loss_box_reg_stage2: 1.294  loss_mask: 0.1619  loss_rpn_cls: 0.02176  loss_rpn_loc: 0.06612  time: 0.6461  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 16:48:56] d2.utils.events INFO:  eta: 11:53:24  iter: 1779  total_loss: 4.611  loss_cls_stage0: 0.4929  loss_box_reg_stage0: 0.4015  loss_cls_stage1: 0.599  loss_box_reg_stage1: 1.007  loss_cls_stage2: 0.5609  loss_box_reg_stage2: 1.275  loss_mask: 0.1486  loss_rpn_cls: 0.03089  loss_rpn_loc: 0.0671  time: 0.6460  data_time: 0.0051  lr: 0.00016  max_mem: 19672M
[07/28 16:49:09] d2.utils.events INFO:  eta: 11:53:28  iter: 1799  total_loss: 4.572  loss_cls_stage0: 0.5073  loss_box_reg_stage0: 0.408  loss_cls_stage1: 0.5774  loss_box_reg_stage1: 0.9646  loss_cls_stage2: 0.5287  loss_box_reg_stage2: 1.244  loss_mask: 0.1459  loss_rpn_cls: 0.0223  loss_rpn_loc: 0.05851  time: 0.6460  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 16:49:22] d2.utils.events INFO:  eta: 11:53:27  iter: 1819  total_loss: 4.403  loss_cls_stage0: 0.4999  loss_box_reg_stage0: 0.382  loss_cls_stage1: 0.5688  loss_box_reg_stage1: 0.9429  loss_cls_stage2: 0.5697  loss_box_reg_stage2: 1.182  loss_mask: 0.1507  loss_rpn_cls: 0.02701  loss_rpn_loc: 0.07104  time: 0.6462  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 16:49:35] d2.utils.events INFO:  eta: 11:53:09  iter: 1839  total_loss: 4.58  loss_cls_stage0: 0.4901  loss_box_reg_stage0: 0.3958  loss_cls_stage1: 0.5788  loss_box_reg_stage1: 1.015  loss_cls_stage2: 0.5562  loss_box_reg_stage2: 1.221  loss_mask: 0.1459  loss_rpn_cls: 0.02506  loss_rpn_loc: 0.06515  time: 0.6462  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 16:49:47] d2.utils.events INFO:  eta: 11:52:02  iter: 1859  total_loss: 4.148  loss_cls_stage0: 0.4312  loss_box_reg_stage0: 0.4156  loss_cls_stage1: 0.4881  loss_box_reg_stage1: 0.9565  loss_cls_stage2: 0.5086  loss_box_reg_stage2: 1.154  loss_mask: 0.1421  loss_rpn_cls: 0.03506  loss_rpn_loc: 0.06102  time: 0.6460  data_time: 0.0054  lr: 0.00016  max_mem: 19672M
[07/28 16:50:00] d2.utils.events INFO:  eta: 11:52:38  iter: 1879  total_loss: 4.342  loss_cls_stage0: 0.4792  loss_box_reg_stage0: 0.4116  loss_cls_stage1: 0.5453  loss_box_reg_stage1: 0.9189  loss_cls_stage2: 0.5354  loss_box_reg_stage2: 1.097  loss_mask: 0.1535  loss_rpn_cls: 0.03185  loss_rpn_loc: 0.06655  time: 0.6461  data_time: 0.0054  lr: 0.00016  max_mem: 19672M
[07/28 16:50:13] d2.utils.events INFO:  eta: 11:52:23  iter: 1899  total_loss: 3.944  loss_cls_stage0: 0.433  loss_box_reg_stage0: 0.3929  loss_cls_stage1: 0.4562  loss_box_reg_stage1: 0.8317  loss_cls_stage2: 0.441  loss_box_reg_stage2: 0.9979  loss_mask: 0.1395  loss_rpn_cls: 0.03612  loss_rpn_loc: 0.06054  time: 0.6459  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 16:50:26] d2.utils.events INFO:  eta: 11:52:14  iter: 1919  total_loss: 4.119  loss_cls_stage0: 0.458  loss_box_reg_stage0: 0.3654  loss_cls_stage1: 0.5252  loss_box_reg_stage1: 0.8656  loss_cls_stage2: 0.5156  loss_box_reg_stage2: 0.9877  loss_mask: 0.13  loss_rpn_cls: 0.03515  loss_rpn_loc: 0.06222  time: 0.6457  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 16:50:39] d2.utils.events INFO:  eta: 11:52:09  iter: 1939  total_loss: 4.277  loss_cls_stage0: 0.4751  loss_box_reg_stage0: 0.4046  loss_cls_stage1: 0.5318  loss_box_reg_stage1: 0.9065  loss_cls_stage2: 0.523  loss_box_reg_stage2: 1.11  loss_mask: 0.1519  loss_rpn_cls: 0.03144  loss_rpn_loc: 0.05936  time: 0.6457  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 16:50:52] d2.utils.events INFO:  eta: 11:52:42  iter: 1959  total_loss: 3.966  loss_cls_stage0: 0.4299  loss_box_reg_stage0: 0.3497  loss_cls_stage1: 0.4793  loss_box_reg_stage1: 0.8775  loss_cls_stage2: 0.4869  loss_box_reg_stage2: 1.117  loss_mask: 0.132  loss_rpn_cls: 0.02397  loss_rpn_loc: 0.06162  time: 0.6461  data_time: 0.0054  lr: 0.00016  max_mem: 19672M
[07/28 16:51:05] d2.utils.events INFO:  eta: 11:51:53  iter: 1979  total_loss: 4.204  loss_cls_stage0: 0.4668  loss_box_reg_stage0: 0.4208  loss_cls_stage1: 0.4916  loss_box_reg_stage1: 0.8891  loss_cls_stage2: 0.4747  loss_box_reg_stage2: 1.135  loss_mask: 0.154  loss_rpn_cls: 0.0261  loss_rpn_loc: 0.0668  time: 0.6462  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 16:51:19] d2.utils.events INFO:  eta: 11:52:16  iter: 1999  total_loss: 3.998  loss_cls_stage0: 0.4398  loss_box_reg_stage0: 0.375  loss_cls_stage1: 0.5246  loss_box_reg_stage1: 0.867  loss_cls_stage2: 0.4696  loss_box_reg_stage2: 1.096  loss_mask: 0.1399  loss_rpn_cls: 0.02476  loss_rpn_loc: 0.05698  time: 0.6464  data_time: 0.0052  lr: 0.00016  max_mem: 19672M
[07/28 16:51:31] d2.utils.events INFO:  eta: 11:51:47  iter: 2019  total_loss: 4.207  loss_cls_stage0: 0.4438  loss_box_reg_stage0: 0.3865  loss_cls_stage1: 0.4883  loss_box_reg_stage1: 0.8527  loss_cls_stage2: 0.4952  loss_box_reg_stage2: 1.131  loss_mask: 0.1529  loss_rpn_cls: 0.02949  loss_rpn_loc: 0.05532  time: 0.6462  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 16:51:44] d2.utils.events INFO:  eta: 11:51:22  iter: 2039  total_loss: 4.668  loss_cls_stage0: 0.4985  loss_box_reg_stage0: 0.4355  loss_cls_stage1: 0.5485  loss_box_reg_stage1: 1.04  loss_cls_stage2: 0.5149  loss_box_reg_stage2: 1.166  loss_mask: 0.1756  loss_rpn_cls: 0.03397  loss_rpn_loc: 0.0815  time: 0.6462  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 16:51:57] d2.utils.events INFO:  eta: 11:51:48  iter: 2059  total_loss: 4.191  loss_cls_stage0: 0.4513  loss_box_reg_stage0: 0.3774  loss_cls_stage1: 0.4937  loss_box_reg_stage1: 0.9405  loss_cls_stage2: 0.4891  loss_box_reg_stage2: 1.171  loss_mask: 0.1329  loss_rpn_cls: 0.02771  loss_rpn_loc: 0.066  time: 0.6461  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 16:52:10] d2.utils.events INFO:  eta: 11:51:35  iter: 2079  total_loss: 4.264  loss_cls_stage0: 0.468  loss_box_reg_stage0: 0.3963  loss_cls_stage1: 0.4841  loss_box_reg_stage1: 0.9417  loss_cls_stage2: 0.4722  loss_box_reg_stage2: 1.126  loss_mask: 0.1467  loss_rpn_cls: 0.03341  loss_rpn_loc: 0.07086  time: 0.6463  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 16:52:24] d2.utils.events INFO:  eta: 11:51:22  iter: 2099  total_loss: 4.365  loss_cls_stage0: 0.4502  loss_box_reg_stage0: 0.3917  loss_cls_stage1: 0.5068  loss_box_reg_stage1: 0.9063  loss_cls_stage2: 0.5027  loss_box_reg_stage2: 1.119  loss_mask: 0.152  loss_rpn_cls: 0.03683  loss_rpn_loc: 0.07053  time: 0.6465  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 16:52:36] d2.utils.events INFO:  eta: 11:50:30  iter: 2119  total_loss: 3.998  loss_cls_stage0: 0.449  loss_box_reg_stage0: 0.3894  loss_cls_stage1: 0.4852  loss_box_reg_stage1: 0.9018  loss_cls_stage2: 0.4554  loss_box_reg_stage2: 1.091  loss_mask: 0.134  loss_rpn_cls: 0.03421  loss_rpn_loc: 0.06989  time: 0.6462  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 16:52:49] d2.utils.events INFO:  eta: 11:50:17  iter: 2139  total_loss: 4.158  loss_cls_stage0: 0.4345  loss_box_reg_stage0: 0.3861  loss_cls_stage1: 0.4959  loss_box_reg_stage1: 0.9003  loss_cls_stage2: 0.4805  loss_box_reg_stage2: 1.064  loss_mask: 0.1368  loss_rpn_cls: 0.02189  loss_rpn_loc: 0.06067  time: 0.6461  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 16:53:01] d2.utils.events INFO:  eta: 11:50:16  iter: 2159  total_loss: 4.293  loss_cls_stage0: 0.4473  loss_box_reg_stage0: 0.3826  loss_cls_stage1: 0.5317  loss_box_reg_stage1: 0.9457  loss_cls_stage2: 0.5142  loss_box_reg_stage2: 1.251  loss_mask: 0.1406  loss_rpn_cls: 0.0236  loss_rpn_loc: 0.0612  time: 0.6460  data_time: 0.0059  lr: 0.00016  max_mem: 19672M
[07/28 16:53:14] d2.utils.events INFO:  eta: 11:50:18  iter: 2179  total_loss: 4.189  loss_cls_stage0: 0.4691  loss_box_reg_stage0: 0.3991  loss_cls_stage1: 0.5472  loss_box_reg_stage1: 0.918  loss_cls_stage2: 0.5179  loss_box_reg_stage2: 1.203  loss_mask: 0.1461  loss_rpn_cls: 0.02281  loss_rpn_loc: 0.05468  time: 0.6460  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 16:53:27] d2.utils.events INFO:  eta: 11:49:43  iter: 2199  total_loss: 3.903  loss_cls_stage0: 0.3946  loss_box_reg_stage0: 0.373  loss_cls_stage1: 0.4569  loss_box_reg_stage1: 0.8479  loss_cls_stage2: 0.4494  loss_box_reg_stage2: 1.135  loss_mask: 0.1227  loss_rpn_cls: 0.02881  loss_rpn_loc: 0.065  time: 0.6458  data_time: 0.0059  lr: 0.00016  max_mem: 19672M
[07/28 16:53:40] d2.utils.events INFO:  eta: 11:49:10  iter: 2219  total_loss: 4.409  loss_cls_stage0: 0.4877  loss_box_reg_stage0: 0.4241  loss_cls_stage1: 0.5261  loss_box_reg_stage1: 0.971  loss_cls_stage2: 0.501  loss_box_reg_stage2: 1.221  loss_mask: 0.15  loss_rpn_cls: 0.02309  loss_rpn_loc: 0.05472  time: 0.6458  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 16:53:53] d2.utils.events INFO:  eta: 11:49:49  iter: 2239  total_loss: 4.341  loss_cls_stage0: 0.4815  loss_box_reg_stage0: 0.4252  loss_cls_stage1: 0.5128  loss_box_reg_stage1: 0.9748  loss_cls_stage2: 0.5045  loss_box_reg_stage2: 1.171  loss_mask: 0.148  loss_rpn_cls: 0.02934  loss_rpn_loc: 0.0784  time: 0.6458  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 16:54:05] d2.utils.events INFO:  eta: 11:49:43  iter: 2259  total_loss: 4.212  loss_cls_stage0: 0.4783  loss_box_reg_stage0: 0.4039  loss_cls_stage1: 0.5463  loss_box_reg_stage1: 0.8956  loss_cls_stage2: 0.4559  loss_box_reg_stage2: 1.111  loss_mask: 0.1424  loss_rpn_cls: 0.03697  loss_rpn_loc: 0.0547  time: 0.6456  data_time: 0.0053  lr: 0.00016  max_mem: 19672M
[07/28 16:54:19] d2.utils.events INFO:  eta: 11:49:35  iter: 2279  total_loss: 4.2  loss_cls_stage0: 0.4359  loss_box_reg_stage0: 0.3894  loss_cls_stage1: 0.499  loss_box_reg_stage1: 0.9862  loss_cls_stage2: 0.4713  loss_box_reg_stage2: 1.19  loss_mask: 0.1343  loss_rpn_cls: 0.02631  loss_rpn_loc: 0.05995  time: 0.6458  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 16:54:32] d2.utils.events INFO:  eta: 11:49:22  iter: 2299  total_loss: 4.579  loss_cls_stage0: 0.4778  loss_box_reg_stage0: 0.4032  loss_cls_stage1: 0.5494  loss_box_reg_stage1: 1.035  loss_cls_stage2: 0.519  loss_box_reg_stage2: 1.34  loss_mask: 0.154  loss_rpn_cls: 0.02866  loss_rpn_loc: 0.07892  time: 0.6459  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 16:54:46] d2.utils.events INFO:  eta: 11:49:50  iter: 2319  total_loss: 4.507  loss_cls_stage0: 0.4666  loss_box_reg_stage0: 0.3979  loss_cls_stage1: 0.5305  loss_box_reg_stage1: 1.022  loss_cls_stage2: 0.5042  loss_box_reg_stage2: 1.272  loss_mask: 0.1318  loss_rpn_cls: 0.02181  loss_rpn_loc: 0.05992  time: 0.6463  data_time: 0.0054  lr: 0.00016  max_mem: 19672M
[07/28 16:54:59] d2.utils.events INFO:  eta: 11:49:48  iter: 2339  total_loss: 4.178  loss_cls_stage0: 0.4414  loss_box_reg_stage0: 0.3933  loss_cls_stage1: 0.4621  loss_box_reg_stage1: 0.9444  loss_cls_stage2: 0.4436  loss_box_reg_stage2: 1.137  loss_mask: 0.145  loss_rpn_cls: 0.02669  loss_rpn_loc: 0.05845  time: 0.6463  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 16:55:12] d2.utils.events INFO:  eta: 11:50:14  iter: 2359  total_loss: 3.905  loss_cls_stage0: 0.428  loss_box_reg_stage0: 0.3847  loss_cls_stage1: 0.4569  loss_box_reg_stage1: 0.8315  loss_cls_stage2: 0.4297  loss_box_reg_stage2: 1.042  loss_mask: 0.1415  loss_rpn_cls: 0.03261  loss_rpn_loc: 0.07042  time: 0.6463  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 16:55:25] d2.utils.events INFO:  eta: 11:50:10  iter: 2379  total_loss: 4.024  loss_cls_stage0: 0.417  loss_box_reg_stage0: 0.3616  loss_cls_stage1: 0.5002  loss_box_reg_stage1: 0.91  loss_cls_stage2: 0.4846  loss_box_reg_stage2: 1.19  loss_mask: 0.1252  loss_rpn_cls: 0.02809  loss_rpn_loc: 0.05936  time: 0.6464  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 16:55:38] d2.utils.events INFO:  eta: 11:49:47  iter: 2399  total_loss: 3.535  loss_cls_stage0: 0.3792  loss_box_reg_stage0: 0.3581  loss_cls_stage1: 0.4087  loss_box_reg_stage1: 0.8014  loss_cls_stage2: 0.3787  loss_box_reg_stage2: 0.9836  loss_mask: 0.135  loss_rpn_cls: 0.0262  loss_rpn_loc: 0.06682  time: 0.6464  data_time: 0.0059  lr: 0.00016  max_mem: 19672M
[07/28 16:55:51] d2.utils.events INFO:  eta: 11:49:40  iter: 2419  total_loss: 3.977  loss_cls_stage0: 0.4245  loss_box_reg_stage0: 0.361  loss_cls_stage1: 0.4368  loss_box_reg_stage1: 0.8692  loss_cls_stage2: 0.4386  loss_box_reg_stage2: 1.13  loss_mask: 0.1176  loss_rpn_cls: 0.02813  loss_rpn_loc: 0.05865  time: 0.6464  data_time: 0.0054  lr: 0.00016  max_mem: 19672M
[07/28 16:56:03] d2.utils.events INFO:  eta: 11:49:15  iter: 2439  total_loss: 3.955  loss_cls_stage0: 0.4394  loss_box_reg_stage0: 0.3813  loss_cls_stage1: 0.5046  loss_box_reg_stage1: 0.8943  loss_cls_stage2: 0.4821  loss_box_reg_stage2: 1.148  loss_mask: 0.1422  loss_rpn_cls: 0.02315  loss_rpn_loc: 0.05855  time: 0.6463  data_time: 0.0054  lr: 0.00016  max_mem: 19672M
[07/28 16:56:16] d2.utils.events INFO:  eta: 11:48:42  iter: 2459  total_loss: 4.029  loss_cls_stage0: 0.4306  loss_box_reg_stage0: 0.3645  loss_cls_stage1: 0.5071  loss_box_reg_stage1: 0.8729  loss_cls_stage2: 0.4869  loss_box_reg_stage2: 1.093  loss_mask: 0.1364  loss_rpn_cls: 0.02506  loss_rpn_loc: 0.06044  time: 0.6463  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 16:56:29] d2.utils.events INFO:  eta: 11:48:41  iter: 2479  total_loss: 4.206  loss_cls_stage0: 0.4578  loss_box_reg_stage0: 0.3884  loss_cls_stage1: 0.4798  loss_box_reg_stage1: 0.925  loss_cls_stage2: 0.4571  loss_box_reg_stage2: 1.082  loss_mask: 0.1494  loss_rpn_cls: 0.02238  loss_rpn_loc: 0.05619  time: 0.6462  data_time: 0.0061  lr: 0.00016  max_mem: 19672M
[07/28 16:56:42] d2.utils.events INFO:  eta: 11:48:23  iter: 2499  total_loss: 4.014  loss_cls_stage0: 0.4018  loss_box_reg_stage0: 0.3418  loss_cls_stage1: 0.4661  loss_box_reg_stage1: 0.892  loss_cls_stage2: 0.4531  loss_box_reg_stage2: 1.222  loss_mask: 0.1245  loss_rpn_cls: 0.02232  loss_rpn_loc: 0.05418  time: 0.6462  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 16:56:54] d2.utils.events INFO:  eta: 11:47:40  iter: 2519  total_loss: 4.155  loss_cls_stage0: 0.4208  loss_box_reg_stage0: 0.3902  loss_cls_stage1: 0.4534  loss_box_reg_stage1: 0.9119  loss_cls_stage2: 0.4234  loss_box_reg_stage2: 1.202  loss_mask: 0.1433  loss_rpn_cls: 0.02261  loss_rpn_loc: 0.05949  time: 0.6460  data_time: 0.0052  lr: 0.00016  max_mem: 19672M
[07/28 16:57:07] d2.utils.events INFO:  eta: 11:48:02  iter: 2539  total_loss: 3.75  loss_cls_stage0: 0.3923  loss_box_reg_stage0: 0.363  loss_cls_stage1: 0.4274  loss_box_reg_stage1: 0.8118  loss_cls_stage2: 0.4237  loss_box_reg_stage2: 1.005  loss_mask: 0.1255  loss_rpn_cls: 0.03521  loss_rpn_loc: 0.08841  time: 0.6461  data_time: 0.0051  lr: 0.00016  max_mem: 19672M
[07/28 16:57:20] d2.utils.events INFO:  eta: 11:48:03  iter: 2559  total_loss: 3.851  loss_cls_stage0: 0.4201  loss_box_reg_stage0: 0.3554  loss_cls_stage1: 0.4695  loss_box_reg_stage1: 0.8569  loss_cls_stage2: 0.4661  loss_box_reg_stage2: 1.091  loss_mask: 0.1406  loss_rpn_cls: 0.03506  loss_rpn_loc: 0.06422  time: 0.6461  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 16:57:33] d2.utils.events INFO:  eta: 11:47:44  iter: 2579  total_loss: 4.013  loss_cls_stage0: 0.4367  loss_box_reg_stage0: 0.3785  loss_cls_stage1: 0.4628  loss_box_reg_stage1: 0.8056  loss_cls_stage2: 0.4377  loss_box_reg_stage2: 0.9723  loss_mask: 0.1464  loss_rpn_cls: 0.02998  loss_rpn_loc: 0.062  time: 0.6461  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 16:57:46] d2.utils.events INFO:  eta: 11:47:28  iter: 2599  total_loss: 3.572  loss_cls_stage0: 0.391  loss_box_reg_stage0: 0.3745  loss_cls_stage1: 0.443  loss_box_reg_stage1: 0.7967  loss_cls_stage2: 0.4058  loss_box_reg_stage2: 1.05  loss_mask: 0.1332  loss_rpn_cls: 0.02763  loss_rpn_loc: 0.04951  time: 0.6460  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 16:57:59] d2.utils.events INFO:  eta: 11:47:21  iter: 2619  total_loss: 4.022  loss_cls_stage0: 0.4589  loss_box_reg_stage0: 0.3859  loss_cls_stage1: 0.4959  loss_box_reg_stage1: 0.9082  loss_cls_stage2: 0.4308  loss_box_reg_stage2: 1.066  loss_mask: 0.1365  loss_rpn_cls: 0.02426  loss_rpn_loc: 0.05981  time: 0.6460  data_time: 0.0054  lr: 0.00016  max_mem: 19672M
[07/28 16:58:12] d2.utils.events INFO:  eta: 11:47:21  iter: 2639  total_loss: 4.002  loss_cls_stage0: 0.4648  loss_box_reg_stage0: 0.3839  loss_cls_stage1: 0.4915  loss_box_reg_stage1: 0.8521  loss_cls_stage2: 0.4612  loss_box_reg_stage2: 1.099  loss_mask: 0.1278  loss_rpn_cls: 0.03437  loss_rpn_loc: 0.06967  time: 0.6460  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 16:58:25] d2.utils.events INFO:  eta: 11:47:10  iter: 2659  total_loss: 4.288  loss_cls_stage0: 0.4422  loss_box_reg_stage0: 0.3854  loss_cls_stage1: 0.5097  loss_box_reg_stage1: 0.9215  loss_cls_stage2: 0.501  loss_box_reg_stage2: 1.202  loss_mask: 0.1311  loss_rpn_cls: 0.02256  loss_rpn_loc: 0.06324  time: 0.6462  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 16:58:39] d2.utils.events INFO:  eta: 11:46:59  iter: 2679  total_loss: 4.186  loss_cls_stage0: 0.4175  loss_box_reg_stage0: 0.3425  loss_cls_stage1: 0.4957  loss_box_reg_stage1: 0.9498  loss_cls_stage2: 0.4734  loss_box_reg_stage2: 1.187  loss_mask: 0.1273  loss_rpn_cls: 0.01877  loss_rpn_loc: 0.05255  time: 0.6463  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 16:58:51] d2.utils.events INFO:  eta: 11:46:50  iter: 2699  total_loss: 4.067  loss_cls_stage0: 0.4727  loss_box_reg_stage0: 0.3838  loss_cls_stage1: 0.5198  loss_box_reg_stage1: 0.898  loss_cls_stage2: 0.5207  loss_box_reg_stage2: 1.131  loss_mask: 0.1302  loss_rpn_cls: 0.02752  loss_rpn_loc: 0.05592  time: 0.6462  data_time: 0.0052  lr: 0.00016  max_mem: 19672M
[07/28 16:59:04] d2.utils.events INFO:  eta: 11:46:59  iter: 2719  total_loss: 4.191  loss_cls_stage0: 0.4672  loss_box_reg_stage0: 0.3916  loss_cls_stage1: 0.5387  loss_box_reg_stage1: 0.8935  loss_cls_stage2: 0.5015  loss_box_reg_stage2: 1.182  loss_mask: 0.1374  loss_rpn_cls: 0.02424  loss_rpn_loc: 0.06291  time: 0.6462  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 16:59:17] d2.utils.events INFO:  eta: 11:46:26  iter: 2739  total_loss: 4.016  loss_cls_stage0: 0.434  loss_box_reg_stage0: 0.3802  loss_cls_stage1: 0.5118  loss_box_reg_stage1: 0.9068  loss_cls_stage2: 0.4619  loss_box_reg_stage2: 1.057  loss_mask: 0.1364  loss_rpn_cls: 0.03302  loss_rpn_loc: 0.05976  time: 0.6461  data_time: 0.0051  lr: 0.00016  max_mem: 19672M
[07/28 16:59:30] d2.utils.events INFO:  eta: 11:45:58  iter: 2759  total_loss: 3.932  loss_cls_stage0: 0.4427  loss_box_reg_stage0: 0.3911  loss_cls_stage1: 0.4369  loss_box_reg_stage1: 0.8907  loss_cls_stage2: 0.4304  loss_box_reg_stage2: 1.145  loss_mask: 0.1514  loss_rpn_cls: 0.02966  loss_rpn_loc: 0.05735  time: 0.6461  data_time: 0.0052  lr: 0.00016  max_mem: 19672M
[07/28 16:59:43] d2.utils.events INFO:  eta: 11:45:54  iter: 2779  total_loss: 3.839  loss_cls_stage0: 0.3971  loss_box_reg_stage0: 0.3709  loss_cls_stage1: 0.4351  loss_box_reg_stage1: 0.9084  loss_cls_stage2: 0.4312  loss_box_reg_stage2: 1.175  loss_mask: 0.1267  loss_rpn_cls: 0.01854  loss_rpn_loc: 0.05943  time: 0.6461  data_time: 0.0053  lr: 0.00016  max_mem: 19672M
[07/28 16:59:56] d2.utils.events INFO:  eta: 11:45:36  iter: 2799  total_loss: 3.928  loss_cls_stage0: 0.4274  loss_box_reg_stage0: 0.3535  loss_cls_stage1: 0.5014  loss_box_reg_stage1: 0.8142  loss_cls_stage2: 0.4761  loss_box_reg_stage2: 1.103  loss_mask: 0.1317  loss_rpn_cls: 0.02164  loss_rpn_loc: 0.05728  time: 0.6461  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 17:00:09] d2.utils.events INFO:  eta: 11:45:23  iter: 2819  total_loss: 3.971  loss_cls_stage0: 0.4239  loss_box_reg_stage0: 0.3939  loss_cls_stage1: 0.4718  loss_box_reg_stage1: 0.8714  loss_cls_stage2: 0.4594  loss_box_reg_stage2: 1.105  loss_mask: 0.1246  loss_rpn_cls: 0.02913  loss_rpn_loc: 0.06951  time: 0.6463  data_time: 0.0060  lr: 0.00016  max_mem: 19672M
[07/28 17:00:22] d2.utils.events INFO:  eta: 11:45:05  iter: 2839  total_loss: 3.967  loss_cls_stage0: 0.4101  loss_box_reg_stage0: 0.3303  loss_cls_stage1: 0.466  loss_box_reg_stage1: 0.8601  loss_cls_stage2: 0.4353  loss_box_reg_stage2: 1.024  loss_mask: 0.1226  loss_rpn_cls: 0.02319  loss_rpn_loc: 0.0606  time: 0.6463  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:00:35] d2.utils.events INFO:  eta: 11:44:59  iter: 2859  total_loss: 3.898  loss_cls_stage0: 0.4157  loss_box_reg_stage0: 0.3802  loss_cls_stage1: 0.4521  loss_box_reg_stage1: 0.8587  loss_cls_stage2: 0.4213  loss_box_reg_stage2: 1.128  loss_mask: 0.1413  loss_rpn_cls: 0.01891  loss_rpn_loc: 0.05666  time: 0.6462  data_time: 0.0053  lr: 0.00016  max_mem: 19672M
[07/28 17:00:49] d2.utils.events INFO:  eta: 11:44:53  iter: 2879  total_loss: 3.892  loss_cls_stage0: 0.4603  loss_box_reg_stage0: 0.3856  loss_cls_stage1: 0.5018  loss_box_reg_stage1: 0.8789  loss_cls_stage2: 0.4747  loss_box_reg_stage2: 1.091  loss_mask: 0.1426  loss_rpn_cls: 0.02045  loss_rpn_loc: 0.05711  time: 0.6465  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 17:01:02] d2.utils.events INFO:  eta: 11:44:41  iter: 2899  total_loss: 4.081  loss_cls_stage0: 0.4059  loss_box_reg_stage0: 0.36  loss_cls_stage1: 0.4396  loss_box_reg_stage1: 0.9026  loss_cls_stage2: 0.4327  loss_box_reg_stage2: 1.16  loss_mask: 0.1185  loss_rpn_cls: 0.02177  loss_rpn_loc: 0.06049  time: 0.6465  data_time: 0.0059  lr: 0.00016  max_mem: 19672M
[07/28 17:01:15] d2.utils.events INFO:  eta: 11:44:39  iter: 2919  total_loss: 3.91  loss_cls_stage0: 0.4102  loss_box_reg_stage0: 0.3556  loss_cls_stage1: 0.4785  loss_box_reg_stage1: 0.9188  loss_cls_stage2: 0.477  loss_box_reg_stage2: 1.143  loss_mask: 0.1207  loss_rpn_cls: 0.02357  loss_rpn_loc: 0.0536  time: 0.6465  data_time: 0.0059  lr: 0.00016  max_mem: 19672M
[07/28 17:01:28] d2.utils.events INFO:  eta: 11:44:31  iter: 2939  total_loss: 3.912  loss_cls_stage0: 0.4393  loss_box_reg_stage0: 0.3812  loss_cls_stage1: 0.4718  loss_box_reg_stage1: 0.8745  loss_cls_stage2: 0.4446  loss_box_reg_stage2: 1.05  loss_mask: 0.1258  loss_rpn_cls: 0.0273  loss_rpn_loc: 0.06062  time: 0.6465  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 17:01:41] d2.utils.events INFO:  eta: 11:44:02  iter: 2959  total_loss: 3.701  loss_cls_stage0: 0.4018  loss_box_reg_stage0: 0.3737  loss_cls_stage1: 0.3968  loss_box_reg_stage1: 0.8328  loss_cls_stage2: 0.3694  loss_box_reg_stage2: 1.016  loss_mask: 0.1479  loss_rpn_cls: 0.02703  loss_rpn_loc: 0.07287  time: 0.6465  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:01:53] d2.utils.events INFO:  eta: 11:43:48  iter: 2979  total_loss: 3.992  loss_cls_stage0: 0.4384  loss_box_reg_stage0: 0.3748  loss_cls_stage1: 0.4583  loss_box_reg_stage1: 0.8479  loss_cls_stage2: 0.4123  loss_box_reg_stage2: 0.991  loss_mask: 0.1294  loss_rpn_cls: 0.02509  loss_rpn_loc: 0.06177  time: 0.6465  data_time: 0.0051  lr: 0.00016  max_mem: 19672M
[07/28 17:02:06] d2.utils.events INFO:  eta: 11:43:29  iter: 2999  total_loss: 3.622  loss_cls_stage0: 0.3596  loss_box_reg_stage0: 0.3523  loss_cls_stage1: 0.3833  loss_box_reg_stage1: 0.8303  loss_cls_stage2: 0.3546  loss_box_reg_stage2: 1.12  loss_mask: 0.1217  loss_rpn_cls: 0.02654  loss_rpn_loc: 0.06021  time: 0.6463  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:02:19] d2.utils.events INFO:  eta: 11:43:16  iter: 3019  total_loss: 3.965  loss_cls_stage0: 0.4298  loss_box_reg_stage0: 0.3727  loss_cls_stage1: 0.474  loss_box_reg_stage1: 0.8502  loss_cls_stage2: 0.4653  loss_box_reg_stage2: 1.134  loss_mask: 0.138  loss_rpn_cls: 0.02391  loss_rpn_loc: 0.05681  time: 0.6463  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 17:02:32] d2.utils.events INFO:  eta: 11:42:59  iter: 3039  total_loss: 4.007  loss_cls_stage0: 0.4082  loss_box_reg_stage0: 0.3498  loss_cls_stage1: 0.4729  loss_box_reg_stage1: 0.9372  loss_cls_stage2: 0.4653  loss_box_reg_stage2: 1.178  loss_mask: 0.1226  loss_rpn_cls: 0.02017  loss_rpn_loc: 0.0596  time: 0.6464  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:02:45] d2.utils.events INFO:  eta: 11:42:54  iter: 3059  total_loss: 3.944  loss_cls_stage0: 0.4035  loss_box_reg_stage0: 0.3539  loss_cls_stage1: 0.4525  loss_box_reg_stage1: 0.8859  loss_cls_stage2: 0.4207  loss_box_reg_stage2: 1.112  loss_mask: 0.125  loss_rpn_cls: 0.02285  loss_rpn_loc: 0.05772  time: 0.6464  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 17:02:58] d2.utils.events INFO:  eta: 11:42:33  iter: 3079  total_loss: 3.719  loss_cls_stage0: 0.393  loss_box_reg_stage0: 0.3457  loss_cls_stage1: 0.4205  loss_box_reg_stage1: 0.8341  loss_cls_stage2: 0.4227  loss_box_reg_stage2: 1.083  loss_mask: 0.1157  loss_rpn_cls: 0.02299  loss_rpn_loc: 0.04861  time: 0.6463  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 17:03:11] d2.utils.events INFO:  eta: 11:42:10  iter: 3099  total_loss: 3.778  loss_cls_stage0: 0.3846  loss_box_reg_stage0: 0.36  loss_cls_stage1: 0.417  loss_box_reg_stage1: 0.8567  loss_cls_stage2: 0.3933  loss_box_reg_stage2: 1.112  loss_mask: 0.1211  loss_rpn_cls: 0.02062  loss_rpn_loc: 0.05698  time: 0.6464  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 17:03:23] d2.utils.events INFO:  eta: 11:41:57  iter: 3119  total_loss: 3.68  loss_cls_stage0: 0.3758  loss_box_reg_stage0: 0.3489  loss_cls_stage1: 0.394  loss_box_reg_stage1: 0.8142  loss_cls_stage2: 0.4025  loss_box_reg_stage2: 0.9951  loss_mask: 0.1189  loss_rpn_cls: 0.03939  loss_rpn_loc: 0.07675  time: 0.6463  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 17:03:36] d2.utils.events INFO:  eta: 11:41:41  iter: 3139  total_loss: 3.65  loss_cls_stage0: 0.395  loss_box_reg_stage0: 0.3691  loss_cls_stage1: 0.4787  loss_box_reg_stage1: 0.8137  loss_cls_stage2: 0.4206  loss_box_reg_stage2: 1.005  loss_mask: 0.1239  loss_rpn_cls: 0.02189  loss_rpn_loc: 0.06381  time: 0.6461  data_time: 0.0054  lr: 0.00016  max_mem: 19672M
[07/28 17:03:49] d2.utils.events INFO:  eta: 11:41:19  iter: 3159  total_loss: 3.684  loss_cls_stage0: 0.3849  loss_box_reg_stage0: 0.3543  loss_cls_stage1: 0.3929  loss_box_reg_stage1: 0.8494  loss_cls_stage2: 0.3758  loss_box_reg_stage2: 1.043  loss_mask: 0.1239  loss_rpn_cls: 0.03944  loss_rpn_loc: 0.06152  time: 0.6461  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 17:04:01] d2.utils.events INFO:  eta: 11:41:02  iter: 3179  total_loss: 4.017  loss_cls_stage0: 0.4226  loss_box_reg_stage0: 0.364  loss_cls_stage1: 0.457  loss_box_reg_stage1: 0.8427  loss_cls_stage2: 0.4218  loss_box_reg_stage2: 1.107  loss_mask: 0.1253  loss_rpn_cls: 0.02994  loss_rpn_loc: 0.08527  time: 0.6460  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 17:04:14] d2.utils.events INFO:  eta: 11:40:59  iter: 3199  total_loss: 3.77  loss_cls_stage0: 0.3969  loss_box_reg_stage0: 0.3599  loss_cls_stage1: 0.4083  loss_box_reg_stage1: 0.8201  loss_cls_stage2: 0.3932  loss_box_reg_stage2: 1.007  loss_mask: 0.1341  loss_rpn_cls: 0.0429  loss_rpn_loc: 0.06459  time: 0.6460  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:04:27] d2.utils.events INFO:  eta: 11:40:44  iter: 3219  total_loss: 3.69  loss_cls_stage0: 0.3765  loss_box_reg_stage0: 0.3321  loss_cls_stage1: 0.4334  loss_box_reg_stage1: 0.8122  loss_cls_stage2: 0.419  loss_box_reg_stage2: 1.014  loss_mask: 0.1248  loss_rpn_cls: 0.02389  loss_rpn_loc: 0.05604  time: 0.6460  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:04:40] d2.utils.events INFO:  eta: 11:40:26  iter: 3239  total_loss: 3.737  loss_cls_stage0: 0.3699  loss_box_reg_stage0: 0.3539  loss_cls_stage1: 0.4302  loss_box_reg_stage1: 0.8764  loss_cls_stage2: 0.4216  loss_box_reg_stage2: 1.149  loss_mask: 0.1216  loss_rpn_cls: 0.02876  loss_rpn_loc: 0.05711  time: 0.6460  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 17:04:53] d2.utils.events INFO:  eta: 11:40:43  iter: 3259  total_loss: 3.974  loss_cls_stage0: 0.4523  loss_box_reg_stage0: 0.3948  loss_cls_stage1: 0.4795  loss_box_reg_stage1: 0.8615  loss_cls_stage2: 0.4364  loss_box_reg_stage2: 1.037  loss_mask: 0.139  loss_rpn_cls: 0.03074  loss_rpn_loc: 0.06215  time: 0.6461  data_time: 0.0060  lr: 0.00016  max_mem: 19672M
[07/28 17:05:06] d2.utils.events INFO:  eta: 11:40:22  iter: 3279  total_loss: 3.912  loss_cls_stage0: 0.4022  loss_box_reg_stage0: 0.3467  loss_cls_stage1: 0.496  loss_box_reg_stage1: 0.8713  loss_cls_stage2: 0.501  loss_box_reg_stage2: 1.095  loss_mask: 0.1199  loss_rpn_cls: 0.0257  loss_rpn_loc: 0.05876  time: 0.6461  data_time: 0.0065  lr: 0.00016  max_mem: 19672M
[07/28 17:05:19] d2.utils.events INFO:  eta: 11:39:59  iter: 3299  total_loss: 3.973  loss_cls_stage0: 0.3979  loss_box_reg_stage0: 0.372  loss_cls_stage1: 0.4299  loss_box_reg_stage1: 0.871  loss_cls_stage2: 0.4407  loss_box_reg_stage2: 1.124  loss_mask: 0.1225  loss_rpn_cls: 0.02519  loss_rpn_loc: 0.08408  time: 0.6461  data_time: 0.0054  lr: 0.00016  max_mem: 19672M
[07/28 17:05:33] d2.utils.events INFO:  eta: 11:39:39  iter: 3319  total_loss: 4.187  loss_cls_stage0: 0.4658  loss_box_reg_stage0: 0.3952  loss_cls_stage1: 0.475  loss_box_reg_stage1: 0.942  loss_cls_stage2: 0.458  loss_box_reg_stage2: 1.084  loss_mask: 0.1408  loss_rpn_cls: 0.02468  loss_rpn_loc: 0.07755  time: 0.6462  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:05:45] d2.utils.events INFO:  eta: 11:39:14  iter: 3339  total_loss: 3.683  loss_cls_stage0: 0.3962  loss_box_reg_stage0: 0.3576  loss_cls_stage1: 0.4321  loss_box_reg_stage1: 0.8613  loss_cls_stage2: 0.4055  loss_box_reg_stage2: 1.056  loss_mask: 0.13  loss_rpn_cls: 0.02342  loss_rpn_loc: 0.05875  time: 0.6462  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 17:05:59] d2.utils.events INFO:  eta: 11:38:51  iter: 3359  total_loss: 3.971  loss_cls_stage0: 0.3971  loss_box_reg_stage0: 0.3656  loss_cls_stage1: 0.4783  loss_box_reg_stage1: 0.8604  loss_cls_stage2: 0.4805  loss_box_reg_stage2: 1.107  loss_mask: 0.129  loss_rpn_cls: 0.02207  loss_rpn_loc: 0.05221  time: 0.6462  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 17:06:11] d2.utils.events INFO:  eta: 11:38:02  iter: 3379  total_loss: 3.732  loss_cls_stage0: 0.3803  loss_box_reg_stage0: 0.353  loss_cls_stage1: 0.4186  loss_box_reg_stage1: 0.8434  loss_cls_stage2: 0.4029  loss_box_reg_stage2: 1.11  loss_mask: 0.1156  loss_rpn_cls: 0.02708  loss_rpn_loc: 0.06399  time: 0.6462  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 17:06:25] d2.utils.events INFO:  eta: 11:37:49  iter: 3399  total_loss: 4.056  loss_cls_stage0: 0.4344  loss_box_reg_stage0: 0.3724  loss_cls_stage1: 0.4908  loss_box_reg_stage1: 0.9351  loss_cls_stage2: 0.4441  loss_box_reg_stage2: 1.167  loss_mask: 0.128  loss_rpn_cls: 0.02574  loss_rpn_loc: 0.0602  time: 0.6463  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:06:37] d2.utils.events INFO:  eta: 11:37:25  iter: 3419  total_loss: 3.713  loss_cls_stage0: 0.3864  loss_box_reg_stage0: 0.3616  loss_cls_stage1: 0.4369  loss_box_reg_stage1: 0.8072  loss_cls_stage2: 0.3871  loss_box_reg_stage2: 1.059  loss_mask: 0.1294  loss_rpn_cls: 0.02913  loss_rpn_loc: 0.06809  time: 0.6463  data_time: 0.0052  lr: 0.00016  max_mem: 19672M
[07/28 17:06:50] d2.utils.events INFO:  eta: 11:37:08  iter: 3439  total_loss: 3.786  loss_cls_stage0: 0.3658  loss_box_reg_stage0: 0.3425  loss_cls_stage1: 0.419  loss_box_reg_stage1: 0.8621  loss_cls_stage2: 0.3902  loss_box_reg_stage2: 1.037  loss_mask: 0.1193  loss_rpn_cls: 0.02286  loss_rpn_loc: 0.05401  time: 0.6462  data_time: 0.0053  lr: 0.00016  max_mem: 19672M
[07/28 17:07:04] d2.utils.events INFO:  eta: 11:37:41  iter: 3459  total_loss: 3.872  loss_cls_stage0: 0.4006  loss_box_reg_stage0: 0.3454  loss_cls_stage1: 0.477  loss_box_reg_stage1: 0.8597  loss_cls_stage2: 0.4158  loss_box_reg_stage2: 1.086  loss_mask: 0.1358  loss_rpn_cls: 0.02113  loss_rpn_loc: 0.06112  time: 0.6463  data_time: 0.0052  lr: 0.00016  max_mem: 19672M
[07/28 17:07:17] d2.utils.events INFO:  eta: 11:37:32  iter: 3479  total_loss: 4.028  loss_cls_stage0: 0.4061  loss_box_reg_stage0: 0.37  loss_cls_stage1: 0.4636  loss_box_reg_stage1: 0.9149  loss_cls_stage2: 0.4345  loss_box_reg_stage2: 1.162  loss_mask: 0.1314  loss_rpn_cls: 0.0237  loss_rpn_loc: 0.05733  time: 0.6463  data_time: 0.0054  lr: 0.00016  max_mem: 19672M
[07/28 17:07:29] d2.utils.events INFO:  eta: 11:37:15  iter: 3499  total_loss: 3.675  loss_cls_stage0: 0.4098  loss_box_reg_stage0: 0.3517  loss_cls_stage1: 0.3976  loss_box_reg_stage1: 0.8334  loss_cls_stage2: 0.3869  loss_box_reg_stage2: 0.9287  loss_mask: 0.1294  loss_rpn_cls: 0.02476  loss_rpn_loc: 0.06459  time: 0.6463  data_time: 0.0053  lr: 0.00016  max_mem: 19672M
[07/28 17:07:42] d2.utils.events INFO:  eta: 11:36:16  iter: 3519  total_loss: 3.512  loss_cls_stage0: 0.3732  loss_box_reg_stage0: 0.3501  loss_cls_stage1: 0.4066  loss_box_reg_stage1: 0.7993  loss_cls_stage2: 0.3728  loss_box_reg_stage2: 0.9531  loss_mask: 0.1182  loss_rpn_cls: 0.02759  loss_rpn_loc: 0.07071  time: 0.6462  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 17:07:55] d2.utils.events INFO:  eta: 11:35:58  iter: 3539  total_loss: 3.482  loss_cls_stage0: 0.3747  loss_box_reg_stage0: 0.3462  loss_cls_stage1: 0.3972  loss_box_reg_stage1: 0.7934  loss_cls_stage2: 0.4182  loss_box_reg_stage2: 1.001  loss_mask: 0.1212  loss_rpn_cls: 0.01764  loss_rpn_loc: 0.05895  time: 0.6463  data_time: 0.0054  lr: 0.00016  max_mem: 19672M
[07/28 17:08:08] d2.utils.events INFO:  eta: 11:35:45  iter: 3559  total_loss: 3.948  loss_cls_stage0: 0.4006  loss_box_reg_stage0: 0.3737  loss_cls_stage1: 0.4355  loss_box_reg_stage1: 0.8543  loss_cls_stage2: 0.4242  loss_box_reg_stage2: 1.146  loss_mask: 0.1336  loss_rpn_cls: 0.02134  loss_rpn_loc: 0.06121  time: 0.6463  data_time: 0.0065  lr: 0.00016  max_mem: 19672M
[07/28 17:08:21] d2.utils.events INFO:  eta: 11:35:37  iter: 3579  total_loss: 4.05  loss_cls_stage0: 0.4229  loss_box_reg_stage0: 0.3826  loss_cls_stage1: 0.4681  loss_box_reg_stage1: 0.8785  loss_cls_stage2: 0.4409  loss_box_reg_stage2: 1.149  loss_mask: 0.1288  loss_rpn_cls: 0.02392  loss_rpn_loc: 0.08172  time: 0.6463  data_time: 0.0051  lr: 0.00016  max_mem: 19672M
[07/28 17:08:34] d2.utils.events INFO:  eta: 11:36:09  iter: 3599  total_loss: 3.613  loss_cls_stage0: 0.3564  loss_box_reg_stage0: 0.333  loss_cls_stage1: 0.3869  loss_box_reg_stage1: 0.7959  loss_cls_stage2: 0.3804  loss_box_reg_stage2: 1.013  loss_mask: 0.1129  loss_rpn_cls: 0.02687  loss_rpn_loc: 0.0588  time: 0.6463  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:08:47] d2.utils.events INFO:  eta: 11:35:56  iter: 3619  total_loss: 3.337  loss_cls_stage0: 0.368  loss_box_reg_stage0: 0.3388  loss_cls_stage1: 0.3734  loss_box_reg_stage1: 0.7435  loss_cls_stage2: 0.3435  loss_box_reg_stage2: 0.9712  loss_mask: 0.1168  loss_rpn_cls: 0.03184  loss_rpn_loc: 0.06101  time: 0.6463  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 17:09:00] d2.utils.events INFO:  eta: 11:35:43  iter: 3639  total_loss: 3.972  loss_cls_stage0: 0.3958  loss_box_reg_stage0: 0.3622  loss_cls_stage1: 0.4629  loss_box_reg_stage1: 0.8246  loss_cls_stage2: 0.4113  loss_box_reg_stage2: 1.057  loss_mask: 0.1179  loss_rpn_cls: 0.02236  loss_rpn_loc: 0.0811  time: 0.6463  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 17:09:13] d2.utils.events INFO:  eta: 11:34:27  iter: 3659  total_loss: 3.471  loss_cls_stage0: 0.3626  loss_box_reg_stage0: 0.3189  loss_cls_stage1: 0.4006  loss_box_reg_stage1: 0.8025  loss_cls_stage2: 0.4024  loss_box_reg_stage2: 1.001  loss_mask: 0.129  loss_rpn_cls: 0.02206  loss_rpn_loc: 0.06037  time: 0.6462  data_time: 0.0060  lr: 0.00016  max_mem: 19672M
[07/28 17:09:25] d2.utils.events INFO:  eta: 11:33:56  iter: 3679  total_loss: 3.354  loss_cls_stage0: 0.363  loss_box_reg_stage0: 0.3367  loss_cls_stage1: 0.3722  loss_box_reg_stage1: 0.7415  loss_cls_stage2: 0.3384  loss_box_reg_stage2: 0.9898  loss_mask: 0.1095  loss_rpn_cls: 0.02287  loss_rpn_loc: 0.05337  time: 0.6461  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:09:38] d2.utils.events INFO:  eta: 11:34:05  iter: 3699  total_loss: 3.557  loss_cls_stage0: 0.3917  loss_box_reg_stage0: 0.3645  loss_cls_stage1: 0.3941  loss_box_reg_stage1: 0.8169  loss_cls_stage2: 0.3683  loss_box_reg_stage2: 1.025  loss_mask: 0.127  loss_rpn_cls: 0.02823  loss_rpn_loc: 0.05396  time: 0.6461  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:09:50] d2.utils.events INFO:  eta: 11:33:26  iter: 3719  total_loss: 3.435  loss_cls_stage0: 0.3948  loss_box_reg_stage0: 0.3394  loss_cls_stage1: 0.4075  loss_box_reg_stage1: 0.768  loss_cls_stage2: 0.3751  loss_box_reg_stage2: 1.02  loss_mask: 0.1274  loss_rpn_cls: 0.02496  loss_rpn_loc: 0.0535  time: 0.6459  data_time: 0.0053  lr: 0.00016  max_mem: 19672M
[07/28 17:10:03] d2.utils.events INFO:  eta: 11:33:13  iter: 3739  total_loss: 3.447  loss_cls_stage0: 0.3584  loss_box_reg_stage0: 0.3406  loss_cls_stage1: 0.3738  loss_box_reg_stage1: 0.8051  loss_cls_stage2: 0.3717  loss_box_reg_stage2: 1.057  loss_mask: 0.1283  loss_rpn_cls: 0.02938  loss_rpn_loc: 0.05922  time: 0.6459  data_time: 0.0054  lr: 0.00016  max_mem: 19672M
[07/28 17:10:16] d2.utils.events INFO:  eta: 11:33:14  iter: 3759  total_loss: 3.612  loss_cls_stage0: 0.3916  loss_box_reg_stage0: 0.3576  loss_cls_stage1: 0.4152  loss_box_reg_stage1: 0.8167  loss_cls_stage2: 0.3756  loss_box_reg_stage2: 1.045  loss_mask: 0.1358  loss_rpn_cls: 0.02503  loss_rpn_loc: 0.05604  time: 0.6459  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:10:30] d2.utils.events INFO:  eta: 11:33:22  iter: 3779  total_loss: 3.687  loss_cls_stage0: 0.4027  loss_box_reg_stage0: 0.3583  loss_cls_stage1: 0.4257  loss_box_reg_stage1: 0.8657  loss_cls_stage2: 0.4117  loss_box_reg_stage2: 1.123  loss_mask: 0.1217  loss_rpn_cls: 0.02313  loss_rpn_loc: 0.05907  time: 0.6460  data_time: 0.0059  lr: 0.00016  max_mem: 19672M
[07/28 17:10:43] d2.utils.events INFO:  eta: 11:33:35  iter: 3799  total_loss: 3.774  loss_cls_stage0: 0.4056  loss_box_reg_stage0: 0.348  loss_cls_stage1: 0.4581  loss_box_reg_stage1: 0.8375  loss_cls_stage2: 0.41  loss_box_reg_stage2: 1.052  loss_mask: 0.1347  loss_rpn_cls: 0.01936  loss_rpn_loc: 0.05138  time: 0.6461  data_time: 0.0060  lr: 0.00016  max_mem: 19672M
[07/28 17:10:56] d2.utils.events INFO:  eta: 11:32:56  iter: 3819  total_loss: 3.64  loss_cls_stage0: 0.4212  loss_box_reg_stage0: 0.3752  loss_cls_stage1: 0.4446  loss_box_reg_stage1: 0.8068  loss_cls_stage2: 0.4318  loss_box_reg_stage2: 1.008  loss_mask: 0.1335  loss_rpn_cls: 0.02128  loss_rpn_loc: 0.05178  time: 0.6461  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:11:08] d2.utils.events INFO:  eta: 11:32:43  iter: 3839  total_loss: 3.765  loss_cls_stage0: 0.4317  loss_box_reg_stage0: 0.3692  loss_cls_stage1: 0.4564  loss_box_reg_stage1: 0.7958  loss_cls_stage2: 0.4044  loss_box_reg_stage2: 1.021  loss_mask: 0.1396  loss_rpn_cls: 0.02119  loss_rpn_loc: 0.06033  time: 0.6459  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:11:21] d2.utils.events INFO:  eta: 11:32:44  iter: 3859  total_loss: 3.697  loss_cls_stage0: 0.331  loss_box_reg_stage0: 0.3272  loss_cls_stage1: 0.3788  loss_box_reg_stage1: 0.8599  loss_cls_stage2: 0.37  loss_box_reg_stage2: 0.9873  loss_mask: 0.1148  loss_rpn_cls: 0.02401  loss_rpn_loc: 0.06158  time: 0.6460  data_time: 0.0060  lr: 0.00016  max_mem: 19672M
[07/28 17:11:34] d2.utils.events INFO:  eta: 11:31:46  iter: 3879  total_loss: 3.437  loss_cls_stage0: 0.3899  loss_box_reg_stage0: 0.3594  loss_cls_stage1: 0.3933  loss_box_reg_stage1: 0.7811  loss_cls_stage2: 0.3681  loss_box_reg_stage2: 0.9811  loss_mask: 0.1233  loss_rpn_cls: 0.02399  loss_rpn_loc: 0.06737  time: 0.6460  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 17:11:47] d2.utils.events INFO:  eta: 11:31:29  iter: 3899  total_loss: 3.626  loss_cls_stage0: 0.4012  loss_box_reg_stage0: 0.3694  loss_cls_stage1: 0.4192  loss_box_reg_stage1: 0.8195  loss_cls_stage2: 0.3874  loss_box_reg_stage2: 0.9738  loss_mask: 0.134  loss_rpn_cls: 0.01823  loss_rpn_loc: 0.05586  time: 0.6460  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:12:00] d2.utils.events INFO:  eta: 11:31:16  iter: 3919  total_loss: 3.315  loss_cls_stage0: 0.3295  loss_box_reg_stage0: 0.3461  loss_cls_stage1: 0.3502  loss_box_reg_stage1: 0.7362  loss_cls_stage2: 0.3616  loss_box_reg_stage2: 0.8686  loss_mask: 0.1234  loss_rpn_cls: 0.02752  loss_rpn_loc: 0.06662  time: 0.6460  data_time: 0.0064  lr: 0.00016  max_mem: 19672M
[07/28 17:12:13] d2.utils.events INFO:  eta: 11:31:01  iter: 3939  total_loss: 3.278  loss_cls_stage0: 0.3507  loss_box_reg_stage0: 0.3375  loss_cls_stage1: 0.3422  loss_box_reg_stage1: 0.7117  loss_cls_stage2: 0.322  loss_box_reg_stage2: 0.8708  loss_mask: 0.1158  loss_rpn_cls: 0.02455  loss_rpn_loc: 0.05786  time: 0.6460  data_time: 0.0062  lr: 0.00016  max_mem: 19672M
[07/28 17:12:26] d2.utils.events INFO:  eta: 11:30:42  iter: 3959  total_loss: 3.681  loss_cls_stage0: 0.4211  loss_box_reg_stage0: 0.3712  loss_cls_stage1: 0.405  loss_box_reg_stage1: 0.8403  loss_cls_stage2: 0.3711  loss_box_reg_stage2: 0.9732  loss_mask: 0.1321  loss_rpn_cls: 0.03442  loss_rpn_loc: 0.07878  time: 0.6459  data_time: 0.0053  lr: 0.00016  max_mem: 19672M
[07/28 17:12:39] d2.utils.events INFO:  eta: 11:30:36  iter: 3979  total_loss: 3.62  loss_cls_stage0: 0.3663  loss_box_reg_stage0: 0.364  loss_cls_stage1: 0.3732  loss_box_reg_stage1: 0.8213  loss_cls_stage2: 0.3665  loss_box_reg_stage2: 1.039  loss_mask: 0.1265  loss_rpn_cls: 0.02508  loss_rpn_loc: 0.05348  time: 0.6461  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 17:12:52] d2.utils.events INFO:  eta: 11:30:28  iter: 3999  total_loss: 3.621  loss_cls_stage0: 0.3661  loss_box_reg_stage0: 0.3376  loss_cls_stage1: 0.3779  loss_box_reg_stage1: 0.7944  loss_cls_stage2: 0.3723  loss_box_reg_stage2: 0.9828  loss_mask: 0.1231  loss_rpn_cls: 0.02205  loss_rpn_loc: 0.05397  time: 0.6461  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:13:06] d2.utils.events INFO:  eta: 11:30:38  iter: 4019  total_loss: 3.117  loss_cls_stage0: 0.3518  loss_box_reg_stage0: 0.3079  loss_cls_stage1: 0.3802  loss_box_reg_stage1: 0.6754  loss_cls_stage2: 0.3596  loss_box_reg_stage2: 0.8488  loss_mask: 0.1075  loss_rpn_cls: 0.02662  loss_rpn_loc: 0.06656  time: 0.6462  data_time: 0.0063  lr: 0.00016  max_mem: 19672M
[07/28 17:13:19] d2.utils.events INFO:  eta: 11:30:47  iter: 4039  total_loss: 3.24  loss_cls_stage0: 0.302  loss_box_reg_stage0: 0.3177  loss_cls_stage1: 0.3169  loss_box_reg_stage1: 0.7408  loss_cls_stage2: 0.3135  loss_box_reg_stage2: 1.02  loss_mask: 0.1122  loss_rpn_cls: 0.02295  loss_rpn_loc: 0.05409  time: 0.6463  data_time: 0.0059  lr: 0.00016  max_mem: 19672M
[07/28 17:13:32] d2.utils.events INFO:  eta: 11:30:19  iter: 4059  total_loss: 3.718  loss_cls_stage0: 0.3652  loss_box_reg_stage0: 0.3466  loss_cls_stage1: 0.3982  loss_box_reg_stage1: 0.8282  loss_cls_stage2: 0.3957  loss_box_reg_stage2: 1.084  loss_mask: 0.1112  loss_rpn_cls: 0.02286  loss_rpn_loc: 0.05461  time: 0.6463  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 17:13:44] d2.utils.events INFO:  eta: 11:30:06  iter: 4079  total_loss: 3.741  loss_cls_stage0: 0.4023  loss_box_reg_stage0: 0.3654  loss_cls_stage1: 0.4367  loss_box_reg_stage1: 0.8405  loss_cls_stage2: 0.3974  loss_box_reg_stage2: 1.05  loss_mask: 0.1372  loss_rpn_cls: 0.02023  loss_rpn_loc: 0.05188  time: 0.6462  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:13:57] d2.utils.events INFO:  eta: 11:29:18  iter: 4099  total_loss: 3.45  loss_cls_stage0: 0.3632  loss_box_reg_stage0: 0.3352  loss_cls_stage1: 0.3748  loss_box_reg_stage1: 0.8239  loss_cls_stage2: 0.3654  loss_box_reg_stage2: 0.963  loss_mask: 0.1155  loss_rpn_cls: 0.02009  loss_rpn_loc: 0.05988  time: 0.6462  data_time: 0.0053  lr: 0.00016  max_mem: 19672M
[07/28 17:14:10] d2.utils.events INFO:  eta: 11:29:15  iter: 4119  total_loss: 3.517  loss_cls_stage0: 0.3246  loss_box_reg_stage0: 0.3335  loss_cls_stage1: 0.359  loss_box_reg_stage1: 0.8308  loss_cls_stage2: 0.3538  loss_box_reg_stage2: 1.037  loss_mask: 0.1159  loss_rpn_cls: 0.02319  loss_rpn_loc: 0.06882  time: 0.6462  data_time: 0.0061  lr: 0.00016  max_mem: 19672M
[07/28 17:14:23] d2.utils.events INFO:  eta: 11:29:41  iter: 4139  total_loss: 3.623  loss_cls_stage0: 0.3683  loss_box_reg_stage0: 0.3365  loss_cls_stage1: 0.418  loss_box_reg_stage1: 0.8265  loss_cls_stage2: 0.3857  loss_box_reg_stage2: 1.012  loss_mask: 0.1105  loss_rpn_cls: 0.02331  loss_rpn_loc: 0.05124  time: 0.6462  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 17:14:36] d2.utils.events INFO:  eta: 11:29:35  iter: 4159  total_loss: 3.534  loss_cls_stage0: 0.3612  loss_box_reg_stage0: 0.3306  loss_cls_stage1: 0.406  loss_box_reg_stage1: 0.8073  loss_cls_stage2: 0.3982  loss_box_reg_stage2: 1.044  loss_mask: 0.1064  loss_rpn_cls: 0.01992  loss_rpn_loc: 0.05152  time: 0.6462  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 17:14:49] d2.utils.events INFO:  eta: 11:29:31  iter: 4179  total_loss: 3.731  loss_cls_stage0: 0.3773  loss_box_reg_stage0: 0.3367  loss_cls_stage1: 0.4201  loss_box_reg_stage1: 0.8979  loss_cls_stage2: 0.3995  loss_box_reg_stage2: 1.116  loss_mask: 0.1248  loss_rpn_cls: 0.01478  loss_rpn_loc: 0.05522  time: 0.6462  data_time: 0.0053  lr: 0.00016  max_mem: 19672M
[07/28 17:15:02] d2.utils.events INFO:  eta: 11:29:16  iter: 4199  total_loss: 3.362  loss_cls_stage0: 0.3615  loss_box_reg_stage0: 0.3263  loss_cls_stage1: 0.3848  loss_box_reg_stage1: 0.7527  loss_cls_stage2: 0.353  loss_box_reg_stage2: 0.9763  loss_mask: 0.1272  loss_rpn_cls: 0.02006  loss_rpn_loc: 0.05061  time: 0.6462  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 17:15:15] d2.utils.events INFO:  eta: 11:28:57  iter: 4219  total_loss: 3.451  loss_cls_stage0: 0.3605  loss_box_reg_stage0: 0.3528  loss_cls_stage1: 0.3837  loss_box_reg_stage1: 0.7857  loss_cls_stage2: 0.348  loss_box_reg_stage2: 0.9467  loss_mask: 0.1309  loss_rpn_cls: 0.02798  loss_rpn_loc: 0.06899  time: 0.6461  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 17:15:27] d2.utils.events INFO:  eta: 11:28:43  iter: 4239  total_loss: 3.517  loss_cls_stage0: 0.3742  loss_box_reg_stage0: 0.352  loss_cls_stage1: 0.415  loss_box_reg_stage1: 0.7666  loss_cls_stage2: 0.4085  loss_box_reg_stage2: 1.032  loss_mask: 0.1375  loss_rpn_cls: 0.0178  loss_rpn_loc: 0.04882  time: 0.6461  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 17:15:41] d2.utils.events INFO:  eta: 11:28:11  iter: 4259  total_loss: 3.682  loss_cls_stage0: 0.395  loss_box_reg_stage0: 0.3613  loss_cls_stage1: 0.4275  loss_box_reg_stage1: 0.7829  loss_cls_stage2: 0.4028  loss_box_reg_stage2: 1.002  loss_mask: 0.1359  loss_rpn_cls: 0.01887  loss_rpn_loc: 0.05116  time: 0.6462  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 17:15:54] d2.utils.events INFO:  eta: 11:28:17  iter: 4279  total_loss: 3.377  loss_cls_stage0: 0.3579  loss_box_reg_stage0: 0.3062  loss_cls_stage1: 0.3783  loss_box_reg_stage1: 0.7378  loss_cls_stage2: 0.3773  loss_box_reg_stage2: 0.908  loss_mask: 0.1011  loss_rpn_cls: 0.0223  loss_rpn_loc: 0.05693  time: 0.6462  data_time: 0.0062  lr: 0.00016  max_mem: 19672M
[07/28 17:16:07] d2.utils.events INFO:  eta: 11:28:05  iter: 4299  total_loss: 3.338  loss_cls_stage0: 0.3557  loss_box_reg_stage0: 0.3259  loss_cls_stage1: 0.3789  loss_box_reg_stage1: 0.7435  loss_cls_stage2: 0.3587  loss_box_reg_stage2: 1.002  loss_mask: 0.1152  loss_rpn_cls: 0.0226  loss_rpn_loc: 0.05897  time: 0.6462  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:16:20] d2.utils.events INFO:  eta: 11:27:58  iter: 4319  total_loss: 3.745  loss_cls_stage0: 0.4028  loss_box_reg_stage0: 0.3331  loss_cls_stage1: 0.4289  loss_box_reg_stage1: 0.8116  loss_cls_stage2: 0.4023  loss_box_reg_stage2: 1.137  loss_mask: 0.1088  loss_rpn_cls: 0.01669  loss_rpn_loc: 0.05179  time: 0.6462  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:16:33] d2.utils.events INFO:  eta: 11:27:45  iter: 4339  total_loss: 3.355  loss_cls_stage0: 0.3407  loss_box_reg_stage0: 0.3231  loss_cls_stage1: 0.3657  loss_box_reg_stage1: 0.7657  loss_cls_stage2: 0.3673  loss_box_reg_stage2: 1.036  loss_mask: 0.103  loss_rpn_cls: 0.03028  loss_rpn_loc: 0.0619  time: 0.6462  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 17:16:46] d2.utils.events INFO:  eta: 11:27:25  iter: 4359  total_loss: 3.719  loss_cls_stage0: 0.3835  loss_box_reg_stage0: 0.3622  loss_cls_stage1: 0.3987  loss_box_reg_stage1: 0.8608  loss_cls_stage2: 0.3832  loss_box_reg_stage2: 1.021  loss_mask: 0.1202  loss_rpn_cls: 0.01856  loss_rpn_loc: 0.04854  time: 0.6462  data_time: 0.0060  lr: 0.00016  max_mem: 19672M
[07/28 17:16:59] d2.utils.events INFO:  eta: 11:27:18  iter: 4379  total_loss: 3.506  loss_cls_stage0: 0.3842  loss_box_reg_stage0: 0.3444  loss_cls_stage1: 0.4283  loss_box_reg_stage1: 0.7614  loss_cls_stage2: 0.388  loss_box_reg_stage2: 0.9885  loss_mask: 0.1181  loss_rpn_cls: 0.02735  loss_rpn_loc: 0.06467  time: 0.6462  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 17:17:11] d2.utils.events INFO:  eta: 11:27:08  iter: 4399  total_loss: 3.859  loss_cls_stage0: 0.3635  loss_box_reg_stage0: 0.3512  loss_cls_stage1: 0.4275  loss_box_reg_stage1: 0.8558  loss_cls_stage2: 0.4334  loss_box_reg_stage2: 1.081  loss_mask: 0.1139  loss_rpn_cls: 0.01512  loss_rpn_loc: 0.051  time: 0.6462  data_time: 0.0054  lr: 0.00016  max_mem: 19672M
[07/28 17:17:25] d2.utils.events INFO:  eta: 11:28:06  iter: 4419  total_loss: 3.633  loss_cls_stage0: 0.3702  loss_box_reg_stage0: 0.3316  loss_cls_stage1: 0.4239  loss_box_reg_stage1: 0.8425  loss_cls_stage2: 0.4051  loss_box_reg_stage2: 1.078  loss_mask: 0.1095  loss_rpn_cls: 0.02389  loss_rpn_loc: 0.06057  time: 0.6464  data_time: 0.0059  lr: 0.00016  max_mem: 19672M
[07/28 17:17:38] d2.utils.events INFO:  eta: 11:27:33  iter: 4439  total_loss: 3.965  loss_cls_stage0: 0.411  loss_box_reg_stage0: 0.3657  loss_cls_stage1: 0.463  loss_box_reg_stage1: 0.8859  loss_cls_stage2: 0.4393  loss_box_reg_stage2: 1.164  loss_mask: 0.1181  loss_rpn_cls: 0.02386  loss_rpn_loc: 0.06346  time: 0.6463  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 17:17:51] d2.utils.events INFO:  eta: 11:27:03  iter: 4459  total_loss: 3.54  loss_cls_stage0: 0.338  loss_box_reg_stage0: 0.3209  loss_cls_stage1: 0.3877  loss_box_reg_stage1: 0.8679  loss_cls_stage2: 0.3813  loss_box_reg_stage2: 1.024  loss_mask: 0.1113  loss_rpn_cls: 0.02079  loss_rpn_loc: 0.05032  time: 0.6463  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 17:18:04] d2.utils.events INFO:  eta: 11:26:50  iter: 4479  total_loss: 3.828  loss_cls_stage0: 0.4202  loss_box_reg_stage0: 0.3671  loss_cls_stage1: 0.4431  loss_box_reg_stage1: 0.8974  loss_cls_stage2: 0.412  loss_box_reg_stage2: 1.094  loss_mask: 0.1308  loss_rpn_cls: 0.019  loss_rpn_loc: 0.05376  time: 0.6464  data_time: 0.0054  lr: 0.00016  max_mem: 19672M
[07/28 17:18:17] d2.utils.events INFO:  eta: 11:27:23  iter: 4499  total_loss: 3.781  loss_cls_stage0: 0.378  loss_box_reg_stage0: 0.3524  loss_cls_stage1: 0.4607  loss_box_reg_stage1: 0.8346  loss_cls_stage2: 0.4497  loss_box_reg_stage2: 1.088  loss_mask: 0.1265  loss_rpn_cls: 0.02122  loss_rpn_loc: 0.04973  time: 0.6464  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 17:18:30] d2.utils.events INFO:  eta: 11:28:01  iter: 4519  total_loss: 3.91  loss_cls_stage0: 0.4231  loss_box_reg_stage0: 0.3612  loss_cls_stage1: 0.4646  loss_box_reg_stage1: 0.8895  loss_cls_stage2: 0.4073  loss_box_reg_stage2: 1.116  loss_mask: 0.1185  loss_rpn_cls: 0.01777  loss_rpn_loc: 0.05733  time: 0.6465  data_time: 0.0059  lr: 0.00016  max_mem: 19672M
[07/28 17:18:44] d2.utils.events INFO:  eta: 11:27:53  iter: 4539  total_loss: 3.462  loss_cls_stage0: 0.3274  loss_box_reg_stage0: 0.3151  loss_cls_stage1: 0.3944  loss_box_reg_stage1: 0.7989  loss_cls_stage2: 0.3802  loss_box_reg_stage2: 0.986  loss_mask: 0.1088  loss_rpn_cls: 0.01864  loss_rpn_loc: 0.04967  time: 0.6466  data_time: 0.0060  lr: 0.00016  max_mem: 19672M
[07/28 17:18:57] d2.utils.events INFO:  eta: 11:27:31  iter: 4559  total_loss: 3.476  loss_cls_stage0: 0.3462  loss_box_reg_stage0: 0.3137  loss_cls_stage1: 0.3817  loss_box_reg_stage1: 0.7824  loss_cls_stage2: 0.3914  loss_box_reg_stage2: 0.9757  loss_mask: 0.1068  loss_rpn_cls: 0.0325  loss_rpn_loc: 0.05865  time: 0.6466  data_time: 0.0059  lr: 0.00016  max_mem: 19672M
[07/28 17:19:09] d2.utils.events INFO:  eta: 11:27:06  iter: 4579  total_loss: 3.421  loss_cls_stage0: 0.3409  loss_box_reg_stage0: 0.3203  loss_cls_stage1: 0.3598  loss_box_reg_stage1: 0.7736  loss_cls_stage2: 0.3762  loss_box_reg_stage2: 1.023  loss_mask: 0.1054  loss_rpn_cls: 0.02428  loss_rpn_loc: 0.04981  time: 0.6464  data_time: 0.0053  lr: 0.00016  max_mem: 19672M
[07/28 17:19:22] d2.utils.events INFO:  eta: 11:27:01  iter: 4599  total_loss: 3.631  loss_cls_stage0: 0.3621  loss_box_reg_stage0: 0.3093  loss_cls_stage1: 0.4066  loss_box_reg_stage1: 0.8063  loss_cls_stage2: 0.3958  loss_box_reg_stage2: 1.003  loss_mask: 0.1099  loss_rpn_cls: 0.02087  loss_rpn_loc: 0.05554  time: 0.6465  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 17:19:35] d2.utils.events INFO:  eta: 11:26:48  iter: 4619  total_loss: 3.292  loss_cls_stage0: 0.3681  loss_box_reg_stage0: 0.3395  loss_cls_stage1: 0.3858  loss_box_reg_stage1: 0.765  loss_cls_stage2: 0.3768  loss_box_reg_stage2: 0.9604  loss_mask: 0.1105  loss_rpn_cls: 0.02318  loss_rpn_loc: 0.05219  time: 0.6465  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 17:19:48] d2.utils.events INFO:  eta: 11:26:34  iter: 4639  total_loss: 3.263  loss_cls_stage0: 0.3638  loss_box_reg_stage0: 0.3259  loss_cls_stage1: 0.3683  loss_box_reg_stage1: 0.7011  loss_cls_stage2: 0.3342  loss_box_reg_stage2: 0.9079  loss_mask: 0.1249  loss_rpn_cls: 0.03491  loss_rpn_loc: 0.07196  time: 0.6464  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:20:01] d2.utils.events INFO:  eta: 11:26:24  iter: 4659  total_loss: 3.495  loss_cls_stage0: 0.3853  loss_box_reg_stage0: 0.3331  loss_cls_stage1: 0.3705  loss_box_reg_stage1: 0.7938  loss_cls_stage2: 0.3624  loss_box_reg_stage2: 1.049  loss_mask: 0.119  loss_rpn_cls: 0.02514  loss_rpn_loc: 0.05588  time: 0.6464  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:20:14] d2.utils.events INFO:  eta: 11:26:08  iter: 4679  total_loss: 3.469  loss_cls_stage0: 0.3562  loss_box_reg_stage0: 0.3227  loss_cls_stage1: 0.4144  loss_box_reg_stage1: 0.7514  loss_cls_stage2: 0.42  loss_box_reg_stage2: 0.9747  loss_mask: 0.1208  loss_rpn_cls: 0.0191  loss_rpn_loc: 0.05758  time: 0.6465  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:20:27] d2.utils.events INFO:  eta: 11:25:23  iter: 4699  total_loss: 3.952  loss_cls_stage0: 0.4159  loss_box_reg_stage0: 0.3589  loss_cls_stage1: 0.4514  loss_box_reg_stage1: 0.8395  loss_cls_stage2: 0.4635  loss_box_reg_stage2: 1.057  loss_mask: 0.131  loss_rpn_cls: 0.02862  loss_rpn_loc: 0.05509  time: 0.6465  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:20:40] d2.utils.events INFO:  eta: 11:25:41  iter: 4719  total_loss: 3.379  loss_cls_stage0: 0.3304  loss_box_reg_stage0: 0.3219  loss_cls_stage1: 0.3597  loss_box_reg_stage1: 0.7588  loss_cls_stage2: 0.3671  loss_box_reg_stage2: 1.018  loss_mask: 0.1049  loss_rpn_cls: 0.02451  loss_rpn_loc: 0.047  time: 0.6464  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:20:53] d2.utils.events INFO:  eta: 11:25:40  iter: 4739  total_loss: 3.59  loss_cls_stage0: 0.3592  loss_box_reg_stage0: 0.3568  loss_cls_stage1: 0.3884  loss_box_reg_stage1: 0.8306  loss_cls_stage2: 0.3846  loss_box_reg_stage2: 1.049  loss_mask: 0.1307  loss_rpn_cls: 0.02196  loss_rpn_loc: 0.05785  time: 0.6464  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 17:21:06] d2.utils.events INFO:  eta: 11:25:39  iter: 4759  total_loss: 3.643  loss_cls_stage0: 0.3715  loss_box_reg_stage0: 0.3364  loss_cls_stage1: 0.4287  loss_box_reg_stage1: 0.8386  loss_cls_stage2: 0.4021  loss_box_reg_stage2: 1.089  loss_mask: 0.1193  loss_rpn_cls: 0.02031  loss_rpn_loc: 0.05583  time: 0.6465  data_time: 0.0059  lr: 0.00016  max_mem: 19672M
[07/28 17:21:18] d2.utils.events INFO:  eta: 11:24:10  iter: 4779  total_loss: 3.513  loss_cls_stage0: 0.3497  loss_box_reg_stage0: 0.3469  loss_cls_stage1: 0.386  loss_box_reg_stage1: 0.7929  loss_cls_stage2: 0.3816  loss_box_reg_stage2: 1.036  loss_mask: 0.1158  loss_rpn_cls: 0.02509  loss_rpn_loc: 0.06769  time: 0.6464  data_time: 0.0059  lr: 0.00016  max_mem: 19672M
[07/28 17:21:32] d2.utils.events INFO:  eta: 11:23:37  iter: 4799  total_loss: 3.196  loss_cls_stage0: 0.3398  loss_box_reg_stage0: 0.3163  loss_cls_stage1: 0.3711  loss_box_reg_stage1: 0.7307  loss_cls_stage2: 0.3367  loss_box_reg_stage2: 0.933  loss_mask: 0.1042  loss_rpn_cls: 0.01613  loss_rpn_loc: 0.04633  time: 0.6464  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 17:21:44] d2.utils.events INFO:  eta: 11:23:24  iter: 4819  total_loss: 3.232  loss_cls_stage0: 0.3424  loss_box_reg_stage0: 0.3193  loss_cls_stage1: 0.3469  loss_box_reg_stage1: 0.7015  loss_cls_stage2: 0.3284  loss_box_reg_stage2: 0.9789  loss_mask: 0.1165  loss_rpn_cls: 0.0198  loss_rpn_loc: 0.05866  time: 0.6463  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:21:57] d2.utils.events INFO:  eta: 11:22:48  iter: 4839  total_loss: 3.64  loss_cls_stage0: 0.3751  loss_box_reg_stage0: 0.3433  loss_cls_stage1: 0.4216  loss_box_reg_stage1: 0.8249  loss_cls_stage2: 0.4174  loss_box_reg_stage2: 1.093  loss_mask: 0.115  loss_rpn_cls: 0.01436  loss_rpn_loc: 0.05636  time: 0.6463  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 17:22:10] d2.utils.events INFO:  eta: 11:23:27  iter: 4859  total_loss: 3.525  loss_cls_stage0: 0.3483  loss_box_reg_stage0: 0.3423  loss_cls_stage1: 0.3675  loss_box_reg_stage1: 0.8551  loss_cls_stage2: 0.3803  loss_box_reg_stage2: 1.185  loss_mask: 0.1124  loss_rpn_cls: 0.01915  loss_rpn_loc: 0.04773  time: 0.6464  data_time: 0.0052  lr: 0.00016  max_mem: 19672M
[07/28 17:22:23] d2.utils.events INFO:  eta: 11:23:38  iter: 4879  total_loss: 3.775  loss_cls_stage0: 0.3502  loss_box_reg_stage0: 0.3323  loss_cls_stage1: 0.4215  loss_box_reg_stage1: 0.8517  loss_cls_stage2: 0.4046  loss_box_reg_stage2: 1.07  loss_mask: 0.1145  loss_rpn_cls: 0.02005  loss_rpn_loc: 0.05426  time: 0.6464  data_time: 0.0054  lr: 0.00016  max_mem: 19672M
[07/28 17:22:36] d2.utils.events INFO:  eta: 11:23:43  iter: 4899  total_loss: 3.343  loss_cls_stage0: 0.3399  loss_box_reg_stage0: 0.3322  loss_cls_stage1: 0.366  loss_box_reg_stage1: 0.7892  loss_cls_stage2: 0.3384  loss_box_reg_stage2: 1.025  loss_mask: 0.1094  loss_rpn_cls: 0.03539  loss_rpn_loc: 0.07005  time: 0.6464  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 17:22:49] d2.utils.events INFO:  eta: 11:23:15  iter: 4919  total_loss: 3.463  loss_cls_stage0: 0.3435  loss_box_reg_stage0: 0.345  loss_cls_stage1: 0.4112  loss_box_reg_stage1: 0.7809  loss_cls_stage2: 0.379  loss_box_reg_stage2: 0.9607  loss_mask: 0.109  loss_rpn_cls: 0.02063  loss_rpn_loc: 0.05294  time: 0.6463  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 17:23:01] d2.utils.events INFO:  eta: 11:23:02  iter: 4939  total_loss: 3.804  loss_cls_stage0: 0.3733  loss_box_reg_stage0: 0.3306  loss_cls_stage1: 0.4178  loss_box_reg_stage1: 0.8744  loss_cls_stage2: 0.3932  loss_box_reg_stage2: 1.048  loss_mask: 0.1182  loss_rpn_cls: 0.02584  loss_rpn_loc: 0.09225  time: 0.6463  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 17:23:14] d2.utils.events INFO:  eta: 11:22:49  iter: 4959  total_loss: 3.479  loss_cls_stage0: 0.3612  loss_box_reg_stage0: 0.3325  loss_cls_stage1: 0.38  loss_box_reg_stage1: 0.8157  loss_cls_stage2: 0.373  loss_box_reg_stage2: 1.026  loss_mask: 0.1114  loss_rpn_cls: 0.03001  loss_rpn_loc: 0.06051  time: 0.6463  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 17:23:28] d2.utils.events INFO:  eta: 11:22:34  iter: 4979  total_loss: 3.398  loss_cls_stage0: 0.3479  loss_box_reg_stage0: 0.315  loss_cls_stage1: 0.3844  loss_box_reg_stage1: 0.8281  loss_cls_stage2: 0.3654  loss_box_reg_stage2: 1.055  loss_mask: 0.1072  loss_rpn_cls: 0.02397  loss_rpn_loc: 0.06081  time: 0.6463  data_time: 0.0060  lr: 0.00016  max_mem: 19672M
[07/28 17:23:40] fvcore.common.checkpoint INFO: Saving checkpoint to ./output/model_0004999.pth
[07/28 17:23:43] d2.data.datasets.coco INFO: Loading datasets/tless/tless_annotations_test.json takes 1.42 seconds.
[07/28 17:23:43] d2.data.datasets.coco INFO: Loaded 10080 images in COCO format from datasets/tless/tless_annotations_test.json
[07/28 17:23:44] d2.data.build INFO: Distribution of instances among all 30 categories:
[36m|  category  | #instances   | category   | #instances   | category   | #instances   |
|:----------:|:-------------|:-----------|:-------------|:-----------|:-------------|
|     1      | 8190         | 2          | 4720         | 3          | 4000         |
|     4      | 6255         | 5          | 1960         | 6          | 1007         |
|     7      | 2511         | 8          | 1512         | 9          | 2511         |
|     10     | 1495         | 11         | 1907         | 12         | 1471         |
|     13     | 1486         | 14         | 1505         | 15         | 1509         |
|     16     | 1993         | 17         | 1507         | 18         | 1495         |
|     19     | 1985         | 20         | 2477         | 21         | 1880         |
|     22     | 1977         | 23         | 2514         | 24         | 1983         |
|     25     | 1001         | 26         | 1008         | 27         | 983          |
|     28     | 1973         | 29         | 1006         | 30         | 1487         |
|            |              |            |              |            |              |
|   total    | 67308        |            |              |            |              |[0m
[07/28 17:23:44] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[07/28 17:23:44] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[07/28 17:23:44] d2.data.common INFO: Serializing 10080 elements to byte tensors and concatenating them all ...
[07/28 17:23:44] d2.data.common INFO: Serialized dataset takes 50.77 MiB
[07/28 17:23:45] d2.evaluation.evaluator INFO: Start inference on 10080 batches
[07/28 17:23:46] d2.evaluation.evaluator INFO: Inference done 11/10080. Dataloading: 0.0008 s/iter. Inference: 0.1040 s/iter. Eval: 0.0073 s/iter. Total: 0.1121 s/iter. ETA=0:18:48
[07/28 17:23:52] d2.evaluation.evaluator INFO: Inference done 56/10080. Dataloading: 0.0009 s/iter. Inference: 0.1054 s/iter. Eval: 0.0056 s/iter. Total: 0.1120 s/iter. ETA=0:18:42
[07/28 17:23:57] d2.evaluation.evaluator INFO: Inference done 100/10080. Dataloading: 0.0010 s/iter. Inference: 0.1061 s/iter. Eval: 0.0058 s/iter. Total: 0.1129 s/iter. ETA=0:18:46
[07/28 17:24:02] d2.evaluation.evaluator INFO: Inference done 144/10080. Dataloading: 0.0010 s/iter. Inference: 0.1062 s/iter. Eval: 0.0059 s/iter. Total: 0.1132 s/iter. ETA=0:18:44
[07/28 17:24:07] d2.evaluation.evaluator INFO: Inference done 190/10080. Dataloading: 0.0010 s/iter. Inference: 0.1059 s/iter. Eval: 0.0056 s/iter. Total: 0.1126 s/iter. ETA=0:18:33
[07/28 17:24:12] d2.evaluation.evaluator INFO: Inference done 235/10080. Dataloading: 0.0010 s/iter. Inference: 0.1059 s/iter. Eval: 0.0055 s/iter. Total: 0.1125 s/iter. ETA=0:18:27
[07/28 17:24:17] d2.evaluation.evaluator INFO: Inference done 280/10080. Dataloading: 0.0010 s/iter. Inference: 0.1058 s/iter. Eval: 0.0054 s/iter. Total: 0.1123 s/iter. ETA=0:18:20
[07/28 17:24:22] d2.evaluation.evaluator INFO: Inference done 325/10080. Dataloading: 0.0010 s/iter. Inference: 0.1057 s/iter. Eval: 0.0053 s/iter. Total: 0.1122 s/iter. ETA=0:18:14
[07/28 17:24:27] d2.evaluation.evaluator INFO: Inference done 370/10080. Dataloading: 0.0010 s/iter. Inference: 0.1058 s/iter. Eval: 0.0053 s/iter. Total: 0.1122 s/iter. ETA=0:18:09
[07/28 17:24:32] d2.evaluation.evaluator INFO: Inference done 415/10080. Dataloading: 0.0010 s/iter. Inference: 0.1058 s/iter. Eval: 0.0053 s/iter. Total: 0.1121 s/iter. ETA=0:18:03
[07/28 17:24:37] d2.evaluation.evaluator INFO: Inference done 460/10080. Dataloading: 0.0010 s/iter. Inference: 0.1058 s/iter. Eval: 0.0052 s/iter. Total: 0.1121 s/iter. ETA=0:17:58
[07/28 17:24:42] d2.evaluation.evaluator INFO: Inference done 506/10080. Dataloading: 0.0010 s/iter. Inference: 0.1058 s/iter. Eval: 0.0051 s/iter. Total: 0.1120 s/iter. ETA=0:17:52
[07/28 17:24:47] d2.evaluation.evaluator INFO: Inference done 552/10080. Dataloading: 0.0010 s/iter. Inference: 0.1058 s/iter. Eval: 0.0050 s/iter. Total: 0.1119 s/iter. ETA=0:17:46
[07/28 17:24:52] d2.evaluation.evaluator INFO: Inference done 596/10080. Dataloading: 0.0010 s/iter. Inference: 0.1059 s/iter. Eval: 0.0051 s/iter. Total: 0.1121 s/iter. ETA=0:17:43
[07/28 17:24:57] d2.evaluation.evaluator INFO: Inference done 641/10080. Dataloading: 0.0010 s/iter. Inference: 0.1059 s/iter. Eval: 0.0051 s/iter. Total: 0.1121 s/iter. ETA=0:17:38
[07/28 17:25:02] d2.evaluation.evaluator INFO: Inference done 686/10080. Dataloading: 0.0011 s/iter. Inference: 0.1059 s/iter. Eval: 0.0051 s/iter. Total: 0.1122 s/iter. ETA=0:17:33
[07/28 17:25:07] d2.evaluation.evaluator INFO: Inference done 732/10080. Dataloading: 0.0011 s/iter. Inference: 0.1058 s/iter. Eval: 0.0051 s/iter. Total: 0.1121 s/iter. ETA=0:17:27
[07/28 17:25:12] d2.evaluation.evaluator INFO: Inference done 777/10080. Dataloading: 0.0011 s/iter. Inference: 0.1059 s/iter. Eval: 0.0051 s/iter. Total: 0.1121 s/iter. ETA=0:17:22
[07/28 17:25:17] d2.evaluation.evaluator INFO: Inference done 822/10080. Dataloading: 0.0011 s/iter. Inference: 0.1059 s/iter. Eval: 0.0051 s/iter. Total: 0.1121 s/iter. ETA=0:17:17
[07/28 17:25:22] d2.evaluation.evaluator INFO: Inference done 867/10080. Dataloading: 0.0011 s/iter. Inference: 0.1059 s/iter. Eval: 0.0051 s/iter. Total: 0.1121 s/iter. ETA=0:17:12
[07/28 17:25:28] d2.evaluation.evaluator INFO: Inference done 912/10080. Dataloading: 0.0011 s/iter. Inference: 0.1059 s/iter. Eval: 0.0051 s/iter. Total: 0.1122 s/iter. ETA=0:17:08
[07/28 17:25:33] d2.evaluation.evaluator INFO: Inference done 958/10080. Dataloading: 0.0011 s/iter. Inference: 0.1059 s/iter. Eval: 0.0051 s/iter. Total: 0.1121 s/iter. ETA=0:17:02
[07/28 17:25:38] d2.evaluation.evaluator INFO: Inference done 1003/10080. Dataloading: 0.0011 s/iter. Inference: 0.1059 s/iter. Eval: 0.0051 s/iter. Total: 0.1121 s/iter. ETA=0:16:57
[07/28 17:25:43] d2.evaluation.evaluator INFO: Inference done 1048/10080. Dataloading: 0.0011 s/iter. Inference: 0.1059 s/iter. Eval: 0.0052 s/iter. Total: 0.1122 s/iter. ETA=0:16:53
[07/28 17:25:48] d2.evaluation.evaluator INFO: Inference done 1093/10080. Dataloading: 0.0011 s/iter. Inference: 0.1058 s/iter. Eval: 0.0052 s/iter. Total: 0.1121 s/iter. ETA=0:16:47
[07/28 17:25:53] d2.evaluation.evaluator INFO: Inference done 1137/10080. Dataloading: 0.0011 s/iter. Inference: 0.1058 s/iter. Eval: 0.0053 s/iter. Total: 0.1123 s/iter. ETA=0:16:44
[07/28 17:25:58] d2.evaluation.evaluator INFO: Inference done 1182/10080. Dataloading: 0.0011 s/iter. Inference: 0.1058 s/iter. Eval: 0.0053 s/iter. Total: 0.1123 s/iter. ETA=0:16:39
[07/28 17:26:03] d2.evaluation.evaluator INFO: Inference done 1227/10080. Dataloading: 0.0011 s/iter. Inference: 0.1058 s/iter. Eval: 0.0054 s/iter. Total: 0.1123 s/iter. ETA=0:16:34
[07/28 17:26:08] d2.evaluation.evaluator INFO: Inference done 1272/10080. Dataloading: 0.0011 s/iter. Inference: 0.1058 s/iter. Eval: 0.0054 s/iter. Total: 0.1123 s/iter. ETA=0:16:28
[07/28 17:26:13] d2.evaluation.evaluator INFO: Inference done 1317/10080. Dataloading: 0.0011 s/iter. Inference: 0.1058 s/iter. Eval: 0.0054 s/iter. Total: 0.1122 s/iter. ETA=0:16:23
[07/28 17:26:18] d2.evaluation.evaluator INFO: Inference done 1363/10080. Dataloading: 0.0011 s/iter. Inference: 0.1057 s/iter. Eval: 0.0054 s/iter. Total: 0.1122 s/iter. ETA=0:16:18
[07/28 17:26:23] d2.evaluation.evaluator INFO: Inference done 1407/10080. Dataloading: 0.0011 s/iter. Inference: 0.1058 s/iter. Eval: 0.0054 s/iter. Total: 0.1123 s/iter. ETA=0:16:13
[07/28 17:26:28] d2.evaluation.evaluator INFO: Inference done 1451/10080. Dataloading: 0.0011 s/iter. Inference: 0.1058 s/iter. Eval: 0.0054 s/iter. Total: 0.1123 s/iter. ETA=0:16:09
[07/28 17:26:33] d2.evaluation.evaluator INFO: Inference done 1496/10080. Dataloading: 0.0011 s/iter. Inference: 0.1058 s/iter. Eval: 0.0054 s/iter. Total: 0.1123 s/iter. ETA=0:16:04
[07/28 17:26:38] d2.evaluation.evaluator INFO: Inference done 1540/10080. Dataloading: 0.0011 s/iter. Inference: 0.1058 s/iter. Eval: 0.0054 s/iter. Total: 0.1124 s/iter. ETA=0:15:59
[07/28 17:26:43] d2.evaluation.evaluator INFO: Inference done 1586/10080. Dataloading: 0.0011 s/iter. Inference: 0.1058 s/iter. Eval: 0.0054 s/iter. Total: 0.1123 s/iter. ETA=0:15:54
[07/28 17:26:48] d2.evaluation.evaluator INFO: Inference done 1631/10080. Dataloading: 0.0011 s/iter. Inference: 0.1058 s/iter. Eval: 0.0054 s/iter. Total: 0.1123 s/iter. ETA=0:15:49
[07/28 17:26:54] d2.evaluation.evaluator INFO: Inference done 1675/10080. Dataloading: 0.0011 s/iter. Inference: 0.1058 s/iter. Eval: 0.0055 s/iter. Total: 0.1124 s/iter. ETA=0:15:44
[07/28 17:26:59] d2.evaluation.evaluator INFO: Inference done 1720/10080. Dataloading: 0.0011 s/iter. Inference: 0.1057 s/iter. Eval: 0.0055 s/iter. Total: 0.1124 s/iter. ETA=0:15:39
[07/28 17:27:04] d2.evaluation.evaluator INFO: Inference done 1764/10080. Dataloading: 0.0011 s/iter. Inference: 0.1058 s/iter. Eval: 0.0055 s/iter. Total: 0.1124 s/iter. ETA=0:15:34
[07/28 17:27:09] d2.evaluation.evaluator INFO: Inference done 1806/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0055 s/iter. Total: 0.1126 s/iter. ETA=0:15:31
[07/28 17:27:14] d2.evaluation.evaluator INFO: Inference done 1852/10080. Dataloading: 0.0011 s/iter. Inference: 0.1059 s/iter. Eval: 0.0055 s/iter. Total: 0.1126 s/iter. ETA=0:15:26
[07/28 17:27:19] d2.evaluation.evaluator INFO: Inference done 1897/10080. Dataloading: 0.0011 s/iter. Inference: 0.1059 s/iter. Eval: 0.0055 s/iter. Total: 0.1126 s/iter. ETA=0:15:21
[07/28 17:27:24] d2.evaluation.evaluator INFO: Inference done 1942/10080. Dataloading: 0.0011 s/iter. Inference: 0.1059 s/iter. Eval: 0.0056 s/iter. Total: 0.1126 s/iter. ETA=0:15:16
[07/28 17:27:29] d2.evaluation.evaluator INFO: Inference done 1988/10080. Dataloading: 0.0011 s/iter. Inference: 0.1059 s/iter. Eval: 0.0055 s/iter. Total: 0.1126 s/iter. ETA=0:15:10
[07/28 17:27:34] d2.evaluation.evaluator INFO: Inference done 2032/10080. Dataloading: 0.0011 s/iter. Inference: 0.1059 s/iter. Eval: 0.0055 s/iter. Total: 0.1126 s/iter. ETA=0:15:06
[07/28 17:27:39] d2.evaluation.evaluator INFO: Inference done 2076/10080. Dataloading: 0.0011 s/iter. Inference: 0.1059 s/iter. Eval: 0.0056 s/iter. Total: 0.1126 s/iter. ETA=0:15:01
[07/28 17:27:44] d2.evaluation.evaluator INFO: Inference done 2120/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0056 s/iter. Total: 0.1127 s/iter. ETA=0:14:56
[07/28 17:27:49] d2.evaluation.evaluator INFO: Inference done 2164/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0056 s/iter. Total: 0.1127 s/iter. ETA=0:14:52
[07/28 17:27:54] d2.evaluation.evaluator INFO: Inference done 2209/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0056 s/iter. Total: 0.1127 s/iter. ETA=0:14:47
[07/28 17:27:59] d2.evaluation.evaluator INFO: Inference done 2253/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0056 s/iter. Total: 0.1128 s/iter. ETA=0:14:42
[07/28 17:28:04] d2.evaluation.evaluator INFO: Inference done 2298/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0056 s/iter. Total: 0.1128 s/iter. ETA=0:14:37
[07/28 17:28:09] d2.evaluation.evaluator INFO: Inference done 2342/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0057 s/iter. Total: 0.1128 s/iter. ETA=0:14:32
[07/28 17:28:14] d2.evaluation.evaluator INFO: Inference done 2386/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0057 s/iter. Total: 0.1128 s/iter. ETA=0:14:27
[07/28 17:28:19] d2.evaluation.evaluator INFO: Inference done 2430/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0057 s/iter. Total: 0.1128 s/iter. ETA=0:14:23
[07/28 17:28:25] d2.evaluation.evaluator INFO: Inference done 2475/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0057 s/iter. Total: 0.1128 s/iter. ETA=0:14:18
[07/28 17:28:30] d2.evaluation.evaluator INFO: Inference done 2519/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0057 s/iter. Total: 0.1129 s/iter. ETA=0:14:13
[07/28 17:28:35] d2.evaluation.evaluator INFO: Inference done 2563/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0057 s/iter. Total: 0.1129 s/iter. ETA=0:14:08
[07/28 17:28:40] d2.evaluation.evaluator INFO: Inference done 2607/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0058 s/iter. Total: 0.1129 s/iter. ETA=0:14:03
[07/28 17:28:45] d2.evaluation.evaluator INFO: Inference done 2651/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0058 s/iter. Total: 0.1130 s/iter. ETA=0:13:59
[07/28 17:28:50] d2.evaluation.evaluator INFO: Inference done 2695/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0058 s/iter. Total: 0.1130 s/iter. ETA=0:13:54
[07/28 17:28:55] d2.evaluation.evaluator INFO: Inference done 2740/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0059 s/iter. Total: 0.1130 s/iter. ETA=0:13:49
[07/28 17:29:00] d2.evaluation.evaluator INFO: Inference done 2786/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0059 s/iter. Total: 0.1129 s/iter. ETA=0:13:43
[07/28 17:29:05] d2.evaluation.evaluator INFO: Inference done 2831/10080. Dataloading: 0.0011 s/iter. Inference: 0.1059 s/iter. Eval: 0.0059 s/iter. Total: 0.1129 s/iter. ETA=0:13:38
[07/28 17:29:10] d2.evaluation.evaluator INFO: Inference done 2875/10080. Dataloading: 0.0011 s/iter. Inference: 0.1059 s/iter. Eval: 0.0059 s/iter. Total: 0.1130 s/iter. ETA=0:13:33
[07/28 17:29:15] d2.evaluation.evaluator INFO: Inference done 2921/10080. Dataloading: 0.0011 s/iter. Inference: 0.1059 s/iter. Eval: 0.0059 s/iter. Total: 0.1129 s/iter. ETA=0:13:28
[07/28 17:29:20] d2.evaluation.evaluator INFO: Inference done 2966/10080. Dataloading: 0.0011 s/iter. Inference: 0.1059 s/iter. Eval: 0.0059 s/iter. Total: 0.1129 s/iter. ETA=0:13:23
[07/28 17:29:25] d2.evaluation.evaluator INFO: Inference done 3012/10080. Dataloading: 0.0011 s/iter. Inference: 0.1059 s/iter. Eval: 0.0058 s/iter. Total: 0.1129 s/iter. ETA=0:13:17
[07/28 17:29:30] d2.evaluation.evaluator INFO: Inference done 3055/10080. Dataloading: 0.0011 s/iter. Inference: 0.1059 s/iter. Eval: 0.0059 s/iter. Total: 0.1129 s/iter. ETA=0:13:13
[07/28 17:29:35] d2.evaluation.evaluator INFO: Inference done 3098/10080. Dataloading: 0.0011 s/iter. Inference: 0.1059 s/iter. Eval: 0.0059 s/iter. Total: 0.1130 s/iter. ETA=0:13:08
[07/28 17:29:40] d2.evaluation.evaluator INFO: Inference done 3141/10080. Dataloading: 0.0011 s/iter. Inference: 0.1059 s/iter. Eval: 0.0060 s/iter. Total: 0.1130 s/iter. ETA=0:13:04
[07/28 17:29:45] d2.evaluation.evaluator INFO: Inference done 3184/10080. Dataloading: 0.0011 s/iter. Inference: 0.1059 s/iter. Eval: 0.0060 s/iter. Total: 0.1131 s/iter. ETA=0:12:59
[07/28 17:29:50] d2.evaluation.evaluator INFO: Inference done 3227/10080. Dataloading: 0.0011 s/iter. Inference: 0.1059 s/iter. Eval: 0.0061 s/iter. Total: 0.1131 s/iter. ETA=0:12:55
[07/28 17:29:55] d2.evaluation.evaluator INFO: Inference done 3271/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0061 s/iter. Total: 0.1132 s/iter. ETA=0:12:50
[07/28 17:30:00] d2.evaluation.evaluator INFO: Inference done 3315/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0061 s/iter. Total: 0.1132 s/iter. ETA=0:12:45
[07/28 17:30:06] d2.evaluation.evaluator INFO: Inference done 3359/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0062 s/iter. Total: 0.1132 s/iter. ETA=0:12:41
[07/28 17:30:11] d2.evaluation.evaluator INFO: Inference done 3403/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0062 s/iter. Total: 0.1132 s/iter. ETA=0:12:36
[07/28 17:30:16] d2.evaluation.evaluator INFO: Inference done 3447/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0062 s/iter. Total: 0.1133 s/iter. ETA=0:12:31
[07/28 17:30:21] d2.evaluation.evaluator INFO: Inference done 3491/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0062 s/iter. Total: 0.1133 s/iter. ETA=0:12:26
[07/28 17:30:26] d2.evaluation.evaluator INFO: Inference done 3534/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0063 s/iter. Total: 0.1134 s/iter. ETA=0:12:22
[07/28 17:30:31] d2.evaluation.evaluator INFO: Inference done 3578/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0063 s/iter. Total: 0.1134 s/iter. ETA=0:12:17
[07/28 17:30:36] d2.evaluation.evaluator INFO: Inference done 3622/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0063 s/iter. Total: 0.1134 s/iter. ETA=0:12:12
[07/28 17:30:41] d2.evaluation.evaluator INFO: Inference done 3665/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0063 s/iter. Total: 0.1135 s/iter. ETA=0:12:07
[07/28 17:30:46] d2.evaluation.evaluator INFO: Inference done 3708/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0063 s/iter. Total: 0.1135 s/iter. ETA=0:12:03
[07/28 17:30:51] d2.evaluation.evaluator INFO: Inference done 3752/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0064 s/iter. Total: 0.1135 s/iter. ETA=0:11:58
[07/28 17:30:56] d2.evaluation.evaluator INFO: Inference done 3796/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0064 s/iter. Total: 0.1135 s/iter. ETA=0:11:53
[07/28 17:31:01] d2.evaluation.evaluator INFO: Inference done 3840/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0064 s/iter. Total: 0.1135 s/iter. ETA=0:11:48
[07/28 17:31:06] d2.evaluation.evaluator INFO: Inference done 3884/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0064 s/iter. Total: 0.1135 s/iter. ETA=0:11:43
[07/28 17:31:11] d2.evaluation.evaluator INFO: Inference done 3929/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0064 s/iter. Total: 0.1135 s/iter. ETA=0:11:38
[07/28 17:31:16] d2.evaluation.evaluator INFO: Inference done 3973/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0064 s/iter. Total: 0.1136 s/iter. ETA=0:11:33
[07/28 17:31:21] d2.evaluation.evaluator INFO: Inference done 4018/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0064 s/iter. Total: 0.1135 s/iter. ETA=0:11:28
[07/28 17:31:27] d2.evaluation.evaluator INFO: Inference done 4061/10080. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0064 s/iter. Total: 0.1136 s/iter. ETA=0:11:23
[07/28 17:31:32] d2.evaluation.evaluator INFO: Inference done 4103/10080. Dataloading: 0.0011 s/iter. Inference: 0.1061 s/iter. Eval: 0.0065 s/iter. Total: 0.1137 s/iter. ETA=0:11:19
[07/28 17:31:37] d2.evaluation.evaluator INFO: Inference done 4144/10080. Dataloading: 0.0011 s/iter. Inference: 0.1061 s/iter. Eval: 0.0066 s/iter. Total: 0.1137 s/iter. ETA=0:11:15
[07/28 17:31:42] d2.evaluation.evaluator INFO: Inference done 4186/10080. Dataloading: 0.0011 s/iter. Inference: 0.1061 s/iter. Eval: 0.0066 s/iter. Total: 0.1138 s/iter. ETA=0:11:10
[07/28 17:31:47] d2.evaluation.evaluator INFO: Inference done 4228/10080. Dataloading: 0.0011 s/iter. Inference: 0.1061 s/iter. Eval: 0.0067 s/iter. Total: 0.1139 s/iter. ETA=0:11:06
[07/28 17:31:52] d2.evaluation.evaluator INFO: Inference done 4270/10080. Dataloading: 0.0011 s/iter. Inference: 0.1061 s/iter. Eval: 0.0067 s/iter. Total: 0.1139 s/iter. ETA=0:11:02
[07/28 17:31:57] d2.evaluation.evaluator INFO: Inference done 4313/10080. Dataloading: 0.0011 s/iter. Inference: 0.1061 s/iter. Eval: 0.0068 s/iter. Total: 0.1140 s/iter. ETA=0:10:57
[07/28 17:32:02] d2.evaluation.evaluator INFO: Inference done 4355/10080. Dataloading: 0.0011 s/iter. Inference: 0.1061 s/iter. Eval: 0.0068 s/iter. Total: 0.1141 s/iter. ETA=0:10:53
[07/28 17:32:07] d2.evaluation.evaluator INFO: Inference done 4396/10080. Dataloading: 0.0011 s/iter. Inference: 0.1061 s/iter. Eval: 0.0069 s/iter. Total: 0.1141 s/iter. ETA=0:10:48
[07/28 17:32:12] d2.evaluation.evaluator INFO: Inference done 4438/10080. Dataloading: 0.0011 s/iter. Inference: 0.1061 s/iter. Eval: 0.0069 s/iter. Total: 0.1142 s/iter. ETA=0:10:44
[07/28 17:32:17] d2.evaluation.evaluator INFO: Inference done 4480/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0070 s/iter. Total: 0.1143 s/iter. ETA=0:10:39
[07/28 17:32:22] d2.evaluation.evaluator INFO: Inference done 4522/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0071 s/iter. Total: 0.1143 s/iter. ETA=0:10:35
[07/28 17:32:27] d2.evaluation.evaluator INFO: Inference done 4566/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0071 s/iter. Total: 0.1144 s/iter. ETA=0:10:30
[07/28 17:32:32] d2.evaluation.evaluator INFO: Inference done 4609/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0071 s/iter. Total: 0.1144 s/iter. ETA=0:10:25
[07/28 17:32:37] d2.evaluation.evaluator INFO: Inference done 4652/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0071 s/iter. Total: 0.1144 s/iter. ETA=0:10:20
[07/28 17:32:42] d2.evaluation.evaluator INFO: Inference done 4694/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0072 s/iter. Total: 0.1144 s/iter. ETA=0:10:16
[07/28 17:32:48] d2.evaluation.evaluator INFO: Inference done 4737/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0072 s/iter. Total: 0.1145 s/iter. ETA=0:10:11
[07/28 17:32:53] d2.evaluation.evaluator INFO: Inference done 4781/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0072 s/iter. Total: 0.1145 s/iter. ETA=0:10:06
[07/28 17:32:58] d2.evaluation.evaluator INFO: Inference done 4825/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0072 s/iter. Total: 0.1145 s/iter. ETA=0:10:01
[07/28 17:33:03] d2.evaluation.evaluator INFO: Inference done 4869/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0072 s/iter. Total: 0.1145 s/iter. ETA=0:09:56
[07/28 17:33:08] d2.evaluation.evaluator INFO: Inference done 4912/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0072 s/iter. Total: 0.1145 s/iter. ETA=0:09:51
[07/28 17:33:13] d2.evaluation.evaluator INFO: Inference done 4956/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0072 s/iter. Total: 0.1145 s/iter. ETA=0:09:46
[07/28 17:33:18] d2.evaluation.evaluator INFO: Inference done 5000/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0072 s/iter. Total: 0.1145 s/iter. ETA=0:09:41
[07/28 17:33:23] d2.evaluation.evaluator INFO: Inference done 5044/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0073 s/iter. Total: 0.1145 s/iter. ETA=0:09:36
[07/28 17:33:28] d2.evaluation.evaluator INFO: Inference done 5089/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0072 s/iter. Total: 0.1145 s/iter. ETA=0:09:31
[07/28 17:33:33] d2.evaluation.evaluator INFO: Inference done 5133/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0072 s/iter. Total: 0.1145 s/iter. ETA=0:09:26
[07/28 17:33:38] d2.evaluation.evaluator INFO: Inference done 5177/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0072 s/iter. Total: 0.1145 s/iter. ETA=0:09:21
[07/28 17:33:43] d2.evaluation.evaluator INFO: Inference done 5221/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0072 s/iter. Total: 0.1145 s/iter. ETA=0:09:16
[07/28 17:33:48] d2.evaluation.evaluator INFO: Inference done 5265/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0072 s/iter. Total: 0.1145 s/iter. ETA=0:09:11
[07/28 17:33:53] d2.evaluation.evaluator INFO: Inference done 5311/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0072 s/iter. Total: 0.1145 s/iter. ETA=0:09:06
[07/28 17:33:58] d2.evaluation.evaluator INFO: Inference done 5351/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0072 s/iter. Total: 0.1146 s/iter. ETA=0:09:01
[07/28 17:34:03] d2.evaluation.evaluator INFO: Inference done 5396/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0072 s/iter. Total: 0.1146 s/iter. ETA=0:08:56
[07/28 17:34:08] d2.evaluation.evaluator INFO: Inference done 5440/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0072 s/iter. Total: 0.1146 s/iter. ETA=0:08:51
[07/28 17:34:13] d2.evaluation.evaluator INFO: Inference done 5484/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0072 s/iter. Total: 0.1146 s/iter. ETA=0:08:46
[07/28 17:34:19] d2.evaluation.evaluator INFO: Inference done 5529/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0072 s/iter. Total: 0.1145 s/iter. ETA=0:08:41
[07/28 17:34:24] d2.evaluation.evaluator INFO: Inference done 5573/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0072 s/iter. Total: 0.1146 s/iter. ETA=0:08:36
[07/28 17:34:29] d2.evaluation.evaluator INFO: Inference done 5616/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0072 s/iter. Total: 0.1146 s/iter. ETA=0:08:31
[07/28 17:34:34] d2.evaluation.evaluator INFO: Inference done 5660/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0072 s/iter. Total: 0.1146 s/iter. ETA=0:08:26
[07/28 17:34:39] d2.evaluation.evaluator INFO: Inference done 5703/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0072 s/iter. Total: 0.1146 s/iter. ETA=0:08:21
[07/28 17:34:44] d2.evaluation.evaluator INFO: Inference done 5747/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0072 s/iter. Total: 0.1146 s/iter. ETA=0:08:16
[07/28 17:34:49] d2.evaluation.evaluator INFO: Inference done 5791/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0072 s/iter. Total: 0.1146 s/iter. ETA=0:08:11
[07/28 17:34:54] d2.evaluation.evaluator INFO: Inference done 5835/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0072 s/iter. Total: 0.1146 s/iter. ETA=0:08:06
[07/28 17:34:59] d2.evaluation.evaluator INFO: Inference done 5879/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0073 s/iter. Total: 0.1146 s/iter. ETA=0:08:01
[07/28 17:35:04] d2.evaluation.evaluator INFO: Inference done 5923/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0073 s/iter. Total: 0.1146 s/iter. ETA=0:07:56
[07/28 17:35:09] d2.evaluation.evaluator INFO: Inference done 5967/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0073 s/iter. Total: 0.1146 s/iter. ETA=0:07:51
[07/28 17:35:14] d2.evaluation.evaluator INFO: Inference done 6011/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0073 s/iter. Total: 0.1146 s/iter. ETA=0:07:46
[07/28 17:35:19] d2.evaluation.evaluator INFO: Inference done 6055/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0073 s/iter. Total: 0.1146 s/iter. ETA=0:07:41
[07/28 17:35:24] d2.evaluation.evaluator INFO: Inference done 6097/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0073 s/iter. Total: 0.1147 s/iter. ETA=0:07:36
[07/28 17:35:30] d2.evaluation.evaluator INFO: Inference done 6139/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0073 s/iter. Total: 0.1147 s/iter. ETA=0:07:32
[07/28 17:35:35] d2.evaluation.evaluator INFO: Inference done 6180/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0074 s/iter. Total: 0.1148 s/iter. ETA=0:07:27
[07/28 17:35:40] d2.evaluation.evaluator INFO: Inference done 6223/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0074 s/iter. Total: 0.1148 s/iter. ETA=0:07:22
[07/28 17:35:45] d2.evaluation.evaluator INFO: Inference done 6265/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0075 s/iter. Total: 0.1148 s/iter. ETA=0:07:18
[07/28 17:35:50] d2.evaluation.evaluator INFO: Inference done 6308/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0075 s/iter. Total: 0.1149 s/iter. ETA=0:07:13
[07/28 17:35:55] d2.evaluation.evaluator INFO: Inference done 6351/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0075 s/iter. Total: 0.1149 s/iter. ETA=0:07:08
[07/28 17:36:00] d2.evaluation.evaluator INFO: Inference done 6395/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0075 s/iter. Total: 0.1149 s/iter. ETA=0:07:03
[07/28 17:36:05] d2.evaluation.evaluator INFO: Inference done 6438/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0075 s/iter. Total: 0.1149 s/iter. ETA=0:06:58
[07/28 17:36:10] d2.evaluation.evaluator INFO: Inference done 6481/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0075 s/iter. Total: 0.1149 s/iter. ETA=0:06:53
[07/28 17:36:15] d2.evaluation.evaluator INFO: Inference done 6525/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0075 s/iter. Total: 0.1149 s/iter. ETA=0:06:48
[07/28 17:36:20] d2.evaluation.evaluator INFO: Inference done 6568/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0075 s/iter. Total: 0.1149 s/iter. ETA=0:06:43
[07/28 17:36:25] d2.evaluation.evaluator INFO: Inference done 6611/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0076 s/iter. Total: 0.1149 s/iter. ETA=0:06:38
[07/28 17:36:30] d2.evaluation.evaluator INFO: Inference done 6655/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0076 s/iter. Total: 0.1149 s/iter. ETA=0:06:33
[07/28 17:36:35] d2.evaluation.evaluator INFO: Inference done 6697/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0076 s/iter. Total: 0.1150 s/iter. ETA=0:06:28
[07/28 17:36:40] d2.evaluation.evaluator INFO: Inference done 6741/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0076 s/iter. Total: 0.1150 s/iter. ETA=0:06:23
[07/28 17:36:45] d2.evaluation.evaluator INFO: Inference done 6784/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0076 s/iter. Total: 0.1150 s/iter. ETA=0:06:19
[07/28 17:36:50] d2.evaluation.evaluator INFO: Inference done 6827/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0076 s/iter. Total: 0.1150 s/iter. ETA=0:06:14
[07/28 17:36:55] d2.evaluation.evaluator INFO: Inference done 6870/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0076 s/iter. Total: 0.1150 s/iter. ETA=0:06:09
[07/28 17:37:00] d2.evaluation.evaluator INFO: Inference done 6913/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0076 s/iter. Total: 0.1150 s/iter. ETA=0:06:04
[07/28 17:37:05] d2.evaluation.evaluator INFO: Inference done 6956/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0076 s/iter. Total: 0.1150 s/iter. ETA=0:05:59
[07/28 17:37:11] d2.evaluation.evaluator INFO: Inference done 7000/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0076 s/iter. Total: 0.1150 s/iter. ETA=0:05:54
[07/28 17:37:16] d2.evaluation.evaluator INFO: Inference done 7045/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0076 s/iter. Total: 0.1150 s/iter. ETA=0:05:49
[07/28 17:37:21] d2.evaluation.evaluator INFO: Inference done 7089/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0076 s/iter. Total: 0.1150 s/iter. ETA=0:05:44
[07/28 17:37:26] d2.evaluation.evaluator INFO: Inference done 7133/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0076 s/iter. Total: 0.1150 s/iter. ETA=0:05:39
[07/28 17:37:31] d2.evaluation.evaluator INFO: Inference done 7177/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0077 s/iter. Total: 0.1150 s/iter. ETA=0:05:33
[07/28 17:37:36] d2.evaluation.evaluator INFO: Inference done 7220/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0077 s/iter. Total: 0.1151 s/iter. ETA=0:05:29
[07/28 17:37:41] d2.evaluation.evaluator INFO: Inference done 7264/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0077 s/iter. Total: 0.1150 s/iter. ETA=0:05:23
[07/28 17:37:46] d2.evaluation.evaluator INFO: Inference done 7308/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0077 s/iter. Total: 0.1150 s/iter. ETA=0:05:18
[07/28 17:37:51] d2.evaluation.evaluator INFO: Inference done 7352/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0077 s/iter. Total: 0.1151 s/iter. ETA=0:05:13
[07/28 17:37:56] d2.evaluation.evaluator INFO: Inference done 7397/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0077 s/iter. Total: 0.1150 s/iter. ETA=0:05:08
[07/28 17:38:01] d2.evaluation.evaluator INFO: Inference done 7442/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0077 s/iter. Total: 0.1150 s/iter. ETA=0:05:03
[07/28 17:38:06] d2.evaluation.evaluator INFO: Inference done 7485/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0077 s/iter. Total: 0.1150 s/iter. ETA=0:04:58
[07/28 17:38:11] d2.evaluation.evaluator INFO: Inference done 7530/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0077 s/iter. Total: 0.1150 s/iter. ETA=0:04:53
[07/28 17:38:16] d2.evaluation.evaluator INFO: Inference done 7575/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0077 s/iter. Total: 0.1150 s/iter. ETA=0:04:48
[07/28 17:38:22] d2.evaluation.evaluator INFO: Inference done 7618/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0077 s/iter. Total: 0.1150 s/iter. ETA=0:04:43
[07/28 17:38:27] d2.evaluation.evaluator INFO: Inference done 7661/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0077 s/iter. Total: 0.1150 s/iter. ETA=0:04:38
[07/28 17:38:32] d2.evaluation.evaluator INFO: Inference done 7704/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0077 s/iter. Total: 0.1151 s/iter. ETA=0:04:33
[07/28 17:38:37] d2.evaluation.evaluator INFO: Inference done 7746/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0077 s/iter. Total: 0.1151 s/iter. ETA=0:04:28
[07/28 17:38:42] d2.evaluation.evaluator INFO: Inference done 7789/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0077 s/iter. Total: 0.1151 s/iter. ETA=0:04:23
[07/28 17:38:47] d2.evaluation.evaluator INFO: Inference done 7831/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0078 s/iter. Total: 0.1151 s/iter. ETA=0:04:18
[07/28 17:38:52] d2.evaluation.evaluator INFO: Inference done 7874/10080. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0078 s/iter. Total: 0.1151 s/iter. ETA=0:04:13
[07/28 17:38:57] d2.evaluation.evaluator INFO: Inference done 7917/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0078 s/iter. Total: 0.1151 s/iter. ETA=0:04:09
[07/28 17:39:02] d2.evaluation.evaluator INFO: Inference done 7960/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0078 s/iter. Total: 0.1152 s/iter. ETA=0:04:04
[07/28 17:39:07] d2.evaluation.evaluator INFO: Inference done 8002/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0078 s/iter. Total: 0.1152 s/iter. ETA=0:03:59
[07/28 17:39:12] d2.evaluation.evaluator INFO: Inference done 8046/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0078 s/iter. Total: 0.1152 s/iter. ETA=0:03:54
[07/28 17:39:17] d2.evaluation.evaluator INFO: Inference done 8089/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0078 s/iter. Total: 0.1152 s/iter. ETA=0:03:49
[07/28 17:39:22] d2.evaluation.evaluator INFO: Inference done 8133/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0078 s/iter. Total: 0.1152 s/iter. ETA=0:03:44
[07/28 17:39:27] d2.evaluation.evaluator INFO: Inference done 8175/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0078 s/iter. Total: 0.1152 s/iter. ETA=0:03:39
[07/28 17:39:32] d2.evaluation.evaluator INFO: Inference done 8218/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0078 s/iter. Total: 0.1152 s/iter. ETA=0:03:34
[07/28 17:39:37] d2.evaluation.evaluator INFO: Inference done 8262/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0078 s/iter. Total: 0.1152 s/iter. ETA=0:03:29
[07/28 17:39:42] d2.evaluation.evaluator INFO: Inference done 8305/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0079 s/iter. Total: 0.1152 s/iter. ETA=0:03:24
[07/28 17:39:47] d2.evaluation.evaluator INFO: Inference done 8348/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0079 s/iter. Total: 0.1153 s/iter. ETA=0:03:19
[07/28 17:39:52] d2.evaluation.evaluator INFO: Inference done 8390/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0079 s/iter. Total: 0.1153 s/iter. ETA=0:03:14
[07/28 17:39:58] d2.evaluation.evaluator INFO: Inference done 8433/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0079 s/iter. Total: 0.1153 s/iter. ETA=0:03:09
[07/28 17:40:03] d2.evaluation.evaluator INFO: Inference done 8476/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0079 s/iter. Total: 0.1153 s/iter. ETA=0:03:04
[07/28 17:40:08] d2.evaluation.evaluator INFO: Inference done 8517/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0079 s/iter. Total: 0.1153 s/iter. ETA=0:03:00
[07/28 17:40:13] d2.evaluation.evaluator INFO: Inference done 8560/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0079 s/iter. Total: 0.1154 s/iter. ETA=0:02:55
[07/28 17:40:18] d2.evaluation.evaluator INFO: Inference done 8602/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0079 s/iter. Total: 0.1154 s/iter. ETA=0:02:50
[07/28 17:40:23] d2.evaluation.evaluator INFO: Inference done 8644/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0080 s/iter. Total: 0.1154 s/iter. ETA=0:02:45
[07/28 17:40:28] d2.evaluation.evaluator INFO: Inference done 8686/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0080 s/iter. Total: 0.1154 s/iter. ETA=0:02:40
[07/28 17:40:33] d2.evaluation.evaluator INFO: Inference done 8728/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0080 s/iter. Total: 0.1155 s/iter. ETA=0:02:36
[07/28 17:40:38] d2.evaluation.evaluator INFO: Inference done 8770/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0080 s/iter. Total: 0.1155 s/iter. ETA=0:02:31
[07/28 17:40:43] d2.evaluation.evaluator INFO: Inference done 8812/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0080 s/iter. Total: 0.1155 s/iter. ETA=0:02:26
[07/28 17:40:48] d2.evaluation.evaluator INFO: Inference done 8855/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0081 s/iter. Total: 0.1155 s/iter. ETA=0:02:21
[07/28 17:40:53] d2.evaluation.evaluator INFO: Inference done 8897/10080. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0081 s/iter. Total: 0.1155 s/iter. ETA=0:02:16
[07/28 17:40:58] d2.evaluation.evaluator INFO: Inference done 8938/10080. Dataloading: 0.0011 s/iter. Inference: 0.1064 s/iter. Eval: 0.0081 s/iter. Total: 0.1156 s/iter. ETA=0:02:11
[07/28 17:41:03] d2.evaluation.evaluator INFO: Inference done 8981/10080. Dataloading: 0.0011 s/iter. Inference: 0.1064 s/iter. Eval: 0.0081 s/iter. Total: 0.1156 s/iter. ETA=0:02:07
[07/28 17:41:08] d2.evaluation.evaluator INFO: Inference done 9022/10080. Dataloading: 0.0011 s/iter. Inference: 0.1064 s/iter. Eval: 0.0081 s/iter. Total: 0.1156 s/iter. ETA=0:02:02
[07/28 17:41:13] d2.evaluation.evaluator INFO: Inference done 9065/10080. Dataloading: 0.0011 s/iter. Inference: 0.1064 s/iter. Eval: 0.0081 s/iter. Total: 0.1156 s/iter. ETA=0:01:57
[07/28 17:41:18] d2.evaluation.evaluator INFO: Inference done 9103/10080. Dataloading: 0.0011 s/iter. Inference: 0.1064 s/iter. Eval: 0.0082 s/iter. Total: 0.1157 s/iter. ETA=0:01:53
[07/28 17:41:24] d2.evaluation.evaluator INFO: Inference done 9146/10080. Dataloading: 0.0011 s/iter. Inference: 0.1064 s/iter. Eval: 0.0082 s/iter. Total: 0.1157 s/iter. ETA=0:01:48
[07/28 17:41:29] d2.evaluation.evaluator INFO: Inference done 9190/10080. Dataloading: 0.0011 s/iter. Inference: 0.1064 s/iter. Eval: 0.0082 s/iter. Total: 0.1157 s/iter. ETA=0:01:42
[07/28 17:41:34] d2.evaluation.evaluator INFO: Inference done 9234/10080. Dataloading: 0.0011 s/iter. Inference: 0.1064 s/iter. Eval: 0.0082 s/iter. Total: 0.1157 s/iter. ETA=0:01:37
[07/28 17:41:39] d2.evaluation.evaluator INFO: Inference done 9277/10080. Dataloading: 0.0011 s/iter. Inference: 0.1064 s/iter. Eval: 0.0082 s/iter. Total: 0.1157 s/iter. ETA=0:01:32
[07/28 17:41:44] d2.evaluation.evaluator INFO: Inference done 9320/10080. Dataloading: 0.0011 s/iter. Inference: 0.1064 s/iter. Eval: 0.0082 s/iter. Total: 0.1157 s/iter. ETA=0:01:27
[07/28 17:41:49] d2.evaluation.evaluator INFO: Inference done 9363/10080. Dataloading: 0.0011 s/iter. Inference: 0.1064 s/iter. Eval: 0.0082 s/iter. Total: 0.1157 s/iter. ETA=0:01:22
[07/28 17:41:54] d2.evaluation.evaluator INFO: Inference done 9405/10080. Dataloading: 0.0011 s/iter. Inference: 0.1064 s/iter. Eval: 0.0082 s/iter. Total: 0.1157 s/iter. ETA=0:01:18
[07/28 17:41:59] d2.evaluation.evaluator INFO: Inference done 9447/10080. Dataloading: 0.0011 s/iter. Inference: 0.1064 s/iter. Eval: 0.0082 s/iter. Total: 0.1158 s/iter. ETA=0:01:13
[07/28 17:42:04] d2.evaluation.evaluator INFO: Inference done 9489/10080. Dataloading: 0.0011 s/iter. Inference: 0.1064 s/iter. Eval: 0.0082 s/iter. Total: 0.1158 s/iter. ETA=0:01:08
[07/28 17:42:09] d2.evaluation.evaluator INFO: Inference done 9533/10080. Dataloading: 0.0011 s/iter. Inference: 0.1064 s/iter. Eval: 0.0082 s/iter. Total: 0.1158 s/iter. ETA=0:01:03
[07/28 17:42:14] d2.evaluation.evaluator INFO: Inference done 9577/10080. Dataloading: 0.0011 s/iter. Inference: 0.1064 s/iter. Eval: 0.0082 s/iter. Total: 0.1158 s/iter. ETA=0:00:58
[07/28 17:42:19] d2.evaluation.evaluator INFO: Inference done 9615/10080. Dataloading: 0.0011 s/iter. Inference: 0.1064 s/iter. Eval: 0.0083 s/iter. Total: 0.1158 s/iter. ETA=0:00:53
[07/28 17:42:24] d2.evaluation.evaluator INFO: Inference done 9655/10080. Dataloading: 0.0011 s/iter. Inference: 0.1064 s/iter. Eval: 0.0083 s/iter. Total: 0.1159 s/iter. ETA=0:00:49
[07/28 17:42:29] d2.evaluation.evaluator INFO: Inference done 9694/10080. Dataloading: 0.0011 s/iter. Inference: 0.1065 s/iter. Eval: 0.0084 s/iter. Total: 0.1160 s/iter. ETA=0:00:44
[07/28 17:42:34] d2.evaluation.evaluator INFO: Inference done 9733/10080. Dataloading: 0.0011 s/iter. Inference: 0.1065 s/iter. Eval: 0.0084 s/iter. Total: 0.1160 s/iter. ETA=0:00:40
[07/28 17:42:39] d2.evaluation.evaluator INFO: Inference done 9771/10080. Dataloading: 0.0011 s/iter. Inference: 0.1065 s/iter. Eval: 0.0085 s/iter. Total: 0.1161 s/iter. ETA=0:00:35
[07/28 17:42:44] d2.evaluation.evaluator INFO: Inference done 9809/10080. Dataloading: 0.0011 s/iter. Inference: 0.1065 s/iter. Eval: 0.0085 s/iter. Total: 0.1161 s/iter. ETA=0:00:31
[07/28 17:42:50] d2.evaluation.evaluator INFO: Inference done 9849/10080. Dataloading: 0.0011 s/iter. Inference: 0.1065 s/iter. Eval: 0.0086 s/iter. Total: 0.1162 s/iter. ETA=0:00:26
[07/28 17:42:55] d2.evaluation.evaluator INFO: Inference done 9888/10080. Dataloading: 0.0011 s/iter. Inference: 0.1065 s/iter. Eval: 0.0086 s/iter. Total: 0.1162 s/iter. ETA=0:00:22
[07/28 17:43:00] d2.evaluation.evaluator INFO: Inference done 9928/10080. Dataloading: 0.0011 s/iter. Inference: 0.1065 s/iter. Eval: 0.0086 s/iter. Total: 0.1163 s/iter. ETA=0:00:17
[07/28 17:43:05] d2.evaluation.evaluator INFO: Inference done 9968/10080. Dataloading: 0.0011 s/iter. Inference: 0.1065 s/iter. Eval: 0.0087 s/iter. Total: 0.1163 s/iter. ETA=0:00:13
[07/28 17:43:10] d2.evaluation.evaluator INFO: Inference done 10009/10080. Dataloading: 0.0011 s/iter. Inference: 0.1065 s/iter. Eval: 0.0087 s/iter. Total: 0.1163 s/iter. ETA=0:00:08
[07/28 17:43:15] d2.evaluation.evaluator INFO: Inference done 10051/10080. Dataloading: 0.0011 s/iter. Inference: 0.1065 s/iter. Eval: 0.0087 s/iter. Total: 0.1164 s/iter. ETA=0:00:03
[07/28 17:43:18] d2.evaluation.evaluator INFO: Total inference time: 0:19:32.605752 (0.116388 s / iter per device, on 1 devices)
[07/28 17:43:18] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:17:53 (0.106518 s / iter per device, on 1 devices)
[07/28 17:43:18] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[07/28 17:43:18] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[07/28 17:43:19] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[07/28 17:43:25] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 6.28 seconds.
[07/28 17:43:25] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[07/28 17:43:26] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.63 seconds.
[07/28 17:43:26] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |
|:------:|:------:|:------:|:-----:|:------:|:------:|
| 51.788 | 65.310 | 57.992 | 4.759 | 46.902 | 55.768 |
[07/28 17:43:26] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP     | category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|:-----------|:-------|
| 1          | 45.447 | 2          | 24.446 | 3          | 28.950 |
| 4          | 28.605 | 5          | 46.194 | 6          | 37.260 |
| 7          | 3.157  | 8          | 47.811 | 9          | 81.475 |
| 10         | 70.051 | 11         | 59.249 | 12         | 71.101 |
| 13         | 31.697 | 14         | 52.877 | 15         | 44.449 |
| 16         | 53.051 | 17         | 69.299 | 18         | 82.434 |
| 19         | 47.096 | 20         | 41.639 | 21         | 44.758 |
| 22         | 47.171 | 23         | 75.588 | 24         | 64.999 |
| 25         | 67.427 | 26         | 56.376 | 27         | 66.183 |
| 28         | 77.695 | 29         | 66.770 | 30         | 20.383 |
[07/28 17:43:30] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *segm*
[07/28 17:43:39] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 9.07 seconds.
[07/28 17:43:39] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[07/28 17:43:39] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.65 seconds.
[07/28 17:43:40] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |
|:------:|:------:|:------:|:-----:|:------:|:------:|
| 42.966 | 60.670 | 49.087 | 0.252 | 31.070 | 52.207 |
[07/28 17:43:40] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|:-----------|:-------|
| 1          | 42.417 | 2          | 21.939 | 3          | 25.152 |
| 4          | 23.552 | 5          | 28.303 | 6          | 28.796 |
| 7          | 1.917  | 8          | 35.739 | 9          | 71.515 |
| 10         | 63.422 | 11         | 54.977 | 12         | 63.965 |
| 13         | 23.287 | 14         | 38.734 | 15         | 41.639 |
| 16         | 49.292 | 17         | 59.155 | 18         | 74.817 |
| 19         | 40.739 | 20         | 32.380 | 21         | 34.601 |
| 22         | 40.487 | 23         | 66.282 | 24         | 56.671 |
| 25         | 60.346 | 26         | 43.890 | 27         | 43.217 |
| 28         | 64.468 | 29         | 43.877 | 30         | 13.412 |
[07/28 17:43:41] d2.evaluation.testing INFO: copypaste: Task: bbox
[07/28 17:43:41] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/28 17:43:41] d2.evaluation.testing INFO: copypaste: 51.7881,65.3095,57.9921,4.7589,46.9016,55.7680
[07/28 17:43:41] d2.evaluation.testing INFO: copypaste: Task: segm
[07/28 17:43:41] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/28 17:43:41] d2.evaluation.testing INFO: copypaste: 42.9662,60.6700,49.0868,0.2519,31.0698,52.2072
[07/28 17:43:41] d2.utils.events INFO:  eta: 11:21:37  iter: 4999  total_loss: 3.729  loss_cls_stage0: 0.3571  loss_box_reg_stage0: 0.3388  loss_cls_stage1: 0.404  loss_box_reg_stage1: 0.8399  loss_cls_stage2: 0.3825  loss_box_reg_stage2: 1.097  loss_mask: 0.124  loss_rpn_cls: 0.0197  loss_rpn_loc: 0.05855  time: 0.6462  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 17:43:53] d2.utils.events INFO:  eta: 11:21:01  iter: 5019  total_loss: 3.474  loss_cls_stage0: 0.3316  loss_box_reg_stage0: 0.3313  loss_cls_stage1: 0.3613  loss_box_reg_stage1: 0.8155  loss_cls_stage2: 0.3326  loss_box_reg_stage2: 1.05  loss_mask: 0.1158  loss_rpn_cls: 0.01974  loss_rpn_loc: 0.04997  time: 0.6462  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 17:44:06] d2.utils.events INFO:  eta: 11:20:05  iter: 5039  total_loss: 3.844  loss_cls_stage0: 0.3556  loss_box_reg_stage0: 0.3516  loss_cls_stage1: 0.4186  loss_box_reg_stage1: 0.8897  loss_cls_stage2: 0.4142  loss_box_reg_stage2: 1.129  loss_mask: 0.1189  loss_rpn_cls: 0.02372  loss_rpn_loc: 0.07573  time: 0.6461  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 17:44:19] d2.utils.events INFO:  eta: 11:19:52  iter: 5059  total_loss: 3.485  loss_cls_stage0: 0.3643  loss_box_reg_stage0: 0.3281  loss_cls_stage1: 0.3924  loss_box_reg_stage1: 0.7673  loss_cls_stage2: 0.3678  loss_box_reg_stage2: 1.011  loss_mask: 0.1083  loss_rpn_cls: 0.01694  loss_rpn_loc: 0.04798  time: 0.6461  data_time: 0.0060  lr: 0.00016  max_mem: 19672M
[07/28 17:44:32] d2.utils.events INFO:  eta: 11:19:49  iter: 5079  total_loss: 3.549  loss_cls_stage0: 0.3664  loss_box_reg_stage0: 0.3256  loss_cls_stage1: 0.4146  loss_box_reg_stage1: 0.7934  loss_cls_stage2: 0.3756  loss_box_reg_stage2: 1.059  loss_mask: 0.112  loss_rpn_cls: 0.01864  loss_rpn_loc: 0.05305  time: 0.6461  data_time: 0.0067  lr: 0.00016  max_mem: 19672M
[07/28 17:44:44] d2.utils.events INFO:  eta: 11:19:40  iter: 5099  total_loss: 3.265  loss_cls_stage0: 0.3303  loss_box_reg_stage0: 0.3154  loss_cls_stage1: 0.36  loss_box_reg_stage1: 0.7323  loss_cls_stage2: 0.3551  loss_box_reg_stage2: 0.9572  loss_mask: 0.1073  loss_rpn_cls: 0.02257  loss_rpn_loc: 0.06016  time: 0.6461  data_time: 0.0061  lr: 0.00016  max_mem: 19672M
[07/28 17:44:57] d2.utils.events INFO:  eta: 11:19:27  iter: 5119  total_loss: 3.568  loss_cls_stage0: 0.3655  loss_box_reg_stage0: 0.3155  loss_cls_stage1: 0.4323  loss_box_reg_stage1: 0.801  loss_cls_stage2: 0.4129  loss_box_reg_stage2: 1.049  loss_mask: 0.1158  loss_rpn_cls: 0.02038  loss_rpn_loc: 0.05343  time: 0.6460  data_time: 0.0060  lr: 0.00016  max_mem: 19672M
[07/28 17:45:10] d2.utils.events INFO:  eta: 11:19:38  iter: 5139  total_loss: 3.519  loss_cls_stage0: 0.3671  loss_box_reg_stage0: 0.3307  loss_cls_stage1: 0.4197  loss_box_reg_stage1: 0.7942  loss_cls_stage2: 0.407  loss_box_reg_stage2: 1.032  loss_mask: 0.1122  loss_rpn_cls: 0.0187  loss_rpn_loc: 0.05053  time: 0.6461  data_time: 0.0054  lr: 0.00016  max_mem: 19672M
[07/28 17:45:23] d2.utils.events INFO:  eta: 11:19:17  iter: 5159  total_loss: 3.514  loss_cls_stage0: 0.3469  loss_box_reg_stage0: 0.3295  loss_cls_stage1: 0.3928  loss_box_reg_stage1: 0.7933  loss_cls_stage2: 0.365  loss_box_reg_stage2: 1.107  loss_mask: 0.1113  loss_rpn_cls: 0.02948  loss_rpn_loc: 0.0843  time: 0.6460  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 17:45:36] d2.utils.events INFO:  eta: 11:18:33  iter: 5179  total_loss: 3.596  loss_cls_stage0: 0.3456  loss_box_reg_stage0: 0.3205  loss_cls_stage1: 0.4185  loss_box_reg_stage1: 0.8012  loss_cls_stage2: 0.4393  loss_box_reg_stage2: 1.037  loss_mask: 0.1087  loss_rpn_cls: 0.02259  loss_rpn_loc: 0.06645  time: 0.6460  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:45:49] d2.utils.events INFO:  eta: 11:18:26  iter: 5199  total_loss: 3.454  loss_cls_stage0: 0.3442  loss_box_reg_stage0: 0.3073  loss_cls_stage1: 0.3701  loss_box_reg_stage1: 0.7938  loss_cls_stage2: 0.3785  loss_box_reg_stage2: 1.001  loss_mask: 0.1072  loss_rpn_cls: 0.01904  loss_rpn_loc: 0.06044  time: 0.6460  data_time: 0.0069  lr: 0.00016  max_mem: 19672M
[07/28 17:46:02] d2.utils.events INFO:  eta: 11:18:30  iter: 5219  total_loss: 3.563  loss_cls_stage0: 0.3721  loss_box_reg_stage0: 0.3434  loss_cls_stage1: 0.4218  loss_box_reg_stage1: 0.849  loss_cls_stage2: 0.4097  loss_box_reg_stage2: 1.134  loss_mask: 0.1054  loss_rpn_cls: 0.02401  loss_rpn_loc: 0.06198  time: 0.6460  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 17:46:15] d2.utils.events INFO:  eta: 11:18:59  iter: 5239  total_loss: 3.683  loss_cls_stage0: 0.3649  loss_box_reg_stage0: 0.3217  loss_cls_stage1: 0.4051  loss_box_reg_stage1: 0.8187  loss_cls_stage2: 0.3855  loss_box_reg_stage2: 1.047  loss_mask: 0.1109  loss_rpn_cls: 0.02238  loss_rpn_loc: 0.05996  time: 0.6461  data_time: 0.0064  lr: 0.00016  max_mem: 19672M
[07/28 17:46:29] d2.utils.events INFO:  eta: 11:18:45  iter: 5259  total_loss: 3.613  loss_cls_stage0: 0.3788  loss_box_reg_stage0: 0.3296  loss_cls_stage1: 0.4227  loss_box_reg_stage1: 0.833  loss_cls_stage2: 0.4033  loss_box_reg_stage2: 1.051  loss_mask: 0.1154  loss_rpn_cls: 0.01937  loss_rpn_loc: 0.05237  time: 0.6462  data_time: 0.0060  lr: 0.00016  max_mem: 19672M
[07/28 17:46:42] d2.utils.events INFO:  eta: 11:18:28  iter: 5279  total_loss: 3.386  loss_cls_stage0: 0.3532  loss_box_reg_stage0: 0.3169  loss_cls_stage1: 0.392  loss_box_reg_stage1: 0.8011  loss_cls_stage2: 0.3827  loss_box_reg_stage2: 1.046  loss_mask: 0.119  loss_rpn_cls: 0.0164  loss_rpn_loc: 0.05001  time: 0.6462  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 17:46:55] d2.utils.events INFO:  eta: 11:19:14  iter: 5299  total_loss: 3.681  loss_cls_stage0: 0.3551  loss_box_reg_stage0: 0.3188  loss_cls_stage1: 0.3999  loss_box_reg_stage1: 0.8308  loss_cls_stage2: 0.3408  loss_box_reg_stage2: 1.12  loss_mask: 0.1034  loss_rpn_cls: 0.01486  loss_rpn_loc: 0.04489  time: 0.6464  data_time: 0.0059  lr: 0.00016  max_mem: 19672M
[07/28 17:47:09] d2.utils.events INFO:  eta: 11:18:44  iter: 5319  total_loss: 3.763  loss_cls_stage0: 0.361  loss_box_reg_stage0: 0.3312  loss_cls_stage1: 0.4327  loss_box_reg_stage1: 0.8846  loss_cls_stage2: 0.4478  loss_box_reg_stage2: 1.151  loss_mask: 0.1015  loss_rpn_cls: 0.0172  loss_rpn_loc: 0.05275  time: 0.6464  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 17:47:21] d2.utils.events INFO:  eta: 11:18:40  iter: 5339  total_loss: 3.246  loss_cls_stage0: 0.3278  loss_box_reg_stage0: 0.34  loss_cls_stage1: 0.3588  loss_box_reg_stage1: 0.7142  loss_cls_stage2: 0.3368  loss_box_reg_stage2: 0.871  loss_mask: 0.1186  loss_rpn_cls: 0.03646  loss_rpn_loc: 0.05262  time: 0.6464  data_time: 0.0060  lr: 0.00016  max_mem: 19672M
[07/28 17:47:34] d2.utils.events INFO:  eta: 11:18:35  iter: 5359  total_loss: 3.313  loss_cls_stage0: 0.3658  loss_box_reg_stage0: 0.3419  loss_cls_stage1: 0.3856  loss_box_reg_stage1: 0.7799  loss_cls_stage2: 0.3472  loss_box_reg_stage2: 0.9703  loss_mask: 0.1071  loss_rpn_cls: 0.02548  loss_rpn_loc: 0.06399  time: 0.6464  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 17:47:47] d2.utils.events INFO:  eta: 11:18:14  iter: 5379  total_loss: 3.164  loss_cls_stage0: 0.3182  loss_box_reg_stage0: 0.3098  loss_cls_stage1: 0.3338  loss_box_reg_stage1: 0.7285  loss_cls_stage2: 0.3051  loss_box_reg_stage2: 0.9305  loss_mask: 0.09838  loss_rpn_cls: 0.02399  loss_rpn_loc: 0.07843  time: 0.6464  data_time: 0.0054  lr: 0.00016  max_mem: 19672M
[07/28 17:48:00] d2.utils.events INFO:  eta: 11:18:01  iter: 5399  total_loss: 3.479  loss_cls_stage0: 0.3515  loss_box_reg_stage0: 0.3352  loss_cls_stage1: 0.3637  loss_box_reg_stage1: 0.8067  loss_cls_stage2: 0.3261  loss_box_reg_stage2: 1.005  loss_mask: 0.1134  loss_rpn_cls: 0.03033  loss_rpn_loc: 0.0645  time: 0.6463  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 17:48:13] d2.utils.events INFO:  eta: 11:17:01  iter: 5419  total_loss: 3.279  loss_cls_stage0: 0.3495  loss_box_reg_stage0: 0.3146  loss_cls_stage1: 0.3836  loss_box_reg_stage1: 0.725  loss_cls_stage2: 0.3655  loss_box_reg_stage2: 0.9602  loss_mask: 0.1091  loss_rpn_cls: 0.02017  loss_rpn_loc: 0.04843  time: 0.6463  data_time: 0.0063  lr: 0.00016  max_mem: 19672M
[07/28 17:48:26] d2.utils.events INFO:  eta: 11:17:14  iter: 5439  total_loss: 3.468  loss_cls_stage0: 0.3474  loss_box_reg_stage0: 0.3237  loss_cls_stage1: 0.3864  loss_box_reg_stage1: 0.8093  loss_cls_stage2: 0.3827  loss_box_reg_stage2: 1.069  loss_mask: 0.1225  loss_rpn_cls: 0.0222  loss_rpn_loc: 0.05262  time: 0.6464  data_time: 0.0065  lr: 0.00016  max_mem: 19672M
[07/28 17:48:40] d2.utils.events INFO:  eta: 11:17:19  iter: 5459  total_loss: 3.659  loss_cls_stage0: 0.3803  loss_box_reg_stage0: 0.3501  loss_cls_stage1: 0.404  loss_box_reg_stage1: 0.8044  loss_cls_stage2: 0.3871  loss_box_reg_stage2: 1.034  loss_mask: 0.1094  loss_rpn_cls: 0.02481  loss_rpn_loc: 0.0565  time: 0.6465  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 17:48:52] d2.utils.events INFO:  eta: 11:16:59  iter: 5479  total_loss: 3.464  loss_cls_stage0: 0.3641  loss_box_reg_stage0: 0.3387  loss_cls_stage1: 0.4006  loss_box_reg_stage1: 0.7769  loss_cls_stage2: 0.3823  loss_box_reg_stage2: 0.9992  loss_mask: 0.108  loss_rpn_cls: 0.02027  loss_rpn_loc: 0.05076  time: 0.6464  data_time: 0.0062  lr: 0.00016  max_mem: 19672M
[07/28 17:49:06] d2.utils.events INFO:  eta: 11:16:57  iter: 5499  total_loss: 3.974  loss_cls_stage0: 0.4066  loss_box_reg_stage0: 0.3544  loss_cls_stage1: 0.4591  loss_box_reg_stage1: 0.9023  loss_cls_stage2: 0.4416  loss_box_reg_stage2: 1.111  loss_mask: 0.1375  loss_rpn_cls: 0.02251  loss_rpn_loc: 0.07877  time: 0.6465  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 17:49:19] d2.utils.events INFO:  eta: 11:15:53  iter: 5519  total_loss: 3.595  loss_cls_stage0: 0.3865  loss_box_reg_stage0: 0.3497  loss_cls_stage1: 0.3948  loss_box_reg_stage1: 0.78  loss_cls_stage2: 0.36  loss_box_reg_stage2: 1.002  loss_mask: 0.1202  loss_rpn_cls: 0.02045  loss_rpn_loc: 0.06981  time: 0.6465  data_time: 0.0064  lr: 0.00016  max_mem: 19672M
[07/28 17:49:31] d2.utils.events INFO:  eta: 11:15:08  iter: 5539  total_loss: 3.24  loss_cls_stage0: 0.3427  loss_box_reg_stage0: 0.3441  loss_cls_stage1: 0.3566  loss_box_reg_stage1: 0.726  loss_cls_stage2: 0.3388  loss_box_reg_stage2: 0.9004  loss_mask: 0.1079  loss_rpn_cls: 0.03061  loss_rpn_loc: 0.08517  time: 0.6464  data_time: 0.0061  lr: 0.00016  max_mem: 19672M
[07/28 17:49:44] d2.utils.events INFO:  eta: 11:15:16  iter: 5559  total_loss: 3.418  loss_cls_stage0: 0.3592  loss_box_reg_stage0: 0.3385  loss_cls_stage1: 0.3662  loss_box_reg_stage1: 0.7697  loss_cls_stage2: 0.3127  loss_box_reg_stage2: 0.9565  loss_mask: 0.1096  loss_rpn_cls: 0.02088  loss_rpn_loc: 0.05416  time: 0.6464  data_time: 0.0059  lr: 0.00016  max_mem: 19672M
[07/28 17:49:57] d2.utils.events INFO:  eta: 11:15:36  iter: 5579  total_loss: 3.409  loss_cls_stage0: 0.3558  loss_box_reg_stage0: 0.3191  loss_cls_stage1: 0.3792  loss_box_reg_stage1: 0.7394  loss_cls_stage2: 0.3582  loss_box_reg_stage2: 0.9722  loss_mask: 0.1035  loss_rpn_cls: 0.01878  loss_rpn_loc: 0.05075  time: 0.6464  data_time: 0.0068  lr: 0.00016  max_mem: 19672M
[07/28 17:50:10] d2.utils.events INFO:  eta: 11:15:03  iter: 5599  total_loss: 3.22  loss_cls_stage0: 0.3664  loss_box_reg_stage0: 0.3411  loss_cls_stage1: 0.3595  loss_box_reg_stage1: 0.7533  loss_cls_stage2: 0.3427  loss_box_reg_stage2: 0.8504  loss_mask: 0.1104  loss_rpn_cls: 0.02025  loss_rpn_loc: 0.04579  time: 0.6464  data_time: 0.0061  lr: 0.00016  max_mem: 19672M
[07/28 17:50:23] d2.utils.events INFO:  eta: 11:14:50  iter: 5619  total_loss: 3.653  loss_cls_stage0: 0.3773  loss_box_reg_stage0: 0.3271  loss_cls_stage1: 0.4005  loss_box_reg_stage1: 0.8352  loss_cls_stage2: 0.3763  loss_box_reg_stage2: 1.083  loss_mask: 0.1035  loss_rpn_cls: 0.0181  loss_rpn_loc: 0.05496  time: 0.6464  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 17:50:36] d2.utils.events INFO:  eta: 11:14:57  iter: 5639  total_loss: 3.46  loss_cls_stage0: 0.3566  loss_box_reg_stage0: 0.3156  loss_cls_stage1: 0.3831  loss_box_reg_stage1: 0.7856  loss_cls_stage2: 0.3557  loss_box_reg_stage2: 1.056  loss_mask: 0.1008  loss_rpn_cls: 0.02258  loss_rpn_loc: 0.05313  time: 0.6465  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 17:50:49] d2.utils.events INFO:  eta: 11:14:42  iter: 5659  total_loss: 3.327  loss_cls_stage0: 0.3181  loss_box_reg_stage0: 0.3261  loss_cls_stage1: 0.3435  loss_box_reg_stage1: 0.7462  loss_cls_stage2: 0.3239  loss_box_reg_stage2: 0.9997  loss_mask: 0.1168  loss_rpn_cls: 0.02766  loss_rpn_loc: 0.05911  time: 0.6465  data_time: 0.0063  lr: 0.00016  max_mem: 19672M
[07/28 17:51:02] d2.utils.events INFO:  eta: 11:14:37  iter: 5679  total_loss: 3.346  loss_cls_stage0: 0.3451  loss_box_reg_stage0: 0.3317  loss_cls_stage1: 0.3779  loss_box_reg_stage1: 0.7752  loss_cls_stage2: 0.3772  loss_box_reg_stage2: 0.9917  loss_mask: 0.1044  loss_rpn_cls: 0.02388  loss_rpn_loc: 0.04991  time: 0.6465  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 17:51:15] d2.utils.events INFO:  eta: 11:14:13  iter: 5699  total_loss: 3.373  loss_cls_stage0: 0.34  loss_box_reg_stage0: 0.3342  loss_cls_stage1: 0.3782  loss_box_reg_stage1: 0.7545  loss_cls_stage2: 0.3597  loss_box_reg_stage2: 1.003  loss_mask: 0.1067  loss_rpn_cls: 0.02917  loss_rpn_loc: 0.05964  time: 0.6464  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 17:51:28] d2.utils.events INFO:  eta: 11:14:22  iter: 5719  total_loss: 3.397  loss_cls_stage0: 0.358  loss_box_reg_stage0: 0.3368  loss_cls_stage1: 0.3723  loss_box_reg_stage1: 0.7686  loss_cls_stage2: 0.3366  loss_box_reg_stage2: 0.8926  loss_mask: 0.1183  loss_rpn_cls: 0.03193  loss_rpn_loc: 0.0887  time: 0.6465  data_time: 0.0072  lr: 0.00016  max_mem: 19672M
[07/28 17:51:41] d2.utils.events INFO:  eta: 11:14:09  iter: 5739  total_loss: 3.314  loss_cls_stage0: 0.3229  loss_box_reg_stage0: 0.3119  loss_cls_stage1: 0.3539  loss_box_reg_stage1: 0.7422  loss_cls_stage2: 0.3447  loss_box_reg_stage2: 0.9818  loss_mask: 0.1097  loss_rpn_cls: 0.02068  loss_rpn_loc: 0.04611  time: 0.6465  data_time: 0.0065  lr: 0.00016  max_mem: 19672M
[07/28 17:51:54] d2.utils.events INFO:  eta: 11:13:56  iter: 5759  total_loss: 3.128  loss_cls_stage0: 0.3351  loss_box_reg_stage0: 0.3092  loss_cls_stage1: 0.3415  loss_box_reg_stage1: 0.7138  loss_cls_stage2: 0.3206  loss_box_reg_stage2: 0.9596  loss_mask: 0.101  loss_rpn_cls: 0.02512  loss_rpn_loc: 0.07251  time: 0.6466  data_time: 0.0070  lr: 0.00016  max_mem: 19672M
[07/28 17:52:08] d2.utils.events INFO:  eta: 11:13:50  iter: 5779  total_loss: 3.06  loss_cls_stage0: 0.3149  loss_box_reg_stage0: 0.3113  loss_cls_stage1: 0.332  loss_box_reg_stage1: 0.6883  loss_cls_stage2: 0.3164  loss_box_reg_stage2: 0.8834  loss_mask: 0.1014  loss_rpn_cls: 0.0166  loss_rpn_loc: 0.04845  time: 0.6466  data_time: 0.0061  lr: 0.00016  max_mem: 19672M
[07/28 17:52:20] d2.utils.events INFO:  eta: 11:13:12  iter: 5799  total_loss: 3.476  loss_cls_stage0: 0.3734  loss_box_reg_stage0: 0.3474  loss_cls_stage1: 0.3856  loss_box_reg_stage1: 0.8052  loss_cls_stage2: 0.3774  loss_box_reg_stage2: 1.026  loss_mask: 0.1157  loss_rpn_cls: 0.02151  loss_rpn_loc: 0.06711  time: 0.6466  data_time: 0.0055  lr: 0.00016  max_mem: 19672M
[07/28 17:52:34] d2.utils.events INFO:  eta: 11:13:24  iter: 5819  total_loss: 3.187  loss_cls_stage0: 0.3251  loss_box_reg_stage0: 0.3061  loss_cls_stage1: 0.3509  loss_box_reg_stage1: 0.7595  loss_cls_stage2: 0.344  loss_box_reg_stage2: 0.9932  loss_mask: 0.1035  loss_rpn_cls: 0.0146  loss_rpn_loc: 0.05144  time: 0.6466  data_time: 0.0059  lr: 0.00016  max_mem: 19672M
[07/28 17:52:47] d2.utils.events INFO:  eta: 11:14:08  iter: 5839  total_loss: 3.483  loss_cls_stage0: 0.354  loss_box_reg_stage0: 0.3439  loss_cls_stage1: 0.3994  loss_box_reg_stage1: 0.834  loss_cls_stage2: 0.3894  loss_box_reg_stage2: 1.084  loss_mask: 0.1123  loss_rpn_cls: 0.01808  loss_rpn_loc: 0.05349  time: 0.6467  data_time: 0.0064  lr: 0.00016  max_mem: 19672M
[07/28 17:53:00] d2.utils.events INFO:  eta: 11:13:22  iter: 5859  total_loss: 3.21  loss_cls_stage0: 0.3028  loss_box_reg_stage0: 0.3123  loss_cls_stage1: 0.3477  loss_box_reg_stage1: 0.7698  loss_cls_stage2: 0.3597  loss_box_reg_stage2: 0.984  loss_mask: 0.1029  loss_rpn_cls: 0.01461  loss_rpn_loc: 0.05533  time: 0.6467  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:53:13] d2.utils.events INFO:  eta: 11:12:45  iter: 5879  total_loss: 3.444  loss_cls_stage0: 0.3116  loss_box_reg_stage0: 0.2988  loss_cls_stage1: 0.3525  loss_box_reg_stage1: 0.7411  loss_cls_stage2: 0.3593  loss_box_reg_stage2: 1.06  loss_mask: 0.1129  loss_rpn_cls: 0.01652  loss_rpn_loc: 0.0548  time: 0.6467  data_time: 0.0060  lr: 0.00016  max_mem: 19672M
[07/28 17:53:27] d2.utils.events INFO:  eta: 11:12:47  iter: 5899  total_loss: 3.693  loss_cls_stage0: 0.4005  loss_box_reg_stage0: 0.3831  loss_cls_stage1: 0.4075  loss_box_reg_stage1: 0.8585  loss_cls_stage2: 0.3956  loss_box_reg_stage2: 1.036  loss_mask: 0.1373  loss_rpn_cls: 0.02312  loss_rpn_loc: 0.05821  time: 0.6468  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 17:53:39] d2.utils.events INFO:  eta: 11:12:43  iter: 5919  total_loss: 3.554  loss_cls_stage0: 0.367  loss_box_reg_stage0: 0.3437  loss_cls_stage1: 0.3723  loss_box_reg_stage1: 0.8036  loss_cls_stage2: 0.3693  loss_box_reg_stage2: 1.013  loss_mask: 0.114  loss_rpn_cls: 0.02594  loss_rpn_loc: 0.07586  time: 0.6468  data_time: 0.0063  lr: 0.00016  max_mem: 19672M
[07/28 17:53:52] d2.utils.events INFO:  eta: 11:13:06  iter: 5939  total_loss: 3.311  loss_cls_stage0: 0.3238  loss_box_reg_stage0: 0.2954  loss_cls_stage1: 0.3574  loss_box_reg_stage1: 0.7451  loss_cls_stage2: 0.3658  loss_box_reg_stage2: 1.003  loss_mask: 0.09728  loss_rpn_cls: 0.01721  loss_rpn_loc: 0.04365  time: 0.6468  data_time: 0.0059  lr: 0.00016  max_mem: 19672M
[07/28 17:54:05] d2.utils.events INFO:  eta: 11:12:55  iter: 5959  total_loss: 3.497  loss_cls_stage0: 0.3376  loss_box_reg_stage0: 0.3001  loss_cls_stage1: 0.3956  loss_box_reg_stage1: 0.8188  loss_cls_stage2: 0.3288  loss_box_reg_stage2: 1.05  loss_mask: 0.09853  loss_rpn_cls: 0.0224  loss_rpn_loc: 0.06119  time: 0.6468  data_time: 0.0065  lr: 0.00016  max_mem: 19672M
[07/28 17:54:18] d2.utils.events INFO:  eta: 11:12:54  iter: 5979  total_loss: 3.417  loss_cls_stage0: 0.3428  loss_box_reg_stage0: 0.3112  loss_cls_stage1: 0.3763  loss_box_reg_stage1: 0.7842  loss_cls_stage2: 0.3776  loss_box_reg_stage2: 1.018  loss_mask: 0.1151  loss_rpn_cls: 0.01713  loss_rpn_loc: 0.04508  time: 0.6468  data_time: 0.0057  lr: 0.00016  max_mem: 19672M
[07/28 17:54:31] d2.utils.events INFO:  eta: 11:13:01  iter: 5999  total_loss: 3.296  loss_cls_stage0: 0.2992  loss_box_reg_stage0: 0.2991  loss_cls_stage1: 0.3221  loss_box_reg_stage1: 0.7883  loss_cls_stage2: 0.3313  loss_box_reg_stage2: 0.9863  loss_mask: 0.1005  loss_rpn_cls: 0.02069  loss_rpn_loc: 0.04997  time: 0.6468  data_time: 0.0059  lr: 0.00016  max_mem: 19672M
[07/28 17:54:44] d2.utils.events INFO:  eta: 11:12:52  iter: 6019  total_loss: 3.29  loss_cls_stage0: 0.3377  loss_box_reg_stage0: 0.3145  loss_cls_stage1: 0.3902  loss_box_reg_stage1: 0.7333  loss_cls_stage2: 0.3896  loss_box_reg_stage2: 0.9848  loss_mask: 0.1045  loss_rpn_cls: 0.0185  loss_rpn_loc: 0.04658  time: 0.6468  data_time: 0.0058  lr: 0.00016  max_mem: 19672M
[07/28 17:54:57] d2.utils.events INFO:  eta: 11:12:42  iter: 6039  total_loss: 3.49  loss_cls_stage0: 0.3946  loss_box_reg_stage0: 0.3389  loss_cls_stage1: 0.426  loss_box_reg_stage1: 0.7603  loss_cls_stage2: 0.4099  loss_box_reg_stage2: 1.029  loss_mask: 0.1192  loss_rpn_cls: 0.016  loss_rpn_loc: 0.05005  time: 0.6468  data_time: 0.0061  lr: 0.00016  max_mem: 19672M
[07/28 17:55:10] d2.utils.events INFO:  eta: 11:12:26  iter: 6059  total_loss: 3.238  loss_cls_stage0: 0.342  loss_box_reg_stage0: 0.3119  loss_cls_stage1: 0.3737  loss_box_reg_stage1: 0.7634  loss_cls_stage2: 0.3944  loss_box_reg_stage2: 0.9531  loss_mask: 0.1  loss_rpn_cls: 0.02187  loss_rpn_loc: 0.05164  time: 0.6467  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:55:22] d2.utils.events INFO:  eta: 11:11:57  iter: 6079  total_loss: 3.456  loss_cls_stage0: 0.3679  loss_box_reg_stage0: 0.3302  loss_cls_stage1: 0.3763  loss_box_reg_stage1: 0.7634  loss_cls_stage2: 0.3565  loss_box_reg_stage2: 1.005  loss_mask: 0.1096  loss_rpn_cls: 0.01803  loss_rpn_loc: 0.04937  time: 0.6467  data_time: 0.0060  lr: 0.00016  max_mem: 19672M
[07/28 17:55:35] d2.utils.events INFO:  eta: 11:11:49  iter: 6099  total_loss: 3.422  loss_cls_stage0: 0.3088  loss_box_reg_stage0: 0.3047  loss_cls_stage1: 0.3589  loss_box_reg_stage1: 0.7722  loss_cls_stage2: 0.3494  loss_box_reg_stage2: 1.04  loss_mask: 0.1071  loss_rpn_cls: 0.01765  loss_rpn_loc: 0.05563  time: 0.6466  data_time: 0.0065  lr: 0.00016  max_mem: 19672M
[07/28 17:55:48] d2.utils.events INFO:  eta: 11:11:42  iter: 6119  total_loss: 3.537  loss_cls_stage0: 0.353  loss_box_reg_stage0: 0.3398  loss_cls_stage1: 0.4096  loss_box_reg_stage1: 0.7914  loss_cls_stage2: 0.3781  loss_box_reg_stage2: 0.9955  loss_mask: 0.1122  loss_rpn_cls: 0.03076  loss_rpn_loc: 0.05845  time: 0.6466  data_time: 0.0067  lr: 0.00016  max_mem: 19672M
[07/28 17:56:00] d2.utils.events INFO:  eta: 11:11:00  iter: 6139  total_loss: 3.403  loss_cls_stage0: 0.362  loss_box_reg_stage0: 0.3201  loss_cls_stage1: 0.4007  loss_box_reg_stage1: 0.8043  loss_cls_stage2: 0.3968  loss_box_reg_stage2: 1.026  loss_mask: 0.1064  loss_rpn_cls: 0.02122  loss_rpn_loc: 0.05154  time: 0.6465  data_time: 0.0056  lr: 0.00016  max_mem: 19672M
[07/28 17:56:09] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 422, in run_step
    loss_dict = self.model(data)
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 167, in forward
    _, detector_losses = self.roi_heads(images, features, proposals, gt_instances)
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/roi_heads/cascade_rcnn.py", line 144, in forward
    losses = self._forward_box(features, proposals, targets)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/roi_heads/cascade_rcnn.py", line 174, in _forward_box
    predictions = self._run_stage(features, proposals, k)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/roi_heads/cascade_rcnn.py", line 268, in _run_stage
    box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/poolers.py", line 261, in forward
    output.index_put_((inds,), pooler(x[level], pooler_fmt_boxes_level))
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/hoenig/.local/lib/python3.8/site-packages/torchvision/ops/roi_align.py", line 61, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/_ops.py", line 143, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.67 GiB total capacity; 19.71 GiB already allocated; 75.31 MiB free; 20.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[07/28 17:56:09] d2.engine.hooks INFO: Overall training speed: 6150 iterations in 1:06:17 (0.6467 s / it)
[07/28 17:56:09] d2.engine.hooks INFO: Total training time: 1:26:23 (0:20:06 on hooks)
[07/28 17:56:09] d2.utils.events INFO:  eta: 11:11:18  iter: 6152  total_loss: 3.418  loss_cls_stage0: 0.3363  loss_box_reg_stage0: 0.2913  loss_cls_stage1: 0.4007  loss_box_reg_stage1: 0.7965  loss_cls_stage2: 0.379  loss_box_reg_stage2: 1.005  loss_mask: 0.1033  loss_rpn_cls: 0.01956  loss_rpn_loc: 0.05741  time: 0.6466  data_time: 0.0059  lr: 0.00016  max_mem: 20180M
[07/29 12:34:42] detectron2 INFO: Rank of current process: 0. World size: 1
[07/29 12:34:42] detectron2 INFO: Environment info:
----------------------  -------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.10 (default, May 26 2023, 14:05:08) [GCC 9.4.0]
numpy                   1.23.5
detectron2              0.6 @/home/hoenig/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 9.4
CUDA compiler           CUDA 11.6
detectron2 arch flags   8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          525.125.06
CUDA_HOME               /usr/local/cuda
Pillow                  9.3.0
torchvision             0.13.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221122
iopath                  0.1.9
cv2                     4.7.0
----------------------  -------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[07/29 12:34:42] detectron2 INFO: Command line arguments: Namespace(config_file='projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_s_3x.py', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=True)
[07/29 12:34:42] detectron2 INFO: Contents of args.config_file=projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_s_3x.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcascade_mask_rcnn_mvitv2_t_3x[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15moptimizer[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mlr_multiplier[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtrain[39m


[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mdepth[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mlast_block_indexes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m([39m[38;5;141m0[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m13[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m15[39m[38;5;15m)[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15minit_checkpoint[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://ImageNetPretrained/mvitv2/MViTv2_S_in1k.pyth[39m[38;5;186m"[39m

[07/29 12:34:42] d2.config.lazy WARNING: The config contains objects that cannot serialize to a valid yaml. ./output/config.yaml is human-readable but cannot be loaded.
[07/29 12:34:42] d2.config.lazy WARNING: Config is saved using cloudpickle at ./output/config.yaml.pkl.
[07/29 12:34:42] detectron2 INFO: Full config saved to ./output/config.yaml
[07/29 12:34:42] d2.utils.env INFO: Using a generated random seed 45765081
[07/29 12:34:43] detectron2 INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): MViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      )
      (blocks): ModuleList(
        (0): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=96, out_features=192, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (2): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (3): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=192, out_features=384, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (4): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (6): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.107)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.120)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.133)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.147)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (12): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.160)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (13): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.173)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (14): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.187)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=384, out_features=768, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (15): MultiScaleBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (scale2_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (scale3_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (scale4_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (scale5_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): CascadeROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): ModuleList(
      (0): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (1): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (2): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
    )
    (box_predictor): ModuleList(
      (0): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=31, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (1): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=31, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (2): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=31, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 30, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[07/29 12:34:56] d2.data.datasets.coco INFO: Loading datasets/tless/tless_annotations_train.json takes 11.67 seconds.
[07/29 12:34:56] d2.data.datasets.coco INFO: Loaded 50000 images in COCO format from datasets/tless/tless_annotations_train.json
[07/29 12:34:59] d2.data.build INFO: Removed 0 images with no usable annotations. 50000 images left.
[07/29 12:35:00] d2.data.build INFO: Distribution of instances among all 30 categories:
[36m|  category  | #instances   | category   | #instances   | category   | #instances   |
|:----------:|:-------------|:-----------|:-------------|:-----------|:-------------|
|     1      | 19873        | 2          | 21790        | 3          | 21461        |
|     4      | 21610        | 5          | 23092        | 6          | 23830        |
|     7      | 26345        | 8          | 26271        | 9          | 22606        |
|     10     | 22289        | 11         | 22313        | 12         | 23290        |
|     13     | 20476        | 14         | 22314        | 15         | 22261        |
|     16     | 21595        | 17         | 24514        | 18         | 25429        |
|     19     | 23097        | 20         | 22733        | 21         | 23375        |
|     22     | 23035        | 23         | 24216        | 24         | 22070        |
|     25     | 23853        | 26         | 24387        | 27         | 25290        |
|     28     | 23366        | 29         | 24099        | 30         | 22921        |
|            |              |            |              |            |              |
|   total    | 693801       |            |              |            |              |[0m
[07/29 12:35:00] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [<detectron2.data.transforms.augmentation_impl.RandomApply object at 0x7f09a420d9a0>, ResizeShortestEdge(short_edge_length=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], max_size=1333, sample_style='choice'), RandomFlip()]
[07/29 12:35:00] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[07/29 12:35:00] d2.data.common INFO: Serializing 50000 elements to byte tensors and concatenating them all ...
[07/29 12:35:05] d2.data.common INFO: Serialized dataset takes 389.07 MiB
[07/29 12:35:07] fvcore.common.checkpoint INFO: [Checkpointer] Loading from ./output/model_0004999.pth ...
[07/29 12:35:07] d2.checkpoint.c2_model_loading INFO: Following weights matched with model:
| Names in Model                                  | Names in Checkpoint                                                                                                                                                                                                       | Shapes                                                                                                                   |
|:------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|
| backbone.bottom_up.blocks.0.attn.*              | backbone.bottom_up.blocks.0.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (96,) (96,96) (288,) (288,96) (111,96) (111,96)     |
| backbone.bottom_up.blocks.0.mlp.fc1.*           | backbone.bottom_up.blocks.0.mlp.fc1.{bias,weight}                                                                                                                                                                         | (384,) (384,96)                                                                                                          |
| backbone.bottom_up.blocks.0.mlp.fc2.*           | backbone.bottom_up.blocks.0.mlp.fc2.{bias,weight}                                                                                                                                                                         | (96,) (96,384)                                                                                                           |
| backbone.bottom_up.blocks.0.norm1.*             | backbone.bottom_up.blocks.0.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| backbone.bottom_up.blocks.0.norm2.*             | backbone.bottom_up.blocks.0.norm2.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| backbone.bottom_up.blocks.1.attn.*              | backbone.bottom_up.blocks.1.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,96) (55,96) (55,96)    |
| backbone.bottom_up.blocks.1.mlp.fc1.*           | backbone.bottom_up.blocks.1.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| backbone.bottom_up.blocks.1.mlp.fc2.*           | backbone.bottom_up.blocks.1.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| backbone.bottom_up.blocks.1.norm1.*             | backbone.bottom_up.blocks.1.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| backbone.bottom_up.blocks.1.norm2.*             | backbone.bottom_up.blocks.1.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| backbone.bottom_up.blocks.1.proj.*              | backbone.bottom_up.blocks.1.proj.{bias,weight}                                                                                                                                                                            | (192,) (192,96)                                                                                                          |
| backbone.bottom_up.blocks.10.attn.*             | backbone.bottom_up.blocks.10.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.10.mlp.fc1.*          | backbone.bottom_up.blocks.10.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.10.mlp.fc2.*          | backbone.bottom_up.blocks.10.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.10.norm1.*            | backbone.bottom_up.blocks.10.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.10.norm2.*            | backbone.bottom_up.blocks.10.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.11.attn.*             | backbone.bottom_up.blocks.11.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.11.mlp.fc1.*          | backbone.bottom_up.blocks.11.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.11.mlp.fc2.*          | backbone.bottom_up.blocks.11.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.11.norm1.*            | backbone.bottom_up.blocks.11.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.11.norm2.*            | backbone.bottom_up.blocks.11.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.12.attn.*             | backbone.bottom_up.blocks.12.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.12.mlp.fc1.*          | backbone.bottom_up.blocks.12.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.12.mlp.fc2.*          | backbone.bottom_up.blocks.12.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.12.norm1.*            | backbone.bottom_up.blocks.12.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.12.norm2.*            | backbone.bottom_up.blocks.12.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.13.attn.*             | backbone.bottom_up.blocks.13.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.13.mlp.fc1.*          | backbone.bottom_up.blocks.13.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.13.mlp.fc2.*          | backbone.bottom_up.blocks.13.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.13.norm1.*            | backbone.bottom_up.blocks.13.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.13.norm2.*            | backbone.bottom_up.blocks.13.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.14.attn.*             | backbone.bottom_up.blocks.14.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.14.mlp.fc1.*          | backbone.bottom_up.blocks.14.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| backbone.bottom_up.blocks.14.mlp.fc2.*          | backbone.bottom_up.blocks.14.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| backbone.bottom_up.blocks.14.norm1.*            | backbone.bottom_up.blocks.14.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.14.norm2.*            | backbone.bottom_up.blocks.14.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| backbone.bottom_up.blocks.14.proj.*             | backbone.bottom_up.blocks.14.proj.{bias,weight}                                                                                                                                                                           | (768,) (768,384)                                                                                                         |
| backbone.bottom_up.blocks.15.attn.*             | backbone.bottom_up.blocks.15.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,768) (13,96) (13,96) |
| backbone.bottom_up.blocks.15.mlp.fc1.*          | backbone.bottom_up.blocks.15.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| backbone.bottom_up.blocks.15.mlp.fc2.*          | backbone.bottom_up.blocks.15.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| backbone.bottom_up.blocks.15.norm1.*            | backbone.bottom_up.blocks.15.norm1.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| backbone.bottom_up.blocks.15.norm2.*            | backbone.bottom_up.blocks.15.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| backbone.bottom_up.blocks.2.attn.*              | backbone.bottom_up.blocks.2.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,192) (55,96) (55,96)   |
| backbone.bottom_up.blocks.2.mlp.fc1.*           | backbone.bottom_up.blocks.2.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| backbone.bottom_up.blocks.2.mlp.fc2.*           | backbone.bottom_up.blocks.2.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| backbone.bottom_up.blocks.2.norm1.*             | backbone.bottom_up.blocks.2.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| backbone.bottom_up.blocks.2.norm2.*             | backbone.bottom_up.blocks.2.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| backbone.bottom_up.blocks.3.attn.*              | backbone.bottom_up.blocks.3.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,192) (55,96) (55,96) |
| backbone.bottom_up.blocks.3.mlp.fc1.*           | backbone.bottom_up.blocks.3.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.3.mlp.fc2.*           | backbone.bottom_up.blocks.3.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.3.norm1.*             | backbone.bottom_up.blocks.3.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| backbone.bottom_up.blocks.3.norm2.*             | backbone.bottom_up.blocks.3.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.3.proj.*              | backbone.bottom_up.blocks.3.proj.{bias,weight}                                                                                                                                                                            | (384,) (384,192)                                                                                                         |
| backbone.bottom_up.blocks.4.attn.*              | backbone.bottom_up.blocks.4.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.4.mlp.fc1.*           | backbone.bottom_up.blocks.4.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.4.mlp.fc2.*           | backbone.bottom_up.blocks.4.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.4.norm1.*             | backbone.bottom_up.blocks.4.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.4.norm2.*             | backbone.bottom_up.blocks.4.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.5.attn.*              | backbone.bottom_up.blocks.5.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.5.mlp.fc1.*           | backbone.bottom_up.blocks.5.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.5.mlp.fc2.*           | backbone.bottom_up.blocks.5.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.5.norm1.*             | backbone.bottom_up.blocks.5.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.5.norm2.*             | backbone.bottom_up.blocks.5.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.6.attn.*              | backbone.bottom_up.blocks.6.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.6.mlp.fc1.*           | backbone.bottom_up.blocks.6.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.6.mlp.fc2.*           | backbone.bottom_up.blocks.6.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.6.norm1.*             | backbone.bottom_up.blocks.6.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.6.norm2.*             | backbone.bottom_up.blocks.6.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.7.attn.*              | backbone.bottom_up.blocks.7.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.7.mlp.fc1.*           | backbone.bottom_up.blocks.7.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.7.mlp.fc2.*           | backbone.bottom_up.blocks.7.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.7.norm1.*             | backbone.bottom_up.blocks.7.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.7.norm2.*             | backbone.bottom_up.blocks.7.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.8.attn.*              | backbone.bottom_up.blocks.8.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.8.mlp.fc1.*           | backbone.bottom_up.blocks.8.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.8.mlp.fc2.*           | backbone.bottom_up.blocks.8.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.8.norm1.*             | backbone.bottom_up.blocks.8.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.8.norm2.*             | backbone.bottom_up.blocks.8.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.9.attn.*              | backbone.bottom_up.blocks.9.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.9.mlp.fc1.*           | backbone.bottom_up.blocks.9.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.9.mlp.fc2.*           | backbone.bottom_up.blocks.9.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.9.norm1.*             | backbone.bottom_up.blocks.9.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.9.norm2.*             | backbone.bottom_up.blocks.9.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.patch_embed.proj.*           | backbone.bottom_up.patch_embed.proj.{bias,weight}                                                                                                                                                                         | (96,) (96,3,7,7)                                                                                                         |
| backbone.bottom_up.scale2_norm.*                | backbone.bottom_up.scale2_norm.{bias,weight}                                                                                                                                                                              | (96,) (96,)                                                                                                              |
| backbone.bottom_up.scale3_norm.*                | backbone.bottom_up.scale3_norm.{bias,weight}                                                                                                                                                                              | (192,) (192,)                                                                                                            |
| backbone.bottom_up.scale4_norm.*                | backbone.bottom_up.scale4_norm.{bias,weight}                                                                                                                                                                              | (384,) (384,)                                                                                                            |
| backbone.bottom_up.scale5_norm.*                | backbone.bottom_up.scale5_norm.{bias,weight}                                                                                                                                                                              | (768,) (768,)                                                                                                            |
| backbone.fpn_lateral2.*                         | backbone.fpn_lateral2.{bias,weight}                                                                                                                                                                                       | (256,) (256,96,1,1)                                                                                                      |
| backbone.fpn_lateral3.*                         | backbone.fpn_lateral3.{bias,weight}                                                                                                                                                                                       | (256,) (256,192,1,1)                                                                                                     |
| backbone.fpn_lateral4.*                         | backbone.fpn_lateral4.{bias,weight}                                                                                                                                                                                       | (256,) (256,384,1,1)                                                                                                     |
| backbone.fpn_lateral5.*                         | backbone.fpn_lateral5.{bias,weight}                                                                                                                                                                                       | (256,) (256,768,1,1)                                                                                                     |
| backbone.fpn_output2.*                          | backbone.fpn_output2.{bias,weight}                                                                                                                                                                                        | (256,) (256,256,3,3)                                                                                                     |
| backbone.fpn_output3.*                          | backbone.fpn_output3.{bias,weight}                                                                                                                                                                                        | (256,) (256,256,3,3)                                                                                                     |
| backbone.fpn_output4.*                          | backbone.fpn_output4.{bias,weight}                                                                                                                                                                                        | (256,) (256,256,3,3)                                                                                                     |
| backbone.fpn_output5.*                          | backbone.fpn_output5.{bias,weight}                                                                                                                                                                                        | (256,) (256,256,3,3)                                                                                                     |
| proposal_generator.rpn_head.anchor_deltas.*     | proposal_generator.rpn_head.anchor_deltas.{bias,weight}                                                                                                                                                                   | (12,) (12,256,1,1)                                                                                                       |
| proposal_generator.rpn_head.conv.conv0.*        | proposal_generator.rpn_head.conv.conv0.{bias,weight}                                                                                                                                                                      | (256,) (256,256,3,3)                                                                                                     |
| proposal_generator.rpn_head.conv.conv1.*        | proposal_generator.rpn_head.conv.conv1.{bias,weight}                                                                                                                                                                      | (256,) (256,256,3,3)                                                                                                     |
| proposal_generator.rpn_head.objectness_logits.* | proposal_generator.rpn_head.objectness_logits.{bias,weight}                                                                                                                                                               | (3,) (3,256,1,1)                                                                                                         |
| roi_heads.box_head.0.conv1.*                    | roi_heads.box_head.0.conv1.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.0.conv2.*                    | roi_heads.box_head.0.conv2.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.0.conv3.*                    | roi_heads.box_head.0.conv3.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.0.conv4.*                    | roi_heads.box_head.0.conv4.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.0.fc1.*                      | roi_heads.box_head.0.fc1.{bias,weight}                                                                                                                                                                                    | (1024,) (1024,12544)                                                                                                     |
| roi_heads.box_head.1.conv1.*                    | roi_heads.box_head.1.conv1.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.1.conv2.*                    | roi_heads.box_head.1.conv2.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.1.conv3.*                    | roi_heads.box_head.1.conv3.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.1.conv4.*                    | roi_heads.box_head.1.conv4.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.1.fc1.*                      | roi_heads.box_head.1.fc1.{bias,weight}                                                                                                                                                                                    | (1024,) (1024,12544)                                                                                                     |
| roi_heads.box_head.2.conv1.*                    | roi_heads.box_head.2.conv1.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.2.conv2.*                    | roi_heads.box_head.2.conv2.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.2.conv3.*                    | roi_heads.box_head.2.conv3.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.2.conv4.*                    | roi_heads.box_head.2.conv4.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.2.fc1.*                      | roi_heads.box_head.2.fc1.{bias,weight}                                                                                                                                                                                    | (1024,) (1024,12544)                                                                                                     |
| roi_heads.box_predictor.0.bbox_pred.*           | roi_heads.box_predictor.0.bbox_pred.{bias,weight}                                                                                                                                                                         | (4,) (4,1024)                                                                                                            |
| roi_heads.box_predictor.0.cls_score.*           | roi_heads.box_predictor.0.cls_score.{bias,weight}                                                                                                                                                                         | (31,) (31,1024)                                                                                                          |
| roi_heads.box_predictor.1.bbox_pred.*           | roi_heads.box_predictor.1.bbox_pred.{bias,weight}                                                                                                                                                                         | (4,) (4,1024)                                                                                                            |
| roi_heads.box_predictor.1.cls_score.*           | roi_heads.box_predictor.1.cls_score.{bias,weight}                                                                                                                                                                         | (31,) (31,1024)                                                                                                          |
| roi_heads.box_predictor.2.bbox_pred.*           | roi_heads.box_predictor.2.bbox_pred.{bias,weight}                                                                                                                                                                         | (4,) (4,1024)                                                                                                            |
| roi_heads.box_predictor.2.cls_score.*           | roi_heads.box_predictor.2.cls_score.{bias,weight}                                                                                                                                                                         | (31,) (31,1024)                                                                                                          |
| roi_heads.mask_head.deconv.*                    | roi_heads.mask_head.deconv.{bias,weight}                                                                                                                                                                                  | (256,) (256,256,2,2)                                                                                                     |
| roi_heads.mask_head.mask_fcn1.*                 | roi_heads.mask_head.mask_fcn1.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                  | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.mask_head.mask_fcn2.*                 | roi_heads.mask_head.mask_fcn2.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                  | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.mask_head.mask_fcn3.*                 | roi_heads.mask_head.mask_fcn3.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                  | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.mask_head.mask_fcn4.*                 | roi_heads.mask_head.mask_fcn4.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                  | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.mask_head.predictor.*                 | roi_heads.mask_head.predictor.{bias,weight}                                                                                                                                                                               | (30,) (30,256,1,1)                                                                                                       |
[07/29 12:35:08] fvcore.common.checkpoint INFO: Loading trainer from ./output/model_0004999.pth ...
[07/29 12:35:08] d2.engine.train_loop INFO: Starting training from iteration 5000
[07/29 12:35:23] d2.utils.events INFO:  eta: 10:31:17  iter: 5019  total_loss: 3.596  loss_cls_stage0: 0.3793  loss_box_reg_stage0: 0.3638  loss_cls_stage1: 0.3725  loss_box_reg_stage1: 0.8104  loss_cls_stage2: 0.3696  loss_box_reg_stage2: 0.9916  loss_mask: 0.1193  loss_rpn_cls: 0.01622  loss_rpn_loc: 0.05055  time: 0.6178  data_time: 0.0144  lr: 0.00016  max_mem: 16762M
[07/29 12:35:36] d2.utils.events INFO:  eta: 11:04:03  iter: 5039  total_loss: 3.468  loss_cls_stage0: 0.3504  loss_box_reg_stage0: 0.3116  loss_cls_stage1: 0.3629  loss_box_reg_stage1: 0.7756  loss_cls_stage2: 0.3452  loss_box_reg_stage2: 1.014  loss_mask: 0.1125  loss_rpn_cls: 0.01857  loss_rpn_loc: 0.05168  time: 0.6282  data_time: 0.0046  lr: 0.00016  max_mem: 16762M
[07/29 12:35:48] d2.utils.events INFO:  eta: 10:49:38  iter: 5059  total_loss: 3.777  loss_cls_stage0: 0.427  loss_box_reg_stage0: 0.3601  loss_cls_stage1: 0.4431  loss_box_reg_stage1: 0.8614  loss_cls_stage2: 0.3934  loss_box_reg_stage2: 0.9691  loss_mask: 0.1257  loss_rpn_cls: 0.02882  loss_rpn_loc: 0.06072  time: 0.6222  data_time: 0.0045  lr: 0.00016  max_mem: 16762M
[07/29 12:36:01] d2.utils.events INFO:  eta: 10:49:26  iter: 5079  total_loss: 3.29  loss_cls_stage0: 0.3161  loss_box_reg_stage0: 0.3195  loss_cls_stage1: 0.3564  loss_box_reg_stage1: 0.7935  loss_cls_stage2: 0.3428  loss_box_reg_stage2: 1.004  loss_mask: 0.1043  loss_rpn_cls: 0.01461  loss_rpn_loc: 0.05077  time: 0.6223  data_time: 0.0046  lr: 0.00016  max_mem: 16762M
[07/29 12:36:13] d2.utils.events INFO:  eta: 10:51:39  iter: 5099  total_loss: 3.424  loss_cls_stage0: 0.3561  loss_box_reg_stage0: 0.3449  loss_cls_stage1: 0.3927  loss_box_reg_stage1: 0.8014  loss_cls_stage2: 0.3553  loss_box_reg_stage2: 0.9461  loss_mask: 0.1166  loss_rpn_cls: 0.02269  loss_rpn_loc: 0.05389  time: 0.6239  data_time: 0.0049  lr: 0.00016  max_mem: 16762M
[07/29 12:36:26] d2.utils.events INFO:  eta: 10:51:53  iter: 5119  total_loss: 3.658  loss_cls_stage0: 0.3446  loss_box_reg_stage0: 0.3408  loss_cls_stage1: 0.4112  loss_box_reg_stage1: 0.8528  loss_cls_stage2: 0.3907  loss_box_reg_stage2: 1.034  loss_mask: 0.1254  loss_rpn_cls: 0.01459  loss_rpn_loc: 0.05029  time: 0.6232  data_time: 0.0045  lr: 0.00016  max_mem: 16762M
[07/29 12:36:38] d2.utils.events INFO:  eta: 10:53:40  iter: 5139  total_loss: 3.429  loss_cls_stage0: 0.3444  loss_box_reg_stage0: 0.3139  loss_cls_stage1: 0.383  loss_box_reg_stage1: 0.7752  loss_cls_stage2: 0.3777  loss_box_reg_stage2: 0.9984  loss_mask: 0.1025  loss_rpn_cls: 0.03023  loss_rpn_loc: 0.05024  time: 0.6241  data_time: 0.0049  lr: 0.00016  max_mem: 16762M
[07/29 12:36:50] d2.utils.events INFO:  eta: 10:51:28  iter: 5159  total_loss: 3.051  loss_cls_stage0: 0.3234  loss_box_reg_stage0: 0.3282  loss_cls_stage1: 0.3279  loss_box_reg_stage1: 0.7049  loss_cls_stage2: 0.317  loss_box_reg_stage2: 0.9073  loss_mask: 0.1042  loss_rpn_cls: 0.02498  loss_rpn_loc: 0.05153  time: 0.6214  data_time: 0.0047  lr: 0.00016  max_mem: 16762M
[07/29 12:37:02] d2.utils.events INFO:  eta: 10:51:07  iter: 5179  total_loss: 3.122  loss_cls_stage0: 0.2907  loss_box_reg_stage0: 0.2889  loss_cls_stage1: 0.3232  loss_box_reg_stage1: 0.7299  loss_cls_stage2: 0.3373  loss_box_reg_stage2: 0.9057  loss_mask: 0.102  loss_rpn_cls: 0.02345  loss_rpn_loc: 0.05439  time: 0.6201  data_time: 0.0048  lr: 0.00016  max_mem: 16762M
[07/29 12:37:15] d2.utils.events INFO:  eta: 10:50:54  iter: 5199  total_loss: 3.587  loss_cls_stage0: 0.3655  loss_box_reg_stage0: 0.3344  loss_cls_stage1: 0.403  loss_box_reg_stage1: 0.8096  loss_cls_stage2: 0.4029  loss_box_reg_stage2: 1.067  loss_mask: 0.1284  loss_rpn_cls: 0.02408  loss_rpn_loc: 0.06366  time: 0.6203  data_time: 0.0046  lr: 0.00016  max_mem: 16762M
[07/29 12:37:28] d2.utils.events INFO:  eta: 10:50:50  iter: 5219  total_loss: 3.595  loss_cls_stage0: 0.3494  loss_box_reg_stage0: 0.3219  loss_cls_stage1: 0.3841  loss_box_reg_stage1: 0.8448  loss_cls_stage2: 0.3527  loss_box_reg_stage2: 1.11  loss_mask: 0.1155  loss_rpn_cls: 0.01732  loss_rpn_loc: 0.06689  time: 0.6212  data_time: 0.0046  lr: 0.00016  max_mem: 16762M
[07/29 12:37:40] d2.utils.events INFO:  eta: 10:53:11  iter: 5239  total_loss: 3.65  loss_cls_stage0: 0.3398  loss_box_reg_stage0: 0.3013  loss_cls_stage1: 0.4137  loss_box_reg_stage1: 0.8424  loss_cls_stage2: 0.4227  loss_box_reg_stage2: 1.135  loss_mask: 0.1146  loss_rpn_cls: 0.02022  loss_rpn_loc: 0.05756  time: 0.6231  data_time: 0.0046  lr: 0.00016  max_mem: 16762M
[07/29 12:37:53] d2.utils.events INFO:  eta: 10:54:10  iter: 5259  total_loss: 3.849  loss_cls_stage0: 0.3676  loss_box_reg_stage0: 0.3311  loss_cls_stage1: 0.4395  loss_box_reg_stage1: 0.8899  loss_cls_stage2: 0.4509  loss_box_reg_stage2: 1.136  loss_mask: 0.1118  loss_rpn_cls: 0.01704  loss_rpn_loc: 0.04992  time: 0.6243  data_time: 0.0047  lr: 0.00016  max_mem: 16762M
[07/29 12:38:06] d2.utils.events INFO:  eta: 10:52:46  iter: 5279  total_loss: 3.623  loss_cls_stage0: 0.3769  loss_box_reg_stage0: 0.3391  loss_cls_stage1: 0.4209  loss_box_reg_stage1: 0.8551  loss_cls_stage2: 0.3861  loss_box_reg_stage2: 1.026  loss_mask: 0.129  loss_rpn_cls: 0.02441  loss_rpn_loc: 0.05468  time: 0.6237  data_time: 0.0046  lr: 0.00016  max_mem: 16762M
[07/29 12:38:18] d2.utils.events INFO:  eta: 10:52:24  iter: 5299  total_loss: 3.41  loss_cls_stage0: 0.3451  loss_box_reg_stage0: 0.3381  loss_cls_stage1: 0.3741  loss_box_reg_stage1: 0.8133  loss_cls_stage2: 0.3532  loss_box_reg_stage2: 1.028  loss_mask: 0.1155  loss_rpn_cls: 0.02241  loss_rpn_loc: 0.06637  time: 0.6239  data_time: 0.0046  lr: 0.00016  max_mem: 16762M
[07/29 12:38:31] d2.utils.events INFO:  eta: 10:52:21  iter: 5319  total_loss: 3.384  loss_cls_stage0: 0.3492  loss_box_reg_stage0: 0.3306  loss_cls_stage1: 0.3612  loss_box_reg_stage1: 0.7638  loss_cls_stage2: 0.3507  loss_box_reg_stage2: 1.041  loss_mask: 0.09933  loss_rpn_cls: 0.02107  loss_rpn_loc: 0.05414  time: 0.6240  data_time: 0.0049  lr: 0.00016  max_mem: 16762M
[07/29 12:38:43] d2.utils.events INFO:  eta: 10:54:06  iter: 5339  total_loss: 3.657  loss_cls_stage0: 0.3378  loss_box_reg_stage0: 0.3155  loss_cls_stage1: 0.3762  loss_box_reg_stage1: 0.8479  loss_cls_stage2: 0.3808  loss_box_reg_stage2: 1.112  loss_mask: 0.1081  loss_rpn_cls: 0.01491  loss_rpn_loc: 0.04665  time: 0.6244  data_time: 0.0045  lr: 0.00016  max_mem: 16762M
[07/29 12:38:56] d2.utils.events INFO:  eta: 10:52:00  iter: 5359  total_loss: 3.398  loss_cls_stage0: 0.3407  loss_box_reg_stage0: 0.3383  loss_cls_stage1: 0.3611  loss_box_reg_stage1: 0.7569  loss_cls_stage2: 0.3412  loss_box_reg_stage2: 1.019  loss_mask: 0.1068  loss_rpn_cls: 0.02815  loss_rpn_loc: 0.05383  time: 0.6238  data_time: 0.0045  lr: 0.00016  max_mem: 16762M
[07/29 12:39:08] d2.utils.events INFO:  eta: 10:54:37  iter: 5379  total_loss: 3.273  loss_cls_stage0: 0.3441  loss_box_reg_stage0: 0.3142  loss_cls_stage1: 0.3627  loss_box_reg_stage1: 0.7188  loss_cls_stage2: 0.3149  loss_box_reg_stage2: 0.894  loss_mask: 0.1131  loss_rpn_cls: 0.01751  loss_rpn_loc: 0.04504  time: 0.6244  data_time: 0.0052  lr: 0.00016  max_mem: 16762M
[07/29 12:39:21] d2.utils.events INFO:  eta: 10:53:51  iter: 5399  total_loss: 3.272  loss_cls_stage0: 0.3284  loss_box_reg_stage0: 0.3037  loss_cls_stage1: 0.3535  loss_box_reg_stage1: 0.7568  loss_cls_stage2: 0.33  loss_box_reg_stage2: 0.9335  loss_mask: 0.09942  loss_rpn_cls: 0.02068  loss_rpn_loc: 0.05165  time: 0.6247  data_time: 0.0048  lr: 0.00016  max_mem: 16762M
[07/29 12:39:33] d2.utils.events INFO:  eta: 10:53:15  iter: 5419  total_loss: 3.82  loss_cls_stage0: 0.3743  loss_box_reg_stage0: 0.344  loss_cls_stage1: 0.4229  loss_box_reg_stage1: 0.9085  loss_cls_stage2: 0.3869  loss_box_reg_stage2: 1.095  loss_mask: 0.1319  loss_rpn_cls: 0.02231  loss_rpn_loc: 0.05294  time: 0.6247  data_time: 0.0047  lr: 0.00016  max_mem: 16762M
[07/29 12:39:46] d2.utils.events INFO:  eta: 10:53:03  iter: 5439  total_loss: 3.505  loss_cls_stage0: 0.3461  loss_box_reg_stage0: 0.3518  loss_cls_stage1: 0.3684  loss_box_reg_stage1: 0.8232  loss_cls_stage2: 0.336  loss_box_reg_stage2: 1.013  loss_mask: 0.107  loss_rpn_cls: 0.02158  loss_rpn_loc: 0.05659  time: 0.6249  data_time: 0.0048  lr: 0.00016  max_mem: 16762M
[07/29 12:39:59] d2.utils.events INFO:  eta: 10:54:17  iter: 5459  total_loss: 3.395  loss_cls_stage0: 0.3441  loss_box_reg_stage0: 0.3263  loss_cls_stage1: 0.3955  loss_box_reg_stage1: 0.7725  loss_cls_stage2: 0.3779  loss_box_reg_stage2: 0.9832  loss_mask: 0.1183  loss_rpn_cls: 0.02023  loss_rpn_loc: 0.04666  time: 0.6259  data_time: 0.0050  lr: 0.00016  max_mem: 16762M
[07/29 12:40:12] d2.utils.events INFO:  eta: 10:54:15  iter: 5479  total_loss: 3.53  loss_cls_stage0: 0.3701  loss_box_reg_stage0: 0.3432  loss_cls_stage1: 0.4078  loss_box_reg_stage1: 0.8063  loss_cls_stage2: 0.3764  loss_box_reg_stage2: 1.015  loss_mask: 0.1092  loss_rpn_cls: 0.02249  loss_rpn_loc: 0.05015  time: 0.6259  data_time: 0.0048  lr: 0.00016  max_mem: 16762M
[07/29 12:40:24] d2.utils.events INFO:  eta: 10:54:44  iter: 5499  total_loss: 3.506  loss_cls_stage0: 0.3353  loss_box_reg_stage0: 0.3265  loss_cls_stage1: 0.3747  loss_box_reg_stage1: 0.7868  loss_cls_stage2: 0.3634  loss_box_reg_stage2: 0.993  loss_mask: 0.1049  loss_rpn_cls: 0.02481  loss_rpn_loc: 0.06758  time: 0.6263  data_time: 0.0049  lr: 0.00016  max_mem: 16762M
[07/29 12:40:37] d2.utils.events INFO:  eta: 10:54:29  iter: 5519  total_loss: 3.093  loss_cls_stage0: 0.3382  loss_box_reg_stage0: 0.3161  loss_cls_stage1: 0.3487  loss_box_reg_stage1: 0.7072  loss_cls_stage2: 0.3149  loss_box_reg_stage2: 0.868  loss_mask: 0.1149  loss_rpn_cls: 0.01856  loss_rpn_loc: 0.04655  time: 0.6261  data_time: 0.0049  lr: 0.00016  max_mem: 16762M
[07/29 12:40:49] d2.utils.events INFO:  eta: 10:54:37  iter: 5539  total_loss: 3.236  loss_cls_stage0: 0.3244  loss_box_reg_stage0: 0.3154  loss_cls_stage1: 0.3243  loss_box_reg_stage1: 0.7412  loss_cls_stage2: 0.3163  loss_box_reg_stage2: 1.006  loss_mask: 0.112  loss_rpn_cls: 0.01765  loss_rpn_loc: 0.05549  time: 0.6263  data_time: 0.0048  lr: 0.00016  max_mem: 16762M
[07/29 12:41:01] d2.utils.events INFO:  eta: 10:54:03  iter: 5559  total_loss: 3.37  loss_cls_stage0: 0.3577  loss_box_reg_stage0: 0.3322  loss_cls_stage1: 0.4059  loss_box_reg_stage1: 0.7903  loss_cls_stage2: 0.3882  loss_box_reg_stage2: 0.9314  loss_mask: 0.09661  loss_rpn_cls: 0.02291  loss_rpn_loc: 0.05716  time: 0.6251  data_time: 0.0044  lr: 0.00016  max_mem: 16762M
[07/29 12:41:14] d2.utils.events INFO:  eta: 10:53:54  iter: 5579  total_loss: 3.3  loss_cls_stage0: 0.3402  loss_box_reg_stage0: 0.3228  loss_cls_stage1: 0.3786  loss_box_reg_stage1: 0.7471  loss_cls_stage2: 0.362  loss_box_reg_stage2: 0.9459  loss_mask: 0.0976  loss_rpn_cls: 0.01946  loss_rpn_loc: 0.05275  time: 0.6251  data_time: 0.0047  lr: 0.00016  max_mem: 16762M
[07/29 12:41:26] d2.utils.events INFO:  eta: 10:53:42  iter: 5599  total_loss: 3.331  loss_cls_stage0: 0.3589  loss_box_reg_stage0: 0.3606  loss_cls_stage1: 0.3493  loss_box_reg_stage1: 0.799  loss_cls_stage2: 0.3248  loss_box_reg_stage2: 0.993  loss_mask: 0.1202  loss_rpn_cls: 0.02192  loss_rpn_loc: 0.07547  time: 0.6250  data_time: 0.0048  lr: 0.00016  max_mem: 16762M
[07/29 12:41:39] d2.utils.events INFO:  eta: 10:54:07  iter: 5619  total_loss: 3.532  loss_cls_stage0: 0.3717  loss_box_reg_stage0: 0.3665  loss_cls_stage1: 0.3884  loss_box_reg_stage1: 0.8105  loss_cls_stage2: 0.3452  loss_box_reg_stage2: 0.9639  loss_mask: 0.1162  loss_rpn_cls: 0.0173  loss_rpn_loc: 0.05112  time: 0.6262  data_time: 0.0045  lr: 0.00016  max_mem: 17807M
[07/29 12:41:52] d2.utils.events INFO:  eta: 10:53:54  iter: 5639  total_loss: 3.161  loss_cls_stage0: 0.3577  loss_box_reg_stage0: 0.3263  loss_cls_stage1: 0.3429  loss_box_reg_stage1: 0.6893  loss_cls_stage2: 0.325  loss_box_reg_stage2: 0.8603  loss_mask: 0.111  loss_rpn_cls: 0.03394  loss_rpn_loc: 0.05404  time: 0.6260  data_time: 0.0048  lr: 0.00016  max_mem: 17807M
[07/29 12:42:04] d2.utils.events INFO:  eta: 10:53:20  iter: 5659  total_loss: 3.219  loss_cls_stage0: 0.3314  loss_box_reg_stage0: 0.3205  loss_cls_stage1: 0.3678  loss_box_reg_stage1: 0.724  loss_cls_stage2: 0.3246  loss_box_reg_stage2: 0.9357  loss_mask: 0.1107  loss_rpn_cls: 0.02315  loss_rpn_loc: 0.05885  time: 0.6256  data_time: 0.0049  lr: 0.00016  max_mem: 17807M
[07/29 12:42:17] d2.utils.events INFO:  eta: 10:53:08  iter: 5679  total_loss: 3.384  loss_cls_stage0: 0.3874  loss_box_reg_stage0: 0.3467  loss_cls_stage1: 0.3978  loss_box_reg_stage1: 0.8137  loss_cls_stage2: 0.3737  loss_box_reg_stage2: 0.9671  loss_mask: 0.1175  loss_rpn_cls: 0.02078  loss_rpn_loc: 0.04452  time: 0.6256  data_time: 0.0046  lr: 0.00016  max_mem: 17807M
[07/29 12:42:29] d2.utils.events INFO:  eta: 10:52:55  iter: 5699  total_loss: 3.387  loss_cls_stage0: 0.3442  loss_box_reg_stage0: 0.3194  loss_cls_stage1: 0.3914  loss_box_reg_stage1: 0.7509  loss_cls_stage2: 0.3603  loss_box_reg_stage2: 0.9975  loss_mask: 0.1105  loss_rpn_cls: 0.01776  loss_rpn_loc: 0.05235  time: 0.6258  data_time: 0.0047  lr: 0.00016  max_mem: 17807M
[07/29 12:42:42] d2.utils.events INFO:  eta: 10:52:29  iter: 5719  total_loss: 3.599  loss_cls_stage0: 0.3587  loss_box_reg_stage0: 0.3385  loss_cls_stage1: 0.3952  loss_box_reg_stage1: 0.8167  loss_cls_stage2: 0.3779  loss_box_reg_stage2: 1.066  loss_mask: 0.1211  loss_rpn_cls: 0.02297  loss_rpn_loc: 0.05188  time: 0.6258  data_time: 0.0051  lr: 0.00016  max_mem: 17807M
[07/29 12:42:54] d2.utils.events INFO:  eta: 10:52:16  iter: 5739  total_loss: 3.529  loss_cls_stage0: 0.3519  loss_box_reg_stage0: 0.3143  loss_cls_stage1: 0.3949  loss_box_reg_stage1: 0.7656  loss_cls_stage2: 0.3684  loss_box_reg_stage2: 1.027  loss_mask: 0.09837  loss_rpn_cls: 0.02154  loss_rpn_loc: 0.05673  time: 0.6258  data_time: 0.0050  lr: 0.00016  max_mem: 17807M
[07/29 12:43:07] d2.utils.events INFO:  eta: 10:52:00  iter: 5759  total_loss: 2.951  loss_cls_stage0: 0.3182  loss_box_reg_stage0: 0.3275  loss_cls_stage1: 0.3052  loss_box_reg_stage1: 0.6722  loss_cls_stage2: 0.2884  loss_box_reg_stage2: 0.846  loss_mask: 0.1033  loss_rpn_cls: 0.03129  loss_rpn_loc: 0.06579  time: 0.6256  data_time: 0.0051  lr: 0.00016  max_mem: 17807M
[07/29 12:43:19] d2.utils.events INFO:  eta: 10:51:51  iter: 5779  total_loss: 3.111  loss_cls_stage0: 0.3223  loss_box_reg_stage0: 0.3287  loss_cls_stage1: 0.3381  loss_box_reg_stage1: 0.7207  loss_cls_stage2: 0.2964  loss_box_reg_stage2: 0.9163  loss_mask: 0.1059  loss_rpn_cls: 0.01846  loss_rpn_loc: 0.05388  time: 0.6258  data_time: 0.0046  lr: 0.00016  max_mem: 17807M
[07/29 12:43:24] d2.engine.hooks INFO: Overall training speed: 785 iterations in 0:08:11 (0.6256 s / it)
[07/29 12:43:24] d2.engine.hooks INFO: Total training time: 0:08:11 (0:00:00 on hooks)
[07/29 12:43:24] d2.utils.events INFO:  eta: 10:51:43  iter: 5787  total_loss: 3.338  loss_cls_stage0: 0.3416  loss_box_reg_stage0: 0.3375  loss_cls_stage1: 0.3583  loss_box_reg_stage1: 0.7723  loss_cls_stage2: 0.3273  loss_box_reg_stage2: 0.9576  loss_mask: 0.1084  loss_rpn_cls: 0.0168  loss_rpn_loc: 0.05388  time: 0.6255  data_time: 0.0047  lr: 0.00016  max_mem: 17807M
[07/29 12:46:15] detectron2 INFO: Rank of current process: 0. World size: 1
[07/29 12:46:15] detectron2 INFO: Environment info:
----------------------  -------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.10 (default, May 26 2023, 14:05:08) [GCC 9.4.0]
numpy                   1.23.5
detectron2              0.6 @/home/hoenig/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 9.4
CUDA compiler           CUDA 11.6
detectron2 arch flags   8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          525.125.06
CUDA_HOME               /usr/local/cuda
Pillow                  9.3.0
torchvision             0.13.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221122
iopath                  0.1.9
cv2                     4.7.0
----------------------  -------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[07/29 12:46:15] detectron2 INFO: Command line arguments: Namespace(config_file='projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_s_3x.py', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=True)
[07/29 12:46:15] detectron2 INFO: Contents of args.config_file=projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_s_3x.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcascade_mask_rcnn_mvitv2_t_3x[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15moptimizer[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mlr_multiplier[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtrain[39m


[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mdepth[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mlast_block_indexes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m([39m[38;5;141m0[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m13[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m15[39m[38;5;15m)[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15minit_checkpoint[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://ImageNetPretrained/mvitv2/MViTv2_S_in1k.pyth[39m[38;5;186m"[39m

[07/29 12:46:15] d2.config.lazy WARNING: The config contains objects that cannot serialize to a valid yaml. ./output/config.yaml is human-readable but cannot be loaded.
[07/29 12:46:15] d2.config.lazy WARNING: Config is saved using cloudpickle at ./output/config.yaml.pkl.
[07/29 12:46:15] detectron2 INFO: Full config saved to ./output/config.yaml
[07/29 12:46:15] d2.utils.env INFO: Using a generated random seed 19019960
[07/29 12:46:16] detectron2 INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): MViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      )
      (blocks): ModuleList(
        (0): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=96, out_features=192, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (2): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (3): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=192, out_features=384, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (4): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (6): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.107)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.120)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.133)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.147)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (12): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.160)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (13): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.173)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (14): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.187)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=384, out_features=768, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (15): MultiScaleBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (scale2_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (scale3_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (scale4_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (scale5_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): CascadeROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): ModuleList(
      (0): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (1): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (2): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
    )
    (box_predictor): ModuleList(
      (0): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=31, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (1): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=31, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (2): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=31, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 30, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[07/29 12:46:28] d2.data.datasets.coco INFO: Loading datasets/tless/tless_annotations_train.json takes 11.39 seconds.
[07/29 12:46:29] d2.data.datasets.coco INFO: Loaded 50000 images in COCO format from datasets/tless/tless_annotations_train.json
[07/29 12:46:32] d2.data.build INFO: Removed 0 images with no usable annotations. 50000 images left.
[07/29 12:46:34] d2.data.build INFO: Distribution of instances among all 30 categories:
[36m|  category  | #instances   | category   | #instances   | category   | #instances   |
|:----------:|:-------------|:-----------|:-------------|:-----------|:-------------|
|     1      | 19873        | 2          | 21790        | 3          | 21461        |
|     4      | 21610        | 5          | 23092        | 6          | 23830        |
|     7      | 26345        | 8          | 26271        | 9          | 22606        |
|     10     | 22289        | 11         | 22313        | 12         | 23290        |
|     13     | 20476        | 14         | 22314        | 15         | 22261        |
|     16     | 21595        | 17         | 24514        | 18         | 25429        |
|     19     | 23097        | 20         | 22733        | 21         | 23375        |
|     22     | 23035        | 23         | 24216        | 24         | 22070        |
|     25     | 23853        | 26         | 24387        | 27         | 25290        |
|     28     | 23366        | 29         | 24099        | 30         | 22921        |
|            |              |            |              |            |              |
|   total    | 693801       |            |              |            |              |[0m
[07/29 12:46:34] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [<detectron2.data.transforms.augmentation_impl.RandomApply object at 0x7f4e8d409c40>, ResizeShortestEdge(short_edge_length=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], max_size=1333, sample_style='choice'), RandomFlip()]
[07/29 12:46:34] d2.config.instantiate ERROR: Error when instantiating detectron2.data.build.build_detection_train_loader!
[07/29 12:47:04] detectron2 INFO: Rank of current process: 0. World size: 1
[07/29 12:47:04] detectron2 INFO: Environment info:
----------------------  -------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.10 (default, May 26 2023, 14:05:08) [GCC 9.4.0]
numpy                   1.23.5
detectron2              0.6 @/home/hoenig/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 9.4
CUDA compiler           CUDA 11.6
detectron2 arch flags   8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          525.125.06
CUDA_HOME               /usr/local/cuda
Pillow                  9.3.0
torchvision             0.13.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221122
iopath                  0.1.9
cv2                     4.7.0
----------------------  -------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[07/29 12:47:04] detectron2 INFO: Command line arguments: Namespace(config_file='projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_s_3x.py', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=True)
[07/29 12:47:04] detectron2 INFO: Contents of args.config_file=projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_s_3x.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcascade_mask_rcnn_mvitv2_t_3x[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15moptimizer[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mlr_multiplier[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtrain[39m


[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mdepth[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mlast_block_indexes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m([39m[38;5;141m0[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m13[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m15[39m[38;5;15m)[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15minit_checkpoint[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://ImageNetPretrained/mvitv2/MViTv2_S_in1k.pyth[39m[38;5;186m"[39m

[07/29 12:47:04] d2.config.lazy WARNING: The config contains objects that cannot serialize to a valid yaml. ./output/config.yaml is human-readable but cannot be loaded.
[07/29 12:47:04] d2.config.lazy WARNING: Config is saved using cloudpickle at ./output/config.yaml.pkl.
[07/29 12:47:04] detectron2 INFO: Full config saved to ./output/config.yaml
[07/29 12:47:04] d2.utils.env INFO: Using a generated random seed 7780854
[07/29 12:47:05] detectron2 INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): MViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      )
      (blocks): ModuleList(
        (0): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=96, out_features=192, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (2): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (3): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=192, out_features=384, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (4): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (6): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.107)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.120)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.133)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.147)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (12): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.160)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (13): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.173)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (14): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.187)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=384, out_features=768, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (15): MultiScaleBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (scale2_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (scale3_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (scale4_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (scale5_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): CascadeROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): ModuleList(
      (0): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (1): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (2): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
    )
    (box_predictor): ModuleList(
      (0): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=31, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (1): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=31, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (2): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=31, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 30, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[07/29 12:47:17] d2.data.datasets.coco INFO: Loading datasets/tless/tless_annotations_train.json takes 11.04 seconds.
[07/29 12:47:17] d2.data.datasets.coco INFO: Loaded 50000 images in COCO format from datasets/tless/tless_annotations_train.json
[07/29 12:47:21] d2.data.build INFO: Removed 0 images with no usable annotations. 50000 images left.
[07/29 12:47:22] d2.data.build INFO: Distribution of instances among all 30 categories:
[36m|  category  | #instances   | category   | #instances   | category   | #instances   |
|:----------:|:-------------|:-----------|:-------------|:-----------|:-------------|
|     1      | 19873        | 2          | 21790        | 3          | 21461        |
|     4      | 21610        | 5          | 23092        | 6          | 23830        |
|     7      | 26345        | 8          | 26271        | 9          | 22606        |
|     10     | 22289        | 11         | 22313        | 12         | 23290        |
|     13     | 20476        | 14         | 22314        | 15         | 22261        |
|     16     | 21595        | 17         | 24514        | 18         | 25429        |
|     19     | 23097        | 20         | 22733        | 21         | 23375        |
|     22     | 23035        | 23         | 24216        | 24         | 22070        |
|     25     | 23853        | 26         | 24387        | 27         | 25290        |
|     28     | 23366        | 29         | 24099        | 30         | 22921        |
|            |              |            |              |            |              |
|   total    | 693801       |            |              |            |              |[0m
[07/29 12:47:22] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [<detectron2.data.transforms.augmentation_impl.RandomApply object at 0x7f12d814c4c0>, ResizeShortestEdge(short_edge_length=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], max_size=1333, sample_style='choice'), RandomFlip()]
[07/29 12:47:22] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[07/29 12:47:22] d2.data.common INFO: Serializing 50000 elements to byte tensors and concatenating them all ...
[07/29 12:47:26] d2.data.common INFO: Serialized dataset takes 389.07 MiB
[07/29 12:47:28] fvcore.common.checkpoint INFO: [Checkpointer] Loading from ./output/model_0004999.pth ...
[07/29 12:47:28] d2.checkpoint.c2_model_loading INFO: Following weights matched with model:
| Names in Model                                  | Names in Checkpoint                                                                                                                                                                                                       | Shapes                                                                                                                   |
|:------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|
| backbone.bottom_up.blocks.0.attn.*              | backbone.bottom_up.blocks.0.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (96,) (96,96) (288,) (288,96) (111,96) (111,96)     |
| backbone.bottom_up.blocks.0.mlp.fc1.*           | backbone.bottom_up.blocks.0.mlp.fc1.{bias,weight}                                                                                                                                                                         | (384,) (384,96)                                                                                                          |
| backbone.bottom_up.blocks.0.mlp.fc2.*           | backbone.bottom_up.blocks.0.mlp.fc2.{bias,weight}                                                                                                                                                                         | (96,) (96,384)                                                                                                           |
| backbone.bottom_up.blocks.0.norm1.*             | backbone.bottom_up.blocks.0.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| backbone.bottom_up.blocks.0.norm2.*             | backbone.bottom_up.blocks.0.norm2.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| backbone.bottom_up.blocks.1.attn.*              | backbone.bottom_up.blocks.1.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,96) (55,96) (55,96)    |
| backbone.bottom_up.blocks.1.mlp.fc1.*           | backbone.bottom_up.blocks.1.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| backbone.bottom_up.blocks.1.mlp.fc2.*           | backbone.bottom_up.blocks.1.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| backbone.bottom_up.blocks.1.norm1.*             | backbone.bottom_up.blocks.1.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| backbone.bottom_up.blocks.1.norm2.*             | backbone.bottom_up.blocks.1.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| backbone.bottom_up.blocks.1.proj.*              | backbone.bottom_up.blocks.1.proj.{bias,weight}                                                                                                                                                                            | (192,) (192,96)                                                                                                          |
| backbone.bottom_up.blocks.10.attn.*             | backbone.bottom_up.blocks.10.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.10.mlp.fc1.*          | backbone.bottom_up.blocks.10.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.10.mlp.fc2.*          | backbone.bottom_up.blocks.10.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.10.norm1.*            | backbone.bottom_up.blocks.10.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.10.norm2.*            | backbone.bottom_up.blocks.10.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.11.attn.*             | backbone.bottom_up.blocks.11.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.11.mlp.fc1.*          | backbone.bottom_up.blocks.11.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.11.mlp.fc2.*          | backbone.bottom_up.blocks.11.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.11.norm1.*            | backbone.bottom_up.blocks.11.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.11.norm2.*            | backbone.bottom_up.blocks.11.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.12.attn.*             | backbone.bottom_up.blocks.12.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.12.mlp.fc1.*          | backbone.bottom_up.blocks.12.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.12.mlp.fc2.*          | backbone.bottom_up.blocks.12.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.12.norm1.*            | backbone.bottom_up.blocks.12.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.12.norm2.*            | backbone.bottom_up.blocks.12.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.13.attn.*             | backbone.bottom_up.blocks.13.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.13.mlp.fc1.*          | backbone.bottom_up.blocks.13.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.13.mlp.fc2.*          | backbone.bottom_up.blocks.13.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.13.norm1.*            | backbone.bottom_up.blocks.13.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.13.norm2.*            | backbone.bottom_up.blocks.13.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.14.attn.*             | backbone.bottom_up.blocks.14.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.14.mlp.fc1.*          | backbone.bottom_up.blocks.14.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| backbone.bottom_up.blocks.14.mlp.fc2.*          | backbone.bottom_up.blocks.14.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| backbone.bottom_up.blocks.14.norm1.*            | backbone.bottom_up.blocks.14.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.14.norm2.*            | backbone.bottom_up.blocks.14.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| backbone.bottom_up.blocks.14.proj.*             | backbone.bottom_up.blocks.14.proj.{bias,weight}                                                                                                                                                                           | (768,) (768,384)                                                                                                         |
| backbone.bottom_up.blocks.15.attn.*             | backbone.bottom_up.blocks.15.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,768) (13,96) (13,96) |
| backbone.bottom_up.blocks.15.mlp.fc1.*          | backbone.bottom_up.blocks.15.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| backbone.bottom_up.blocks.15.mlp.fc2.*          | backbone.bottom_up.blocks.15.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| backbone.bottom_up.blocks.15.norm1.*            | backbone.bottom_up.blocks.15.norm1.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| backbone.bottom_up.blocks.15.norm2.*            | backbone.bottom_up.blocks.15.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| backbone.bottom_up.blocks.2.attn.*              | backbone.bottom_up.blocks.2.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,192) (55,96) (55,96)   |
| backbone.bottom_up.blocks.2.mlp.fc1.*           | backbone.bottom_up.blocks.2.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| backbone.bottom_up.blocks.2.mlp.fc2.*           | backbone.bottom_up.blocks.2.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| backbone.bottom_up.blocks.2.norm1.*             | backbone.bottom_up.blocks.2.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| backbone.bottom_up.blocks.2.norm2.*             | backbone.bottom_up.blocks.2.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| backbone.bottom_up.blocks.3.attn.*              | backbone.bottom_up.blocks.3.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,192) (55,96) (55,96) |
| backbone.bottom_up.blocks.3.mlp.fc1.*           | backbone.bottom_up.blocks.3.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.3.mlp.fc2.*           | backbone.bottom_up.blocks.3.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.3.norm1.*             | backbone.bottom_up.blocks.3.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| backbone.bottom_up.blocks.3.norm2.*             | backbone.bottom_up.blocks.3.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.3.proj.*              | backbone.bottom_up.blocks.3.proj.{bias,weight}                                                                                                                                                                            | (384,) (384,192)                                                                                                         |
| backbone.bottom_up.blocks.4.attn.*              | backbone.bottom_up.blocks.4.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.4.mlp.fc1.*           | backbone.bottom_up.blocks.4.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.4.mlp.fc2.*           | backbone.bottom_up.blocks.4.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.4.norm1.*             | backbone.bottom_up.blocks.4.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.4.norm2.*             | backbone.bottom_up.blocks.4.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.5.attn.*              | backbone.bottom_up.blocks.5.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.5.mlp.fc1.*           | backbone.bottom_up.blocks.5.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.5.mlp.fc2.*           | backbone.bottom_up.blocks.5.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.5.norm1.*             | backbone.bottom_up.blocks.5.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.5.norm2.*             | backbone.bottom_up.blocks.5.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.6.attn.*              | backbone.bottom_up.blocks.6.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.6.mlp.fc1.*           | backbone.bottom_up.blocks.6.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.6.mlp.fc2.*           | backbone.bottom_up.blocks.6.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.6.norm1.*             | backbone.bottom_up.blocks.6.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.6.norm2.*             | backbone.bottom_up.blocks.6.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.7.attn.*              | backbone.bottom_up.blocks.7.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.7.mlp.fc1.*           | backbone.bottom_up.blocks.7.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.7.mlp.fc2.*           | backbone.bottom_up.blocks.7.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.7.norm1.*             | backbone.bottom_up.blocks.7.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.7.norm2.*             | backbone.bottom_up.blocks.7.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.8.attn.*              | backbone.bottom_up.blocks.8.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.8.mlp.fc1.*           | backbone.bottom_up.blocks.8.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.8.mlp.fc2.*           | backbone.bottom_up.blocks.8.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.8.norm1.*             | backbone.bottom_up.blocks.8.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.8.norm2.*             | backbone.bottom_up.blocks.8.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.9.attn.*              | backbone.bottom_up.blocks.9.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.9.mlp.fc1.*           | backbone.bottom_up.blocks.9.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.9.mlp.fc2.*           | backbone.bottom_up.blocks.9.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.9.norm1.*             | backbone.bottom_up.blocks.9.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.9.norm2.*             | backbone.bottom_up.blocks.9.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.patch_embed.proj.*           | backbone.bottom_up.patch_embed.proj.{bias,weight}                                                                                                                                                                         | (96,) (96,3,7,7)                                                                                                         |
| backbone.bottom_up.scale2_norm.*                | backbone.bottom_up.scale2_norm.{bias,weight}                                                                                                                                                                              | (96,) (96,)                                                                                                              |
| backbone.bottom_up.scale3_norm.*                | backbone.bottom_up.scale3_norm.{bias,weight}                                                                                                                                                                              | (192,) (192,)                                                                                                            |
| backbone.bottom_up.scale4_norm.*                | backbone.bottom_up.scale4_norm.{bias,weight}                                                                                                                                                                              | (384,) (384,)                                                                                                            |
| backbone.bottom_up.scale5_norm.*                | backbone.bottom_up.scale5_norm.{bias,weight}                                                                                                                                                                              | (768,) (768,)                                                                                                            |
| backbone.fpn_lateral2.*                         | backbone.fpn_lateral2.{bias,weight}                                                                                                                                                                                       | (256,) (256,96,1,1)                                                                                                      |
| backbone.fpn_lateral3.*                         | backbone.fpn_lateral3.{bias,weight}                                                                                                                                                                                       | (256,) (256,192,1,1)                                                                                                     |
| backbone.fpn_lateral4.*                         | backbone.fpn_lateral4.{bias,weight}                                                                                                                                                                                       | (256,) (256,384,1,1)                                                                                                     |
| backbone.fpn_lateral5.*                         | backbone.fpn_lateral5.{bias,weight}                                                                                                                                                                                       | (256,) (256,768,1,1)                                                                                                     |
| backbone.fpn_output2.*                          | backbone.fpn_output2.{bias,weight}                                                                                                                                                                                        | (256,) (256,256,3,3)                                                                                                     |
| backbone.fpn_output3.*                          | backbone.fpn_output3.{bias,weight}                                                                                                                                                                                        | (256,) (256,256,3,3)                                                                                                     |
| backbone.fpn_output4.*                          | backbone.fpn_output4.{bias,weight}                                                                                                                                                                                        | (256,) (256,256,3,3)                                                                                                     |
| backbone.fpn_output5.*                          | backbone.fpn_output5.{bias,weight}                                                                                                                                                                                        | (256,) (256,256,3,3)                                                                                                     |
| proposal_generator.rpn_head.anchor_deltas.*     | proposal_generator.rpn_head.anchor_deltas.{bias,weight}                                                                                                                                                                   | (12,) (12,256,1,1)                                                                                                       |
| proposal_generator.rpn_head.conv.conv0.*        | proposal_generator.rpn_head.conv.conv0.{bias,weight}                                                                                                                                                                      | (256,) (256,256,3,3)                                                                                                     |
| proposal_generator.rpn_head.conv.conv1.*        | proposal_generator.rpn_head.conv.conv1.{bias,weight}                                                                                                                                                                      | (256,) (256,256,3,3)                                                                                                     |
| proposal_generator.rpn_head.objectness_logits.* | proposal_generator.rpn_head.objectness_logits.{bias,weight}                                                                                                                                                               | (3,) (3,256,1,1)                                                                                                         |
| roi_heads.box_head.0.conv1.*                    | roi_heads.box_head.0.conv1.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.0.conv2.*                    | roi_heads.box_head.0.conv2.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.0.conv3.*                    | roi_heads.box_head.0.conv3.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.0.conv4.*                    | roi_heads.box_head.0.conv4.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.0.fc1.*                      | roi_heads.box_head.0.fc1.{bias,weight}                                                                                                                                                                                    | (1024,) (1024,12544)                                                                                                     |
| roi_heads.box_head.1.conv1.*                    | roi_heads.box_head.1.conv1.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.1.conv2.*                    | roi_heads.box_head.1.conv2.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.1.conv3.*                    | roi_heads.box_head.1.conv3.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.1.conv4.*                    | roi_heads.box_head.1.conv4.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.1.fc1.*                      | roi_heads.box_head.1.fc1.{bias,weight}                                                                                                                                                                                    | (1024,) (1024,12544)                                                                                                     |
| roi_heads.box_head.2.conv1.*                    | roi_heads.box_head.2.conv1.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.2.conv2.*                    | roi_heads.box_head.2.conv2.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.2.conv3.*                    | roi_heads.box_head.2.conv3.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.2.conv4.*                    | roi_heads.box_head.2.conv4.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.2.fc1.*                      | roi_heads.box_head.2.fc1.{bias,weight}                                                                                                                                                                                    | (1024,) (1024,12544)                                                                                                     |
| roi_heads.box_predictor.0.bbox_pred.*           | roi_heads.box_predictor.0.bbox_pred.{bias,weight}                                                                                                                                                                         | (4,) (4,1024)                                                                                                            |
| roi_heads.box_predictor.0.cls_score.*           | roi_heads.box_predictor.0.cls_score.{bias,weight}                                                                                                                                                                         | (31,) (31,1024)                                                                                                          |
| roi_heads.box_predictor.1.bbox_pred.*           | roi_heads.box_predictor.1.bbox_pred.{bias,weight}                                                                                                                                                                         | (4,) (4,1024)                                                                                                            |
| roi_heads.box_predictor.1.cls_score.*           | roi_heads.box_predictor.1.cls_score.{bias,weight}                                                                                                                                                                         | (31,) (31,1024)                                                                                                          |
| roi_heads.box_predictor.2.bbox_pred.*           | roi_heads.box_predictor.2.bbox_pred.{bias,weight}                                                                                                                                                                         | (4,) (4,1024)                                                                                                            |
| roi_heads.box_predictor.2.cls_score.*           | roi_heads.box_predictor.2.cls_score.{bias,weight}                                                                                                                                                                         | (31,) (31,1024)                                                                                                          |
| roi_heads.mask_head.deconv.*                    | roi_heads.mask_head.deconv.{bias,weight}                                                                                                                                                                                  | (256,) (256,256,2,2)                                                                                                     |
| roi_heads.mask_head.mask_fcn1.*                 | roi_heads.mask_head.mask_fcn1.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                  | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.mask_head.mask_fcn2.*                 | roi_heads.mask_head.mask_fcn2.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                  | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.mask_head.mask_fcn3.*                 | roi_heads.mask_head.mask_fcn3.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                  | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.mask_head.mask_fcn4.*                 | roi_heads.mask_head.mask_fcn4.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                  | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.mask_head.predictor.*                 | roi_heads.mask_head.predictor.{bias,weight}                                                                                                                                                                               | (30,) (30,256,1,1)                                                                                                       |
[07/29 12:47:29] fvcore.common.checkpoint INFO: Loading trainer from ./output/model_0004999.pth ...
[07/29 12:47:29] d2.engine.train_loop INFO: Starting training from iteration 5000
[07/29 12:47:44] d2.utils.events INFO:  eta: 2 days, 16:00:27  iter: 5019  total_loss: 3.445  loss_cls_stage0: 0.3682  loss_box_reg_stage0: 0.3329  loss_cls_stage1: 0.3611  loss_box_reg_stage1: 0.7536  loss_cls_stage2: 0.3578  loss_box_reg_stage2: 0.9468  loss_mask: 0.12  loss_rpn_cls: 0.02465  loss_rpn_loc: 0.04846  time: 0.6216  data_time: 0.0165  lr: 0.00016  max_mem: 16131M
[07/29 12:47:56] d2.utils.events INFO:  eta: 2 days, 15:41:07  iter: 5039  total_loss: 3.463  loss_cls_stage0: 0.3364  loss_box_reg_stage0: 0.3449  loss_cls_stage1: 0.3826  loss_box_reg_stage1: 0.7956  loss_cls_stage2: 0.3534  loss_box_reg_stage2: 0.9893  loss_mask: 0.1149  loss_rpn_cls: 0.01981  loss_rpn_loc: 0.0487  time: 0.6139  data_time: 0.0049  lr: 0.00016  max_mem: 16131M
[07/29 12:48:08] d2.utils.events INFO:  eta: 2 days, 16:15:01  iter: 5059  total_loss: 3.66  loss_cls_stage0: 0.3386  loss_box_reg_stage0: 0.3136  loss_cls_stage1: 0.3745  loss_box_reg_stage1: 0.8603  loss_cls_stage2: 0.3921  loss_box_reg_stage2: 1.066  loss_mask: 0.1163  loss_rpn_cls: 0.01805  loss_rpn_loc: 0.04751  time: 0.6195  data_time: 0.0051  lr: 0.00016  max_mem: 16131M
[07/29 12:48:18] d2.engine.hooks INFO: Overall training speed: 73 iterations in 0:00:45 (0.6293 s / it)
[07/29 12:48:18] d2.engine.hooks INFO: Total training time: 0:00:45 (0:00:00 on hooks)
[07/29 12:48:18] d2.utils.events INFO:  eta: 2 days, 16:24:46  iter: 5075  total_loss: 3.297  loss_cls_stage0: 0.3363  loss_box_reg_stage0: 0.3005  loss_cls_stage1: 0.357  loss_box_reg_stage1: 0.7971  loss_cls_stage2: 0.3267  loss_box_reg_stage2: 1.155  loss_mask: 0.1021  loss_rpn_cls: 0.01286  loss_rpn_loc: 0.04558  time: 0.6202  data_time: 0.0053  lr: 0.00016  max_mem: 16131M
[07/29 12:48:26] detectron2 INFO: Rank of current process: 0. World size: 1
[07/29 12:48:27] detectron2 INFO: Environment info:
----------------------  -------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.10 (default, May 26 2023, 14:05:08) [GCC 9.4.0]
numpy                   1.23.5
detectron2              0.6 @/home/hoenig/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 9.4
CUDA compiler           CUDA 11.6
detectron2 arch flags   8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          525.125.06
CUDA_HOME               /usr/local/cuda
Pillow                  9.3.0
torchvision             0.13.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221122
iopath                  0.1.9
cv2                     4.7.0
----------------------  -------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[07/29 12:48:27] detectron2 INFO: Command line arguments: Namespace(config_file='projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_s_3x.py', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=True)
[07/29 12:48:27] detectron2 INFO: Contents of args.config_file=projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_s_3x.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcascade_mask_rcnn_mvitv2_t_3x[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15moptimizer[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mlr_multiplier[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtrain[39m


[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mdepth[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mlast_block_indexes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m([39m[38;5;141m0[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m13[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m15[39m[38;5;15m)[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15minit_checkpoint[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://ImageNetPretrained/mvitv2/MViTv2_S_in1k.pyth[39m[38;5;186m"[39m

[07/29 12:48:27] d2.config.lazy WARNING: The config contains objects that cannot serialize to a valid yaml. ./output/config.yaml is human-readable but cannot be loaded.
[07/29 12:48:27] d2.config.lazy WARNING: Config is saved using cloudpickle at ./output/config.yaml.pkl.
[07/29 12:48:27] detectron2 INFO: Full config saved to ./output/config.yaml
[07/29 12:48:27] d2.utils.env INFO: Using a generated random seed 30155866
[07/29 12:48:27] detectron2 INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): MViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      )
      (blocks): ModuleList(
        (0): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=96, out_features=192, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (2): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (3): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=192, out_features=384, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (4): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (6): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.107)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.120)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.133)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.147)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (12): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.160)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (13): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.173)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (14): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.187)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=384, out_features=768, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (15): MultiScaleBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (scale2_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (scale3_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (scale4_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (scale5_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): CascadeROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): ModuleList(
      (0): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (1): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (2): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
    )
    (box_predictor): ModuleList(
      (0): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=31, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (1): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=31, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (2): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=31, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 30, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[07/29 12:48:45] detectron2 INFO: Rank of current process: 0. World size: 1
[07/29 12:48:46] detectron2 INFO: Environment info:
----------------------  -------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.10 (default, May 26 2023, 14:05:08) [GCC 9.4.0]
numpy                   1.23.5
detectron2              0.6 @/home/hoenig/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 9.4
CUDA compiler           CUDA 11.6
detectron2 arch flags   8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          525.125.06
CUDA_HOME               /usr/local/cuda
Pillow                  9.3.0
torchvision             0.13.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221122
iopath                  0.1.9
cv2                     4.7.0
----------------------  -------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[07/29 12:48:46] detectron2 INFO: Command line arguments: Namespace(config_file='projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_s_3x.py', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=True)
[07/29 12:48:46] detectron2 INFO: Contents of args.config_file=projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_s_3x.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcascade_mask_rcnn_mvitv2_t_3x[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15moptimizer[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mlr_multiplier[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtrain[39m


[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mdepth[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mlast_block_indexes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m([39m[38;5;141m0[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m13[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m15[39m[38;5;15m)[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15minit_checkpoint[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://ImageNetPretrained/mvitv2/MViTv2_S_in1k.pyth[39m[38;5;186m"[39m

[07/29 12:48:46] d2.config.lazy WARNING: The config contains objects that cannot serialize to a valid yaml. ./output/config.yaml is human-readable but cannot be loaded.
[07/29 12:48:46] d2.config.lazy WARNING: Config is saved using cloudpickle at ./output/config.yaml.pkl.
[07/29 12:48:46] detectron2 INFO: Full config saved to ./output/config.yaml
[07/29 12:48:46] d2.utils.env INFO: Using a generated random seed 49594310
[07/29 12:48:47] detectron2 INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): MViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      )
      (blocks): ModuleList(
        (0): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=96, out_features=192, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (2): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (3): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=192, out_features=384, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (4): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (6): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.107)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.120)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.133)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.147)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (12): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.160)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (13): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.173)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (14): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.187)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=384, out_features=768, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (15): MultiScaleBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (scale2_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (scale3_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (scale4_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (scale5_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): CascadeROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): ModuleList(
      (0): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (1): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (2): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
    )
    (box_predictor): ModuleList(
      (0): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=31, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (1): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=31, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (2): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=31, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 30, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[07/29 12:49:00] d2.data.datasets.coco INFO: Loading datasets/tless/tless_annotations_train.json takes 11.94 seconds.
[07/29 12:49:00] d2.data.datasets.coco INFO: Loaded 50000 images in COCO format from datasets/tless/tless_annotations_train.json
[07/29 12:49:03] d2.data.build INFO: Removed 0 images with no usable annotations. 50000 images left.
[07/29 12:49:05] d2.data.build INFO: Distribution of instances among all 30 categories:
[36m|  category  | #instances   | category   | #instances   | category   | #instances   |
|:----------:|:-------------|:-----------|:-------------|:-----------|:-------------|
|     1      | 19873        | 2          | 21790        | 3          | 21461        |
|     4      | 21610        | 5          | 23092        | 6          | 23830        |
|     7      | 26345        | 8          | 26271        | 9          | 22606        |
|     10     | 22289        | 11         | 22313        | 12         | 23290        |
|     13     | 20476        | 14         | 22314        | 15         | 22261        |
|     16     | 21595        | 17         | 24514        | 18         | 25429        |
|     19     | 23097        | 20         | 22733        | 21         | 23375        |
|     22     | 23035        | 23         | 24216        | 24         | 22070        |
|     25     | 23853        | 26         | 24387        | 27         | 25290        |
|     28     | 23366        | 29         | 24099        | 30         | 22921        |
|            |              |            |              |            |              |
|   total    | 693801       |            |              |            |              |[0m
[07/29 12:49:05] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [<detectron2.data.transforms.augmentation_impl.RandomApply object at 0x7fba3113d490>, ResizeShortestEdge(short_edge_length=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], max_size=1333, sample_style='choice'), RandomFlip()]
[07/29 12:49:05] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[07/29 12:49:05] d2.data.common INFO: Serializing 50000 elements to byte tensors and concatenating them all ...
[07/29 12:49:08] d2.data.common INFO: Serialized dataset takes 389.07 MiB
[07/29 12:49:10] fvcore.common.checkpoint INFO: [Checkpointer] Loading from ./output/model_0004999.pth ...
[07/29 12:49:10] d2.checkpoint.c2_model_loading INFO: Following weights matched with model:
| Names in Model                                  | Names in Checkpoint                                                                                                                                                                                                       | Shapes                                                                                                                   |
|:------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|
| backbone.bottom_up.blocks.0.attn.*              | backbone.bottom_up.blocks.0.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (96,) (96,96) (288,) (288,96) (111,96) (111,96)     |
| backbone.bottom_up.blocks.0.mlp.fc1.*           | backbone.bottom_up.blocks.0.mlp.fc1.{bias,weight}                                                                                                                                                                         | (384,) (384,96)                                                                                                          |
| backbone.bottom_up.blocks.0.mlp.fc2.*           | backbone.bottom_up.blocks.0.mlp.fc2.{bias,weight}                                                                                                                                                                         | (96,) (96,384)                                                                                                           |
| backbone.bottom_up.blocks.0.norm1.*             | backbone.bottom_up.blocks.0.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| backbone.bottom_up.blocks.0.norm2.*             | backbone.bottom_up.blocks.0.norm2.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| backbone.bottom_up.blocks.1.attn.*              | backbone.bottom_up.blocks.1.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,96) (55,96) (55,96)    |
| backbone.bottom_up.blocks.1.mlp.fc1.*           | backbone.bottom_up.blocks.1.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| backbone.bottom_up.blocks.1.mlp.fc2.*           | backbone.bottom_up.blocks.1.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| backbone.bottom_up.blocks.1.norm1.*             | backbone.bottom_up.blocks.1.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| backbone.bottom_up.blocks.1.norm2.*             | backbone.bottom_up.blocks.1.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| backbone.bottom_up.blocks.1.proj.*              | backbone.bottom_up.blocks.1.proj.{bias,weight}                                                                                                                                                                            | (192,) (192,96)                                                                                                          |
| backbone.bottom_up.blocks.10.attn.*             | backbone.bottom_up.blocks.10.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.10.mlp.fc1.*          | backbone.bottom_up.blocks.10.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.10.mlp.fc2.*          | backbone.bottom_up.blocks.10.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.10.norm1.*            | backbone.bottom_up.blocks.10.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.10.norm2.*            | backbone.bottom_up.blocks.10.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.11.attn.*             | backbone.bottom_up.blocks.11.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.11.mlp.fc1.*          | backbone.bottom_up.blocks.11.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.11.mlp.fc2.*          | backbone.bottom_up.blocks.11.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.11.norm1.*            | backbone.bottom_up.blocks.11.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.11.norm2.*            | backbone.bottom_up.blocks.11.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.12.attn.*             | backbone.bottom_up.blocks.12.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.12.mlp.fc1.*          | backbone.bottom_up.blocks.12.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.12.mlp.fc2.*          | backbone.bottom_up.blocks.12.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.12.norm1.*            | backbone.bottom_up.blocks.12.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.12.norm2.*            | backbone.bottom_up.blocks.12.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.13.attn.*             | backbone.bottom_up.blocks.13.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.13.mlp.fc1.*          | backbone.bottom_up.blocks.13.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.13.mlp.fc2.*          | backbone.bottom_up.blocks.13.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.13.norm1.*            | backbone.bottom_up.blocks.13.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.13.norm2.*            | backbone.bottom_up.blocks.13.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.14.attn.*             | backbone.bottom_up.blocks.14.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.14.mlp.fc1.*          | backbone.bottom_up.blocks.14.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| backbone.bottom_up.blocks.14.mlp.fc2.*          | backbone.bottom_up.blocks.14.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| backbone.bottom_up.blocks.14.norm1.*            | backbone.bottom_up.blocks.14.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.14.norm2.*            | backbone.bottom_up.blocks.14.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| backbone.bottom_up.blocks.14.proj.*             | backbone.bottom_up.blocks.14.proj.{bias,weight}                                                                                                                                                                           | (768,) (768,384)                                                                                                         |
| backbone.bottom_up.blocks.15.attn.*             | backbone.bottom_up.blocks.15.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,768) (13,96) (13,96) |
| backbone.bottom_up.blocks.15.mlp.fc1.*          | backbone.bottom_up.blocks.15.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| backbone.bottom_up.blocks.15.mlp.fc2.*          | backbone.bottom_up.blocks.15.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| backbone.bottom_up.blocks.15.norm1.*            | backbone.bottom_up.blocks.15.norm1.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| backbone.bottom_up.blocks.15.norm2.*            | backbone.bottom_up.blocks.15.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| backbone.bottom_up.blocks.2.attn.*              | backbone.bottom_up.blocks.2.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,192) (55,96) (55,96)   |
| backbone.bottom_up.blocks.2.mlp.fc1.*           | backbone.bottom_up.blocks.2.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| backbone.bottom_up.blocks.2.mlp.fc2.*           | backbone.bottom_up.blocks.2.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| backbone.bottom_up.blocks.2.norm1.*             | backbone.bottom_up.blocks.2.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| backbone.bottom_up.blocks.2.norm2.*             | backbone.bottom_up.blocks.2.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| backbone.bottom_up.blocks.3.attn.*              | backbone.bottom_up.blocks.3.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,192) (55,96) (55,96) |
| backbone.bottom_up.blocks.3.mlp.fc1.*           | backbone.bottom_up.blocks.3.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.3.mlp.fc2.*           | backbone.bottom_up.blocks.3.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.3.norm1.*             | backbone.bottom_up.blocks.3.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| backbone.bottom_up.blocks.3.norm2.*             | backbone.bottom_up.blocks.3.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.3.proj.*              | backbone.bottom_up.blocks.3.proj.{bias,weight}                                                                                                                                                                            | (384,) (384,192)                                                                                                         |
| backbone.bottom_up.blocks.4.attn.*              | backbone.bottom_up.blocks.4.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.4.mlp.fc1.*           | backbone.bottom_up.blocks.4.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.4.mlp.fc2.*           | backbone.bottom_up.blocks.4.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.4.norm1.*             | backbone.bottom_up.blocks.4.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.4.norm2.*             | backbone.bottom_up.blocks.4.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.5.attn.*              | backbone.bottom_up.blocks.5.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.5.mlp.fc1.*           | backbone.bottom_up.blocks.5.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.5.mlp.fc2.*           | backbone.bottom_up.blocks.5.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.5.norm1.*             | backbone.bottom_up.blocks.5.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.5.norm2.*             | backbone.bottom_up.blocks.5.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.6.attn.*              | backbone.bottom_up.blocks.6.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.6.mlp.fc1.*           | backbone.bottom_up.blocks.6.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.6.mlp.fc2.*           | backbone.bottom_up.blocks.6.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.6.norm1.*             | backbone.bottom_up.blocks.6.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.6.norm2.*             | backbone.bottom_up.blocks.6.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.7.attn.*              | backbone.bottom_up.blocks.7.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.7.mlp.fc1.*           | backbone.bottom_up.blocks.7.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.7.mlp.fc2.*           | backbone.bottom_up.blocks.7.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.7.norm1.*             | backbone.bottom_up.blocks.7.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.7.norm2.*             | backbone.bottom_up.blocks.7.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.8.attn.*              | backbone.bottom_up.blocks.8.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.8.mlp.fc1.*           | backbone.bottom_up.blocks.8.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.8.mlp.fc2.*           | backbone.bottom_up.blocks.8.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.8.norm1.*             | backbone.bottom_up.blocks.8.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.8.norm2.*             | backbone.bottom_up.blocks.8.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.9.attn.*              | backbone.bottom_up.blocks.9.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.9.mlp.fc1.*           | backbone.bottom_up.blocks.9.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.9.mlp.fc2.*           | backbone.bottom_up.blocks.9.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.9.norm1.*             | backbone.bottom_up.blocks.9.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.9.norm2.*             | backbone.bottom_up.blocks.9.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.patch_embed.proj.*           | backbone.bottom_up.patch_embed.proj.{bias,weight}                                                                                                                                                                         | (96,) (96,3,7,7)                                                                                                         |
| backbone.bottom_up.scale2_norm.*                | backbone.bottom_up.scale2_norm.{bias,weight}                                                                                                                                                                              | (96,) (96,)                                                                                                              |
| backbone.bottom_up.scale3_norm.*                | backbone.bottom_up.scale3_norm.{bias,weight}                                                                                                                                                                              | (192,) (192,)                                                                                                            |
| backbone.bottom_up.scale4_norm.*                | backbone.bottom_up.scale4_norm.{bias,weight}                                                                                                                                                                              | (384,) (384,)                                                                                                            |
| backbone.bottom_up.scale5_norm.*                | backbone.bottom_up.scale5_norm.{bias,weight}                                                                                                                                                                              | (768,) (768,)                                                                                                            |
| backbone.fpn_lateral2.*                         | backbone.fpn_lateral2.{bias,weight}                                                                                                                                                                                       | (256,) (256,96,1,1)                                                                                                      |
| backbone.fpn_lateral3.*                         | backbone.fpn_lateral3.{bias,weight}                                                                                                                                                                                       | (256,) (256,192,1,1)                                                                                                     |
| backbone.fpn_lateral4.*                         | backbone.fpn_lateral4.{bias,weight}                                                                                                                                                                                       | (256,) (256,384,1,1)                                                                                                     |
| backbone.fpn_lateral5.*                         | backbone.fpn_lateral5.{bias,weight}                                                                                                                                                                                       | (256,) (256,768,1,1)                                                                                                     |
| backbone.fpn_output2.*                          | backbone.fpn_output2.{bias,weight}                                                                                                                                                                                        | (256,) (256,256,3,3)                                                                                                     |
| backbone.fpn_output3.*                          | backbone.fpn_output3.{bias,weight}                                                                                                                                                                                        | (256,) (256,256,3,3)                                                                                                     |
| backbone.fpn_output4.*                          | backbone.fpn_output4.{bias,weight}                                                                                                                                                                                        | (256,) (256,256,3,3)                                                                                                     |
| backbone.fpn_output5.*                          | backbone.fpn_output5.{bias,weight}                                                                                                                                                                                        | (256,) (256,256,3,3)                                                                                                     |
| proposal_generator.rpn_head.anchor_deltas.*     | proposal_generator.rpn_head.anchor_deltas.{bias,weight}                                                                                                                                                                   | (12,) (12,256,1,1)                                                                                                       |
| proposal_generator.rpn_head.conv.conv0.*        | proposal_generator.rpn_head.conv.conv0.{bias,weight}                                                                                                                                                                      | (256,) (256,256,3,3)                                                                                                     |
| proposal_generator.rpn_head.conv.conv1.*        | proposal_generator.rpn_head.conv.conv1.{bias,weight}                                                                                                                                                                      | (256,) (256,256,3,3)                                                                                                     |
| proposal_generator.rpn_head.objectness_logits.* | proposal_generator.rpn_head.objectness_logits.{bias,weight}                                                                                                                                                               | (3,) (3,256,1,1)                                                                                                         |
| roi_heads.box_head.0.conv1.*                    | roi_heads.box_head.0.conv1.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.0.conv2.*                    | roi_heads.box_head.0.conv2.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.0.conv3.*                    | roi_heads.box_head.0.conv3.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.0.conv4.*                    | roi_heads.box_head.0.conv4.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.0.fc1.*                      | roi_heads.box_head.0.fc1.{bias,weight}                                                                                                                                                                                    | (1024,) (1024,12544)                                                                                                     |
| roi_heads.box_head.1.conv1.*                    | roi_heads.box_head.1.conv1.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.1.conv2.*                    | roi_heads.box_head.1.conv2.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.1.conv3.*                    | roi_heads.box_head.1.conv3.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.1.conv4.*                    | roi_heads.box_head.1.conv4.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.1.fc1.*                      | roi_heads.box_head.1.fc1.{bias,weight}                                                                                                                                                                                    | (1024,) (1024,12544)                                                                                                     |
| roi_heads.box_head.2.conv1.*                    | roi_heads.box_head.2.conv1.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.2.conv2.*                    | roi_heads.box_head.2.conv2.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.2.conv3.*                    | roi_heads.box_head.2.conv3.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.2.conv4.*                    | roi_heads.box_head.2.conv4.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.2.fc1.*                      | roi_heads.box_head.2.fc1.{bias,weight}                                                                                                                                                                                    | (1024,) (1024,12544)                                                                                                     |
| roi_heads.box_predictor.0.bbox_pred.*           | roi_heads.box_predictor.0.bbox_pred.{bias,weight}                                                                                                                                                                         | (4,) (4,1024)                                                                                                            |
| roi_heads.box_predictor.0.cls_score.*           | roi_heads.box_predictor.0.cls_score.{bias,weight}                                                                                                                                                                         | (31,) (31,1024)                                                                                                          |
| roi_heads.box_predictor.1.bbox_pred.*           | roi_heads.box_predictor.1.bbox_pred.{bias,weight}                                                                                                                                                                         | (4,) (4,1024)                                                                                                            |
| roi_heads.box_predictor.1.cls_score.*           | roi_heads.box_predictor.1.cls_score.{bias,weight}                                                                                                                                                                         | (31,) (31,1024)                                                                                                          |
| roi_heads.box_predictor.2.bbox_pred.*           | roi_heads.box_predictor.2.bbox_pred.{bias,weight}                                                                                                                                                                         | (4,) (4,1024)                                                                                                            |
| roi_heads.box_predictor.2.cls_score.*           | roi_heads.box_predictor.2.cls_score.{bias,weight}                                                                                                                                                                         | (31,) (31,1024)                                                                                                          |
| roi_heads.mask_head.deconv.*                    | roi_heads.mask_head.deconv.{bias,weight}                                                                                                                                                                                  | (256,) (256,256,2,2)                                                                                                     |
| roi_heads.mask_head.mask_fcn1.*                 | roi_heads.mask_head.mask_fcn1.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                  | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.mask_head.mask_fcn2.*                 | roi_heads.mask_head.mask_fcn2.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                  | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.mask_head.mask_fcn3.*                 | roi_heads.mask_head.mask_fcn3.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                  | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.mask_head.mask_fcn4.*                 | roi_heads.mask_head.mask_fcn4.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                  | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.mask_head.predictor.*                 | roi_heads.mask_head.predictor.{bias,weight}                                                                                                                                                                               | (30,) (30,256,1,1)                                                                                                       |
[07/29 12:49:11] fvcore.common.checkpoint INFO: Loading trainer from ./output/model_0004999.pth ...
[07/29 12:49:11] d2.engine.train_loop INFO: Starting training from iteration 5000
[07/29 12:49:27] d2.utils.events INFO:  eta: 11:08:05  iter: 5019  total_loss: 3.136  loss_cls_stage0: 0.3261  loss_box_reg_stage0: 0.3092  loss_cls_stage1: 0.3558  loss_box_reg_stage1: 0.7296  loss_cls_stage2: 0.3652  loss_box_reg_stage2: 0.9185  loss_mask: 0.1007  loss_rpn_cls: 0.01892  loss_rpn_loc: 0.05094  time: 0.6477  data_time: 0.0182  lr: 0.00016  max_mem: 18161M
[07/29 12:49:39] d2.utils.events INFO:  eta: 11:04:23  iter: 5039  total_loss: 3.849  loss_cls_stage0: 0.3786  loss_box_reg_stage0: 0.3559  loss_cls_stage1: 0.4051  loss_box_reg_stage1: 0.8747  loss_cls_stage2: 0.3959  loss_box_reg_stage2: 1.16  loss_mask: 0.1236  loss_rpn_cls: 0.02797  loss_rpn_loc: 0.06623  time: 0.6320  data_time: 0.0049  lr: 0.00016  max_mem: 18161M
[07/29 12:49:52] d2.utils.events INFO:  eta: 11:07:07  iter: 5059  total_loss: 3.579  loss_cls_stage0: 0.3921  loss_box_reg_stage0: 0.3367  loss_cls_stage1: 0.3888  loss_box_reg_stage1: 0.8163  loss_cls_stage2: 0.3549  loss_box_reg_stage2: 1.076  loss_mask: 0.1098  loss_rpn_cls: 0.02056  loss_rpn_loc: 0.0496  time: 0.6374  data_time: 0.0047  lr: 0.00016  max_mem: 18161M
[07/29 12:50:05] d2.utils.events INFO:  eta: 11:06:52  iter: 5079  total_loss: 3.524  loss_cls_stage0: 0.3543  loss_box_reg_stage0: 0.3249  loss_cls_stage1: 0.3537  loss_box_reg_stage1: 0.8325  loss_cls_stage2: 0.3777  loss_box_reg_stage2: 1.054  loss_mask: 0.1175  loss_rpn_cls: 0.01425  loss_rpn_loc: 0.05318  time: 0.6377  data_time: 0.0049  lr: 0.00016  max_mem: 18161M
[07/29 12:50:17] d2.utils.events INFO:  eta: 11:05:31  iter: 5099  total_loss: 3.68  loss_cls_stage0: 0.3861  loss_box_reg_stage0: 0.3487  loss_cls_stage1: 0.3981  loss_box_reg_stage1: 0.8376  loss_cls_stage2: 0.3858  loss_box_reg_stage2: 1.099  loss_mask: 0.1111  loss_rpn_cls: 0.01597  loss_rpn_loc: 0.06064  time: 0.6334  data_time: 0.0047  lr: 0.00016  max_mem: 18161M
[07/29 12:50:29] d2.utils.events INFO:  eta: 11:05:11  iter: 5119  total_loss: 3.426  loss_cls_stage0: 0.3618  loss_box_reg_stage0: 0.3205  loss_cls_stage1: 0.4025  loss_box_reg_stage1: 0.7617  loss_cls_stage2: 0.3852  loss_box_reg_stage2: 1.005  loss_mask: 0.1141  loss_rpn_cls: 0.02259  loss_rpn_loc: 0.05081  time: 0.6314  data_time: 0.0048  lr: 0.00016  max_mem: 18161M
[07/29 12:50:43] d2.utils.events INFO:  eta: 11:05:15  iter: 5139  total_loss: 3.568  loss_cls_stage0: 0.3145  loss_box_reg_stage0: 0.323  loss_cls_stage1: 0.3423  loss_box_reg_stage1: 0.7804  loss_cls_stage2: 0.3378  loss_box_reg_stage2: 1.083  loss_mask: 0.1145  loss_rpn_cls: 0.01588  loss_rpn_loc: 0.05782  time: 0.6349  data_time: 0.0046  lr: 0.00016  max_mem: 18161M
[07/29 12:50:55] d2.utils.events INFO:  eta: 11:02:42  iter: 5159  total_loss: 3.463  loss_cls_stage0: 0.3461  loss_box_reg_stage0: 0.3277  loss_cls_stage1: 0.3871  loss_box_reg_stage1: 0.7855  loss_cls_stage2: 0.3921  loss_box_reg_stage2: 1.034  loss_mask: 0.1083  loss_rpn_cls: 0.02437  loss_rpn_loc: 0.05926  time: 0.6323  data_time: 0.0045  lr: 0.00016  max_mem: 18161M
[07/29 12:51:07] d2.utils.events INFO:  eta: 11:01:26  iter: 5179  total_loss: 3.804  loss_cls_stage0: 0.4045  loss_box_reg_stage0: 0.3369  loss_cls_stage1: 0.4483  loss_box_reg_stage1: 0.8699  loss_cls_stage2: 0.418  loss_box_reg_stage2: 1.073  loss_mask: 0.1217  loss_rpn_cls: 0.02737  loss_rpn_loc: 0.04726  time: 0.6313  data_time: 0.0044  lr: 0.00016  max_mem: 18161M
[07/29 12:51:20] d2.utils.events INFO:  eta: 11:01:00  iter: 5199  total_loss: 3.512  loss_cls_stage0: 0.3799  loss_box_reg_stage0: 0.3504  loss_cls_stage1: 0.4005  loss_box_reg_stage1: 0.7507  loss_cls_stage2: 0.3777  loss_box_reg_stage2: 0.9409  loss_mask: 0.126  loss_rpn_cls: 0.02122  loss_rpn_loc: 0.05976  time: 0.6311  data_time: 0.0048  lr: 0.00016  max_mem: 18161M
[07/29 12:51:33] d2.utils.events INFO:  eta: 11:01:01  iter: 5219  total_loss: 3.403  loss_cls_stage0: 0.382  loss_box_reg_stage0: 0.3235  loss_cls_stage1: 0.3984  loss_box_reg_stage1: 0.7295  loss_cls_stage2: 0.3832  loss_box_reg_stage2: 0.9342  loss_mask: 0.1107  loss_rpn_cls: 0.02  loss_rpn_loc: 0.05305  time: 0.6314  data_time: 0.0052  lr: 0.00016  max_mem: 18161M
[07/29 12:51:45] d2.utils.events INFO:  eta: 11:00:48  iter: 5239  total_loss: 3.706  loss_cls_stage0: 0.3467  loss_box_reg_stage0: 0.3461  loss_cls_stage1: 0.3854  loss_box_reg_stage1: 0.8357  loss_cls_stage2: 0.3609  loss_box_reg_stage2: 1.103  loss_mask: 0.1155  loss_rpn_cls: 0.02038  loss_rpn_loc: 0.04846  time: 0.6302  data_time: 0.0049  lr: 0.00016  max_mem: 18161M
[07/29 12:51:57] d2.utils.events INFO:  eta: 11:00:12  iter: 5259  total_loss: 2.974  loss_cls_stage0: 0.3063  loss_box_reg_stage0: 0.2907  loss_cls_stage1: 0.3339  loss_box_reg_stage1: 0.7201  loss_cls_stage2: 0.3216  loss_box_reg_stage2: 0.9115  loss_mask: 0.09724  loss_rpn_cls: 0.02415  loss_rpn_loc: 0.05686  time: 0.6285  data_time: 0.0049  lr: 0.00016  max_mem: 18161M
[07/29 12:52:10] d2.utils.events INFO:  eta: 11:00:09  iter: 5279  total_loss: 3.309  loss_cls_stage0: 0.3333  loss_box_reg_stage0: 0.3113  loss_cls_stage1: 0.3542  loss_box_reg_stage1: 0.7318  loss_cls_stage2: 0.3482  loss_box_reg_stage2: 0.9283  loss_mask: 0.1103  loss_rpn_cls: 0.02093  loss_rpn_loc: 0.05449  time: 0.6290  data_time: 0.0048  lr: 0.00016  max_mem: 18161M
[07/29 12:52:23] d2.utils.events INFO:  eta: 10:59:56  iter: 5299  total_loss: 3.168  loss_cls_stage0: 0.3107  loss_box_reg_stage0: 0.2972  loss_cls_stage1: 0.3475  loss_box_reg_stage1: 0.7551  loss_cls_stage2: 0.3033  loss_box_reg_stage2: 0.9023  loss_mask: 0.09888  loss_rpn_cls: 0.01909  loss_rpn_loc: 0.05362  time: 0.6291  data_time: 0.0048  lr: 0.00016  max_mem: 18161M
[07/29 12:52:35] d2.utils.events INFO:  eta: 10:58:17  iter: 5319  total_loss: 3.343  loss_cls_stage0: 0.3146  loss_box_reg_stage0: 0.3043  loss_cls_stage1: 0.3512  loss_box_reg_stage1: 0.7768  loss_cls_stage2: 0.3434  loss_box_reg_stage2: 0.9961  loss_mask: 0.1116  loss_rpn_cls: 0.01586  loss_rpn_loc: 0.04494  time: 0.6277  data_time: 0.0047  lr: 0.00016  max_mem: 18335M
[07/29 12:52:47] d2.utils.events INFO:  eta: 10:58:04  iter: 5339  total_loss: 3.32  loss_cls_stage0: 0.3373  loss_box_reg_stage0: 0.3239  loss_cls_stage1: 0.3581  loss_box_reg_stage1: 0.7489  loss_cls_stage2: 0.3249  loss_box_reg_stage2: 0.9738  loss_mask: 0.1082  loss_rpn_cls: 0.01986  loss_rpn_loc: 0.05662  time: 0.6282  data_time: 0.0049  lr: 0.00016  max_mem: 18335M
[07/29 12:52:59] d2.utils.events INFO:  eta: 10:56:03  iter: 5359  total_loss: 3.695  loss_cls_stage0: 0.3834  loss_box_reg_stage0: 0.3506  loss_cls_stage1: 0.405  loss_box_reg_stage1: 0.8535  loss_cls_stage2: 0.3745  loss_box_reg_stage2: 1.071  loss_mask: 0.1183  loss_rpn_cls: 0.02341  loss_rpn_loc: 0.055  time: 0.6266  data_time: 0.0047  lr: 0.00016  max_mem: 18335M
[07/29 12:53:12] d2.utils.events INFO:  eta: 10:55:50  iter: 5379  total_loss: 3.551  loss_cls_stage0: 0.3854  loss_box_reg_stage0: 0.3474  loss_cls_stage1: 0.4095  loss_box_reg_stage1: 0.8334  loss_cls_stage2: 0.4124  loss_box_reg_stage2: 1.063  loss_mask: 0.1137  loss_rpn_cls: 0.01845  loss_rpn_loc: 0.04711  time: 0.6269  data_time: 0.0048  lr: 0.00016  max_mem: 18335M
[07/29 12:53:24] d2.utils.events INFO:  eta: 10:54:57  iter: 5399  total_loss: 3.473  loss_cls_stage0: 0.3439  loss_box_reg_stage0: 0.3417  loss_cls_stage1: 0.3701  loss_box_reg_stage1: 0.7932  loss_cls_stage2: 0.3514  loss_box_reg_stage2: 0.9973  loss_mask: 0.1179  loss_rpn_cls: 0.03149  loss_rpn_loc: 0.05716  time: 0.6252  data_time: 0.0051  lr: 0.00016  max_mem: 18335M
[07/29 12:53:36] d2.utils.events INFO:  eta: 10:54:01  iter: 5419  total_loss: 3.438  loss_cls_stage0: 0.3113  loss_box_reg_stage0: 0.2977  loss_cls_stage1: 0.3466  loss_box_reg_stage1: 0.754  loss_cls_stage2: 0.3326  loss_box_reg_stage2: 0.9956  loss_mask: 0.1114  loss_rpn_cls: 0.02723  loss_rpn_loc: 0.07008  time: 0.6240  data_time: 0.0045  lr: 0.00016  max_mem: 18335M
[07/29 12:53:48] d2.utils.events INFO:  eta: 10:54:15  iter: 5439  total_loss: 3.384  loss_cls_stage0: 0.3431  loss_box_reg_stage0: 0.3201  loss_cls_stage1: 0.3816  loss_box_reg_stage1: 0.771  loss_cls_stage2: 0.3738  loss_box_reg_stage2: 1.009  loss_mask: 0.1081  loss_rpn_cls: 0.02406  loss_rpn_loc: 0.0576  time: 0.6238  data_time: 0.0049  lr: 0.00016  max_mem: 18335M
[07/29 12:54:01] d2.utils.events INFO:  eta: 10:53:05  iter: 5459  total_loss: 3.38  loss_cls_stage0: 0.3335  loss_box_reg_stage0: 0.3128  loss_cls_stage1: 0.3751  loss_box_reg_stage1: 0.7707  loss_cls_stage2: 0.3653  loss_box_reg_stage2: 0.9801  loss_mask: 0.1103  loss_rpn_cls: 0.01735  loss_rpn_loc: 0.05657  time: 0.6234  data_time: 0.0052  lr: 0.00016  max_mem: 18335M
[07/29 12:54:13] d2.utils.events INFO:  eta: 10:52:52  iter: 5479  total_loss: 3.18  loss_cls_stage0: 0.3085  loss_box_reg_stage0: 0.328  loss_cls_stage1: 0.331  loss_box_reg_stage1: 0.7483  loss_cls_stage2: 0.3208  loss_box_reg_stage2: 0.9902  loss_mask: 0.1022  loss_rpn_cls: 0.02322  loss_rpn_loc: 0.05629  time: 0.6232  data_time: 0.0048  lr: 0.00016  max_mem: 18335M
[07/29 12:54:26] d2.utils.events INFO:  eta: 10:52:07  iter: 5499  total_loss: 3.275  loss_cls_stage0: 0.358  loss_box_reg_stage0: 0.3324  loss_cls_stage1: 0.3667  loss_box_reg_stage1: 0.748  loss_cls_stage2: 0.3489  loss_box_reg_stage2: 0.9279  loss_mask: 0.1153  loss_rpn_cls: 0.02197  loss_rpn_loc: 0.06009  time: 0.6231  data_time: 0.0049  lr: 0.00016  max_mem: 18335M
[07/29 12:54:38] d2.utils.events INFO:  eta: 10:51:55  iter: 5519  total_loss: 3.227  loss_cls_stage0: 0.3082  loss_box_reg_stage0: 0.298  loss_cls_stage1: 0.3293  loss_box_reg_stage1: 0.7608  loss_cls_stage2: 0.317  loss_box_reg_stage2: 1.014  loss_mask: 0.09793  loss_rpn_cls: 0.02176  loss_rpn_loc: 0.04742  time: 0.6232  data_time: 0.0050  lr: 0.00016  max_mem: 18335M
[07/29 12:54:51] d2.utils.events INFO:  eta: 10:51:42  iter: 5539  total_loss: 3.661  loss_cls_stage0: 0.3486  loss_box_reg_stage0: 0.3091  loss_cls_stage1: 0.4461  loss_box_reg_stage1: 0.8272  loss_cls_stage2: 0.41  loss_box_reg_stage2: 1.111  loss_mask: 0.1033  loss_rpn_cls: 0.02089  loss_rpn_loc: 0.06152  time: 0.6232  data_time: 0.0047  lr: 0.00016  max_mem: 18335M
[07/29 12:55:03] d2.utils.events INFO:  eta: 10:52:02  iter: 5559  total_loss: 3.932  loss_cls_stage0: 0.3818  loss_box_reg_stage0: 0.3345  loss_cls_stage1: 0.4484  loss_box_reg_stage1: 0.8686  loss_cls_stage2: 0.4184  loss_box_reg_stage2: 1.094  loss_mask: 0.1199  loss_rpn_cls: 0.02381  loss_rpn_loc: 0.05598  time: 0.6239  data_time: 0.0053  lr: 0.00016  max_mem: 18335M
[07/29 12:55:16] d2.utils.events INFO:  eta: 10:51:30  iter: 5579  total_loss: 3.435  loss_cls_stage0: 0.358  loss_box_reg_stage0: 0.3414  loss_cls_stage1: 0.3638  loss_box_reg_stage1: 0.8228  loss_cls_stage2: 0.3545  loss_box_reg_stage2: 1.021  loss_mask: 0.1089  loss_rpn_cls: 0.01852  loss_rpn_loc: 0.05754  time: 0.6240  data_time: 0.0047  lr: 0.00016  max_mem: 18906M
[07/29 12:55:28] d2.utils.events INFO:  eta: 10:51:04  iter: 5599  total_loss: 3.451  loss_cls_stage0: 0.3235  loss_box_reg_stage0: 0.3388  loss_cls_stage1: 0.3653  loss_box_reg_stage1: 0.7988  loss_cls_stage2: 0.3541  loss_box_reg_stage2: 1.026  loss_mask: 0.1104  loss_rpn_cls: 0.02576  loss_rpn_loc: 0.05342  time: 0.6239  data_time: 0.0051  lr: 0.00016  max_mem: 18906M
[07/29 12:55:41] d2.utils.events INFO:  eta: 10:50:48  iter: 5619  total_loss: 3.24  loss_cls_stage0: 0.3395  loss_box_reg_stage0: 0.3415  loss_cls_stage1: 0.349  loss_box_reg_stage1: 0.7567  loss_cls_stage2: 0.3423  loss_box_reg_stage2: 0.9319  loss_mask: 0.1074  loss_rpn_cls: 0.02395  loss_rpn_loc: 0.05969  time: 0.6235  data_time: 0.0050  lr: 0.00016  max_mem: 18906M
[07/29 12:55:53] d2.utils.events INFO:  eta: 10:50:12  iter: 5639  total_loss: 3.07  loss_cls_stage0: 0.3017  loss_box_reg_stage0: 0.298  loss_cls_stage1: 0.3193  loss_box_reg_stage1: 0.6985  loss_cls_stage2: 0.3136  loss_box_reg_stage2: 0.9711  loss_mask: 0.09259  loss_rpn_cls: 0.01357  loss_rpn_loc: 0.04294  time: 0.6231  data_time: 0.0050  lr: 0.00016  max_mem: 18906M
[07/29 12:56:06] d2.utils.events INFO:  eta: 10:50:20  iter: 5659  total_loss: 3.76  loss_cls_stage0: 0.3769  loss_box_reg_stage0: 0.3549  loss_cls_stage1: 0.4341  loss_box_reg_stage1: 0.8489  loss_cls_stage2: 0.3821  loss_box_reg_stage2: 1.021  loss_mask: 0.1018  loss_rpn_cls: 0.02143  loss_rpn_loc: 0.0569  time: 0.6236  data_time: 0.0047  lr: 0.00016  max_mem: 18906M
[07/29 12:56:18] d2.utils.events INFO:  eta: 10:49:10  iter: 5679  total_loss: 3.446  loss_cls_stage0: 0.3853  loss_box_reg_stage0: 0.363  loss_cls_stage1: 0.4134  loss_box_reg_stage1: 0.7912  loss_cls_stage2: 0.3512  loss_box_reg_stage2: 1.025  loss_mask: 0.1241  loss_rpn_cls: 0.01756  loss_rpn_loc: 0.05084  time: 0.6228  data_time: 0.0045  lr: 0.00016  max_mem: 18906M
[07/29 12:56:30] d2.utils.events INFO:  eta: 10:48:48  iter: 5699  total_loss: 3.401  loss_cls_stage0: 0.3185  loss_box_reg_stage0: 0.3164  loss_cls_stage1: 0.3331  loss_box_reg_stage1: 0.7687  loss_cls_stage2: 0.3399  loss_box_reg_stage2: 0.9926  loss_mask: 0.09947  loss_rpn_cls: 0.01614  loss_rpn_loc: 0.05384  time: 0.6227  data_time: 0.0046  lr: 0.00016  max_mem: 18906M
[07/29 12:56:42] d2.utils.events INFO:  eta: 10:48:25  iter: 5719  total_loss: 3.111  loss_cls_stage0: 0.3223  loss_box_reg_stage0: 0.3131  loss_cls_stage1: 0.3734  loss_box_reg_stage1: 0.7142  loss_cls_stage2: 0.3516  loss_box_reg_stage2: 0.9206  loss_mask: 0.1061  loss_rpn_cls: 0.01531  loss_rpn_loc: 0.04594  time: 0.6222  data_time: 0.0048  lr: 0.00016  max_mem: 18906M
[07/29 12:56:54] d2.utils.events INFO:  eta: 10:48:08  iter: 5739  total_loss: 3.544  loss_cls_stage0: 0.3572  loss_box_reg_stage0: 0.3103  loss_cls_stage1: 0.4194  loss_box_reg_stage1: 0.7617  loss_cls_stage2: 0.3787  loss_box_reg_stage2: 1.019  loss_mask: 0.1141  loss_rpn_cls: 0.02689  loss_rpn_loc: 0.0489  time: 0.6220  data_time: 0.0047  lr: 0.00016  max_mem: 18906M
[07/29 12:57:07] d2.utils.events INFO:  eta: 10:48:20  iter: 5759  total_loss: 3.203  loss_cls_stage0: 0.336  loss_box_reg_stage0: 0.3135  loss_cls_stage1: 0.3508  loss_box_reg_stage1: 0.7753  loss_cls_stage2: 0.3162  loss_box_reg_stage2: 0.9201  loss_mask: 0.1143  loss_rpn_cls: 0.02106  loss_rpn_loc: 0.05827  time: 0.6225  data_time: 0.0052  lr: 0.00016  max_mem: 18906M
[07/29 12:57:21] d2.utils.events INFO:  eta: 10:49:24  iter: 5779  total_loss: 3.619  loss_cls_stage0: 0.3584  loss_box_reg_stage0: 0.3255  loss_cls_stage1: 0.3861  loss_box_reg_stage1: 0.7871  loss_cls_stage2: 0.3492  loss_box_reg_stage2: 1.003  loss_mask: 0.1178  loss_rpn_cls: 0.01889  loss_rpn_loc: 0.05671  time: 0.6240  data_time: 0.0050  lr: 0.00016  max_mem: 18906M
[07/29 12:57:34] d2.utils.events INFO:  eta: 10:49:41  iter: 5799  total_loss: 3.601  loss_cls_stage0: 0.3481  loss_box_reg_stage0: 0.3188  loss_cls_stage1: 0.3971  loss_box_reg_stage1: 0.8027  loss_cls_stage2: 0.3874  loss_box_reg_stage2: 1.115  loss_mask: 0.1074  loss_rpn_cls: 0.02095  loss_rpn_loc: 0.05603  time: 0.6244  data_time: 0.0046  lr: 0.00016  max_mem: 18906M
[07/29 12:57:46] d2.utils.events INFO:  eta: 10:48:58  iter: 5819  total_loss: 3.557  loss_cls_stage0: 0.3666  loss_box_reg_stage0: 0.3224  loss_cls_stage1: 0.4114  loss_box_reg_stage1: 0.8131  loss_cls_stage2: 0.3672  loss_box_reg_stage2: 1.098  loss_mask: 0.1103  loss_rpn_cls: 0.02416  loss_rpn_loc: 0.06432  time: 0.6242  data_time: 0.0047  lr: 0.00016  max_mem: 18906M
[07/29 12:57:59] d2.utils.events INFO:  eta: 10:49:05  iter: 5839  total_loss: 3.49  loss_cls_stage0: 0.3376  loss_box_reg_stage0: 0.3027  loss_cls_stage1: 0.3857  loss_box_reg_stage1: 0.7827  loss_cls_stage2: 0.3893  loss_box_reg_stage2: 1.014  loss_mask: 0.09542  loss_rpn_cls: 0.01774  loss_rpn_loc: 0.0489  time: 0.6247  data_time: 0.0048  lr: 0.00016  max_mem: 18906M
[07/29 12:58:11] d2.utils.events INFO:  eta: 10:48:16  iter: 5859  total_loss: 3.728  loss_cls_stage0: 0.3735  loss_box_reg_stage0: 0.3204  loss_cls_stage1: 0.402  loss_box_reg_stage1: 0.8866  loss_cls_stage2: 0.4006  loss_box_reg_stage2: 1.141  loss_mask: 0.1067  loss_rpn_cls: 0.01779  loss_rpn_loc: 0.05062  time: 0.6247  data_time: 0.0046  lr: 0.00016  max_mem: 18906M
[07/29 12:58:25] d2.utils.events INFO:  eta: 10:48:20  iter: 5879  total_loss: 3.432  loss_cls_stage0: 0.3483  loss_box_reg_stage0: 0.3167  loss_cls_stage1: 0.3801  loss_box_reg_stage1: 0.8419  loss_cls_stage2: 0.3523  loss_box_reg_stage2: 1.091  loss_mask: 0.09801  loss_rpn_cls: 0.01918  loss_rpn_loc: 0.04918  time: 0.6254  data_time: 0.0046  lr: 0.00016  max_mem: 18906M
[07/29 12:58:37] d2.utils.events INFO:  eta: 10:48:27  iter: 5899  total_loss: 3.449  loss_cls_stage0: 0.3168  loss_box_reg_stage0: 0.2988  loss_cls_stage1: 0.3753  loss_box_reg_stage1: 0.7952  loss_cls_stage2: 0.3682  loss_box_reg_stage2: 1.09  loss_mask: 0.09581  loss_rpn_cls: 0.02024  loss_rpn_loc: 0.05286  time: 0.6252  data_time: 0.0046  lr: 0.00016  max_mem: 18906M
[07/29 12:58:49] d2.utils.events INFO:  eta: 10:48:14  iter: 5919  total_loss: 3.594  loss_cls_stage0: 0.3503  loss_box_reg_stage0: 0.3236  loss_cls_stage1: 0.3921  loss_box_reg_stage1: 0.7799  loss_cls_stage2: 0.3844  loss_box_reg_stage2: 1.043  loss_mask: 0.1133  loss_rpn_cls: 0.02459  loss_rpn_loc: 0.058  time: 0.6252  data_time: 0.0048  lr: 0.00016  max_mem: 18906M
[07/29 12:59:02] d2.utils.events INFO:  eta: 10:48:13  iter: 5939  total_loss: 3.414  loss_cls_stage0: 0.3401  loss_box_reg_stage0: 0.3293  loss_cls_stage1: 0.3795  loss_box_reg_stage1: 0.8046  loss_cls_stage2: 0.356  loss_box_reg_stage2: 1.006  loss_mask: 0.1017  loss_rpn_cls: 0.01989  loss_rpn_loc: 0.05298  time: 0.6256  data_time: 0.0045  lr: 0.00016  max_mem: 18906M
[07/29 12:59:15] d2.utils.events INFO:  eta: 10:48:12  iter: 5959  total_loss: 3.51  loss_cls_stage0: 0.3779  loss_box_reg_stage0: 0.3087  loss_cls_stage1: 0.4177  loss_box_reg_stage1: 0.7794  loss_cls_stage2: 0.4047  loss_box_reg_stage2: 1.009  loss_mask: 0.1009  loss_rpn_cls: 0.01812  loss_rpn_loc: 0.04706  time: 0.6258  data_time: 0.0051  lr: 0.00016  max_mem: 18906M
[07/29 12:59:27] d2.utils.events INFO:  eta: 10:47:36  iter: 5979  total_loss: 3.552  loss_cls_stage0: 0.3292  loss_box_reg_stage0: 0.3202  loss_cls_stage1: 0.3433  loss_box_reg_stage1: 0.8483  loss_cls_stage2: 0.3654  loss_box_reg_stage2: 1.049  loss_mask: 0.1099  loss_rpn_cls: 0.02177  loss_rpn_loc: 0.05121  time: 0.6256  data_time: 0.0047  lr: 0.00016  max_mem: 18906M
[07/29 12:59:40] d2.utils.events INFO:  eta: 10:46:52  iter: 5999  total_loss: 3.39  loss_cls_stage0: 0.3442  loss_box_reg_stage0: 0.3278  loss_cls_stage1: 0.3768  loss_box_reg_stage1: 0.7748  loss_cls_stage2: 0.335  loss_box_reg_stage2: 0.9202  loss_mask: 0.1031  loss_rpn_cls: 0.02673  loss_rpn_loc: 0.05194  time: 0.6254  data_time: 0.0048  lr: 0.00016  max_mem: 18906M
[07/29 12:59:52] d2.utils.events INFO:  eta: 10:46:39  iter: 6019  total_loss: 3.572  loss_cls_stage0: 0.3931  loss_box_reg_stage0: 0.3172  loss_cls_stage1: 0.4114  loss_box_reg_stage1: 0.792  loss_cls_stage2: 0.3694  loss_box_reg_stage2: 1.001  loss_mask: 0.1076  loss_rpn_cls: 0.01711  loss_rpn_loc: 0.05195  time: 0.6255  data_time: 0.0049  lr: 0.00016  max_mem: 18906M
[07/29 13:00:05] d2.utils.events INFO:  eta: 10:46:28  iter: 6039  total_loss: 3.283  loss_cls_stage0: 0.3143  loss_box_reg_stage0: 0.3008  loss_cls_stage1: 0.3345  loss_box_reg_stage1: 0.7948  loss_cls_stage2: 0.3133  loss_box_reg_stage2: 1.027  loss_mask: 0.1011  loss_rpn_cls: 0.01419  loss_rpn_loc: 0.05338  time: 0.6258  data_time: 0.0052  lr: 0.00016  max_mem: 18906M
[07/29 13:00:18] d2.utils.events INFO:  eta: 10:46:13  iter: 6059  total_loss: 3.365  loss_cls_stage0: 0.3255  loss_box_reg_stage0: 0.2969  loss_cls_stage1: 0.3699  loss_box_reg_stage1: 0.7615  loss_cls_stage2: 0.3506  loss_box_reg_stage2: 1.06  loss_mask: 0.0931  loss_rpn_cls: 0.02017  loss_rpn_loc: 0.05429  time: 0.6261  data_time: 0.0051  lr: 0.00016  max_mem: 18906M
[07/29 13:00:31] d2.utils.events INFO:  eta: 10:46:03  iter: 6079  total_loss: 3.603  loss_cls_stage0: 0.3389  loss_box_reg_stage0: 0.3061  loss_cls_stage1: 0.3978  loss_box_reg_stage1: 0.8272  loss_cls_stage2: 0.3745  loss_box_reg_stage2: 1.06  loss_mask: 0.09298  loss_rpn_cls: 0.01853  loss_rpn_loc: 0.05629  time: 0.6266  data_time: 0.0051  lr: 0.00016  max_mem: 18906M
[07/29 13:00:44] d2.utils.events INFO:  eta: 10:46:49  iter: 6099  total_loss: 3.64  loss_cls_stage0: 0.3639  loss_box_reg_stage0: 0.3422  loss_cls_stage1: 0.3826  loss_box_reg_stage1: 0.8601  loss_cls_stage2: 0.3526  loss_box_reg_stage2: 1.047  loss_mask: 0.1137  loss_rpn_cls: 0.02053  loss_rpn_loc: 0.07449  time: 0.6270  data_time: 0.0046  lr: 0.00016  max_mem: 18906M
[07/29 13:00:57] d2.utils.events INFO:  eta: 10:46:46  iter: 6119  total_loss: 3.613  loss_cls_stage0: 0.3513  loss_box_reg_stage0: 0.3345  loss_cls_stage1: 0.384  loss_box_reg_stage1: 0.8643  loss_cls_stage2: 0.3635  loss_box_reg_stage2: 1.064  loss_mask: 0.1061  loss_rpn_cls: 0.02162  loss_rpn_loc: 0.05101  time: 0.6273  data_time: 0.0047  lr: 0.00016  max_mem: 18906M
[07/29 13:01:09] d2.utils.events INFO:  eta: 10:46:18  iter: 6139  total_loss: 3.459  loss_cls_stage0: 0.3465  loss_box_reg_stage0: 0.3183  loss_cls_stage1: 0.407  loss_box_reg_stage1: 0.7845  loss_cls_stage2: 0.399  loss_box_reg_stage2: 1.085  loss_mask: 0.1087  loss_rpn_cls: 0.01422  loss_rpn_loc: 0.04586  time: 0.6270  data_time: 0.0048  lr: 0.00016  max_mem: 18906M
[07/29 13:01:22] d2.utils.events INFO:  eta: 10:46:16  iter: 6159  total_loss: 3.397  loss_cls_stage0: 0.3373  loss_box_reg_stage0: 0.3187  loss_cls_stage1: 0.3677  loss_box_reg_stage1: 0.8024  loss_cls_stage2: 0.3806  loss_box_reg_stage2: 0.9975  loss_mask: 0.1026  loss_rpn_cls: 0.02429  loss_rpn_loc: 0.05852  time: 0.6273  data_time: 0.0051  lr: 0.00016  max_mem: 18906M
[07/29 13:01:35] d2.utils.events INFO:  eta: 10:46:18  iter: 6179  total_loss: 3.449  loss_cls_stage0: 0.3281  loss_box_reg_stage0: 0.3108  loss_cls_stage1: 0.3681  loss_box_reg_stage1: 0.816  loss_cls_stage2: 0.366  loss_box_reg_stage2: 1.048  loss_mask: 0.1048  loss_rpn_cls: 0.02372  loss_rpn_loc: 0.05295  time: 0.6276  data_time: 0.0049  lr: 0.00016  max_mem: 18906M
[07/29 13:01:47] d2.utils.events INFO:  eta: 10:46:05  iter: 6199  total_loss: 3.142  loss_cls_stage0: 0.3093  loss_box_reg_stage0: 0.2871  loss_cls_stage1: 0.3329  loss_box_reg_stage1: 0.7246  loss_cls_stage2: 0.3143  loss_box_reg_stage2: 0.9378  loss_mask: 0.09972  loss_rpn_cls: 0.0203  loss_rpn_loc: 0.04673  time: 0.6275  data_time: 0.0051  lr: 0.00016  max_mem: 18906M
[07/29 13:02:00] d2.utils.events INFO:  eta: 10:45:45  iter: 6219  total_loss: 3.449  loss_cls_stage0: 0.3681  loss_box_reg_stage0: 0.348  loss_cls_stage1: 0.3936  loss_box_reg_stage1: 0.7753  loss_cls_stage2: 0.3548  loss_box_reg_stage2: 1.007  loss_mask: 0.1196  loss_rpn_cls: 0.02139  loss_rpn_loc: 0.05819  time: 0.6275  data_time: 0.0050  lr: 0.00016  max_mem: 18906M
[07/29 13:02:13] d2.utils.events INFO:  eta: 10:45:40  iter: 6239  total_loss: 3.278  loss_cls_stage0: 0.3203  loss_box_reg_stage0: 0.3045  loss_cls_stage1: 0.3521  loss_box_reg_stage1: 0.7721  loss_cls_stage2: 0.345  loss_box_reg_stage2: 0.9437  loss_mask: 0.09804  loss_rpn_cls: 0.01728  loss_rpn_loc: 0.04325  time: 0.6276  data_time: 0.0052  lr: 0.00016  max_mem: 18906M
[07/29 13:02:26] d2.utils.events INFO:  eta: 10:46:03  iter: 6259  total_loss: 3.539  loss_cls_stage0: 0.3337  loss_box_reg_stage0: 0.3412  loss_cls_stage1: 0.3795  loss_box_reg_stage1: 0.8537  loss_cls_stage2: 0.355  loss_box_reg_stage2: 1.052  loss_mask: 0.1077  loss_rpn_cls: 0.01361  loss_rpn_loc: 0.04764  time: 0.6280  data_time: 0.0050  lr: 0.00016  max_mem: 18906M
[07/29 13:02:39] d2.utils.events INFO:  eta: 10:46:22  iter: 6279  total_loss: 3.725  loss_cls_stage0: 0.3589  loss_box_reg_stage0: 0.346  loss_cls_stage1: 0.4054  loss_box_reg_stage1: 0.9062  loss_cls_stage2: 0.3916  loss_box_reg_stage2: 1.182  loss_mask: 0.1081  loss_rpn_cls: 0.01712  loss_rpn_loc: 0.05847  time: 0.6284  data_time: 0.0048  lr: 0.00016  max_mem: 18906M
[07/29 13:02:51] d2.utils.events INFO:  eta: 10:46:10  iter: 6299  total_loss: 3.742  loss_cls_stage0: 0.3685  loss_box_reg_stage0: 0.3339  loss_cls_stage1: 0.4072  loss_box_reg_stage1: 0.8476  loss_cls_stage2: 0.3695  loss_box_reg_stage2: 1.084  loss_mask: 0.1078  loss_rpn_cls: 0.02168  loss_rpn_loc: 0.04948  time: 0.6285  data_time: 0.0053  lr: 0.00016  max_mem: 18906M
[07/29 13:03:04] d2.utils.events INFO:  eta: 10:45:37  iter: 6319  total_loss: 3.262  loss_cls_stage0: 0.3203  loss_box_reg_stage0: 0.293  loss_cls_stage1: 0.3656  loss_box_reg_stage1: 0.7569  loss_cls_stage2: 0.3301  loss_box_reg_stage2: 0.9951  loss_mask: 0.09844  loss_rpn_cls: 0.01813  loss_rpn_loc: 0.05726  time: 0.6283  data_time: 0.0045  lr: 0.00016  max_mem: 18906M
[07/29 13:03:16] d2.utils.events INFO:  eta: 10:45:24  iter: 6339  total_loss: 3.197  loss_cls_stage0: 0.3101  loss_box_reg_stage0: 0.3001  loss_cls_stage1: 0.3173  loss_box_reg_stage1: 0.7705  loss_cls_stage2: 0.3057  loss_box_reg_stage2: 1.021  loss_mask: 0.09567  loss_rpn_cls: 0.02067  loss_rpn_loc: 0.04603  time: 0.6283  data_time: 0.0047  lr: 0.00016  max_mem: 18906M
[07/29 13:03:29] d2.utils.events INFO:  eta: 10:45:44  iter: 6359  total_loss: 3.063  loss_cls_stage0: 0.3099  loss_box_reg_stage0: 0.2998  loss_cls_stage1: 0.3311  loss_box_reg_stage1: 0.7079  loss_cls_stage2: 0.3176  loss_box_reg_stage2: 0.9431  loss_mask: 0.1007  loss_rpn_cls: 0.02228  loss_rpn_loc: 0.05516  time: 0.6280  data_time: 0.0046  lr: 0.00016  max_mem: 18906M
[07/29 13:03:41] d2.utils.events INFO:  eta: 10:45:24  iter: 6379  total_loss: 3.125  loss_cls_stage0: 0.3032  loss_box_reg_stage0: 0.3223  loss_cls_stage1: 0.3284  loss_box_reg_stage1: 0.7455  loss_cls_stage2: 0.308  loss_box_reg_stage2: 1.028  loss_mask: 0.1097  loss_rpn_cls: 0.01937  loss_rpn_loc: 0.06031  time: 0.6278  data_time: 0.0048  lr: 0.00016  max_mem: 18906M
[07/29 13:03:54] d2.utils.events INFO:  eta: 10:45:40  iter: 6399  total_loss: 2.96  loss_cls_stage0: 0.3089  loss_box_reg_stage0: 0.2977  loss_cls_stage1: 0.2913  loss_box_reg_stage1: 0.6689  loss_cls_stage2: 0.2883  loss_box_reg_stage2: 0.869  loss_mask: 0.09877  loss_rpn_cls: 0.02333  loss_rpn_loc: 0.06875  time: 0.6280  data_time: 0.0050  lr: 0.00016  max_mem: 18906M
[07/29 13:04:07] d2.utils.events INFO:  eta: 10:46:16  iter: 6419  total_loss: 3.37  loss_cls_stage0: 0.3392  loss_box_reg_stage0: 0.2998  loss_cls_stage1: 0.3717  loss_box_reg_stage1: 0.7644  loss_cls_stage2: 0.3571  loss_box_reg_stage2: 1.04  loss_mask: 0.1093  loss_rpn_cls: 0.01738  loss_rpn_loc: 0.04629  time: 0.6283  data_time: 0.0051  lr: 0.00016  max_mem: 18906M
[07/29 13:04:19] d2.utils.events INFO:  eta: 10:45:31  iter: 6439  total_loss: 3.321  loss_cls_stage0: 0.3275  loss_box_reg_stage0: 0.2966  loss_cls_stage1: 0.3554  loss_box_reg_stage1: 0.7685  loss_cls_stage2: 0.3289  loss_box_reg_stage2: 0.9844  loss_mask: 0.09776  loss_rpn_cls: 0.01722  loss_rpn_loc: 0.04587  time: 0.6282  data_time: 0.0045  lr: 0.00016  max_mem: 18906M
[07/29 13:04:32] d2.utils.events INFO:  eta: 10:46:08  iter: 6459  total_loss: 3.548  loss_cls_stage0: 0.3425  loss_box_reg_stage0: 0.3293  loss_cls_stage1: 0.3687  loss_box_reg_stage1: 0.8267  loss_cls_stage2: 0.3488  loss_box_reg_stage2: 1.044  loss_mask: 0.1089  loss_rpn_cls: 0.01488  loss_rpn_loc: 0.07642  time: 0.6280  data_time: 0.0051  lr: 0.00016  max_mem: 18906M
[07/29 13:04:44] d2.utils.events INFO:  eta: 10:45:58  iter: 6479  total_loss: 3.385  loss_cls_stage0: 0.3526  loss_box_reg_stage0: 0.331  loss_cls_stage1: 0.3666  loss_box_reg_stage1: 0.7817  loss_cls_stage2: 0.3541  loss_box_reg_stage2: 0.9771  loss_mask: 0.1027  loss_rpn_cls: 0.01939  loss_rpn_loc: 0.04436  time: 0.6282  data_time: 0.0054  lr: 0.00016  max_mem: 18906M
[07/29 13:04:57] d2.utils.events INFO:  eta: 10:45:49  iter: 6499  total_loss: 3.097  loss_cls_stage0: 0.3048  loss_box_reg_stage0: 0.2753  loss_cls_stage1: 0.3331  loss_box_reg_stage1: 0.7033  loss_cls_stage2: 0.3363  loss_box_reg_stage2: 0.9641  loss_mask: 0.1019  loss_rpn_cls: 0.01785  loss_rpn_loc: 0.04987  time: 0.6280  data_time: 0.0047  lr: 0.00016  max_mem: 18906M
[07/29 13:05:09] d2.utils.events INFO:  eta: 10:46:31  iter: 6519  total_loss: 3.441  loss_cls_stage0: 0.3251  loss_box_reg_stage0: 0.3294  loss_cls_stage1: 0.3921  loss_box_reg_stage1: 0.7985  loss_cls_stage2: 0.3651  loss_box_reg_stage2: 1.043  loss_mask: 0.101  loss_rpn_cls: 0.01441  loss_rpn_loc: 0.0521  time: 0.6282  data_time: 0.0051  lr: 0.00016  max_mem: 18906M
[07/29 13:05:22] d2.utils.events INFO:  eta: 10:45:50  iter: 6539  total_loss: 3.303  loss_cls_stage0: 0.3638  loss_box_reg_stage0: 0.3373  loss_cls_stage1: 0.3714  loss_box_reg_stage1: 0.7384  loss_cls_stage2: 0.3334  loss_box_reg_stage2: 0.8858  loss_mask: 0.1059  loss_rpn_cls: 0.03069  loss_rpn_loc: 0.06147  time: 0.6281  data_time: 0.0046  lr: 0.00016  max_mem: 18906M
[07/29 13:05:34] d2.utils.events INFO:  eta: 10:45:19  iter: 6559  total_loss: 3.369  loss_cls_stage0: 0.319  loss_box_reg_stage0: 0.3187  loss_cls_stage1: 0.3558  loss_box_reg_stage1: 0.7539  loss_cls_stage2: 0.3494  loss_box_reg_stage2: 0.9876  loss_mask: 0.09513  loss_rpn_cls: 0.0197  loss_rpn_loc: 0.05733  time: 0.6281  data_time: 0.0045  lr: 0.00016  max_mem: 18906M
[07/29 13:05:48] d2.utils.events INFO:  eta: 10:45:41  iter: 6579  total_loss: 3.619  loss_cls_stage0: 0.3376  loss_box_reg_stage0: 0.328  loss_cls_stage1: 0.3855  loss_box_reg_stage1: 0.8262  loss_cls_stage2: 0.3612  loss_box_reg_stage2: 1.016  loss_mask: 0.1038  loss_rpn_cls: 0.01835  loss_rpn_loc: 0.057  time: 0.6284  data_time: 0.0048  lr: 0.00016  max_mem: 18906M
[07/29 13:06:00] d2.utils.events INFO:  eta: 10:45:00  iter: 6599  total_loss: 3.534  loss_cls_stage0: 0.3949  loss_box_reg_stage0: 0.3512  loss_cls_stage1: 0.4373  loss_box_reg_stage1: 0.8364  loss_cls_stage2: 0.3958  loss_box_reg_stage2: 1.041  loss_mask: 0.1046  loss_rpn_cls: 0.01829  loss_rpn_loc: 0.05383  time: 0.6282  data_time: 0.0048  lr: 0.00016  max_mem: 18906M
[07/29 13:06:13] d2.utils.events INFO:  eta: 10:45:28  iter: 6619  total_loss: 3.261  loss_cls_stage0: 0.3326  loss_box_reg_stage0: 0.2854  loss_cls_stage1: 0.3874  loss_box_reg_stage1: 0.7708  loss_cls_stage2: 0.319  loss_box_reg_stage2: 1.032  loss_mask: 0.09915  loss_rpn_cls: 0.01738  loss_rpn_loc: 0.05457  time: 0.6284  data_time: 0.0051  lr: 0.00016  max_mem: 18906M
[07/29 13:06:26] d2.utils.events INFO:  eta: 10:45:53  iter: 6639  total_loss: 3.432  loss_cls_stage0: 0.3527  loss_box_reg_stage0: 0.3378  loss_cls_stage1: 0.3896  loss_box_reg_stage1: 0.8133  loss_cls_stage2: 0.351  loss_box_reg_stage2: 0.9859  loss_mask: 0.1051  loss_rpn_cls: 0.01496  loss_rpn_loc: 0.04852  time: 0.6286  data_time: 0.0051  lr: 0.00016  max_mem: 18906M
[07/29 13:06:38] d2.utils.events INFO:  eta: 10:45:23  iter: 6659  total_loss: 3.406  loss_cls_stage0: 0.3277  loss_box_reg_stage0: 0.3336  loss_cls_stage1: 0.3526  loss_box_reg_stage1: 0.7879  loss_cls_stage2: 0.3523  loss_box_reg_stage2: 1.016  loss_mask: 0.09669  loss_rpn_cls: 0.01722  loss_rpn_loc: 0.05366  time: 0.6287  data_time: 0.0045  lr: 0.00016  max_mem: 18906M
[07/29 13:06:51] d2.utils.events INFO:  eta: 10:45:38  iter: 6679  total_loss: 3.256  loss_cls_stage0: 0.3797  loss_box_reg_stage0: 0.3368  loss_cls_stage1: 0.374  loss_box_reg_stage1: 0.7307  loss_cls_stage2: 0.3287  loss_box_reg_stage2: 0.9157  loss_mask: 0.1026  loss_rpn_cls: 0.02322  loss_rpn_loc: 0.05949  time: 0.6286  data_time: 0.0050  lr: 0.00016  max_mem: 18906M
[07/29 13:07:03] d2.utils.events INFO:  eta: 10:46:22  iter: 6699  total_loss: 3.228  loss_cls_stage0: 0.3197  loss_box_reg_stage0: 0.3128  loss_cls_stage1: 0.3596  loss_box_reg_stage1: 0.7182  loss_cls_stage2: 0.3409  loss_box_reg_stage2: 0.9684  loss_mask: 0.1017  loss_rpn_cls: 0.01539  loss_rpn_loc: 0.04928  time: 0.6287  data_time: 0.0046  lr: 0.00016  max_mem: 18906M
[07/29 13:07:16] d2.utils.events INFO:  eta: 10:46:27  iter: 6719  total_loss: 3.485  loss_cls_stage0: 0.352  loss_box_reg_stage0: 0.3254  loss_cls_stage1: 0.3773  loss_box_reg_stage1: 0.7565  loss_cls_stage2: 0.3696  loss_box_reg_stage2: 1.049  loss_mask: 0.1146  loss_rpn_cls: 0.02599  loss_rpn_loc: 0.06747  time: 0.6286  data_time: 0.0050  lr: 0.00016  max_mem: 18906M
[07/29 13:07:28] d2.utils.events INFO:  eta: 10:46:16  iter: 6739  total_loss: 3.453  loss_cls_stage0: 0.3548  loss_box_reg_stage0: 0.3186  loss_cls_stage1: 0.3725  loss_box_reg_stage1: 0.7789  loss_cls_stage2: 0.3516  loss_box_reg_stage2: 1.048  loss_mask: 0.1126  loss_rpn_cls: 0.01619  loss_rpn_loc: 0.04925  time: 0.6286  data_time: 0.0046  lr: 0.00016  max_mem: 18906M
[07/29 13:07:41] d2.utils.events INFO:  eta: 10:46:01  iter: 6759  total_loss: 3.178  loss_cls_stage0: 0.3491  loss_box_reg_stage0: 0.2857  loss_cls_stage1: 0.372  loss_box_reg_stage1: 0.7352  loss_cls_stage2: 0.3615  loss_box_reg_stage2: 0.9579  loss_mask: 0.08984  loss_rpn_cls: 0.02411  loss_rpn_loc: 0.04938  time: 0.6286  data_time: 0.0044  lr: 0.00016  max_mem: 18906M
[07/29 13:07:53] d2.utils.events INFO:  eta: 10:45:09  iter: 6779  total_loss: 3.482  loss_cls_stage0: 0.3308  loss_box_reg_stage0: 0.309  loss_cls_stage1: 0.363  loss_box_reg_stage1: 0.7647  loss_cls_stage2: 0.36  loss_box_reg_stage2: 1.01  loss_mask: 0.1008  loss_rpn_cls: 0.0191  loss_rpn_loc: 0.05314  time: 0.6285  data_time: 0.0049  lr: 0.00016  max_mem: 18906M
[07/29 13:08:05] d2.utils.events INFO:  eta: 10:44:07  iter: 6799  total_loss: 3.048  loss_cls_stage0: 0.3068  loss_box_reg_stage0: 0.2932  loss_cls_stage1: 0.3322  loss_box_reg_stage1: 0.7136  loss_cls_stage2: 0.2836  loss_box_reg_stage2: 0.9028  loss_mask: 0.09649  loss_rpn_cls: 0.01637  loss_rpn_loc: 0.04409  time: 0.6281  data_time: 0.0045  lr: 0.00016  max_mem: 18906M
[07/29 13:08:18] d2.utils.events INFO:  eta: 10:44:32  iter: 6819  total_loss: 3.202  loss_cls_stage0: 0.3332  loss_box_reg_stage0: 0.3267  loss_cls_stage1: 0.3351  loss_box_reg_stage1: 0.7312  loss_cls_stage2: 0.3383  loss_box_reg_stage2: 0.9229  loss_mask: 0.09571  loss_rpn_cls: 0.02082  loss_rpn_loc: 0.06199  time: 0.6283  data_time: 0.0049  lr: 0.00016  max_mem: 18906M
[07/29 13:08:31] d2.utils.events INFO:  eta: 10:44:31  iter: 6839  total_loss: 3.26  loss_cls_stage0: 0.3316  loss_box_reg_stage0: 0.3298  loss_cls_stage1: 0.3611  loss_box_reg_stage1: 0.7964  loss_cls_stage2: 0.3415  loss_box_reg_stage2: 1.045  loss_mask: 0.1074  loss_rpn_cls: 0.0198  loss_rpn_loc: 0.07798  time: 0.6285  data_time: 0.0045  lr: 0.00016  max_mem: 18906M
[07/29 13:08:44] d2.utils.events INFO:  eta: 10:44:58  iter: 6859  total_loss: 3.617  loss_cls_stage0: 0.3617  loss_box_reg_stage0: 0.3286  loss_cls_stage1: 0.3757  loss_box_reg_stage1: 0.7873  loss_cls_stage2: 0.3378  loss_box_reg_stage2: 0.9719  loss_mask: 0.1046  loss_rpn_cls: 0.0276  loss_rpn_loc: 0.06282  time: 0.6284  data_time: 0.0047  lr: 0.00016  max_mem: 18906M
[07/29 13:08:56] d2.utils.events INFO:  eta: 10:44:45  iter: 6879  total_loss: 3.591  loss_cls_stage0: 0.3899  loss_box_reg_stage0: 0.3466  loss_cls_stage1: 0.4349  loss_box_reg_stage1: 0.8708  loss_cls_stage2: 0.3953  loss_box_reg_stage2: 1.073  loss_mask: 0.1142  loss_rpn_cls: 0.01817  loss_rpn_loc: 0.05623  time: 0.6284  data_time: 0.0050  lr: 0.00016  max_mem: 18906M
[07/29 13:09:09] d2.utils.events INFO:  eta: 10:44:34  iter: 6899  total_loss: 3.416  loss_cls_stage0: 0.3486  loss_box_reg_stage0: 0.337  loss_cls_stage1: 0.3901  loss_box_reg_stage1: 0.8255  loss_cls_stage2: 0.3582  loss_box_reg_stage2: 1.08  loss_mask: 0.09807  loss_rpn_cls: 0.01628  loss_rpn_loc: 0.049  time: 0.6285  data_time: 0.0049  lr: 0.00016  max_mem: 18906M
[07/29 13:09:22] d2.utils.events INFO:  eta: 10:44:41  iter: 6919  total_loss: 3.399  loss_cls_stage0: 0.3279  loss_box_reg_stage0: 0.2898  loss_cls_stage1: 0.3677  loss_box_reg_stage1: 0.7789  loss_cls_stage2: 0.3746  loss_box_reg_stage2: 1.071  loss_mask: 0.09003  loss_rpn_cls: 0.0163  loss_rpn_loc: 0.05284  time: 0.6286  data_time: 0.0051  lr: 0.00016  max_mem: 18906M
[07/29 13:09:34] d2.utils.events INFO:  eta: 10:44:16  iter: 6939  total_loss: 3.412  loss_cls_stage0: 0.3318  loss_box_reg_stage0: 0.308  loss_cls_stage1: 0.3692  loss_box_reg_stage1: 0.8135  loss_cls_stage2: 0.3414  loss_box_reg_stage2: 1.04  loss_mask: 0.1011  loss_rpn_cls: 0.01918  loss_rpn_loc: 0.06162  time: 0.6286  data_time: 0.0053  lr: 0.00016  max_mem: 18906M
[07/29 13:09:47] d2.utils.events INFO:  eta: 10:43:56  iter: 6959  total_loss: 3.646  loss_cls_stage0: 0.3729  loss_box_reg_stage0: 0.3256  loss_cls_stage1: 0.403  loss_box_reg_stage1: 0.8218  loss_cls_stage2: 0.397  loss_box_reg_stage2: 1.031  loss_mask: 0.102  loss_rpn_cls: 0.02317  loss_rpn_loc: 0.05547  time: 0.6285  data_time: 0.0048  lr: 0.00016  max_mem: 18906M
[07/29 13:09:59] d2.utils.events INFO:  eta: 10:43:43  iter: 6979  total_loss: 3.39  loss_cls_stage0: 0.3072  loss_box_reg_stage0: 0.2887  loss_cls_stage1: 0.3476  loss_box_reg_stage1: 0.8098  loss_cls_stage2: 0.3263  loss_box_reg_stage2: 1.019  loss_mask: 0.09697  loss_rpn_cls: 0.01597  loss_rpn_loc: 0.04899  time: 0.6285  data_time: 0.0049  lr: 0.00016  max_mem: 18906M
[07/29 13:10:12] d2.utils.events INFO:  eta: 10:44:17  iter: 6999  total_loss: 3.401  loss_cls_stage0: 0.3216  loss_box_reg_stage0: 0.303  loss_cls_stage1: 0.3737  loss_box_reg_stage1: 0.836  loss_cls_stage2: 0.3836  loss_box_reg_stage2: 1.164  loss_mask: 0.1022  loss_rpn_cls: 0.02046  loss_rpn_loc: 0.05506  time: 0.6287  data_time: 0.0052  lr: 0.00016  max_mem: 18906M
[07/29 13:10:25] d2.utils.events INFO:  eta: 10:43:53  iter: 7019  total_loss: 3.553  loss_cls_stage0: 0.3423  loss_box_reg_stage0: 0.3478  loss_cls_stage1: 0.3588  loss_box_reg_stage1: 0.8383  loss_cls_stage2: 0.3376  loss_box_reg_stage2: 1.047  loss_mask: 0.1094  loss_rpn_cls: 0.02272  loss_rpn_loc: 0.08242  time: 0.6287  data_time: 0.0049  lr: 0.00016  max_mem: 18906M
[07/29 13:10:37] d2.utils.events INFO:  eta: 10:43:34  iter: 7039  total_loss: 3.522  loss_cls_stage0: 0.3378  loss_box_reg_stage0: 0.3161  loss_cls_stage1: 0.3645  loss_box_reg_stage1: 0.8033  loss_cls_stage2: 0.3465  loss_box_reg_stage2: 0.9999  loss_mask: 0.1048  loss_rpn_cls: 0.02435  loss_rpn_loc: 0.05243  time: 0.6287  data_time: 0.0050  lr: 0.00016  max_mem: 18906M
[07/29 13:10:50] d2.utils.events INFO:  eta: 10:43:04  iter: 7059  total_loss: 3.51  loss_cls_stage0: 0.3514  loss_box_reg_stage0: 0.3184  loss_cls_stage1: 0.4062  loss_box_reg_stage1: 0.8295  loss_cls_stage2: 0.3635  loss_box_reg_stage2: 1.008  loss_mask: 0.1084  loss_rpn_cls: 0.02227  loss_rpn_loc: 0.06444  time: 0.6288  data_time: 0.0050  lr: 0.00016  max_mem: 18906M
[07/29 13:11:03] d2.utils.events INFO:  eta: 10:42:36  iter: 7079  total_loss: 3.43  loss_cls_stage0: 0.3585  loss_box_reg_stage0: 0.3367  loss_cls_stage1: 0.3854  loss_box_reg_stage1: 0.7885  loss_cls_stage2: 0.3498  loss_box_reg_stage2: 1.078  loss_mask: 0.09015  loss_rpn_cls: 0.01914  loss_rpn_loc: 0.05819  time: 0.6287  data_time: 0.0046  lr: 0.00016  max_mem: 18906M
[07/29 13:11:15] d2.utils.events INFO:  eta: 10:42:16  iter: 7099  total_loss: 3.592  loss_cls_stage0: 0.3555  loss_box_reg_stage0: 0.3286  loss_cls_stage1: 0.3761  loss_box_reg_stage1: 0.8589  loss_cls_stage2: 0.3661  loss_box_reg_stage2: 1.045  loss_mask: 0.1069  loss_rpn_cls: 0.01602  loss_rpn_loc: 0.0514  time: 0.6288  data_time: 0.0050  lr: 0.00016  max_mem: 18906M
[07/29 13:11:28] d2.utils.events INFO:  eta: 10:42:11  iter: 7119  total_loss: 3.265  loss_cls_stage0: 0.3062  loss_box_reg_stage0: 0.3173  loss_cls_stage1: 0.3282  loss_box_reg_stage1: 0.7436  loss_cls_stage2: 0.3146  loss_box_reg_stage2: 0.9697  loss_mask: 0.09705  loss_rpn_cls: 0.01803  loss_rpn_loc: 0.05802  time: 0.6288  data_time: 0.0051  lr: 0.00016  max_mem: 18906M
[07/29 13:11:41] d2.utils.events INFO:  eta: 10:42:43  iter: 7139  total_loss: 3.268  loss_cls_stage0: 0.3472  loss_box_reg_stage0: 0.326  loss_cls_stage1: 0.3715  loss_box_reg_stage1: 0.7619  loss_cls_stage2: 0.3661  loss_box_reg_stage2: 0.9899  loss_mask: 0.1006  loss_rpn_cls: 0.01708  loss_rpn_loc: 0.04635  time: 0.6291  data_time: 0.0052  lr: 0.00016  max_mem: 18906M
[07/29 13:11:54] d2.utils.events INFO:  eta: 10:42:23  iter: 7159  total_loss: 3.361  loss_cls_stage0: 0.3495  loss_box_reg_stage0: 0.3147  loss_cls_stage1: 0.3561  loss_box_reg_stage1: 0.8138  loss_cls_stage2: 0.3506  loss_box_reg_stage2: 1.046  loss_mask: 0.1005  loss_rpn_cls: 0.01932  loss_rpn_loc: 0.0508  time: 0.6291  data_time: 0.0049  lr: 0.00016  max_mem: 18906M
[07/29 13:12:06] d2.utils.events INFO:  eta: 10:41:41  iter: 7179  total_loss: 3.606  loss_cls_stage0: 0.3559  loss_box_reg_stage0: 0.3263  loss_cls_stage1: 0.3654  loss_box_reg_stage1: 0.8093  loss_cls_stage2: 0.3591  loss_box_reg_stage2: 1.047  loss_mask: 0.09793  loss_rpn_cls: 0.02367  loss_rpn_loc: 0.06046  time: 0.6290  data_time: 0.0048  lr: 0.00016  max_mem: 18906M
[07/29 13:12:18] d2.utils.events INFO:  eta: 10:41:20  iter: 7199  total_loss: 3.611  loss_cls_stage0: 0.3449  loss_box_reg_stage0: 0.336  loss_cls_stage1: 0.4041  loss_box_reg_stage1: 0.8505  loss_cls_stage2: 0.372  loss_box_reg_stage2: 1.051  loss_mask: 0.09853  loss_rpn_cls: 0.0179  loss_rpn_loc: 0.05843  time: 0.6288  data_time: 0.0049  lr: 0.00016  max_mem: 18906M
[07/29 13:12:31] d2.utils.events INFO:  eta: 10:41:05  iter: 7219  total_loss: 3.557  loss_cls_stage0: 0.3596  loss_box_reg_stage0: 0.3384  loss_cls_stage1: 0.3932  loss_box_reg_stage1: 0.8098  loss_cls_stage2: 0.3753  loss_box_reg_stage2: 1.021  loss_mask: 0.1013  loss_rpn_cls: 0.01796  loss_rpn_loc: 0.05911  time: 0.6288  data_time: 0.0046  lr: 0.00016  max_mem: 18906M
[07/29 13:12:43] d2.utils.events INFO:  eta: 10:40:33  iter: 7239  total_loss: 3.495  loss_cls_stage0: 0.339  loss_box_reg_stage0: 0.3102  loss_cls_stage1: 0.405  loss_box_reg_stage1: 0.8225  loss_cls_stage2: 0.404  loss_box_reg_stage2: 1.076  loss_mask: 0.09416  loss_rpn_cls: 0.01977  loss_rpn_loc: 0.07558  time: 0.6287  data_time: 0.0047  lr: 0.00016  max_mem: 18906M
[07/29 13:12:56] d2.utils.events INFO:  eta: 10:40:03  iter: 7259  total_loss: 3.595  loss_cls_stage0: 0.3449  loss_box_reg_stage0: 0.3254  loss_cls_stage1: 0.3876  loss_box_reg_stage1: 0.8315  loss_cls_stage2: 0.3918  loss_box_reg_stage2: 1.028  loss_mask: 0.09713  loss_rpn_cls: 0.0286  loss_rpn_loc: 0.06469  time: 0.6287  data_time: 0.0046  lr: 0.00016  max_mem: 18906M
[07/29 13:13:08] d2.utils.events INFO:  eta: 10:39:04  iter: 7279  total_loss: 3.301  loss_cls_stage0: 0.3242  loss_box_reg_stage0: 0.2989  loss_cls_stage1: 0.3812  loss_box_reg_stage1: 0.7381  loss_cls_stage2: 0.3539  loss_box_reg_stage2: 0.9599  loss_mask: 0.1064  loss_rpn_cls: 0.02287  loss_rpn_loc: 0.04763  time: 0.6286  data_time: 0.0051  lr: 0.00016  max_mem: 18906M
[07/29 13:13:21] d2.utils.events INFO:  eta: 10:38:14  iter: 7299  total_loss: 3.444  loss_cls_stage0: 0.3562  loss_box_reg_stage0: 0.304  loss_cls_stage1: 0.4183  loss_box_reg_stage1: 0.776  loss_cls_stage2: 0.3813  loss_box_reg_stage2: 1.076  loss_mask: 0.1005  loss_rpn_cls: 0.01822  loss_rpn_loc: 0.04618  time: 0.6287  data_time: 0.0046  lr: 0.00016  max_mem: 18906M
[07/29 13:13:34] d2.utils.events INFO:  eta: 10:39:25  iter: 7319  total_loss: 3.177  loss_cls_stage0: 0.2939  loss_box_reg_stage0: 0.2965  loss_cls_stage1: 0.3118  loss_box_reg_stage1: 0.7538  loss_cls_stage2: 0.3109  loss_box_reg_stage2: 1.039  loss_mask: 0.08928  loss_rpn_cls: 0.01693  loss_rpn_loc: 0.04935  time: 0.6288  data_time: 0.0051  lr: 0.00016  max_mem: 18906M
[07/29 13:13:47] d2.utils.events INFO:  eta: 10:38:53  iter: 7339  total_loss: 3.665  loss_cls_stage0: 0.3466  loss_box_reg_stage0: 0.3433  loss_cls_stage1: 0.4219  loss_box_reg_stage1: 0.8405  loss_cls_stage2: 0.3934  loss_box_reg_stage2: 1.078  loss_mask: 0.1055  loss_rpn_cls: 0.01827  loss_rpn_loc: 0.05625  time: 0.6288  data_time: 0.0048  lr: 0.00016  max_mem: 18906M
[07/29 13:13:59] d2.utils.events INFO:  eta: 10:39:10  iter: 7359  total_loss: 3.51  loss_cls_stage0: 0.3317  loss_box_reg_stage0: 0.3199  loss_cls_stage1: 0.374  loss_box_reg_stage1: 0.8244  loss_cls_stage2: 0.3795  loss_box_reg_stage2: 1.065  loss_mask: 0.1157  loss_rpn_cls: 0.02124  loss_rpn_loc: 0.0611  time: 0.6289  data_time: 0.0050  lr: 0.00016  max_mem: 18906M
[07/29 13:14:12] d2.utils.events INFO:  eta: 10:39:03  iter: 7379  total_loss: 3.19  loss_cls_stage0: 0.3267  loss_box_reg_stage0: 0.3199  loss_cls_stage1: 0.3646  loss_box_reg_stage1: 0.7251  loss_cls_stage2: 0.3563  loss_box_reg_stage2: 0.9133  loss_mask: 0.1081  loss_rpn_cls: 0.02844  loss_rpn_loc: 0.05158  time: 0.6289  data_time: 0.0049  lr: 0.00016  max_mem: 18906M
[07/29 13:14:25] d2.utils.events INFO:  eta: 10:38:51  iter: 7399  total_loss: 3.657  loss_cls_stage0: 0.3645  loss_box_reg_stage0: 0.3357  loss_cls_stage1: 0.3856  loss_box_reg_stage1: 0.7533  loss_cls_stage2: 0.3738  loss_box_reg_stage2: 1.021  loss_mask: 0.1034  loss_rpn_cls: 0.02187  loss_rpn_loc: 0.05431  time: 0.6290  data_time: 0.0050  lr: 0.00016  max_mem: 18906M
[07/29 13:14:38] d2.utils.events INFO:  eta: 10:38:19  iter: 7419  total_loss: 3.581  loss_cls_stage0: 0.3728  loss_box_reg_stage0: 0.3491  loss_cls_stage1: 0.3856  loss_box_reg_stage1: 0.8348  loss_cls_stage2: 0.3609  loss_box_reg_stage2: 1.04  loss_mask: 0.1173  loss_rpn_cls: 0.02754  loss_rpn_loc: 0.05952  time: 0.6291  data_time: 0.0047  lr: 0.00016  max_mem: 18906M
[07/29 13:14:50] d2.utils.events INFO:  eta: 10:38:06  iter: 7439  total_loss: 3.373  loss_cls_stage0: 0.3134  loss_box_reg_stage0: 0.3234  loss_cls_stage1: 0.3386  loss_box_reg_stage1: 0.7843  loss_cls_stage2: 0.3371  loss_box_reg_stage2: 1.048  loss_mask: 0.1097  loss_rpn_cls: 0.02036  loss_rpn_loc: 0.04641  time: 0.6290  data_time: 0.0048  lr: 0.00016  max_mem: 18906M
[07/29 13:15:02] d2.utils.events INFO:  eta: 10:38:07  iter: 7459  total_loss: 3.331  loss_cls_stage0: 0.3283  loss_box_reg_stage0: 0.2959  loss_cls_stage1: 0.3591  loss_box_reg_stage1: 0.7355  loss_cls_stage2: 0.3903  loss_box_reg_stage2: 0.9848  loss_mask: 0.08834  loss_rpn_cls: 0.02713  loss_rpn_loc: 0.05772  time: 0.6289  data_time: 0.0050  lr: 0.00016  max_mem: 18906M
[07/29 13:15:15] d2.utils.events INFO:  eta: 10:37:58  iter: 7479  total_loss: 3.618  loss_cls_stage0: 0.3565  loss_box_reg_stage0: 0.3372  loss_cls_stage1: 0.414  loss_box_reg_stage1: 0.8777  loss_cls_stage2: 0.3981  loss_box_reg_stage2: 1.131  loss_mask: 0.1058  loss_rpn_cls: 0.0175  loss_rpn_loc: 0.05348  time: 0.6290  data_time: 0.0048  lr: 0.00016  max_mem: 18906M
[07/29 13:15:28] d2.utils.events INFO:  eta: 10:37:54  iter: 7499  total_loss: 3.365  loss_cls_stage0: 0.299  loss_box_reg_stage0: 0.2904  loss_cls_stage1: 0.3231  loss_box_reg_stage1: 0.8054  loss_cls_stage2: 0.3245  loss_box_reg_stage2: 1.066  loss_mask: 0.08644  loss_rpn_cls: 0.02012  loss_rpn_loc: 0.05176  time: 0.6290  data_time: 0.0049  lr: 0.00016  max_mem: 18906M
[07/29 13:15:40] d2.utils.events INFO:  eta: 10:37:20  iter: 7519  total_loss: 3.69  loss_cls_stage0: 0.3824  loss_box_reg_stage0: 0.3581  loss_cls_stage1: 0.3889  loss_box_reg_stage1: 0.8576  loss_cls_stage2: 0.3585  loss_box_reg_stage2: 1.055  loss_mask: 0.1073  loss_rpn_cls: 0.02294  loss_rpn_loc: 0.05269  time: 0.6290  data_time: 0.0048  lr: 0.00016  max_mem: 18906M
[07/29 13:15:53] d2.utils.events INFO:  eta: 10:37:29  iter: 7539  total_loss: 3.392  loss_cls_stage0: 0.3367  loss_box_reg_stage0: 0.317  loss_cls_stage1: 0.354  loss_box_reg_stage1: 0.7928  loss_cls_stage2: 0.3458  loss_box_reg_stage2: 0.9778  loss_mask: 0.1038  loss_rpn_cls: 0.03232  loss_rpn_loc: 0.08948  time: 0.6289  data_time: 0.0050  lr: 0.00016  max_mem: 18906M
[07/29 13:16:05] d2.utils.events INFO:  eta: 10:37:03  iter: 7559  total_loss: 3.246  loss_cls_stage0: 0.3119  loss_box_reg_stage0: 0.292  loss_cls_stage1: 0.3432  loss_box_reg_stage1: 0.7269  loss_cls_stage2: 0.3289  loss_box_reg_stage2: 0.974  loss_mask: 0.1019  loss_rpn_cls: 0.02051  loss_rpn_loc: 0.0509  time: 0.6288  data_time: 0.0050  lr: 0.00016  max_mem: 18906M
[07/29 13:16:17] d2.utils.events INFO:  eta: 10:36:01  iter: 7579  total_loss: 3.144  loss_cls_stage0: 0.2935  loss_box_reg_stage0: 0.285  loss_cls_stage1: 0.3458  loss_box_reg_stage1: 0.7891  loss_cls_stage2: 0.3411  loss_box_reg_stage2: 1.03  loss_mask: 0.08759  loss_rpn_cls: 0.01832  loss_rpn_loc: 0.05084  time: 0.6286  data_time: 0.0047  lr: 0.00016  max_mem: 18906M
[07/29 13:16:30] d2.utils.events INFO:  eta: 10:36:29  iter: 7599  total_loss: 3.448  loss_cls_stage0: 0.3395  loss_box_reg_stage0: 0.3182  loss_cls_stage1: 0.3511  loss_box_reg_stage1: 0.8402  loss_cls_stage2: 0.3479  loss_box_reg_stage2: 1.024  loss_mask: 0.09636  loss_rpn_cls: 0.02494  loss_rpn_loc: 0.05191  time: 0.6286  data_time: 0.0048  lr: 0.00016  max_mem: 18906M
[07/29 13:16:42] d2.utils.events INFO:  eta: 10:34:56  iter: 7619  total_loss: 3.518  loss_cls_stage0: 0.333  loss_box_reg_stage0: 0.3022  loss_cls_stage1: 0.3817  loss_box_reg_stage1: 0.8316  loss_cls_stage2: 0.3699  loss_box_reg_stage2: 1.15  loss_mask: 0.09194  loss_rpn_cls: 0.01663  loss_rpn_loc: 0.04356  time: 0.6285  data_time: 0.0047  lr: 0.00016  max_mem: 18906M
[07/29 13:16:54] d2.utils.events INFO:  eta: 10:33:14  iter: 7639  total_loss: 3.436  loss_cls_stage0: 0.3098  loss_box_reg_stage0: 0.3005  loss_cls_stage1: 0.3632  loss_box_reg_stage1: 0.7962  loss_cls_stage2: 0.362  loss_box_reg_stage2: 1.056  loss_mask: 0.09247  loss_rpn_cls: 0.02291  loss_rpn_loc: 0.04529  time: 0.6284  data_time: 0.0051  lr: 0.00016  max_mem: 18906M
[07/29 13:17:08] d2.utils.events INFO:  eta: 10:34:03  iter: 7659  total_loss: 3.635  loss_cls_stage0: 0.3668  loss_box_reg_stage0: 0.3243  loss_cls_stage1: 0.3822  loss_box_reg_stage1: 0.8006  loss_cls_stage2: 0.3857  loss_box_reg_stage2: 1.065  loss_mask: 0.1158  loss_rpn_cls: 0.02053  loss_rpn_loc: 0.0483  time: 0.6287  data_time: 0.0051  lr: 0.00016  max_mem: 18906M
[07/29 13:17:21] d2.utils.events INFO:  eta: 10:34:33  iter: 7679  total_loss: 3.473  loss_cls_stage0: 0.3626  loss_box_reg_stage0: 0.326  loss_cls_stage1: 0.3797  loss_box_reg_stage1: 0.7913  loss_cls_stage2: 0.3527  loss_box_reg_stage2: 1.036  loss_mask: 0.1012  loss_rpn_cls: 0.01874  loss_rpn_loc: 0.05615  time: 0.6288  data_time: 0.0051  lr: 0.00016  max_mem: 18906M
[07/29 13:17:33] d2.utils.events INFO:  eta: 10:33:25  iter: 7699  total_loss: 3.442  loss_cls_stage0: 0.3392  loss_box_reg_stage0: 0.3116  loss_cls_stage1: 0.3856  loss_box_reg_stage1: 0.7737  loss_cls_stage2: 0.3501  loss_box_reg_stage2: 1.021  loss_mask: 0.1052  loss_rpn_cls: 0.0226  loss_rpn_loc: 0.05005  time: 0.6287  data_time: 0.0047  lr: 0.00016  max_mem: 18906M
[07/29 13:17:45] d2.utils.events INFO:  eta: 10:32:54  iter: 7719  total_loss: 2.74  loss_cls_stage0: 0.273  loss_box_reg_stage0: 0.2768  loss_cls_stage1: 0.261  loss_box_reg_stage1: 0.6252  loss_cls_stage2: 0.2459  loss_box_reg_stage2: 0.8543  loss_mask: 0.08881  loss_rpn_cls: 0.01998  loss_rpn_loc: 0.04724  time: 0.6287  data_time: 0.0050  lr: 0.00016  max_mem: 18906M
[07/29 13:17:58] d2.utils.events INFO:  eta: 10:32:42  iter: 7739  total_loss: 3.425  loss_cls_stage0: 0.3121  loss_box_reg_stage0: 0.2901  loss_cls_stage1: 0.3393  loss_box_reg_stage1: 0.7905  loss_cls_stage2: 0.339  loss_box_reg_stage2: 1.13  loss_mask: 0.09028  loss_rpn_cls: 0.02219  loss_rpn_loc: 0.04744  time: 0.6285  data_time: 0.0049  lr: 0.00016  max_mem: 18906M
[07/29 13:18:10] d2.utils.events INFO:  eta: 10:31:52  iter: 7759  total_loss: 3.417  loss_cls_stage0: 0.322  loss_box_reg_stage0: 0.2941  loss_cls_stage1: 0.3652  loss_box_reg_stage1: 0.7853  loss_cls_stage2: 0.3591  loss_box_reg_stage2: 1.054  loss_mask: 0.1075  loss_rpn_cls: 0.01474  loss_rpn_loc: 0.05036  time: 0.6284  data_time: 0.0043  lr: 0.00016  max_mem: 18906M
[07/29 13:18:22] d2.utils.events INFO:  eta: 10:31:21  iter: 7779  total_loss: 3.469  loss_cls_stage0: 0.3543  loss_box_reg_stage0: 0.3098  loss_cls_stage1: 0.3534  loss_box_reg_stage1: 0.759  loss_cls_stage2: 0.3523  loss_box_reg_stage2: 1.006  loss_mask: 0.09902  loss_rpn_cls: 0.0185  loss_rpn_loc: 0.04966  time: 0.6283  data_time: 0.0045  lr: 0.00016  max_mem: 18906M
[07/29 13:18:34] d2.utils.events INFO:  eta: 10:31:13  iter: 7799  total_loss: 3.572  loss_cls_stage0: 0.3261  loss_box_reg_stage0: 0.3293  loss_cls_stage1: 0.3646  loss_box_reg_stage1: 0.8322  loss_cls_stage2: 0.3626  loss_box_reg_stage2: 1.071  loss_mask: 0.09274  loss_rpn_cls: 0.01519  loss_rpn_loc: 0.05586  time: 0.6281  data_time: 0.0046  lr: 0.00016  max_mem: 18906M
[07/29 13:18:46] d2.utils.events INFO:  eta: 10:30:39  iter: 7819  total_loss: 3.226  loss_cls_stage0: 0.318  loss_box_reg_stage0: 0.3106  loss_cls_stage1: 0.3451  loss_box_reg_stage1: 0.766  loss_cls_stage2: 0.3229  loss_box_reg_stage2: 0.9297  loss_mask: 0.09133  loss_rpn_cls: 0.02255  loss_rpn_loc: 0.05077  time: 0.6280  data_time: 0.0052  lr: 0.00016  max_mem: 18906M
[07/29 13:18:59] d2.utils.events INFO:  eta: 10:29:55  iter: 7839  total_loss: 3.221  loss_cls_stage0: 0.3361  loss_box_reg_stage0: 0.2949  loss_cls_stage1: 0.3363  loss_box_reg_stage1: 0.7156  loss_cls_stage2: 0.3382  loss_box_reg_stage2: 0.9679  loss_mask: 0.09885  loss_rpn_cls: 0.02216  loss_rpn_loc: 0.05582  time: 0.6279  data_time: 0.0048  lr: 0.00016  max_mem: 18906M
[07/29 13:19:12] d2.utils.events INFO:  eta: 10:29:15  iter: 7859  total_loss: 3.205  loss_cls_stage0: 0.3187  loss_box_reg_stage0: 0.2894  loss_cls_stage1: 0.3367  loss_box_reg_stage1: 0.7155  loss_cls_stage2: 0.3164  loss_box_reg_stage2: 0.9475  loss_mask: 0.0878  loss_rpn_cls: 0.01573  loss_rpn_loc: 0.05389  time: 0.6280  data_time: 0.0045  lr: 0.00016  max_mem: 19303M
[07/29 13:19:24] d2.utils.events INFO:  eta: 10:28:22  iter: 7879  total_loss: 2.913  loss_cls_stage0: 0.2783  loss_box_reg_stage0: 0.2838  loss_cls_stage1: 0.2832  loss_box_reg_stage1: 0.6346  loss_cls_stage2: 0.2765  loss_box_reg_stage2: 0.9295  loss_mask: 0.08979  loss_rpn_cls: 0.01696  loss_rpn_loc: 0.06049  time: 0.6279  data_time: 0.0046  lr: 0.00016  max_mem: 19303M
[07/29 13:19:36] d2.utils.events INFO:  eta: 10:28:00  iter: 7899  total_loss: 3.183  loss_cls_stage0: 0.3105  loss_box_reg_stage0: 0.3146  loss_cls_stage1: 0.342  loss_box_reg_stage1: 0.7618  loss_cls_stage2: 0.312  loss_box_reg_stage2: 1.028  loss_mask: 0.09521  loss_rpn_cls: 0.02028  loss_rpn_loc: 0.05123  time: 0.6279  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 13:19:49] d2.utils.events INFO:  eta: 10:26:55  iter: 7919  total_loss: 3.293  loss_cls_stage0: 0.3058  loss_box_reg_stage0: 0.2966  loss_cls_stage1: 0.3362  loss_box_reg_stage1: 0.745  loss_cls_stage2: 0.3732  loss_box_reg_stage2: 0.9644  loss_mask: 0.08947  loss_rpn_cls: 0.01492  loss_rpn_loc: 0.04552  time: 0.6278  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 13:20:01] d2.utils.events INFO:  eta: 10:26:53  iter: 7939  total_loss: 3.224  loss_cls_stage0: 0.3197  loss_box_reg_stage0: 0.2985  loss_cls_stage1: 0.3272  loss_box_reg_stage1: 0.7316  loss_cls_stage2: 0.3345  loss_box_reg_stage2: 0.9626  loss_mask: 0.07765  loss_rpn_cls: 0.01507  loss_rpn_loc: 0.06263  time: 0.6278  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 13:20:14] d2.utils.events INFO:  eta: 10:26:29  iter: 7959  total_loss: 3.586  loss_cls_stage0: 0.3592  loss_box_reg_stage0: 0.32  loss_cls_stage1: 0.4036  loss_box_reg_stage1: 0.8374  loss_cls_stage2: 0.3613  loss_box_reg_stage2: 1.057  loss_mask: 0.1043  loss_rpn_cls: 0.01592  loss_rpn_loc: 0.04976  time: 0.6278  data_time: 0.0047  lr: 0.00016  max_mem: 19303M
[07/29 13:20:26] d2.utils.events INFO:  eta: 10:26:35  iter: 7979  total_loss: 3.283  loss_cls_stage0: 0.3299  loss_box_reg_stage0: 0.2884  loss_cls_stage1: 0.375  loss_box_reg_stage1: 0.773  loss_cls_stage2: 0.3384  loss_box_reg_stage2: 1.017  loss_mask: 0.08855  loss_rpn_cls: 0.019  loss_rpn_loc: 0.05804  time: 0.6278  data_time: 0.0046  lr: 0.00016  max_mem: 19303M
[07/29 13:20:39] d2.utils.events INFO:  eta: 10:25:52  iter: 7999  total_loss: 3.496  loss_cls_stage0: 0.3233  loss_box_reg_stage0: 0.3067  loss_cls_stage1: 0.331  loss_box_reg_stage1: 0.8975  loss_cls_stage2: 0.3283  loss_box_reg_stage2: 1.154  loss_mask: 0.09572  loss_rpn_cls: 0.01832  loss_rpn_loc: 0.0493  time: 0.6278  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 13:20:52] d2.utils.events INFO:  eta: 10:26:24  iter: 8019  total_loss: 3.518  loss_cls_stage0: 0.3373  loss_box_reg_stage0: 0.3088  loss_cls_stage1: 0.3809  loss_box_reg_stage1: 0.8517  loss_cls_stage2: 0.3447  loss_box_reg_stage2: 1.063  loss_mask: 0.09901  loss_rpn_cls: 0.0133  loss_rpn_loc: 0.04199  time: 0.6279  data_time: 0.0047  lr: 0.00016  max_mem: 19303M
[07/29 13:21:05] d2.utils.events INFO:  eta: 10:26:23  iter: 8039  total_loss: 3.327  loss_cls_stage0: 0.3531  loss_box_reg_stage0: 0.3235  loss_cls_stage1: 0.3257  loss_box_reg_stage1: 0.8052  loss_cls_stage2: 0.3156  loss_box_reg_stage2: 1.037  loss_mask: 0.1039  loss_rpn_cls: 0.01908  loss_rpn_loc: 0.04678  time: 0.6280  data_time: 0.0050  lr: 0.00016  max_mem: 19303M
[07/29 13:21:17] d2.utils.events INFO:  eta: 10:25:59  iter: 8059  total_loss: 3.111  loss_cls_stage0: 0.3399  loss_box_reg_stage0: 0.3035  loss_cls_stage1: 0.3545  loss_box_reg_stage1: 0.7454  loss_cls_stage2: 0.2942  loss_box_reg_stage2: 0.9469  loss_mask: 0.09879  loss_rpn_cls: 0.01836  loss_rpn_loc: 0.04929  time: 0.6280  data_time: 0.0052  lr: 0.00016  max_mem: 19303M
[07/29 13:21:30] d2.utils.events INFO:  eta: 10:26:07  iter: 8079  total_loss: 3.014  loss_cls_stage0: 0.3153  loss_box_reg_stage0: 0.3117  loss_cls_stage1: 0.339  loss_box_reg_stage1: 0.7222  loss_cls_stage2: 0.3027  loss_box_reg_stage2: 0.9202  loss_mask: 0.09862  loss_rpn_cls: 0.02519  loss_rpn_loc: 0.06557  time: 0.6280  data_time: 0.0050  lr: 0.00016  max_mem: 19303M
[07/29 13:21:43] d2.utils.events INFO:  eta: 10:25:46  iter: 8099  total_loss: 3.226  loss_cls_stage0: 0.3214  loss_box_reg_stage0: 0.2891  loss_cls_stage1: 0.3705  loss_box_reg_stage1: 0.6977  loss_cls_stage2: 0.3399  loss_box_reg_stage2: 0.8945  loss_mask: 0.09507  loss_rpn_cls: 0.02059  loss_rpn_loc: 0.0575  time: 0.6281  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 13:21:56] d2.utils.events INFO:  eta: 10:25:41  iter: 8119  total_loss: 3.524  loss_cls_stage0: 0.3465  loss_box_reg_stage0: 0.3187  loss_cls_stage1: 0.3993  loss_box_reg_stage1: 0.796  loss_cls_stage2: 0.3844  loss_box_reg_stage2: 1.039  loss_mask: 0.09988  loss_rpn_cls: 0.01822  loss_rpn_loc: 0.04782  time: 0.6281  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 13:22:08] d2.utils.events INFO:  eta: 10:24:14  iter: 8139  total_loss: 3.125  loss_cls_stage0: 0.2686  loss_box_reg_stage0: 0.2863  loss_cls_stage1: 0.3242  loss_box_reg_stage1: 0.7542  loss_cls_stage2: 0.3117  loss_box_reg_stage2: 0.95  loss_mask: 0.09019  loss_rpn_cls: 0.01768  loss_rpn_loc: 0.07048  time: 0.6281  data_time: 0.0051  lr: 0.00016  max_mem: 19303M
[07/29 13:22:21] d2.utils.events INFO:  eta: 10:23:45  iter: 8159  total_loss: 3.244  loss_cls_stage0: 0.3315  loss_box_reg_stage0: 0.3065  loss_cls_stage1: 0.3595  loss_box_reg_stage1: 0.7585  loss_cls_stage2: 0.3223  loss_box_reg_stage2: 0.9717  loss_mask: 0.09002  loss_rpn_cls: 0.0288  loss_rpn_loc: 0.07379  time: 0.6281  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 13:22:33] d2.utils.events INFO:  eta: 10:23:25  iter: 8179  total_loss: 3.052  loss_cls_stage0: 0.3058  loss_box_reg_stage0: 0.3107  loss_cls_stage1: 0.3327  loss_box_reg_stage1: 0.7051  loss_cls_stage2: 0.299  loss_box_reg_stage2: 0.9204  loss_mask: 0.1  loss_rpn_cls: 0.02271  loss_rpn_loc: 0.05713  time: 0.6280  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 13:22:46] d2.utils.events INFO:  eta: 10:23:19  iter: 8199  total_loss: 3.004  loss_cls_stage0: 0.2826  loss_box_reg_stage0: 0.291  loss_cls_stage1: 0.2951  loss_box_reg_stage1: 0.7061  loss_cls_stage2: 0.2908  loss_box_reg_stage2: 0.9091  loss_mask: 0.0921  loss_rpn_cls: 0.0151  loss_rpn_loc: 0.04829  time: 0.6280  data_time: 0.0050  lr: 0.00016  max_mem: 19303M
[07/29 13:22:58] d2.utils.events INFO:  eta: 10:23:40  iter: 8219  total_loss: 3.13  loss_cls_stage0: 0.3006  loss_box_reg_stage0: 0.2877  loss_cls_stage1: 0.3259  loss_box_reg_stage1: 0.7308  loss_cls_stage2: 0.3017  loss_box_reg_stage2: 1.01  loss_mask: 0.09115  loss_rpn_cls: 0.01854  loss_rpn_loc: 0.06404  time: 0.6281  data_time: 0.0054  lr: 0.00016  max_mem: 19303M
[07/29 13:23:11] d2.utils.events INFO:  eta: 10:23:33  iter: 8239  total_loss: 3.09  loss_cls_stage0: 0.2917  loss_box_reg_stage0: 0.2696  loss_cls_stage1: 0.3201  loss_box_reg_stage1: 0.7129  loss_cls_stage2: 0.3018  loss_box_reg_stage2: 0.9834  loss_mask: 0.0836  loss_rpn_cls: 0.01687  loss_rpn_loc: 0.04523  time: 0.6282  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 13:23:24] d2.utils.events INFO:  eta: 10:23:32  iter: 8259  total_loss: 3.023  loss_cls_stage0: 0.3025  loss_box_reg_stage0: 0.2913  loss_cls_stage1: 0.3262  loss_box_reg_stage1: 0.6527  loss_cls_stage2: 0.3132  loss_box_reg_stage2: 0.8555  loss_mask: 0.08996  loss_rpn_cls: 0.01509  loss_rpn_loc: 0.04844  time: 0.6283  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 13:23:37] d2.utils.events INFO:  eta: 10:23:21  iter: 8279  total_loss: 3.331  loss_cls_stage0: 0.3341  loss_box_reg_stage0: 0.3171  loss_cls_stage1: 0.3328  loss_box_reg_stage1: 0.7877  loss_cls_stage2: 0.3186  loss_box_reg_stage2: 0.9953  loss_mask: 0.09881  loss_rpn_cls: 0.02257  loss_rpn_loc: 0.05187  time: 0.6284  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 13:23:50] d2.utils.events INFO:  eta: 10:23:34  iter: 8299  total_loss: 3.164  loss_cls_stage0: 0.2987  loss_box_reg_stage0: 0.2708  loss_cls_stage1: 0.3271  loss_box_reg_stage1: 0.7329  loss_cls_stage2: 0.3209  loss_box_reg_stage2: 0.9732  loss_mask: 0.09338  loss_rpn_cls: 0.01272  loss_rpn_loc: 0.04321  time: 0.6284  data_time: 0.0051  lr: 0.00016  max_mem: 19303M
[07/29 13:24:03] d2.utils.events INFO:  eta: 10:23:14  iter: 8319  total_loss: 3.613  loss_cls_stage0: 0.3552  loss_box_reg_stage0: 0.3219  loss_cls_stage1: 0.3912  loss_box_reg_stage1: 0.8221  loss_cls_stage2: 0.3957  loss_box_reg_stage2: 0.9584  loss_mask: 0.1214  loss_rpn_cls: 0.02096  loss_rpn_loc: 0.06887  time: 0.6285  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 13:24:15] d2.utils.events INFO:  eta: 10:22:43  iter: 8339  total_loss: 3.088  loss_cls_stage0: 0.2929  loss_box_reg_stage0: 0.295  loss_cls_stage1: 0.325  loss_box_reg_stage1: 0.7441  loss_cls_stage2: 0.3041  loss_box_reg_stage2: 0.9385  loss_mask: 0.08968  loss_rpn_cls: 0.02005  loss_rpn_loc: 0.05109  time: 0.6284  data_time: 0.0051  lr: 0.00016  max_mem: 19303M
[07/29 13:24:27] d2.utils.events INFO:  eta: 10:22:17  iter: 8359  total_loss: 2.998  loss_cls_stage0: 0.3225  loss_box_reg_stage0: 0.2914  loss_cls_stage1: 0.3353  loss_box_reg_stage1: 0.6514  loss_cls_stage2: 0.3125  loss_box_reg_stage2: 0.9314  loss_mask: 0.08554  loss_rpn_cls: 0.01972  loss_rpn_loc: 0.07859  time: 0.6283  data_time: 0.0050  lr: 0.00016  max_mem: 19303M
[07/29 13:24:40] d2.utils.events INFO:  eta: 10:22:49  iter: 8379  total_loss: 3.178  loss_cls_stage0: 0.3407  loss_box_reg_stage0: 0.3035  loss_cls_stage1: 0.3568  loss_box_reg_stage1: 0.7692  loss_cls_stage2: 0.3132  loss_box_reg_stage2: 0.972  loss_mask: 0.09569  loss_rpn_cls: 0.01867  loss_rpn_loc: 0.06657  time: 0.6284  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 13:24:52] d2.utils.events INFO:  eta: 10:22:01  iter: 8399  total_loss: 3.319  loss_cls_stage0: 0.3384  loss_box_reg_stage0: 0.3239  loss_cls_stage1: 0.3483  loss_box_reg_stage1: 0.7887  loss_cls_stage2: 0.3205  loss_box_reg_stage2: 0.9609  loss_mask: 0.09876  loss_rpn_cls: 0.02311  loss_rpn_loc: 0.04704  time: 0.6283  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 13:25:05] d2.utils.events INFO:  eta: 10:21:48  iter: 8419  total_loss: 3.159  loss_cls_stage0: 0.3103  loss_box_reg_stage0: 0.3221  loss_cls_stage1: 0.3223  loss_box_reg_stage1: 0.7464  loss_cls_stage2: 0.3024  loss_box_reg_stage2: 0.927  loss_mask: 0.09941  loss_rpn_cls: 0.02526  loss_rpn_loc: 0.05818  time: 0.6282  data_time: 0.0051  lr: 0.00016  max_mem: 19303M
[07/29 13:25:17] d2.utils.events INFO:  eta: 10:22:05  iter: 8439  total_loss: 3.167  loss_cls_stage0: 0.2786  loss_box_reg_stage0: 0.2992  loss_cls_stage1: 0.3059  loss_box_reg_stage1: 0.7501  loss_cls_stage2: 0.3003  loss_box_reg_stage2: 0.9585  loss_mask: 0.09505  loss_rpn_cls: 0.01762  loss_rpn_loc: 0.05178  time: 0.6281  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 13:25:29] d2.utils.events INFO:  eta: 10:21:42  iter: 8459  total_loss: 3.447  loss_cls_stage0: 0.3546  loss_box_reg_stage0: 0.327  loss_cls_stage1: 0.3681  loss_box_reg_stage1: 0.8587  loss_cls_stage2: 0.3343  loss_box_reg_stage2: 1.077  loss_mask: 0.09789  loss_rpn_cls: 0.02204  loss_rpn_loc: 0.05431  time: 0.6280  data_time: 0.0046  lr: 0.00016  max_mem: 19303M
[07/29 13:25:41] d2.utils.events INFO:  eta: 10:20:22  iter: 8479  total_loss: 3.247  loss_cls_stage0: 0.3181  loss_box_reg_stage0: 0.3181  loss_cls_stage1: 0.3468  loss_box_reg_stage1: 0.7574  loss_cls_stage2: 0.3324  loss_box_reg_stage2: 0.9812  loss_mask: 0.1042  loss_rpn_cls: 0.02136  loss_rpn_loc: 0.05345  time: 0.6278  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 13:25:53] d2.utils.events INFO:  eta: 10:19:28  iter: 8499  total_loss: 3.158  loss_cls_stage0: 0.3277  loss_box_reg_stage0: 0.3021  loss_cls_stage1: 0.3372  loss_box_reg_stage1: 0.7218  loss_cls_stage2: 0.3274  loss_box_reg_stage2: 0.9331  loss_mask: 0.1017  loss_rpn_cls: 0.01989  loss_rpn_loc: 0.04358  time: 0.6277  data_time: 0.0047  lr: 0.00016  max_mem: 19303M
[07/29 13:26:06] d2.utils.events INFO:  eta: 10:19:25  iter: 8519  total_loss: 3.345  loss_cls_stage0: 0.3032  loss_box_reg_stage0: 0.311  loss_cls_stage1: 0.3363  loss_box_reg_stage1: 0.7948  loss_cls_stage2: 0.3294  loss_box_reg_stage2: 1.034  loss_mask: 0.09563  loss_rpn_cls: 0.01121  loss_rpn_loc: 0.04256  time: 0.6278  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 13:26:19] d2.utils.events INFO:  eta: 10:19:25  iter: 8539  total_loss: 3.59  loss_cls_stage0: 0.3635  loss_box_reg_stage0: 0.3401  loss_cls_stage1: 0.4247  loss_box_reg_stage1: 0.7813  loss_cls_stage2: 0.3955  loss_box_reg_stage2: 1.021  loss_mask: 0.1009  loss_rpn_cls: 0.01345  loss_rpn_loc: 0.04803  time: 0.6278  data_time: 0.0050  lr: 0.00016  max_mem: 19303M
[07/29 13:26:31] d2.utils.events INFO:  eta: 10:19:31  iter: 8559  total_loss: 3.269  loss_cls_stage0: 0.3126  loss_box_reg_stage0: 0.2809  loss_cls_stage1: 0.3543  loss_box_reg_stage1: 0.7605  loss_cls_stage2: 0.3472  loss_box_reg_stage2: 1.014  loss_mask: 0.09103  loss_rpn_cls: 0.01961  loss_rpn_loc: 0.04752  time: 0.6278  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 13:26:44] d2.utils.events INFO:  eta: 10:19:19  iter: 8579  total_loss: 3.195  loss_cls_stage0: 0.3125  loss_box_reg_stage0: 0.2947  loss_cls_stage1: 0.3438  loss_box_reg_stage1: 0.7066  loss_cls_stage2: 0.3238  loss_box_reg_stage2: 0.918  loss_mask: 0.09945  loss_rpn_cls: 0.0211  loss_rpn_loc: 0.0623  time: 0.6278  data_time: 0.0047  lr: 0.00016  max_mem: 19303M
[07/29 13:26:56] d2.utils.events INFO:  eta: 10:19:52  iter: 8599  total_loss: 3.1  loss_cls_stage0: 0.3105  loss_box_reg_stage0: 0.2863  loss_cls_stage1: 0.3438  loss_box_reg_stage1: 0.7194  loss_cls_stage2: 0.3047  loss_box_reg_stage2: 0.9227  loss_mask: 0.09112  loss_rpn_cls: 0.01766  loss_rpn_loc: 0.04523  time: 0.6278  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 13:27:09] d2.utils.events INFO:  eta: 10:19:42  iter: 8619  total_loss: 3.06  loss_cls_stage0: 0.3315  loss_box_reg_stage0: 0.3102  loss_cls_stage1: 0.3265  loss_box_reg_stage1: 0.7309  loss_cls_stage2: 0.2818  loss_box_reg_stage2: 0.9011  loss_mask: 0.09425  loss_rpn_cls: 0.01406  loss_rpn_loc: 0.05016  time: 0.6278  data_time: 0.0052  lr: 0.00016  max_mem: 19303M
[07/29 13:27:22] d2.utils.events INFO:  eta: 10:20:06  iter: 8639  total_loss: 3.299  loss_cls_stage0: 0.3297  loss_box_reg_stage0: 0.3169  loss_cls_stage1: 0.3419  loss_box_reg_stage1: 0.7818  loss_cls_stage2: 0.3177  loss_box_reg_stage2: 0.9941  loss_mask: 0.1071  loss_rpn_cls: 0.01461  loss_rpn_loc: 0.08032  time: 0.6279  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 13:27:34] d2.utils.events INFO:  eta: 10:18:27  iter: 8659  total_loss: 3.221  loss_cls_stage0: 0.322  loss_box_reg_stage0: 0.3157  loss_cls_stage1: 0.3444  loss_box_reg_stage1: 0.7505  loss_cls_stage2: 0.3333  loss_box_reg_stage2: 0.9725  loss_mask: 0.1043  loss_rpn_cls: 0.02271  loss_rpn_loc: 0.0533  time: 0.6277  data_time: 0.0050  lr: 0.00016  max_mem: 19303M
[07/29 13:27:47] d2.utils.events INFO:  eta: 10:18:11  iter: 8679  total_loss: 3.164  loss_cls_stage0: 0.3045  loss_box_reg_stage0: 0.2857  loss_cls_stage1: 0.3604  loss_box_reg_stage1: 0.773  loss_cls_stage2: 0.3488  loss_box_reg_stage2: 1.007  loss_mask: 0.08958  loss_rpn_cls: 0.02063  loss_rpn_loc: 0.04364  time: 0.6278  data_time: 0.0046  lr: 0.00016  max_mem: 19303M
[07/29 13:27:59] d2.utils.events INFO:  eta: 10:18:20  iter: 8699  total_loss: 3.455  loss_cls_stage0: 0.3057  loss_box_reg_stage0: 0.3143  loss_cls_stage1: 0.3841  loss_box_reg_stage1: 0.7998  loss_cls_stage2: 0.3867  loss_box_reg_stage2: 1.062  loss_mask: 0.09861  loss_rpn_cls: 0.01796  loss_rpn_loc: 0.05427  time: 0.6278  data_time: 0.0050  lr: 0.00016  max_mem: 19303M
[07/29 13:28:12] d2.utils.events INFO:  eta: 10:17:46  iter: 8719  total_loss: 2.917  loss_cls_stage0: 0.2453  loss_box_reg_stage0: 0.2689  loss_cls_stage1: 0.2605  loss_box_reg_stage1: 0.6752  loss_cls_stage2: 0.2595  loss_box_reg_stage2: 0.9334  loss_mask: 0.08667  loss_rpn_cls: 0.01604  loss_rpn_loc: 0.04486  time: 0.6277  data_time: 0.0045  lr: 0.00016  max_mem: 19303M
[07/29 13:28:24] d2.utils.events INFO:  eta: 10:17:33  iter: 8739  total_loss: 3.285  loss_cls_stage0: 0.3318  loss_box_reg_stage0: 0.3128  loss_cls_stage1: 0.3533  loss_box_reg_stage1: 0.7819  loss_cls_stage2: 0.3438  loss_box_reg_stage2: 0.9494  loss_mask: 0.08915  loss_rpn_cls: 0.02039  loss_rpn_loc: 0.05955  time: 0.6276  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 13:28:36] d2.utils.events INFO:  eta: 10:18:13  iter: 8759  total_loss: 3.093  loss_cls_stage0: 0.3  loss_box_reg_stage0: 0.3048  loss_cls_stage1: 0.3199  loss_box_reg_stage1: 0.749  loss_cls_stage2: 0.3073  loss_box_reg_stage2: 0.9964  loss_mask: 0.09963  loss_rpn_cls: 0.01665  loss_rpn_loc: 0.04434  time: 0.6276  data_time: 0.0046  lr: 0.00016  max_mem: 19303M
[07/29 13:28:49] d2.utils.events INFO:  eta: 10:18:02  iter: 8779  total_loss: 2.928  loss_cls_stage0: 0.2644  loss_box_reg_stage0: 0.2871  loss_cls_stage1: 0.2786  loss_box_reg_stage1: 0.6538  loss_cls_stage2: 0.2831  loss_box_reg_stage2: 0.887  loss_mask: 0.09063  loss_rpn_cls: 0.01776  loss_rpn_loc: 0.04909  time: 0.6276  data_time: 0.0046  lr: 0.00016  max_mem: 19303M
[07/29 13:29:01] d2.utils.events INFO:  eta: 10:18:03  iter: 8799  total_loss: 3.263  loss_cls_stage0: 0.2973  loss_box_reg_stage0: 0.3004  loss_cls_stage1: 0.3289  loss_box_reg_stage1: 0.7619  loss_cls_stage2: 0.3044  loss_box_reg_stage2: 1.008  loss_mask: 0.1011  loss_rpn_cls: 0.01862  loss_rpn_loc: 0.05042  time: 0.6275  data_time: 0.0046  lr: 0.00016  max_mem: 19303M
[07/29 13:29:13] d2.utils.events INFO:  eta: 10:18:18  iter: 8819  total_loss: 3.538  loss_cls_stage0: 0.3517  loss_box_reg_stage0: 0.3272  loss_cls_stage1: 0.3554  loss_box_reg_stage1: 0.8082  loss_cls_stage2: 0.3387  loss_box_reg_stage2: 1.022  loss_mask: 0.1102  loss_rpn_cls: 0.02089  loss_rpn_loc: 0.05408  time: 0.6275  data_time: 0.0047  lr: 0.00016  max_mem: 19303M
[07/29 13:29:26] d2.utils.events INFO:  eta: 10:17:59  iter: 8839  total_loss: 3.471  loss_cls_stage0: 0.3382  loss_box_reg_stage0: 0.3286  loss_cls_stage1: 0.3593  loss_box_reg_stage1: 0.8165  loss_cls_stage2: 0.325  loss_box_reg_stage2: 1.05  loss_mask: 0.09864  loss_rpn_cls: 0.01958  loss_rpn_loc: 0.06705  time: 0.6275  data_time: 0.0047  lr: 0.00016  max_mem: 19303M
[07/29 13:29:39] d2.utils.events INFO:  eta: 10:19:07  iter: 8859  total_loss: 3.329  loss_cls_stage0: 0.3038  loss_box_reg_stage0: 0.3042  loss_cls_stage1: 0.3409  loss_box_reg_stage1: 0.8221  loss_cls_stage2: 0.3112  loss_box_reg_stage2: 1.036  loss_mask: 0.09426  loss_rpn_cls: 0.01379  loss_rpn_loc: 0.05228  time: 0.6275  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 13:29:52] d2.utils.events INFO:  eta: 10:19:59  iter: 8879  total_loss: 3.388  loss_cls_stage0: 0.3321  loss_box_reg_stage0: 0.3067  loss_cls_stage1: 0.3764  loss_box_reg_stage1: 0.7756  loss_cls_stage2: 0.3355  loss_box_reg_stage2: 0.9933  loss_mask: 0.09603  loss_rpn_cls: 0.01349  loss_rpn_loc: 0.04217  time: 0.6277  data_time: 0.0052  lr: 0.00016  max_mem: 19303M
[07/29 13:30:04] d2.utils.events INFO:  eta: 10:19:20  iter: 8899  total_loss: 3.367  loss_cls_stage0: 0.2675  loss_box_reg_stage0: 0.2745  loss_cls_stage1: 0.3579  loss_box_reg_stage1: 0.7678  loss_cls_stage2: 0.3636  loss_box_reg_stage2: 1.079  loss_mask: 0.08764  loss_rpn_cls: 0.0182  loss_rpn_loc: 0.05541  time: 0.6276  data_time: 0.0045  lr: 0.00016  max_mem: 19303M
[07/29 13:30:16] d2.utils.events INFO:  eta: 10:19:12  iter: 8919  total_loss: 3.617  loss_cls_stage0: 0.3324  loss_box_reg_stage0: 0.3081  loss_cls_stage1: 0.3742  loss_box_reg_stage1: 0.7859  loss_cls_stage2: 0.3461  loss_box_reg_stage2: 1.051  loss_mask: 0.1081  loss_rpn_cls: 0.02217  loss_rpn_loc: 0.05078  time: 0.6275  data_time: 0.0047  lr: 0.00016  max_mem: 19303M
[07/29 13:30:29] d2.utils.events INFO:  eta: 10:18:41  iter: 8939  total_loss: 2.946  loss_cls_stage0: 0.3064  loss_box_reg_stage0: 0.2961  loss_cls_stage1: 0.3117  loss_box_reg_stage1: 0.6861  loss_cls_stage2: 0.3106  loss_box_reg_stage2: 0.9333  loss_mask: 0.09491  loss_rpn_cls: 0.01867  loss_rpn_loc: 0.04043  time: 0.6275  data_time: 0.0050  lr: 0.00016  max_mem: 19303M
[07/29 13:30:41] d2.utils.events INFO:  eta: 10:18:37  iter: 8959  total_loss: 3.413  loss_cls_stage0: 0.3172  loss_box_reg_stage0: 0.3114  loss_cls_stage1: 0.3606  loss_box_reg_stage1: 0.8006  loss_cls_stage2: 0.3475  loss_box_reg_stage2: 1.053  loss_mask: 0.09556  loss_rpn_cls: 0.01475  loss_rpn_loc: 0.04811  time: 0.6274  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 13:30:53] d2.utils.events INFO:  eta: 10:18:09  iter: 8979  total_loss: 3.393  loss_cls_stage0: 0.3237  loss_box_reg_stage0: 0.2944  loss_cls_stage1: 0.3527  loss_box_reg_stage1: 0.8076  loss_cls_stage2: 0.354  loss_box_reg_stage2: 1.014  loss_mask: 0.09839  loss_rpn_cls: 0.02606  loss_rpn_loc: 0.05669  time: 0.6273  data_time: 0.0053  lr: 0.00016  max_mem: 19303M
[07/29 13:31:06] d2.utils.events INFO:  eta: 10:17:48  iter: 8999  total_loss: 3.578  loss_cls_stage0: 0.3204  loss_box_reg_stage0: 0.3051  loss_cls_stage1: 0.3593  loss_box_reg_stage1: 0.8265  loss_cls_stage2: 0.3418  loss_box_reg_stage2: 1.077  loss_mask: 0.09054  loss_rpn_cls: 0.01943  loss_rpn_loc: 0.05957  time: 0.6272  data_time: 0.0050  lr: 0.00016  max_mem: 19303M
[07/29 13:31:18] d2.utils.events INFO:  eta: 10:16:47  iter: 9019  total_loss: 3.15  loss_cls_stage0: 0.3173  loss_box_reg_stage0: 0.2955  loss_cls_stage1: 0.3505  loss_box_reg_stage1: 0.6995  loss_cls_stage2: 0.3278  loss_box_reg_stage2: 0.8801  loss_mask: 0.09262  loss_rpn_cls: 0.02154  loss_rpn_loc: 0.05038  time: 0.6271  data_time: 0.0046  lr: 0.00016  max_mem: 19303M
[07/29 13:31:30] d2.utils.events INFO:  eta: 10:15:53  iter: 9039  total_loss: 3.01  loss_cls_stage0: 0.2856  loss_box_reg_stage0: 0.2796  loss_cls_stage1: 0.2794  loss_box_reg_stage1: 0.7049  loss_cls_stage2: 0.2846  loss_box_reg_stage2: 0.9567  loss_mask: 0.08607  loss_rpn_cls: 0.02032  loss_rpn_loc: 0.04436  time: 0.6271  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 13:31:42] d2.utils.events INFO:  eta: 10:15:05  iter: 9059  total_loss: 3.302  loss_cls_stage0: 0.3409  loss_box_reg_stage0: 0.2924  loss_cls_stage1: 0.3786  loss_box_reg_stage1: 0.7323  loss_cls_stage2: 0.3305  loss_box_reg_stage2: 1.002  loss_mask: 0.109  loss_rpn_cls: 0.02284  loss_rpn_loc: 0.06608  time: 0.6269  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 13:31:54] d2.utils.events INFO:  eta: 10:14:51  iter: 9079  total_loss: 3.09  loss_cls_stage0: 0.308  loss_box_reg_stage0: 0.2996  loss_cls_stage1: 0.3041  loss_box_reg_stage1: 0.7081  loss_cls_stage2: 0.2882  loss_box_reg_stage2: 0.9124  loss_mask: 0.08902  loss_rpn_cls: 0.01925  loss_rpn_loc: 0.04862  time: 0.6268  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 13:32:07] d2.utils.events INFO:  eta: 10:14:21  iter: 9099  total_loss: 2.893  loss_cls_stage0: 0.2728  loss_box_reg_stage0: 0.2754  loss_cls_stage1: 0.311  loss_box_reg_stage1: 0.6221  loss_cls_stage2: 0.3076  loss_box_reg_stage2: 0.8795  loss_mask: 0.08232  loss_rpn_cls: 0.0185  loss_rpn_loc: 0.05092  time: 0.6268  data_time: 0.0046  lr: 0.00016  max_mem: 19303M
[07/29 13:32:19] d2.utils.events INFO:  eta: 10:13:24  iter: 9119  total_loss: 3.206  loss_cls_stage0: 0.3265  loss_box_reg_stage0: 0.3173  loss_cls_stage1: 0.3328  loss_box_reg_stage1: 0.7424  loss_cls_stage2: 0.3133  loss_box_reg_stage2: 0.9694  loss_mask: 0.1001  loss_rpn_cls: 0.01809  loss_rpn_loc: 0.05899  time: 0.6268  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 13:32:31] d2.utils.events INFO:  eta: 10:13:11  iter: 9139  total_loss: 3.107  loss_cls_stage0: 0.3037  loss_box_reg_stage0: 0.2914  loss_cls_stage1: 0.3158  loss_box_reg_stage1: 0.6615  loss_cls_stage2: 0.3034  loss_box_reg_stage2: 0.9138  loss_mask: 0.09753  loss_rpn_cls: 0.02375  loss_rpn_loc: 0.04608  time: 0.6267  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 13:32:44] d2.utils.events INFO:  eta: 10:12:39  iter: 9159  total_loss: 3.257  loss_cls_stage0: 0.3301  loss_box_reg_stage0: 0.3261  loss_cls_stage1: 0.3462  loss_box_reg_stage1: 0.7388  loss_cls_stage2: 0.3377  loss_box_reg_stage2: 0.95  loss_mask: 0.09439  loss_rpn_cls: 0.01855  loss_rpn_loc: 0.05134  time: 0.6267  data_time: 0.0047  lr: 0.00016  max_mem: 19303M
[07/29 13:32:56] d2.utils.events INFO:  eta: 10:12:26  iter: 9179  total_loss: 3.251  loss_cls_stage0: 0.3452  loss_box_reg_stage0: 0.3377  loss_cls_stage1: 0.3263  loss_box_reg_stage1: 0.8005  loss_cls_stage2: 0.3115  loss_box_reg_stage2: 0.9512  loss_mask: 0.1138  loss_rpn_cls: 0.0214  loss_rpn_loc: 0.05709  time: 0.6265  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 13:33:08] d2.utils.events INFO:  eta: 10:11:26  iter: 9199  total_loss: 3.15  loss_cls_stage0: 0.2928  loss_box_reg_stage0: 0.3149  loss_cls_stage1: 0.3171  loss_box_reg_stage1: 0.7464  loss_cls_stage2: 0.318  loss_box_reg_stage2: 0.9332  loss_mask: 0.09478  loss_rpn_cls: 0.02567  loss_rpn_loc: 0.0817  time: 0.6265  data_time: 0.0051  lr: 0.00016  max_mem: 19303M
[07/29 13:33:21] d2.utils.events INFO:  eta: 10:10:53  iter: 9219  total_loss: 3.195  loss_cls_stage0: 0.3035  loss_box_reg_stage0: 0.3079  loss_cls_stage1: 0.3103  loss_box_reg_stage1: 0.7358  loss_cls_stage2: 0.3058  loss_box_reg_stage2: 0.9709  loss_mask: 0.1003  loss_rpn_cls: 0.01578  loss_rpn_loc: 0.05657  time: 0.6266  data_time: 0.0051  lr: 0.00016  max_mem: 19303M
[07/29 13:33:34] d2.utils.events INFO:  eta: 10:10:38  iter: 9239  total_loss: 3.199  loss_cls_stage0: 0.2826  loss_box_reg_stage0: 0.292  loss_cls_stage1: 0.3007  loss_box_reg_stage1: 0.7756  loss_cls_stage2: 0.3279  loss_box_reg_stage2: 1.099  loss_mask: 0.0844  loss_rpn_cls: 0.01364  loss_rpn_loc: 0.04464  time: 0.6266  data_time: 0.0047  lr: 0.00016  max_mem: 19303M
[07/29 13:33:47] d2.utils.events INFO:  eta: 10:10:26  iter: 9259  total_loss: 3.077  loss_cls_stage0: 0.3002  loss_box_reg_stage0: 0.2934  loss_cls_stage1: 0.3218  loss_box_reg_stage1: 0.703  loss_cls_stage2: 0.3106  loss_box_reg_stage2: 0.9317  loss_mask: 0.09002  loss_rpn_cls: 0.0135  loss_rpn_loc: 0.04508  time: 0.6267  data_time: 0.0051  lr: 0.00016  max_mem: 19303M
[07/29 13:33:59] d2.utils.events INFO:  eta: 10:10:11  iter: 9279  total_loss: 2.995  loss_cls_stage0: 0.2631  loss_box_reg_stage0: 0.2703  loss_cls_stage1: 0.2972  loss_box_reg_stage1: 0.703  loss_cls_stage2: 0.2851  loss_box_reg_stage2: 0.9192  loss_mask: 0.07616  loss_rpn_cls: 0.01894  loss_rpn_loc: 0.04808  time: 0.6267  data_time: 0.0050  lr: 0.00016  max_mem: 19303M
[07/29 13:34:12] d2.utils.events INFO:  eta: 10:10:00  iter: 9299  total_loss: 2.668  loss_cls_stage0: 0.2872  loss_box_reg_stage0: 0.2697  loss_cls_stage1: 0.2842  loss_box_reg_stage1: 0.6077  loss_cls_stage2: 0.2692  loss_box_reg_stage2: 0.7616  loss_mask: 0.09091  loss_rpn_cls: 0.02716  loss_rpn_loc: 0.04926  time: 0.6268  data_time: 0.0051  lr: 0.00016  max_mem: 19303M
[07/29 13:34:25] d2.utils.events INFO:  eta: 10:09:41  iter: 9319  total_loss: 2.852  loss_cls_stage0: 0.3101  loss_box_reg_stage0: 0.2836  loss_cls_stage1: 0.3364  loss_box_reg_stage1: 0.6751  loss_cls_stage2: 0.3211  loss_box_reg_stage2: 0.8245  loss_mask: 0.08845  loss_rpn_cls: 0.0241  loss_rpn_loc: 0.04532  time: 0.6268  data_time: 0.0050  lr: 0.00016  max_mem: 19303M
[07/29 13:34:37] d2.utils.events INFO:  eta: 10:09:28  iter: 9339  total_loss: 3.095  loss_cls_stage0: 0.2852  loss_box_reg_stage0: 0.2987  loss_cls_stage1: 0.3067  loss_box_reg_stage1: 0.7017  loss_cls_stage2: 0.3027  loss_box_reg_stage2: 0.9087  loss_mask: 0.0963  loss_rpn_cls: 0.02675  loss_rpn_loc: 0.05572  time: 0.6267  data_time: 0.0050  lr: 0.00016  max_mem: 19303M
[07/29 13:34:49] d2.utils.events INFO:  eta: 10:09:22  iter: 9359  total_loss: 3.1  loss_cls_stage0: 0.2921  loss_box_reg_stage0: 0.3122  loss_cls_stage1: 0.3018  loss_box_reg_stage1: 0.7742  loss_cls_stage2: 0.2872  loss_box_reg_stage2: 0.9641  loss_mask: 0.09847  loss_rpn_cls: 0.02158  loss_rpn_loc: 0.05169  time: 0.6267  data_time: 0.0053  lr: 0.00016  max_mem: 19303M
[07/29 13:35:02] d2.utils.events INFO:  eta: 10:08:39  iter: 9379  total_loss: 2.94  loss_cls_stage0: 0.2726  loss_box_reg_stage0: 0.2714  loss_cls_stage1: 0.2676  loss_box_reg_stage1: 0.7367  loss_cls_stage2: 0.2729  loss_box_reg_stage2: 0.993  loss_mask: 0.08024  loss_rpn_cls: 0.01923  loss_rpn_loc: 0.0452  time: 0.6266  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 13:35:14] d2.utils.events INFO:  eta: 10:08:35  iter: 9399  total_loss: 3.05  loss_cls_stage0: 0.2964  loss_box_reg_stage0: 0.3008  loss_cls_stage1: 0.3078  loss_box_reg_stage1: 0.7351  loss_cls_stage2: 0.2882  loss_box_reg_stage2: 0.931  loss_mask: 0.09555  loss_rpn_cls: 0.01557  loss_rpn_loc: 0.04374  time: 0.6266  data_time: 0.0047  lr: 0.00016  max_mem: 19303M
[07/29 13:35:26] d2.utils.events INFO:  eta: 10:08:30  iter: 9419  total_loss: 2.888  loss_cls_stage0: 0.2988  loss_box_reg_stage0: 0.2902  loss_cls_stage1: 0.3297  loss_box_reg_stage1: 0.6469  loss_cls_stage2: 0.3115  loss_box_reg_stage2: 0.8226  loss_mask: 0.1004  loss_rpn_cls: 0.02776  loss_rpn_loc: 0.04597  time: 0.6265  data_time: 0.0051  lr: 0.00016  max_mem: 19303M
[07/29 13:35:39] d2.utils.events INFO:  eta: 10:08:17  iter: 9439  total_loss: 3.234  loss_cls_stage0: 0.3098  loss_box_reg_stage0: 0.3203  loss_cls_stage1: 0.3495  loss_box_reg_stage1: 0.7446  loss_cls_stage2: 0.3281  loss_box_reg_stage2: 0.9865  loss_mask: 0.1002  loss_rpn_cls: 0.01857  loss_rpn_loc: 0.0555  time: 0.6266  data_time: 0.0051  lr: 0.00016  max_mem: 19303M
[07/29 13:35:52] d2.utils.events INFO:  eta: 10:08:17  iter: 9459  total_loss: 3.205  loss_cls_stage0: 0.2926  loss_box_reg_stage0: 0.2763  loss_cls_stage1: 0.3271  loss_box_reg_stage1: 0.7491  loss_cls_stage2: 0.3172  loss_box_reg_stage2: 0.9886  loss_mask: 0.09111  loss_rpn_cls: 0.01374  loss_rpn_loc: 0.04141  time: 0.6266  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 13:36:04] d2.utils.events INFO:  eta: 10:08:05  iter: 9479  total_loss: 3.154  loss_cls_stage0: 0.2745  loss_box_reg_stage0: 0.2928  loss_cls_stage1: 0.32  loss_box_reg_stage1: 0.7494  loss_cls_stage2: 0.3264  loss_box_reg_stage2: 0.9701  loss_mask: 0.09586  loss_rpn_cls: 0.02864  loss_rpn_loc: 0.06681  time: 0.6265  data_time: 0.0050  lr: 0.00016  max_mem: 19303M
[07/29 13:36:17] d2.utils.events INFO:  eta: 10:07:54  iter: 9499  total_loss: 2.965  loss_cls_stage0: 0.2721  loss_box_reg_stage0: 0.2971  loss_cls_stage1: 0.2774  loss_box_reg_stage1: 0.6996  loss_cls_stage2: 0.2816  loss_box_reg_stage2: 0.9299  loss_mask: 0.08509  loss_rpn_cls: 0.02319  loss_rpn_loc: 0.05085  time: 0.6265  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 13:36:29] d2.utils.events INFO:  eta: 10:07:40  iter: 9519  total_loss: 3.157  loss_cls_stage0: 0.2893  loss_box_reg_stage0: 0.2952  loss_cls_stage1: 0.3251  loss_box_reg_stage1: 0.7391  loss_cls_stage2: 0.3197  loss_box_reg_stage2: 0.9683  loss_mask: 0.09075  loss_rpn_cls: 0.01815  loss_rpn_loc: 0.0482  time: 0.6265  data_time: 0.0050  lr: 0.00016  max_mem: 19303M
[07/29 13:36:42] d2.utils.events INFO:  eta: 10:07:15  iter: 9539  total_loss: 3.078  loss_cls_stage0: 0.2906  loss_box_reg_stage0: 0.2826  loss_cls_stage1: 0.3226  loss_box_reg_stage1: 0.6933  loss_cls_stage2: 0.3299  loss_box_reg_stage2: 0.9611  loss_mask: 0.09351  loss_rpn_cls: 0.01526  loss_rpn_loc: 0.04623  time: 0.6265  data_time: 0.0054  lr: 0.00016  max_mem: 19303M
[07/29 13:36:54] d2.utils.events INFO:  eta: 10:07:02  iter: 9559  total_loss: 3.703  loss_cls_stage0: 0.3478  loss_box_reg_stage0: 0.3349  loss_cls_stage1: 0.3904  loss_box_reg_stage1: 0.8271  loss_cls_stage2: 0.3773  loss_box_reg_stage2: 1.078  loss_mask: 0.1002  loss_rpn_cls: 0.01942  loss_rpn_loc: 0.04547  time: 0.6265  data_time: 0.0050  lr: 0.00016  max_mem: 19303M
[07/29 13:37:07] d2.utils.events INFO:  eta: 10:07:00  iter: 9579  total_loss: 3.195  loss_cls_stage0: 0.3206  loss_box_reg_stage0: 0.304  loss_cls_stage1: 0.3151  loss_box_reg_stage1: 0.7576  loss_cls_stage2: 0.3039  loss_box_reg_stage2: 0.9567  loss_mask: 0.1088  loss_rpn_cls: 0.01806  loss_rpn_loc: 0.04974  time: 0.6265  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 13:37:19] d2.utils.events INFO:  eta: 10:06:47  iter: 9599  total_loss: 2.412  loss_cls_stage0: 0.2319  loss_box_reg_stage0: 0.2523  loss_cls_stage1: 0.24  loss_box_reg_stage1: 0.5662  loss_cls_stage2: 0.237  loss_box_reg_stage2: 0.7365  loss_mask: 0.07563  loss_rpn_cls: 0.01737  loss_rpn_loc: 0.03911  time: 0.6265  data_time: 0.0045  lr: 0.00016  max_mem: 19303M
[07/29 13:37:32] d2.utils.events INFO:  eta: 10:06:40  iter: 9619  total_loss: 2.995  loss_cls_stage0: 0.3037  loss_box_reg_stage0: 0.2812  loss_cls_stage1: 0.3344  loss_box_reg_stage1: 0.6803  loss_cls_stage2: 0.3401  loss_box_reg_stage2: 0.9218  loss_mask: 0.08691  loss_rpn_cls: 0.01904  loss_rpn_loc: 0.05219  time: 0.6265  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 13:37:45] d2.utils.events INFO:  eta: 10:06:23  iter: 9639  total_loss: 3.158  loss_cls_stage0: 0.3007  loss_box_reg_stage0: 0.2927  loss_cls_stage1: 0.3347  loss_box_reg_stage1: 0.6953  loss_cls_stage2: 0.3029  loss_box_reg_stage2: 0.8825  loss_mask: 0.08576  loss_rpn_cls: 0.02269  loss_rpn_loc: 0.0533  time: 0.6265  data_time: 0.0050  lr: 0.00016  max_mem: 19303M
[07/29 13:37:57] d2.utils.events INFO:  eta: 10:06:21  iter: 9659  total_loss: 3.303  loss_cls_stage0: 0.3309  loss_box_reg_stage0: 0.3053  loss_cls_stage1: 0.3339  loss_box_reg_stage1: 0.8028  loss_cls_stage2: 0.3171  loss_box_reg_stage2: 1.053  loss_mask: 0.09402  loss_rpn_cls: 0.01657  loss_rpn_loc: 0.04474  time: 0.6265  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 13:38:09] d2.utils.events INFO:  eta: 10:05:59  iter: 9679  total_loss: 3.141  loss_cls_stage0: 0.309  loss_box_reg_stage0: 0.2848  loss_cls_stage1: 0.3219  loss_box_reg_stage1: 0.746  loss_cls_stage2: 0.2981  loss_box_reg_stage2: 0.9472  loss_mask: 0.08057  loss_rpn_cls: 0.01687  loss_rpn_loc: 0.04611  time: 0.6265  data_time: 0.0047  lr: 0.00016  max_mem: 19303M
[07/29 13:38:22] d2.utils.events INFO:  eta: 10:05:44  iter: 9699  total_loss: 3.288  loss_cls_stage0: 0.2981  loss_box_reg_stage0: 0.2957  loss_cls_stage1: 0.3669  loss_box_reg_stage1: 0.7551  loss_cls_stage2: 0.3383  loss_box_reg_stage2: 1.003  loss_mask: 0.0915  loss_rpn_cls: 0.02161  loss_rpn_loc: 0.05742  time: 0.6264  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 13:38:34] d2.utils.events INFO:  eta: 10:05:32  iter: 9719  total_loss: 3.281  loss_cls_stage0: 0.2994  loss_box_reg_stage0: 0.2859  loss_cls_stage1: 0.3379  loss_box_reg_stage1: 0.7874  loss_cls_stage2: 0.3196  loss_box_reg_stage2: 1.004  loss_mask: 0.09595  loss_rpn_cls: 0.01743  loss_rpn_loc: 0.05479  time: 0.6264  data_time: 0.0051  lr: 0.00016  max_mem: 19303M
[07/29 13:38:46] d2.utils.events INFO:  eta: 10:05:09  iter: 9739  total_loss: 3.431  loss_cls_stage0: 0.3278  loss_box_reg_stage0: 0.3014  loss_cls_stage1: 0.3387  loss_box_reg_stage1: 0.7532  loss_cls_stage2: 0.3394  loss_box_reg_stage2: 1.074  loss_mask: 0.09328  loss_rpn_cls: 0.02517  loss_rpn_loc: 0.06787  time: 0.6264  data_time: 0.0050  lr: 0.00016  max_mem: 19303M
[07/29 13:38:59] d2.utils.events INFO:  eta: 10:05:02  iter: 9759  total_loss: 3.079  loss_cls_stage0: 0.2813  loss_box_reg_stage0: 0.2955  loss_cls_stage1: 0.2936  loss_box_reg_stage1: 0.7133  loss_cls_stage2: 0.2956  loss_box_reg_stage2: 0.9721  loss_mask: 0.08378  loss_rpn_cls: 0.02474  loss_rpn_loc: 0.04402  time: 0.6264  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 13:39:12] d2.utils.events INFO:  eta: 10:04:54  iter: 9779  total_loss: 2.988  loss_cls_stage0: 0.2986  loss_box_reg_stage0: 0.2704  loss_cls_stage1: 0.3221  loss_box_reg_stage1: 0.7157  loss_cls_stage2: 0.3046  loss_box_reg_stage2: 0.9304  loss_mask: 0.09685  loss_rpn_cls: 0.02618  loss_rpn_loc: 0.05061  time: 0.6265  data_time: 0.0047  lr: 0.00016  max_mem: 19303M
[07/29 13:39:25] d2.utils.events INFO:  eta: 10:05:23  iter: 9799  total_loss: 2.944  loss_cls_stage0: 0.2791  loss_box_reg_stage0: 0.2861  loss_cls_stage1: 0.2868  loss_box_reg_stage1: 0.6896  loss_cls_stage2: 0.2825  loss_box_reg_stage2: 0.89  loss_mask: 0.09118  loss_rpn_cls: 0.02066  loss_rpn_loc: 0.05494  time: 0.6266  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 13:39:37] d2.utils.events INFO:  eta: 10:04:51  iter: 9819  total_loss: 2.97  loss_cls_stage0: 0.3103  loss_box_reg_stage0: 0.2975  loss_cls_stage1: 0.3035  loss_box_reg_stage1: 0.6976  loss_cls_stage2: 0.2878  loss_box_reg_stage2: 0.8798  loss_mask: 0.0892  loss_rpn_cls: 0.01595  loss_rpn_loc: 0.05097  time: 0.6265  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 13:39:50] d2.utils.events INFO:  eta: 10:04:52  iter: 9839  total_loss: 3.42  loss_cls_stage0: 0.3217  loss_box_reg_stage0: 0.304  loss_cls_stage1: 0.354  loss_box_reg_stage1: 0.812  loss_cls_stage2: 0.3225  loss_box_reg_stage2: 1.142  loss_mask: 0.08483  loss_rpn_cls: 0.02196  loss_rpn_loc: 0.05772  time: 0.6265  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 13:40:02] d2.utils.events INFO:  eta: 10:04:04  iter: 9859  total_loss: 3.289  loss_cls_stage0: 0.3491  loss_box_reg_stage0: 0.3104  loss_cls_stage1: 0.3734  loss_box_reg_stage1: 0.7186  loss_cls_stage2: 0.3612  loss_box_reg_stage2: 0.969  loss_mask: 0.1016  loss_rpn_cls: 0.0236  loss_rpn_loc: 0.05341  time: 0.6265  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 13:40:15] d2.utils.events INFO:  eta: 10:03:42  iter: 9879  total_loss: 3.513  loss_cls_stage0: 0.34  loss_box_reg_stage0: 0.3003  loss_cls_stage1: 0.374  loss_box_reg_stage1: 0.8428  loss_cls_stage2: 0.3437  loss_box_reg_stage2: 1.077  loss_mask: 0.08892  loss_rpn_cls: 0.01754  loss_rpn_loc: 0.04924  time: 0.6265  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 13:40:27] d2.utils.events INFO:  eta: 10:03:05  iter: 9899  total_loss: 3.09  loss_cls_stage0: 0.2973  loss_box_reg_stage0: 0.3024  loss_cls_stage1: 0.325  loss_box_reg_stage1: 0.7023  loss_cls_stage2: 0.3079  loss_box_reg_stage2: 0.8866  loss_mask: 0.09851  loss_rpn_cls: 0.01881  loss_rpn_loc: 0.05009  time: 0.6264  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 13:40:39] d2.utils.events INFO:  eta: 10:03:03  iter: 9919  total_loss: 3.317  loss_cls_stage0: 0.3044  loss_box_reg_stage0: 0.2902  loss_cls_stage1: 0.3465  loss_box_reg_stage1: 0.8108  loss_cls_stage2: 0.3515  loss_box_reg_stage2: 1.066  loss_mask: 0.08575  loss_rpn_cls: 0.0127  loss_rpn_loc: 0.04839  time: 0.6264  data_time: 0.0051  lr: 0.00016  max_mem: 19303M
[07/29 13:40:52] d2.utils.events INFO:  eta: 10:03:05  iter: 9939  total_loss: 3.139  loss_cls_stage0: 0.2891  loss_box_reg_stage0: 0.3145  loss_cls_stage1: 0.3115  loss_box_reg_stage1: 0.7291  loss_cls_stage2: 0.305  loss_box_reg_stage2: 0.9488  loss_mask: 0.08904  loss_rpn_cls: 0.03762  loss_rpn_loc: 0.06267  time: 0.6264  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 13:41:05] d2.utils.events INFO:  eta: 10:02:59  iter: 9959  total_loss: 3.145  loss_cls_stage0: 0.2592  loss_box_reg_stage0: 0.2778  loss_cls_stage1: 0.2953  loss_box_reg_stage1: 0.7191  loss_cls_stage2: 0.2918  loss_box_reg_stage2: 0.9689  loss_mask: 0.09095  loss_rpn_cls: 0.01545  loss_rpn_loc: 0.04779  time: 0.6264  data_time: 0.0050  lr: 0.00016  max_mem: 19303M
[07/29 13:41:17] d2.utils.events INFO:  eta: 10:02:48  iter: 9979  total_loss: 3.443  loss_cls_stage0: 0.3092  loss_box_reg_stage0: 0.2997  loss_cls_stage1: 0.3226  loss_box_reg_stage1: 0.8038  loss_cls_stage2: 0.3332  loss_box_reg_stage2: 1.087  loss_mask: 0.08849  loss_rpn_cls: 0.02729  loss_rpn_loc: 0.05065  time: 0.6264  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 13:41:30] fvcore.common.checkpoint INFO: Saving checkpoint to ./output/model_0009999.pth
[07/29 13:41:32] d2.data.datasets.coco INFO: Loading datasets/tless/tless_annotations_test.json takes 1.34 seconds.
[07/29 13:41:32] d2.data.datasets.coco INFO: Loaded 10080 images in COCO format from datasets/tless/tless_annotations_test.json
[07/29 13:41:33] d2.data.build INFO: Distribution of instances among all 30 categories:
[36m|  category  | #instances   | category   | #instances   | category   | #instances   |
|:----------:|:-------------|:-----------|:-------------|:-----------|:-------------|
|     1      | 8190         | 2          | 4720         | 3          | 4000         |
|     4      | 6255         | 5          | 1960         | 6          | 1007         |
|     7      | 2511         | 8          | 1512         | 9          | 2511         |
|     10     | 1495         | 11         | 1907         | 12         | 1471         |
|     13     | 1486         | 14         | 1505         | 15         | 1509         |
|     16     | 1993         | 17         | 1507         | 18         | 1495         |
|     19     | 1985         | 20         | 2477         | 21         | 1880         |
|     22     | 1977         | 23         | 2514         | 24         | 1983         |
|     25     | 1001         | 26         | 1008         | 27         | 983          |
|     28     | 1973         | 29         | 1006         | 30         | 1487         |
|            |              |            |              |            |              |
|   total    | 67308        |            |              |            |              |[0m
[07/29 13:41:33] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[07/29 13:41:33] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[07/29 13:41:33] d2.data.common INFO: Serializing 10080 elements to byte tensors and concatenating them all ...
[07/29 13:41:33] d2.data.common INFO: Serialized dataset takes 50.77 MiB
[07/29 13:41:34] d2.evaluation.evaluator INFO: Start inference on 10080 batches
[07/29 13:41:36] d2.evaluation.evaluator INFO: Inference done 11/10080. Dataloading: 0.0007 s/iter. Inference: 0.1018 s/iter. Eval: 0.0026 s/iter. Total: 0.1052 s/iter. ETA=0:17:38
[07/29 13:41:41] d2.evaluation.evaluator INFO: Inference done 59/10080. Dataloading: 0.0008 s/iter. Inference: 0.1018 s/iter. Eval: 0.0025 s/iter. Total: 0.1052 s/iter. ETA=0:17:33
[07/29 13:41:46] d2.evaluation.evaluator INFO: Inference done 107/10080. Dataloading: 0.0009 s/iter. Inference: 0.1017 s/iter. Eval: 0.0024 s/iter. Total: 0.1050 s/iter. ETA=0:17:27
[07/29 13:41:51] d2.evaluation.evaluator INFO: Inference done 155/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0025 s/iter. Total: 0.1052 s/iter. ETA=0:17:24
[07/29 13:41:56] d2.evaluation.evaluator INFO: Inference done 203/10080. Dataloading: 0.0009 s/iter. Inference: 0.1017 s/iter. Eval: 0.0024 s/iter. Total: 0.1051 s/iter. ETA=0:17:17
[07/29 13:42:01] d2.evaluation.evaluator INFO: Inference done 252/10080. Dataloading: 0.0008 s/iter. Inference: 0.1015 s/iter. Eval: 0.0025 s/iter. Total: 0.1049 s/iter. ETA=0:17:10
[07/29 13:42:06] d2.evaluation.evaluator INFO: Inference done 301/10080. Dataloading: 0.0008 s/iter. Inference: 0.1013 s/iter. Eval: 0.0025 s/iter. Total: 0.1047 s/iter. ETA=0:17:03
[07/29 13:42:11] d2.evaluation.evaluator INFO: Inference done 350/10080. Dataloading: 0.0008 s/iter. Inference: 0.1012 s/iter. Eval: 0.0025 s/iter. Total: 0.1046 s/iter. ETA=0:16:57
[07/29 13:42:16] d2.evaluation.evaluator INFO: Inference done 397/10080. Dataloading: 0.0008 s/iter. Inference: 0.1013 s/iter. Eval: 0.0026 s/iter. Total: 0.1048 s/iter. ETA=0:16:54
[07/29 13:42:21] d2.evaluation.evaluator INFO: Inference done 445/10080. Dataloading: 0.0008 s/iter. Inference: 0.1014 s/iter. Eval: 0.0027 s/iter. Total: 0.1050 s/iter. ETA=0:16:51
[07/29 13:42:26] d2.evaluation.evaluator INFO: Inference done 493/10080. Dataloading: 0.0008 s/iter. Inference: 0.1014 s/iter. Eval: 0.0028 s/iter. Total: 0.1051 s/iter. ETA=0:16:47
[07/29 13:42:31] d2.evaluation.evaluator INFO: Inference done 541/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0027 s/iter. Total: 0.1050 s/iter. ETA=0:16:41
[07/29 13:42:37] d2.evaluation.evaluator INFO: Inference done 590/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0026 s/iter. Total: 0.1049 s/iter. ETA=0:16:35
[07/29 13:42:42] d2.evaluation.evaluator INFO: Inference done 638/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0026 s/iter. Total: 0.1049 s/iter. ETA=0:16:30
[07/29 13:42:47] d2.evaluation.evaluator INFO: Inference done 686/10080. Dataloading: 0.0009 s/iter. Inference: 0.1013 s/iter. Eval: 0.0025 s/iter. Total: 0.1048 s/iter. ETA=0:16:24
[07/29 13:42:52] d2.evaluation.evaluator INFO: Inference done 734/10080. Dataloading: 0.0009 s/iter. Inference: 0.1013 s/iter. Eval: 0.0025 s/iter. Total: 0.1048 s/iter. ETA=0:16:19
[07/29 13:42:57] d2.evaluation.evaluator INFO: Inference done 782/10080. Dataloading: 0.0009 s/iter. Inference: 0.1013 s/iter. Eval: 0.0026 s/iter. Total: 0.1048 s/iter. ETA=0:16:14
[07/29 13:43:02] d2.evaluation.evaluator INFO: Inference done 829/10080. Dataloading: 0.0009 s/iter. Inference: 0.1013 s/iter. Eval: 0.0026 s/iter. Total: 0.1049 s/iter. ETA=0:16:10
[07/29 13:43:07] d2.evaluation.evaluator INFO: Inference done 877/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0026 s/iter. Total: 0.1049 s/iter. ETA=0:16:05
[07/29 13:43:12] d2.evaluation.evaluator INFO: Inference done 925/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0026 s/iter. Total: 0.1049 s/iter. ETA=0:16:00
[07/29 13:43:17] d2.evaluation.evaluator INFO: Inference done 973/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0026 s/iter. Total: 0.1049 s/iter. ETA=0:15:55
[07/29 13:43:22] d2.evaluation.evaluator INFO: Inference done 1021/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0026 s/iter. Total: 0.1049 s/iter. ETA=0:15:50
[07/29 13:43:27] d2.evaluation.evaluator INFO: Inference done 1070/10080. Dataloading: 0.0009 s/iter. Inference: 0.1013 s/iter. Eval: 0.0026 s/iter. Total: 0.1049 s/iter. ETA=0:15:44
[07/29 13:43:32] d2.evaluation.evaluator INFO: Inference done 1119/10080. Dataloading: 0.0009 s/iter. Inference: 0.1013 s/iter. Eval: 0.0026 s/iter. Total: 0.1048 s/iter. ETA=0:15:39
[07/29 13:43:37] d2.evaluation.evaluator INFO: Inference done 1167/10080. Dataloading: 0.0009 s/iter. Inference: 0.1013 s/iter. Eval: 0.0026 s/iter. Total: 0.1048 s/iter. ETA=0:15:34
[07/29 13:43:42] d2.evaluation.evaluator INFO: Inference done 1215/10080. Dataloading: 0.0009 s/iter. Inference: 0.1013 s/iter. Eval: 0.0027 s/iter. Total: 0.1049 s/iter. ETA=0:15:29
[07/29 13:43:47] d2.evaluation.evaluator INFO: Inference done 1264/10080. Dataloading: 0.0009 s/iter. Inference: 0.1012 s/iter. Eval: 0.0027 s/iter. Total: 0.1048 s/iter. ETA=0:15:24
[07/29 13:43:52] d2.evaluation.evaluator INFO: Inference done 1313/10080. Dataloading: 0.0009 s/iter. Inference: 0.1012 s/iter. Eval: 0.0027 s/iter. Total: 0.1048 s/iter. ETA=0:15:18
[07/29 13:43:57] d2.evaluation.evaluator INFO: Inference done 1361/10080. Dataloading: 0.0009 s/iter. Inference: 0.1012 s/iter. Eval: 0.0027 s/iter. Total: 0.1048 s/iter. ETA=0:15:13
[07/29 13:44:02] d2.evaluation.evaluator INFO: Inference done 1409/10080. Dataloading: 0.0009 s/iter. Inference: 0.1012 s/iter. Eval: 0.0027 s/iter. Total: 0.1048 s/iter. ETA=0:15:09
[07/29 13:44:07] d2.evaluation.evaluator INFO: Inference done 1456/10080. Dataloading: 0.0009 s/iter. Inference: 0.1013 s/iter. Eval: 0.0027 s/iter. Total: 0.1049 s/iter. ETA=0:15:04
[07/29 13:44:12] d2.evaluation.evaluator INFO: Inference done 1503/10080. Dataloading: 0.0009 s/iter. Inference: 0.1013 s/iter. Eval: 0.0028 s/iter. Total: 0.1050 s/iter. ETA=0:15:00
[07/29 13:44:18] d2.evaluation.evaluator INFO: Inference done 1551/10080. Dataloading: 0.0009 s/iter. Inference: 0.1013 s/iter. Eval: 0.0028 s/iter. Total: 0.1050 s/iter. ETA=0:14:55
[07/29 13:44:23] d2.evaluation.evaluator INFO: Inference done 1599/10080. Dataloading: 0.0009 s/iter. Inference: 0.1013 s/iter. Eval: 0.0028 s/iter. Total: 0.1050 s/iter. ETA=0:14:50
[07/29 13:44:28] d2.evaluation.evaluator INFO: Inference done 1647/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0028 s/iter. Total: 0.1051 s/iter. ETA=0:14:45
[07/29 13:44:33] d2.evaluation.evaluator INFO: Inference done 1695/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0028 s/iter. Total: 0.1051 s/iter. ETA=0:14:40
[07/29 13:44:38] d2.evaluation.evaluator INFO: Inference done 1743/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0028 s/iter. Total: 0.1051 s/iter. ETA=0:14:36
[07/29 13:44:43] d2.evaluation.evaluator INFO: Inference done 1790/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0028 s/iter. Total: 0.1051 s/iter. ETA=0:14:31
[07/29 13:44:48] d2.evaluation.evaluator INFO: Inference done 1838/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0028 s/iter. Total: 0.1052 s/iter. ETA=0:14:26
[07/29 13:44:53] d2.evaluation.evaluator INFO: Inference done 1886/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0029 s/iter. Total: 0.1052 s/iter. ETA=0:14:21
[07/29 13:44:58] d2.evaluation.evaluator INFO: Inference done 1934/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0029 s/iter. Total: 0.1052 s/iter. ETA=0:14:17
[07/29 13:45:03] d2.evaluation.evaluator INFO: Inference done 1982/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0029 s/iter. Total: 0.1052 s/iter. ETA=0:14:12
[07/29 13:45:08] d2.evaluation.evaluator INFO: Inference done 2030/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0029 s/iter. Total: 0.1052 s/iter. ETA=0:14:07
[07/29 13:45:13] d2.evaluation.evaluator INFO: Inference done 2078/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0029 s/iter. Total: 0.1052 s/iter. ETA=0:14:02
[07/29 13:45:18] d2.evaluation.evaluator INFO: Inference done 2126/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0029 s/iter. Total: 0.1053 s/iter. ETA=0:13:57
[07/29 13:45:23] d2.evaluation.evaluator INFO: Inference done 2173/10080. Dataloading: 0.0009 s/iter. Inference: 0.1015 s/iter. Eval: 0.0029 s/iter. Total: 0.1053 s/iter. ETA=0:13:52
[07/29 13:45:29] d2.evaluation.evaluator INFO: Inference done 2221/10080. Dataloading: 0.0009 s/iter. Inference: 0.1015 s/iter. Eval: 0.0029 s/iter. Total: 0.1053 s/iter. ETA=0:13:47
[07/29 13:45:34] d2.evaluation.evaluator INFO: Inference done 2269/10080. Dataloading: 0.0009 s/iter. Inference: 0.1015 s/iter. Eval: 0.0029 s/iter. Total: 0.1053 s/iter. ETA=0:13:42
[07/29 13:45:39] d2.evaluation.evaluator INFO: Inference done 2316/10080. Dataloading: 0.0009 s/iter. Inference: 0.1015 s/iter. Eval: 0.0029 s/iter. Total: 0.1054 s/iter. ETA=0:13:37
[07/29 13:45:44] d2.evaluation.evaluator INFO: Inference done 2363/10080. Dataloading: 0.0009 s/iter. Inference: 0.1015 s/iter. Eval: 0.0029 s/iter. Total: 0.1054 s/iter. ETA=0:13:33
[07/29 13:45:49] d2.evaluation.evaluator INFO: Inference done 2410/10080. Dataloading: 0.0009 s/iter. Inference: 0.1015 s/iter. Eval: 0.0029 s/iter. Total: 0.1054 s/iter. ETA=0:13:28
[07/29 13:45:54] d2.evaluation.evaluator INFO: Inference done 2457/10080. Dataloading: 0.0009 s/iter. Inference: 0.1016 s/iter. Eval: 0.0030 s/iter. Total: 0.1055 s/iter. ETA=0:13:24
[07/29 13:45:59] d2.evaluation.evaluator INFO: Inference done 2504/10080. Dataloading: 0.0009 s/iter. Inference: 0.1016 s/iter. Eval: 0.0030 s/iter. Total: 0.1055 s/iter. ETA=0:13:19
[07/29 13:46:04] d2.evaluation.evaluator INFO: Inference done 2551/10080. Dataloading: 0.0009 s/iter. Inference: 0.1016 s/iter. Eval: 0.0030 s/iter. Total: 0.1056 s/iter. ETA=0:13:14
[07/29 13:46:09] d2.evaluation.evaluator INFO: Inference done 2599/10080. Dataloading: 0.0009 s/iter. Inference: 0.1016 s/iter. Eval: 0.0031 s/iter. Total: 0.1056 s/iter. ETA=0:13:09
[07/29 13:46:14] d2.evaluation.evaluator INFO: Inference done 2646/10080. Dataloading: 0.0009 s/iter. Inference: 0.1016 s/iter. Eval: 0.0031 s/iter. Total: 0.1056 s/iter. ETA=0:13:05
[07/29 13:46:19] d2.evaluation.evaluator INFO: Inference done 2693/10080. Dataloading: 0.0009 s/iter. Inference: 0.1016 s/iter. Eval: 0.0031 s/iter. Total: 0.1056 s/iter. ETA=0:13:00
[07/29 13:46:24] d2.evaluation.evaluator INFO: Inference done 2740/10080. Dataloading: 0.0009 s/iter. Inference: 0.1016 s/iter. Eval: 0.0032 s/iter. Total: 0.1057 s/iter. ETA=0:12:55
[07/29 13:46:29] d2.evaluation.evaluator INFO: Inference done 2787/10080. Dataloading: 0.0009 s/iter. Inference: 0.1016 s/iter. Eval: 0.0032 s/iter. Total: 0.1057 s/iter. ETA=0:12:51
[07/29 13:46:34] d2.evaluation.evaluator INFO: Inference done 2834/10080. Dataloading: 0.0009 s/iter. Inference: 0.1016 s/iter. Eval: 0.0032 s/iter. Total: 0.1057 s/iter. ETA=0:12:46
[07/29 13:46:39] d2.evaluation.evaluator INFO: Inference done 2882/10080. Dataloading: 0.0009 s/iter. Inference: 0.1016 s/iter. Eval: 0.0032 s/iter. Total: 0.1057 s/iter. ETA=0:12:41
[07/29 13:46:44] d2.evaluation.evaluator INFO: Inference done 2930/10080. Dataloading: 0.0009 s/iter. Inference: 0.1015 s/iter. Eval: 0.0032 s/iter. Total: 0.1057 s/iter. ETA=0:12:36
[07/29 13:46:50] d2.evaluation.evaluator INFO: Inference done 2978/10080. Dataloading: 0.0009 s/iter. Inference: 0.1015 s/iter. Eval: 0.0033 s/iter. Total: 0.1057 s/iter. ETA=0:12:30
[07/29 13:46:55] d2.evaluation.evaluator INFO: Inference done 3026/10080. Dataloading: 0.0009 s/iter. Inference: 0.1015 s/iter. Eval: 0.0033 s/iter. Total: 0.1057 s/iter. ETA=0:12:25
[07/29 13:47:00] d2.evaluation.evaluator INFO: Inference done 3073/10080. Dataloading: 0.0009 s/iter. Inference: 0.1015 s/iter. Eval: 0.0033 s/iter. Total: 0.1058 s/iter. ETA=0:12:21
[07/29 13:47:05] d2.evaluation.evaluator INFO: Inference done 3120/10080. Dataloading: 0.0009 s/iter. Inference: 0.1015 s/iter. Eval: 0.0033 s/iter. Total: 0.1058 s/iter. ETA=0:12:16
[07/29 13:47:10] d2.evaluation.evaluator INFO: Inference done 3167/10080. Dataloading: 0.0009 s/iter. Inference: 0.1016 s/iter. Eval: 0.0033 s/iter. Total: 0.1058 s/iter. ETA=0:12:11
[07/29 13:47:15] d2.evaluation.evaluator INFO: Inference done 3214/10080. Dataloading: 0.0009 s/iter. Inference: 0.1016 s/iter. Eval: 0.0034 s/iter. Total: 0.1059 s/iter. ETA=0:12:06
[07/29 13:47:20] d2.evaluation.evaluator INFO: Inference done 3262/10080. Dataloading: 0.0009 s/iter. Inference: 0.1016 s/iter. Eval: 0.0034 s/iter. Total: 0.1059 s/iter. ETA=0:12:01
[07/29 13:47:25] d2.evaluation.evaluator INFO: Inference done 3309/10080. Dataloading: 0.0009 s/iter. Inference: 0.1016 s/iter. Eval: 0.0034 s/iter. Total: 0.1059 s/iter. ETA=0:11:56
[07/29 13:47:30] d2.evaluation.evaluator INFO: Inference done 3356/10080. Dataloading: 0.0009 s/iter. Inference: 0.1016 s/iter. Eval: 0.0034 s/iter. Total: 0.1059 s/iter. ETA=0:11:52
[07/29 13:47:35] d2.evaluation.evaluator INFO: Inference done 3403/10080. Dataloading: 0.0009 s/iter. Inference: 0.1016 s/iter. Eval: 0.0034 s/iter. Total: 0.1059 s/iter. ETA=0:11:47
[07/29 13:47:40] d2.evaluation.evaluator INFO: Inference done 3450/10080. Dataloading: 0.0009 s/iter. Inference: 0.1015 s/iter. Eval: 0.0034 s/iter. Total: 0.1059 s/iter. ETA=0:11:42
[07/29 13:47:45] d2.evaluation.evaluator INFO: Inference done 3497/10080. Dataloading: 0.0009 s/iter. Inference: 0.1016 s/iter. Eval: 0.0035 s/iter. Total: 0.1060 s/iter. ETA=0:11:37
[07/29 13:47:50] d2.evaluation.evaluator INFO: Inference done 3540/10080. Dataloading: 0.0009 s/iter. Inference: 0.1017 s/iter. Eval: 0.0035 s/iter. Total: 0.1061 s/iter. ETA=0:11:33
[07/29 13:47:55] d2.evaluation.evaluator INFO: Inference done 3587/10080. Dataloading: 0.0009 s/iter. Inference: 0.1017 s/iter. Eval: 0.0035 s/iter. Total: 0.1061 s/iter. ETA=0:11:29
[07/29 13:48:00] d2.evaluation.evaluator INFO: Inference done 3634/10080. Dataloading: 0.0009 s/iter. Inference: 0.1017 s/iter. Eval: 0.0035 s/iter. Total: 0.1061 s/iter. ETA=0:11:24
[07/29 13:48:05] d2.evaluation.evaluator INFO: Inference done 3681/10080. Dataloading: 0.0009 s/iter. Inference: 0.1017 s/iter. Eval: 0.0035 s/iter. Total: 0.1062 s/iter. ETA=0:11:19
[07/29 13:48:10] d2.evaluation.evaluator INFO: Inference done 3728/10080. Dataloading: 0.0009 s/iter. Inference: 0.1017 s/iter. Eval: 0.0036 s/iter. Total: 0.1062 s/iter. ETA=0:11:14
[07/29 13:48:15] d2.evaluation.evaluator INFO: Inference done 3775/10080. Dataloading: 0.0009 s/iter. Inference: 0.1017 s/iter. Eval: 0.0036 s/iter. Total: 0.1062 s/iter. ETA=0:11:09
[07/29 13:48:21] d2.evaluation.evaluator INFO: Inference done 3822/10080. Dataloading: 0.0009 s/iter. Inference: 0.1017 s/iter. Eval: 0.0036 s/iter. Total: 0.1062 s/iter. ETA=0:11:04
[07/29 13:48:26] d2.evaluation.evaluator INFO: Inference done 3869/10080. Dataloading: 0.0009 s/iter. Inference: 0.1017 s/iter. Eval: 0.0036 s/iter. Total: 0.1062 s/iter. ETA=0:10:59
[07/29 13:48:31] d2.evaluation.evaluator INFO: Inference done 3915/10080. Dataloading: 0.0009 s/iter. Inference: 0.1017 s/iter. Eval: 0.0036 s/iter. Total: 0.1063 s/iter. ETA=0:10:55
[07/29 13:48:36] d2.evaluation.evaluator INFO: Inference done 3962/10080. Dataloading: 0.0009 s/iter. Inference: 0.1017 s/iter. Eval: 0.0037 s/iter. Total: 0.1063 s/iter. ETA=0:10:50
[07/29 13:48:41] d2.evaluation.evaluator INFO: Inference done 4009/10080. Dataloading: 0.0009 s/iter. Inference: 0.1017 s/iter. Eval: 0.0037 s/iter. Total: 0.1063 s/iter. ETA=0:10:45
[07/29 13:48:46] d2.evaluation.evaluator INFO: Inference done 4055/10080. Dataloading: 0.0009 s/iter. Inference: 0.1017 s/iter. Eval: 0.0037 s/iter. Total: 0.1063 s/iter. ETA=0:10:40
[07/29 13:48:51] d2.evaluation.evaluator INFO: Inference done 4101/10080. Dataloading: 0.0009 s/iter. Inference: 0.1017 s/iter. Eval: 0.0037 s/iter. Total: 0.1064 s/iter. ETA=0:10:36
[07/29 13:48:56] d2.evaluation.evaluator INFO: Inference done 4147/10080. Dataloading: 0.0009 s/iter. Inference: 0.1017 s/iter. Eval: 0.0037 s/iter. Total: 0.1064 s/iter. ETA=0:10:31
[07/29 13:49:01] d2.evaluation.evaluator INFO: Inference done 4193/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0038 s/iter. Total: 0.1065 s/iter. ETA=0:10:26
[07/29 13:49:06] d2.evaluation.evaluator INFO: Inference done 4239/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0038 s/iter. Total: 0.1065 s/iter. ETA=0:10:22
[07/29 13:49:11] d2.evaluation.evaluator INFO: Inference done 4285/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0038 s/iter. Total: 0.1066 s/iter. ETA=0:10:17
[07/29 13:49:16] d2.evaluation.evaluator INFO: Inference done 4330/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0039 s/iter. Total: 0.1066 s/iter. ETA=0:10:12
[07/29 13:49:21] d2.evaluation.evaluator INFO: Inference done 4375/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0039 s/iter. Total: 0.1067 s/iter. ETA=0:10:08
[07/29 13:49:26] d2.evaluation.evaluator INFO: Inference done 4419/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0040 s/iter. Total: 0.1067 s/iter. ETA=0:10:04
[07/29 13:49:31] d2.evaluation.evaluator INFO: Inference done 4464/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0040 s/iter. Total: 0.1068 s/iter. ETA=0:09:59
[07/29 13:49:37] d2.evaluation.evaluator INFO: Inference done 4509/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0041 s/iter. Total: 0.1069 s/iter. ETA=0:09:55
[07/29 13:49:42] d2.evaluation.evaluator INFO: Inference done 4556/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0041 s/iter. Total: 0.1069 s/iter. ETA=0:09:50
[07/29 13:49:47] d2.evaluation.evaluator INFO: Inference done 4604/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1069 s/iter. ETA=0:09:45
[07/29 13:49:52] d2.evaluation.evaluator INFO: Inference done 4651/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1069 s/iter. ETA=0:09:40
[07/29 13:49:57] d2.evaluation.evaluator INFO: Inference done 4698/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1069 s/iter. ETA=0:09:35
[07/29 13:50:02] d2.evaluation.evaluator INFO: Inference done 4745/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1069 s/iter. ETA=0:09:30
[07/29 13:50:07] d2.evaluation.evaluator INFO: Inference done 4793/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1069 s/iter. ETA=0:09:25
[07/29 13:50:12] d2.evaluation.evaluator INFO: Inference done 4841/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1069 s/iter. ETA=0:09:19
[07/29 13:50:17] d2.evaluation.evaluator INFO: Inference done 4889/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1069 s/iter. ETA=0:09:14
[07/29 13:50:22] d2.evaluation.evaluator INFO: Inference done 4936/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1069 s/iter. ETA=0:09:09
[07/29 13:50:27] d2.evaluation.evaluator INFO: Inference done 4982/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0042 s/iter. Total: 0.1069 s/iter. ETA=0:09:05
[07/29 13:50:32] d2.evaluation.evaluator INFO: Inference done 5028/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0042 s/iter. Total: 0.1069 s/iter. ETA=0:09:00
[07/29 13:50:37] d2.evaluation.evaluator INFO: Inference done 5075/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0042 s/iter. Total: 0.1069 s/iter. ETA=0:08:55
[07/29 13:50:42] d2.evaluation.evaluator INFO: Inference done 5123/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0042 s/iter. Total: 0.1069 s/iter. ETA=0:08:50
[07/29 13:50:47] d2.evaluation.evaluator INFO: Inference done 5171/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0042 s/iter. Total: 0.1069 s/iter. ETA=0:08:44
[07/29 13:50:52] d2.evaluation.evaluator INFO: Inference done 5219/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1069 s/iter. ETA=0:08:39
[07/29 13:50:58] d2.evaluation.evaluator INFO: Inference done 5267/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1069 s/iter. ETA=0:08:34
[07/29 13:51:03] d2.evaluation.evaluator INFO: Inference done 5315/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1069 s/iter. ETA=0:08:29
[07/29 13:51:08] d2.evaluation.evaluator INFO: Inference done 5363/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1068 s/iter. ETA=0:08:23
[07/29 13:51:13] d2.evaluation.evaluator INFO: Inference done 5411/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1068 s/iter. ETA=0:08:18
[07/29 13:51:18] d2.evaluation.evaluator INFO: Inference done 5459/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1068 s/iter. ETA=0:08:13
[07/29 13:51:23] d2.evaluation.evaluator INFO: Inference done 5506/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1068 s/iter. ETA=0:08:08
[07/29 13:51:28] d2.evaluation.evaluator INFO: Inference done 5553/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1068 s/iter. ETA=0:08:03
[07/29 13:51:33] d2.evaluation.evaluator INFO: Inference done 5600/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1068 s/iter. ETA=0:07:58
[07/29 13:51:38] d2.evaluation.evaluator INFO: Inference done 5648/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1068 s/iter. ETA=0:07:53
[07/29 13:51:43] d2.evaluation.evaluator INFO: Inference done 5696/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1068 s/iter. ETA=0:07:48
[07/29 13:51:48] d2.evaluation.evaluator INFO: Inference done 5744/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0040 s/iter. Total: 0.1068 s/iter. ETA=0:07:43
[07/29 13:51:53] d2.evaluation.evaluator INFO: Inference done 5791/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1068 s/iter. ETA=0:07:38
[07/29 13:51:58] d2.evaluation.evaluator INFO: Inference done 5838/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1068 s/iter. ETA=0:07:33
[07/29 13:52:03] d2.evaluation.evaluator INFO: Inference done 5885/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1068 s/iter. ETA=0:07:28
[07/29 13:52:08] d2.evaluation.evaluator INFO: Inference done 5931/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1069 s/iter. ETA=0:07:23
[07/29 13:52:13] d2.evaluation.evaluator INFO: Inference done 5977/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1069 s/iter. ETA=0:07:18
[07/29 13:52:18] d2.evaluation.evaluator INFO: Inference done 6024/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0041 s/iter. Total: 0.1069 s/iter. ETA=0:07:13
[07/29 13:52:24] d2.evaluation.evaluator INFO: Inference done 6070/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0041 s/iter. Total: 0.1069 s/iter. ETA=0:07:08
[07/29 13:52:29] d2.evaluation.evaluator INFO: Inference done 6116/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0041 s/iter. Total: 0.1069 s/iter. ETA=0:07:03
[07/29 13:52:34] d2.evaluation.evaluator INFO: Inference done 6162/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0041 s/iter. Total: 0.1070 s/iter. ETA=0:06:59
[07/29 13:52:39] d2.evaluation.evaluator INFO: Inference done 6207/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0042 s/iter. Total: 0.1070 s/iter. ETA=0:06:54
[07/29 13:52:44] d2.evaluation.evaluator INFO: Inference done 6253/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0042 s/iter. Total: 0.1070 s/iter. ETA=0:06:49
[07/29 13:52:49] d2.evaluation.evaluator INFO: Inference done 6299/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0042 s/iter. Total: 0.1070 s/iter. ETA=0:06:44
[07/29 13:52:54] d2.evaluation.evaluator INFO: Inference done 6345/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0042 s/iter. Total: 0.1071 s/iter. ETA=0:06:39
[07/29 13:52:59] d2.evaluation.evaluator INFO: Inference done 6391/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0042 s/iter. Total: 0.1071 s/iter. ETA=0:06:34
[07/29 13:53:04] d2.evaluation.evaluator INFO: Inference done 6437/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0043 s/iter. Total: 0.1071 s/iter. ETA=0:06:30
[07/29 13:53:09] d2.evaluation.evaluator INFO: Inference done 6483/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0043 s/iter. Total: 0.1071 s/iter. ETA=0:06:25
[07/29 13:53:14] d2.evaluation.evaluator INFO: Inference done 6529/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0043 s/iter. Total: 0.1071 s/iter. ETA=0:06:20
[07/29 13:53:19] d2.evaluation.evaluator INFO: Inference done 6575/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0043 s/iter. Total: 0.1072 s/iter. ETA=0:06:15
[07/29 13:53:24] d2.evaluation.evaluator INFO: Inference done 6621/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0043 s/iter. Total: 0.1072 s/iter. ETA=0:06:10
[07/29 13:53:29] d2.evaluation.evaluator INFO: Inference done 6667/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0043 s/iter. Total: 0.1072 s/iter. ETA=0:06:05
[07/29 13:53:34] d2.evaluation.evaluator INFO: Inference done 6713/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0043 s/iter. Total: 0.1072 s/iter. ETA=0:06:00
[07/29 13:53:39] d2.evaluation.evaluator INFO: Inference done 6760/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0044 s/iter. Total: 0.1072 s/iter. ETA=0:05:55
[07/29 13:53:44] d2.evaluation.evaluator INFO: Inference done 6807/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0044 s/iter. Total: 0.1072 s/iter. ETA=0:05:50
[07/29 13:53:49] d2.evaluation.evaluator INFO: Inference done 6853/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0044 s/iter. Total: 0.1072 s/iter. ETA=0:05:46
[07/29 13:53:55] d2.evaluation.evaluator INFO: Inference done 6899/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0044 s/iter. Total: 0.1072 s/iter. ETA=0:05:41
[07/29 13:54:00] d2.evaluation.evaluator INFO: Inference done 6945/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0044 s/iter. Total: 0.1073 s/iter. ETA=0:05:36
[07/29 13:54:05] d2.evaluation.evaluator INFO: Inference done 6991/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0044 s/iter. Total: 0.1073 s/iter. ETA=0:05:31
[07/29 13:54:10] d2.evaluation.evaluator INFO: Inference done 7038/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0044 s/iter. Total: 0.1073 s/iter. ETA=0:05:26
[07/29 13:54:15] d2.evaluation.evaluator INFO: Inference done 7084/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0044 s/iter. Total: 0.1073 s/iter. ETA=0:05:21
[07/29 13:54:20] d2.evaluation.evaluator INFO: Inference done 7131/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0044 s/iter. Total: 0.1073 s/iter. ETA=0:05:16
[07/29 13:54:25] d2.evaluation.evaluator INFO: Inference done 7178/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0044 s/iter. Total: 0.1073 s/iter. ETA=0:05:11
[07/29 13:54:30] d2.evaluation.evaluator INFO: Inference done 7225/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0044 s/iter. Total: 0.1073 s/iter. ETA=0:05:06
[07/29 13:54:35] d2.evaluation.evaluator INFO: Inference done 7272/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0044 s/iter. Total: 0.1073 s/iter. ETA=0:05:01
[07/29 13:54:40] d2.evaluation.evaluator INFO: Inference done 7319/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0044 s/iter. Total: 0.1073 s/iter. ETA=0:04:56
[07/29 13:54:45] d2.evaluation.evaluator INFO: Inference done 7366/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0044 s/iter. Total: 0.1073 s/iter. ETA=0:04:51
[07/29 13:54:50] d2.evaluation.evaluator INFO: Inference done 7413/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0044 s/iter. Total: 0.1073 s/iter. ETA=0:04:46
[07/29 13:54:55] d2.evaluation.evaluator INFO: Inference done 7460/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0044 s/iter. Total: 0.1073 s/iter. ETA=0:04:41
[07/29 13:55:00] d2.evaluation.evaluator INFO: Inference done 7507/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0044 s/iter. Total: 0.1073 s/iter. ETA=0:04:36
[07/29 13:55:05] d2.evaluation.evaluator INFO: Inference done 7555/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0044 s/iter. Total: 0.1073 s/iter. ETA=0:04:30
[07/29 13:55:10] d2.evaluation.evaluator INFO: Inference done 7602/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0045 s/iter. Total: 0.1073 s/iter. ETA=0:04:25
[07/29 13:55:15] d2.evaluation.evaluator INFO: Inference done 7649/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0045 s/iter. Total: 0.1073 s/iter. ETA=0:04:20
[07/29 13:55:20] d2.evaluation.evaluator INFO: Inference done 7695/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0045 s/iter. Total: 0.1073 s/iter. ETA=0:04:15
[07/29 13:55:25] d2.evaluation.evaluator INFO: Inference done 7741/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0045 s/iter. Total: 0.1073 s/iter. ETA=0:04:11
[07/29 13:55:30] d2.evaluation.evaluator INFO: Inference done 7787/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0045 s/iter. Total: 0.1073 s/iter. ETA=0:04:06
[07/29 13:55:36] d2.evaluation.evaluator INFO: Inference done 7833/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0045 s/iter. Total: 0.1074 s/iter. ETA=0:04:01
[07/29 13:55:41] d2.evaluation.evaluator INFO: Inference done 7879/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0045 s/iter. Total: 0.1074 s/iter. ETA=0:03:56
[07/29 13:55:46] d2.evaluation.evaluator INFO: Inference done 7925/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0045 s/iter. Total: 0.1074 s/iter. ETA=0:03:51
[07/29 13:55:51] d2.evaluation.evaluator INFO: Inference done 7971/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0045 s/iter. Total: 0.1074 s/iter. ETA=0:03:46
[07/29 13:55:56] d2.evaluation.evaluator INFO: Inference done 8017/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0045 s/iter. Total: 0.1074 s/iter. ETA=0:03:41
[07/29 13:56:01] d2.evaluation.evaluator INFO: Inference done 8063/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0045 s/iter. Total: 0.1074 s/iter. ETA=0:03:36
[07/29 13:56:06] d2.evaluation.evaluator INFO: Inference done 8109/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0045 s/iter. Total: 0.1074 s/iter. ETA=0:03:31
[07/29 13:56:11] d2.evaluation.evaluator INFO: Inference done 8156/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0045 s/iter. Total: 0.1074 s/iter. ETA=0:03:26
[07/29 13:56:16] d2.evaluation.evaluator INFO: Inference done 8202/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0045 s/iter. Total: 0.1074 s/iter. ETA=0:03:21
[07/29 13:56:21] d2.evaluation.evaluator INFO: Inference done 8248/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0045 s/iter. Total: 0.1074 s/iter. ETA=0:03:16
[07/29 13:56:26] d2.evaluation.evaluator INFO: Inference done 8294/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0046 s/iter. Total: 0.1075 s/iter. ETA=0:03:11
[07/29 13:56:31] d2.evaluation.evaluator INFO: Inference done 8340/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0046 s/iter. Total: 0.1075 s/iter. ETA=0:03:06
[07/29 13:56:36] d2.evaluation.evaluator INFO: Inference done 8384/10080. Dataloading: 0.0009 s/iter. Inference: 0.1020 s/iter. Eval: 0.0046 s/iter. Total: 0.1075 s/iter. ETA=0:03:02
[07/29 13:56:41] d2.evaluation.evaluator INFO: Inference done 8430/10080. Dataloading: 0.0009 s/iter. Inference: 0.1020 s/iter. Eval: 0.0046 s/iter. Total: 0.1076 s/iter. ETA=0:02:57
[07/29 13:56:46] d2.evaluation.evaluator INFO: Inference done 8476/10080. Dataloading: 0.0009 s/iter. Inference: 0.1020 s/iter. Eval: 0.0046 s/iter. Total: 0.1076 s/iter. ETA=0:02:52
[07/29 13:56:51] d2.evaluation.evaluator INFO: Inference done 8522/10080. Dataloading: 0.0009 s/iter. Inference: 0.1020 s/iter. Eval: 0.0046 s/iter. Total: 0.1076 s/iter. ETA=0:02:47
[07/29 13:56:56] d2.evaluation.evaluator INFO: Inference done 8569/10080. Dataloading: 0.0009 s/iter. Inference: 0.1020 s/iter. Eval: 0.0046 s/iter. Total: 0.1076 s/iter. ETA=0:02:42
[07/29 13:57:02] d2.evaluation.evaluator INFO: Inference done 8614/10080. Dataloading: 0.0009 s/iter. Inference: 0.1020 s/iter. Eval: 0.0046 s/iter. Total: 0.1076 s/iter. ETA=0:02:37
[07/29 13:57:07] d2.evaluation.evaluator INFO: Inference done 8659/10080. Dataloading: 0.0009 s/iter. Inference: 0.1020 s/iter. Eval: 0.0046 s/iter. Total: 0.1076 s/iter. ETA=0:02:32
[07/29 13:57:12] d2.evaluation.evaluator INFO: Inference done 8705/10080. Dataloading: 0.0009 s/iter. Inference: 0.1020 s/iter. Eval: 0.0047 s/iter. Total: 0.1076 s/iter. ETA=0:02:28
[07/29 13:57:17] d2.evaluation.evaluator INFO: Inference done 8750/10080. Dataloading: 0.0009 s/iter. Inference: 0.1020 s/iter. Eval: 0.0047 s/iter. Total: 0.1077 s/iter. ETA=0:02:23
[07/29 13:57:22] d2.evaluation.evaluator INFO: Inference done 8795/10080. Dataloading: 0.0009 s/iter. Inference: 0.1020 s/iter. Eval: 0.0047 s/iter. Total: 0.1077 s/iter. ETA=0:02:18
[07/29 13:57:27] d2.evaluation.evaluator INFO: Inference done 8840/10080. Dataloading: 0.0009 s/iter. Inference: 0.1021 s/iter. Eval: 0.0047 s/iter. Total: 0.1077 s/iter. ETA=0:02:13
[07/29 13:57:32] d2.evaluation.evaluator INFO: Inference done 8884/10080. Dataloading: 0.0009 s/iter. Inference: 0.1021 s/iter. Eval: 0.0047 s/iter. Total: 0.1077 s/iter. ETA=0:02:08
[07/29 13:57:37] d2.evaluation.evaluator INFO: Inference done 8928/10080. Dataloading: 0.0009 s/iter. Inference: 0.1021 s/iter. Eval: 0.0048 s/iter. Total: 0.1078 s/iter. ETA=0:02:04
[07/29 13:57:42] d2.evaluation.evaluator INFO: Inference done 8972/10080. Dataloading: 0.0009 s/iter. Inference: 0.1021 s/iter. Eval: 0.0048 s/iter. Total: 0.1078 s/iter. ETA=0:01:59
[07/29 13:57:47] d2.evaluation.evaluator INFO: Inference done 9016/10080. Dataloading: 0.0009 s/iter. Inference: 0.1021 s/iter. Eval: 0.0048 s/iter. Total: 0.1078 s/iter. ETA=0:01:54
[07/29 13:57:52] d2.evaluation.evaluator INFO: Inference done 9062/10080. Dataloading: 0.0009 s/iter. Inference: 0.1021 s/iter. Eval: 0.0048 s/iter. Total: 0.1079 s/iter. ETA=0:01:49
[07/29 13:57:57] d2.evaluation.evaluator INFO: Inference done 9108/10080. Dataloading: 0.0009 s/iter. Inference: 0.1021 s/iter. Eval: 0.0048 s/iter. Total: 0.1079 s/iter. ETA=0:01:44
[07/29 13:58:02] d2.evaluation.evaluator INFO: Inference done 9155/10080. Dataloading: 0.0009 s/iter. Inference: 0.1021 s/iter. Eval: 0.0048 s/iter. Total: 0.1079 s/iter. ETA=0:01:39
[07/29 13:58:07] d2.evaluation.evaluator INFO: Inference done 9202/10080. Dataloading: 0.0009 s/iter. Inference: 0.1021 s/iter. Eval: 0.0048 s/iter. Total: 0.1079 s/iter. ETA=0:01:34
[07/29 13:58:12] d2.evaluation.evaluator INFO: Inference done 9249/10080. Dataloading: 0.0009 s/iter. Inference: 0.1021 s/iter. Eval: 0.0048 s/iter. Total: 0.1079 s/iter. ETA=0:01:29
[07/29 13:58:17] d2.evaluation.evaluator INFO: Inference done 9296/10080. Dataloading: 0.0009 s/iter. Inference: 0.1021 s/iter. Eval: 0.0048 s/iter. Total: 0.1079 s/iter. ETA=0:01:24
[07/29 13:58:22] d2.evaluation.evaluator INFO: Inference done 9342/10080. Dataloading: 0.0009 s/iter. Inference: 0.1021 s/iter. Eval: 0.0048 s/iter. Total: 0.1079 s/iter. ETA=0:01:19
[07/29 13:58:27] d2.evaluation.evaluator INFO: Inference done 9388/10080. Dataloading: 0.0009 s/iter. Inference: 0.1021 s/iter. Eval: 0.0048 s/iter. Total: 0.1079 s/iter. ETA=0:01:14
[07/29 13:58:32] d2.evaluation.evaluator INFO: Inference done 9434/10080. Dataloading: 0.0009 s/iter. Inference: 0.1021 s/iter. Eval: 0.0048 s/iter. Total: 0.1079 s/iter. ETA=0:01:09
[07/29 13:58:37] d2.evaluation.evaluator INFO: Inference done 9480/10080. Dataloading: 0.0009 s/iter. Inference: 0.1021 s/iter. Eval: 0.0048 s/iter. Total: 0.1079 s/iter. ETA=0:01:04
[07/29 13:58:43] d2.evaluation.evaluator INFO: Inference done 9527/10080. Dataloading: 0.0009 s/iter. Inference: 0.1021 s/iter. Eval: 0.0048 s/iter. Total: 0.1079 s/iter. ETA=0:00:59
[07/29 13:58:48] d2.evaluation.evaluator INFO: Inference done 9574/10080. Dataloading: 0.0009 s/iter. Inference: 0.1021 s/iter. Eval: 0.0048 s/iter. Total: 0.1079 s/iter. ETA=0:00:54
[07/29 13:58:53] d2.evaluation.evaluator INFO: Inference done 9617/10080. Dataloading: 0.0009 s/iter. Inference: 0.1021 s/iter. Eval: 0.0049 s/iter. Total: 0.1079 s/iter. ETA=0:00:49
[07/29 13:58:58] d2.evaluation.evaluator INFO: Inference done 9660/10080. Dataloading: 0.0009 s/iter. Inference: 0.1021 s/iter. Eval: 0.0049 s/iter. Total: 0.1080 s/iter. ETA=0:00:45
[07/29 13:59:03] d2.evaluation.evaluator INFO: Inference done 9702/10080. Dataloading: 0.0009 s/iter. Inference: 0.1021 s/iter. Eval: 0.0049 s/iter. Total: 0.1080 s/iter. ETA=0:00:40
[07/29 13:59:08] d2.evaluation.evaluator INFO: Inference done 9744/10080. Dataloading: 0.0009 s/iter. Inference: 0.1021 s/iter. Eval: 0.0050 s/iter. Total: 0.1081 s/iter. ETA=0:00:36
[07/29 13:59:13] d2.evaluation.evaluator INFO: Inference done 9788/10080. Dataloading: 0.0009 s/iter. Inference: 0.1021 s/iter. Eval: 0.0050 s/iter. Total: 0.1081 s/iter. ETA=0:00:31
[07/29 13:59:18] d2.evaluation.evaluator INFO: Inference done 9831/10080. Dataloading: 0.0009 s/iter. Inference: 0.1022 s/iter. Eval: 0.0050 s/iter. Total: 0.1082 s/iter. ETA=0:00:26
[07/29 13:59:23] d2.evaluation.evaluator INFO: Inference done 9874/10080. Dataloading: 0.0009 s/iter. Inference: 0.1022 s/iter. Eval: 0.0051 s/iter. Total: 0.1082 s/iter. ETA=0:00:22
[07/29 13:59:28] d2.evaluation.evaluator INFO: Inference done 9918/10080. Dataloading: 0.0009 s/iter. Inference: 0.1022 s/iter. Eval: 0.0051 s/iter. Total: 0.1082 s/iter. ETA=0:00:17
[07/29 13:59:33] d2.evaluation.evaluator INFO: Inference done 9962/10080. Dataloading: 0.0009 s/iter. Inference: 0.1022 s/iter. Eval: 0.0051 s/iter. Total: 0.1083 s/iter. ETA=0:00:12
[07/29 13:59:38] d2.evaluation.evaluator INFO: Inference done 10006/10080. Dataloading: 0.0009 s/iter. Inference: 0.1022 s/iter. Eval: 0.0052 s/iter. Total: 0.1083 s/iter. ETA=0:00:08
[07/29 13:59:43] d2.evaluation.evaluator INFO: Inference done 10051/10080. Dataloading: 0.0009 s/iter. Inference: 0.1022 s/iter. Eval: 0.0052 s/iter. Total: 0.1083 s/iter. ETA=0:00:03
[07/29 13:59:47] d2.evaluation.evaluator INFO: Total inference time: 0:18:11.424471 (0.108330 s / iter per device, on 1 devices)
[07/29 13:59:47] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:17:09 (0.102173 s / iter per device, on 1 devices)
[07/29 13:59:47] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[07/29 13:59:47] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[07/29 13:59:47] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[07/29 13:59:52] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 5.15 seconds.
[07/29 13:59:52] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[07/29 13:59:52] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.44 seconds.
[07/29 13:59:52] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |
|:------:|:------:|:------:|:-----:|:------:|:------:|
| 55.258 | 69.698 | 61.468 | 4.881 | 46.935 | 57.285 |
[07/29 13:59:52] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP     | category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|:-----------|:-------|
| 1          | 38.482 | 2          | 26.050 | 3          | 47.880 |
| 4          | 36.818 | 5          | 44.357 | 6          | 59.579 |
| 7          | 7.143  | 8          | 27.833 | 9          | 68.644 |
| 10         | 63.956 | 11         | 68.603 | 12         | 72.789 |
| 13         | 52.377 | 14         | 60.804 | 15         | 60.209 |
| 16         | 58.717 | 17         | 57.914 | 18         | 80.285 |
| 19         | 55.633 | 20         | 49.766 | 21         | 47.284 |
| 22         | 48.438 | 23         | 72.503 | 24         | 77.795 |
| 25         | 71.230 | 26         | 68.959 | 27         | 54.030 |
| 28         | 77.989 | 29         | 69.258 | 30         | 32.399 |
[07/29 13:59:55] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *segm*
[07/29 14:00:03] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 7.81 seconds.
[07/29 14:00:03] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[07/29 14:00:04] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.46 seconds.
[07/29 14:00:04] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |
|:------:|:------:|:------:|:-----:|:------:|:------:|
| 46.487 | 65.282 | 52.656 | 0.418 | 33.923 | 53.211 |
[07/29 14:00:04] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|:-----------|:-------|
| 1          | 34.942 | 2          | 25.031 | 3          | 43.868 |
| 4          | 33.045 | 5          | 21.192 | 6          | 39.738 |
| 7          | 2.841  | 8          | 18.317 | 9          | 62.045 |
| 10         | 57.921 | 11         | 62.270 | 12         | 62.836 |
| 13         | 45.017 | 14         | 56.961 | 15         | 55.948 |
| 16         | 54.797 | 17         | 52.857 | 18         | 72.432 |
| 19         | 48.595 | 20         | 40.230 | 21         | 42.155 |
| 22         | 42.177 | 23         | 63.630 | 24         | 72.867 |
| 25         | 64.031 | 26         | 55.530 | 27         | 37.919 |
| 28         | 61.242 | 29         | 48.485 | 30         | 15.695 |
[07/29 14:00:05] d2.evaluation.testing INFO: copypaste: Task: bbox
[07/29 14:00:05] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/29 14:00:05] d2.evaluation.testing INFO: copypaste: 55.2575,69.6979,61.4681,4.8812,46.9352,57.2855
[07/29 14:00:05] d2.evaluation.testing INFO: copypaste: Task: segm
[07/29 14:00:05] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/29 14:00:05] d2.evaluation.testing INFO: copypaste: 46.4872,65.2819,52.6565,0.4180,33.9229,53.2106
[07/29 14:00:05] d2.utils.events INFO:  eta: 10:02:39  iter: 9999  total_loss: 2.765  loss_cls_stage0: 0.2541  loss_box_reg_stage0: 0.2456  loss_cls_stage1: 0.2684  loss_box_reg_stage1: 0.6735  loss_cls_stage2: 0.265  loss_box_reg_stage2: 0.9592  loss_mask: 0.08532  loss_rpn_cls: 0.0219  loss_rpn_loc: 0.05654  time: 0.6264  data_time: 0.0047  lr: 0.00016  max_mem: 19303M
[07/29 14:00:17] d2.utils.events INFO:  eta: 10:02:24  iter: 10019  total_loss: 3.263  loss_cls_stage0: 0.3043  loss_box_reg_stage0: 0.3014  loss_cls_stage1: 0.3291  loss_box_reg_stage1: 0.7857  loss_cls_stage2: 0.3043  loss_box_reg_stage2: 0.9907  loss_mask: 0.09572  loss_rpn_cls: 0.02244  loss_rpn_loc: 0.06069  time: 0.6263  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 14:00:29] d2.utils.events INFO:  eta: 10:02:10  iter: 10039  total_loss: 3.059  loss_cls_stage0: 0.2896  loss_box_reg_stage0: 0.2862  loss_cls_stage1: 0.3661  loss_box_reg_stage1: 0.7181  loss_cls_stage2: 0.3511  loss_box_reg_stage2: 0.8985  loss_mask: 0.0865  loss_rpn_cls: 0.01673  loss_rpn_loc: 0.04366  time: 0.6263  data_time: 0.0047  lr: 0.00016  max_mem: 19303M
[07/29 14:00:42] d2.utils.events INFO:  eta: 10:02:13  iter: 10059  total_loss: 3.375  loss_cls_stage0: 0.3679  loss_box_reg_stage0: 0.3518  loss_cls_stage1: 0.3763  loss_box_reg_stage1: 0.7662  loss_cls_stage2: 0.362  loss_box_reg_stage2: 0.9963  loss_mask: 0.1053  loss_rpn_cls: 0.02053  loss_rpn_loc: 0.04578  time: 0.6263  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 14:00:54] d2.utils.events INFO:  eta: 10:01:47  iter: 10079  total_loss: 3.223  loss_cls_stage0: 0.2999  loss_box_reg_stage0: 0.294  loss_cls_stage1: 0.3561  loss_box_reg_stage1: 0.8113  loss_cls_stage2: 0.3013  loss_box_reg_stage2: 0.9509  loss_mask: 0.08978  loss_rpn_cls: 0.01398  loss_rpn_loc: 0.05  time: 0.6262  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 14:01:07] d2.utils.events INFO:  eta: 10:01:48  iter: 10099  total_loss: 2.941  loss_cls_stage0: 0.2778  loss_box_reg_stage0: 0.2571  loss_cls_stage1: 0.3182  loss_box_reg_stage1: 0.6915  loss_cls_stage2: 0.3106  loss_box_reg_stage2: 0.9162  loss_mask: 0.08177  loss_rpn_cls: 0.01316  loss_rpn_loc: 0.05603  time: 0.6262  data_time: 0.0050  lr: 0.00016  max_mem: 19303M
[07/29 14:01:20] d2.utils.events INFO:  eta: 10:01:56  iter: 10119  total_loss: 3.453  loss_cls_stage0: 0.3373  loss_box_reg_stage0: 0.3072  loss_cls_stage1: 0.3756  loss_box_reg_stage1: 0.7998  loss_cls_stage2: 0.3571  loss_box_reg_stage2: 1.109  loss_mask: 0.1037  loss_rpn_cls: 0.01545  loss_rpn_loc: 0.05166  time: 0.6263  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 14:01:32] d2.utils.events INFO:  eta: 10:01:51  iter: 10139  total_loss: 3.369  loss_cls_stage0: 0.3187  loss_box_reg_stage0: 0.3132  loss_cls_stage1: 0.3189  loss_box_reg_stage1: 0.774  loss_cls_stage2: 0.3187  loss_box_reg_stage2: 1.068  loss_mask: 0.09977  loss_rpn_cls: 0.0189  loss_rpn_loc: 0.049  time: 0.6263  data_time: 0.0055  lr: 0.00016  max_mem: 19303M
[07/29 14:01:45] d2.utils.events INFO:  eta: 10:02:11  iter: 10159  total_loss: 3.417  loss_cls_stage0: 0.3015  loss_box_reg_stage0: 0.2911  loss_cls_stage1: 0.3401  loss_box_reg_stage1: 0.793  loss_cls_stage2: 0.3622  loss_box_reg_stage2: 1.046  loss_mask: 0.09221  loss_rpn_cls: 0.01605  loss_rpn_loc: 0.04729  time: 0.6263  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 14:01:57] d2.utils.events INFO:  eta: 10:02:49  iter: 10179  total_loss: 3.131  loss_cls_stage0: 0.2825  loss_box_reg_stage0: 0.2779  loss_cls_stage1: 0.3231  loss_box_reg_stage1: 0.6972  loss_cls_stage2: 0.3125  loss_box_reg_stage2: 0.9688  loss_mask: 0.09545  loss_rpn_cls: 0.02225  loss_rpn_loc: 0.05219  time: 0.6263  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 14:02:10] d2.utils.events INFO:  eta: 10:02:15  iter: 10199  total_loss: 2.932  loss_cls_stage0: 0.2951  loss_box_reg_stage0: 0.2703  loss_cls_stage1: 0.3092  loss_box_reg_stage1: 0.7219  loss_cls_stage2: 0.306  loss_box_reg_stage2: 0.9383  loss_mask: 0.08936  loss_rpn_cls: 0.02344  loss_rpn_loc: 0.04567  time: 0.6262  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 14:02:22] d2.utils.events INFO:  eta: 10:01:48  iter: 10219  total_loss: 3.457  loss_cls_stage0: 0.3183  loss_box_reg_stage0: 0.3058  loss_cls_stage1: 0.3606  loss_box_reg_stage1: 0.8045  loss_cls_stage2: 0.3339  loss_box_reg_stage2: 1.035  loss_mask: 0.09607  loss_rpn_cls: 0.01606  loss_rpn_loc: 0.05465  time: 0.6262  data_time: 0.0046  lr: 0.00016  max_mem: 19303M
[07/29 14:02:34] d2.utils.events INFO:  eta: 10:01:35  iter: 10239  total_loss: 2.982  loss_cls_stage0: 0.288  loss_box_reg_stage0: 0.2914  loss_cls_stage1: 0.3101  loss_box_reg_stage1: 0.7384  loss_cls_stage2: 0.3055  loss_box_reg_stage2: 0.9446  loss_mask: 0.08649  loss_rpn_cls: 0.01443  loss_rpn_loc: 0.04354  time: 0.6262  data_time: 0.0045  lr: 0.00016  max_mem: 19303M
[07/29 14:02:47] d2.utils.events INFO:  eta: 10:00:37  iter: 10259  total_loss: 3.42  loss_cls_stage0: 0.3027  loss_box_reg_stage0: 0.2896  loss_cls_stage1: 0.3401  loss_box_reg_stage1: 0.7868  loss_cls_stage2: 0.347  loss_box_reg_stage2: 1.035  loss_mask: 0.09878  loss_rpn_cls: 0.0134  loss_rpn_loc: 0.04894  time: 0.6261  data_time: 0.0047  lr: 0.00016  max_mem: 19303M
[07/29 14:02:59] d2.utils.events INFO:  eta: 10:00:55  iter: 10279  total_loss: 3.201  loss_cls_stage0: 0.3215  loss_box_reg_stage0: 0.294  loss_cls_stage1: 0.3667  loss_box_reg_stage1: 0.7623  loss_cls_stage2: 0.3799  loss_box_reg_stage2: 0.9974  loss_mask: 0.09463  loss_rpn_cls: 0.01699  loss_rpn_loc: 0.04633  time: 0.6262  data_time: 0.0052  lr: 0.00016  max_mem: 19303M
[07/29 14:03:12] d2.utils.events INFO:  eta: 10:00:58  iter: 10299  total_loss: 3.147  loss_cls_stage0: 0.2775  loss_box_reg_stage0: 0.2795  loss_cls_stage1: 0.3307  loss_box_reg_stage1: 0.7401  loss_cls_stage2: 0.3139  loss_box_reg_stage2: 0.9926  loss_mask: 0.0972  loss_rpn_cls: 0.01805  loss_rpn_loc: 0.04949  time: 0.6262  data_time: 0.0051  lr: 0.00016  max_mem: 19303M
[07/29 14:03:25] d2.utils.events INFO:  eta: 10:01:34  iter: 10319  total_loss: 3.113  loss_cls_stage0: 0.2778  loss_box_reg_stage0: 0.2692  loss_cls_stage1: 0.3053  loss_box_reg_stage1: 0.7127  loss_cls_stage2: 0.3091  loss_box_reg_stage2: 0.9728  loss_mask: 0.08193  loss_rpn_cls: 0.01574  loss_rpn_loc: 0.05728  time: 0.6263  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 14:03:38] d2.utils.events INFO:  eta: 10:01:54  iter: 10339  total_loss: 3.217  loss_cls_stage0: 0.3018  loss_box_reg_stage0: 0.2997  loss_cls_stage1: 0.3422  loss_box_reg_stage1: 0.7683  loss_cls_stage2: 0.2977  loss_box_reg_stage2: 1.017  loss_mask: 0.08598  loss_rpn_cls: 0.01517  loss_rpn_loc: 0.0459  time: 0.6263  data_time: 0.0051  lr: 0.00016  max_mem: 19303M
[07/29 14:03:51] d2.utils.events INFO:  eta: 10:02:03  iter: 10359  total_loss: 3.732  loss_cls_stage0: 0.3349  loss_box_reg_stage0: 0.316  loss_cls_stage1: 0.3799  loss_box_reg_stage1: 0.9111  loss_cls_stage2: 0.383  loss_box_reg_stage2: 1.191  loss_mask: 0.09767  loss_rpn_cls: 0.02498  loss_rpn_loc: 0.07671  time: 0.6264  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 14:04:03] d2.utils.events INFO:  eta: 10:01:48  iter: 10379  total_loss: 3.249  loss_cls_stage0: 0.2895  loss_box_reg_stage0: 0.3117  loss_cls_stage1: 0.3572  loss_box_reg_stage1: 0.81  loss_cls_stage2: 0.3203  loss_box_reg_stage2: 1.004  loss_mask: 0.09437  loss_rpn_cls: 0.02012  loss_rpn_loc: 0.04642  time: 0.6263  data_time: 0.0050  lr: 0.00016  max_mem: 19303M
[07/29 14:04:16] d2.utils.events INFO:  eta: 10:01:45  iter: 10399  total_loss: 3.102  loss_cls_stage0: 0.2801  loss_box_reg_stage0: 0.2733  loss_cls_stage1: 0.3145  loss_box_reg_stage1: 0.774  loss_cls_stage2: 0.3202  loss_box_reg_stage2: 1.024  loss_mask: 0.08196  loss_rpn_cls: 0.01079  loss_rpn_loc: 0.04887  time: 0.6264  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 14:04:28] d2.utils.events INFO:  eta: 10:01:27  iter: 10419  total_loss: 3.215  loss_cls_stage0: 0.313  loss_box_reg_stage0: 0.2839  loss_cls_stage1: 0.3364  loss_box_reg_stage1: 0.7886  loss_cls_stage2: 0.3412  loss_box_reg_stage2: 1.087  loss_mask: 0.08568  loss_rpn_cls: 0.01643  loss_rpn_loc: 0.04916  time: 0.6263  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 14:04:40] d2.utils.events INFO:  eta: 10:01:01  iter: 10439  total_loss: 3.218  loss_cls_stage0: 0.3126  loss_box_reg_stage0: 0.2899  loss_cls_stage1: 0.3487  loss_box_reg_stage1: 0.7247  loss_cls_stage2: 0.3382  loss_box_reg_stage2: 1.005  loss_mask: 0.08634  loss_rpn_cls: 0.01626  loss_rpn_loc: 0.05977  time: 0.6263  data_time: 0.0047  lr: 0.00016  max_mem: 19303M
[07/29 14:04:53] d2.utils.events INFO:  eta: 10:00:24  iter: 10459  total_loss: 3.252  loss_cls_stage0: 0.3017  loss_box_reg_stage0: 0.2885  loss_cls_stage1: 0.3484  loss_box_reg_stage1: 0.731  loss_cls_stage2: 0.3526  loss_box_reg_stage2: 1.004  loss_mask: 0.08554  loss_rpn_cls: 0.01466  loss_rpn_loc: 0.04403  time: 0.6263  data_time: 0.0048  lr: 0.00016  max_mem: 19303M
[07/29 14:05:06] d2.utils.events INFO:  eta: 10:01:02  iter: 10479  total_loss: 3.241  loss_cls_stage0: 0.3166  loss_box_reg_stage0: 0.2902  loss_cls_stage1: 0.3459  loss_box_reg_stage1: 0.7728  loss_cls_stage2: 0.3157  loss_box_reg_stage2: 1.052  loss_mask: 0.08509  loss_rpn_cls: 0.02151  loss_rpn_loc: 0.04803  time: 0.6264  data_time: 0.0050  lr: 0.00016  max_mem: 19303M
[07/29 14:05:19] d2.utils.events INFO:  eta: 10:00:49  iter: 10499  total_loss: 3.359  loss_cls_stage0: 0.3224  loss_box_reg_stage0: 0.3101  loss_cls_stage1: 0.3449  loss_box_reg_stage1: 0.8129  loss_cls_stage2: 0.3411  loss_box_reg_stage2: 1.052  loss_mask: 0.09229  loss_rpn_cls: 0.01154  loss_rpn_loc: 0.04546  time: 0.6264  data_time: 0.0052  lr: 0.00016  max_mem: 19303M
[07/29 14:05:31] d2.utils.events INFO:  eta: 10:00:30  iter: 10519  total_loss: 3.386  loss_cls_stage0: 0.2989  loss_box_reg_stage0: 0.2841  loss_cls_stage1: 0.3478  loss_box_reg_stage1: 0.7385  loss_cls_stage2: 0.34  loss_box_reg_stage2: 1.107  loss_mask: 0.0956  loss_rpn_cls: 0.01702  loss_rpn_loc: 0.04848  time: 0.6264  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 14:05:44] d2.utils.events INFO:  eta: 10:00:22  iter: 10539  total_loss: 3.197  loss_cls_stage0: 0.2903  loss_box_reg_stage0: 0.2931  loss_cls_stage1: 0.3402  loss_box_reg_stage1: 0.785  loss_cls_stage2: 0.3276  loss_box_reg_stage2: 0.9823  loss_mask: 0.08378  loss_rpn_cls: 0.01738  loss_rpn_loc: 0.05003  time: 0.6264  data_time: 0.0053  lr: 0.00016  max_mem: 19303M
[07/29 14:05:56] d2.utils.events INFO:  eta: 10:00:10  iter: 10559  total_loss: 2.847  loss_cls_stage0: 0.2703  loss_box_reg_stage0: 0.2632  loss_cls_stage1: 0.3054  loss_box_reg_stage1: 0.6659  loss_cls_stage2: 0.2645  loss_box_reg_stage2: 0.8263  loss_mask: 0.08285  loss_rpn_cls: 0.0328  loss_rpn_loc: 0.05385  time: 0.6264  data_time: 0.0050  lr: 0.00016  max_mem: 19303M
[07/29 14:06:09] d2.utils.events INFO:  eta: 9:59:57  iter: 10579  total_loss: 3.147  loss_cls_stage0: 0.2931  loss_box_reg_stage0: 0.2736  loss_cls_stage1: 0.3135  loss_box_reg_stage1: 0.6966  loss_cls_stage2: 0.2936  loss_box_reg_stage2: 0.9828  loss_mask: 0.08288  loss_rpn_cls: 0.01505  loss_rpn_loc: 0.04771  time: 0.6263  data_time: 0.0050  lr: 0.00016  max_mem: 19303M
[07/29 14:06:21] d2.utils.events INFO:  eta: 9:59:33  iter: 10599  total_loss: 2.922  loss_cls_stage0: 0.2909  loss_box_reg_stage0: 0.2694  loss_cls_stage1: 0.2948  loss_box_reg_stage1: 0.6829  loss_cls_stage2: 0.2966  loss_box_reg_stage2: 0.8699  loss_mask: 0.08331  loss_rpn_cls: 0.02155  loss_rpn_loc: 0.05457  time: 0.6263  data_time: 0.0045  lr: 0.00016  max_mem: 19303M
[07/29 14:06:33] d2.utils.events INFO:  eta: 9:59:07  iter: 10619  total_loss: 3.184  loss_cls_stage0: 0.3014  loss_box_reg_stage0: 0.281  loss_cls_stage1: 0.3319  loss_box_reg_stage1: 0.7466  loss_cls_stage2: 0.3262  loss_box_reg_stage2: 1.032  loss_mask: 0.08684  loss_rpn_cls: 0.01734  loss_rpn_loc: 0.0488  time: 0.6263  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 14:06:46] d2.utils.events INFO:  eta: 9:58:55  iter: 10639  total_loss: 2.996  loss_cls_stage0: 0.3019  loss_box_reg_stage0: 0.2921  loss_cls_stage1: 0.3182  loss_box_reg_stage1: 0.6604  loss_cls_stage2: 0.3263  loss_box_reg_stage2: 0.8637  loss_mask: 0.08862  loss_rpn_cls: 0.01853  loss_rpn_loc: 0.05595  time: 0.6263  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 14:06:58] d2.utils.events INFO:  eta: 9:58:54  iter: 10659  total_loss: 3.101  loss_cls_stage0: 0.2868  loss_box_reg_stage0: 0.2766  loss_cls_stage1: 0.3258  loss_box_reg_stage1: 0.7535  loss_cls_stage2: 0.334  loss_box_reg_stage2: 1.056  loss_mask: 0.08845  loss_rpn_cls: 0.02678  loss_rpn_loc: 0.04455  time: 0.6263  data_time: 0.0049  lr: 0.00016  max_mem: 19303M
[07/29 14:07:11] d2.utils.events INFO:  eta: 9:58:39  iter: 10679  total_loss: 2.795  loss_cls_stage0: 0.2814  loss_box_reg_stage0: 0.2866  loss_cls_stage1: 0.287  loss_box_reg_stage1: 0.6481  loss_cls_stage2: 0.2862  loss_box_reg_stage2: 0.8589  loss_mask: 0.09064  loss_rpn_cls: 0.02475  loss_rpn_loc: 0.05816  time: 0.6262  data_time: 0.0047  lr: 0.00016  max_mem: 19303M
[07/29 14:07:23] d2.utils.events INFO:  eta: 9:58:26  iter: 10699  total_loss: 2.866  loss_cls_stage0: 0.2639  loss_box_reg_stage0: 0.2705  loss_cls_stage1: 0.2835  loss_box_reg_stage1: 0.7033  loss_cls_stage2: 0.2841  loss_box_reg_stage2: 0.9233  loss_mask: 0.08427  loss_rpn_cls: 0.01783  loss_rpn_loc: 0.06658  time: 0.6262  data_time: 0.0047  lr: 0.00016  max_mem: 19303M
[07/29 14:07:36] d2.utils.events INFO:  eta: 9:58:16  iter: 10719  total_loss: 2.99  loss_cls_stage0: 0.2719  loss_box_reg_stage0: 0.2651  loss_cls_stage1: 0.2957  loss_box_reg_stage1: 0.7015  loss_cls_stage2: 0.2975  loss_box_reg_stage2: 0.9737  loss_mask: 0.07921  loss_rpn_cls: 0.01303  loss_rpn_loc: 0.03953  time: 0.6263  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:07:48] d2.utils.events INFO:  eta: 9:58:10  iter: 10739  total_loss: 3.132  loss_cls_stage0: 0.3024  loss_box_reg_stage0: 0.2856  loss_cls_stage1: 0.3587  loss_box_reg_stage1: 0.7555  loss_cls_stage2: 0.3339  loss_box_reg_stage2: 1.076  loss_mask: 0.08777  loss_rpn_cls: 0.01604  loss_rpn_loc: 0.05438  time: 0.6263  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:08:01] d2.utils.events INFO:  eta: 9:57:57  iter: 10759  total_loss: 3.006  loss_cls_stage0: 0.2941  loss_box_reg_stage0: 0.2747  loss_cls_stage1: 0.3115  loss_box_reg_stage1: 0.7251  loss_cls_stage2: 0.2865  loss_box_reg_stage2: 0.9083  loss_mask: 0.08684  loss_rpn_cls: 0.02147  loss_rpn_loc: 0.04923  time: 0.6263  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:08:14] d2.utils.events INFO:  eta: 9:57:44  iter: 10779  total_loss: 3.042  loss_cls_stage0: 0.3155  loss_box_reg_stage0: 0.2958  loss_cls_stage1: 0.3224  loss_box_reg_stage1: 0.6864  loss_cls_stage2: 0.3173  loss_box_reg_stage2: 0.8492  loss_mask: 0.09739  loss_rpn_cls: 0.02573  loss_rpn_loc: 0.05504  time: 0.6263  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:08:26] d2.utils.events INFO:  eta: 9:57:23  iter: 10799  total_loss: 2.967  loss_cls_stage0: 0.3059  loss_box_reg_stage0: 0.2899  loss_cls_stage1: 0.2917  loss_box_reg_stage1: 0.6633  loss_cls_stage2: 0.2636  loss_box_reg_stage2: 0.8936  loss_mask: 0.08632  loss_rpn_cls: 0.02184  loss_rpn_loc: 0.0466  time: 0.6262  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:08:38] d2.utils.events INFO:  eta: 9:55:52  iter: 10819  total_loss: 3.143  loss_cls_stage0: 0.3185  loss_box_reg_stage0: 0.3034  loss_cls_stage1: 0.3125  loss_box_reg_stage1: 0.7202  loss_cls_stage2: 0.2985  loss_box_reg_stage2: 0.9293  loss_mask: 0.09173  loss_rpn_cls: 0.01749  loss_rpn_loc: 0.04977  time: 0.6261  data_time: 0.0045  lr: 0.00016  max_mem: 19679M
[07/29 14:08:50] d2.utils.events INFO:  eta: 9:55:01  iter: 10839  total_loss: 2.92  loss_cls_stage0: 0.2608  loss_box_reg_stage0: 0.2686  loss_cls_stage1: 0.2805  loss_box_reg_stage1: 0.7072  loss_cls_stage2: 0.281  loss_box_reg_stage2: 0.9631  loss_mask: 0.07588  loss_rpn_cls: 0.0151  loss_rpn_loc: 0.04799  time: 0.6261  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 14:09:03] d2.utils.events INFO:  eta: 9:55:11  iter: 10859  total_loss: 3.019  loss_cls_stage0: 0.3152  loss_box_reg_stage0: 0.2991  loss_cls_stage1: 0.3004  loss_box_reg_stage1: 0.7647  loss_cls_stage2: 0.2892  loss_box_reg_stage2: 0.967  loss_mask: 0.08863  loss_rpn_cls: 0.01816  loss_rpn_loc: 0.05504  time: 0.6261  data_time: 0.0043  lr: 0.00016  max_mem: 19679M
[07/29 14:09:15] d2.utils.events INFO:  eta: 9:54:46  iter: 10879  total_loss: 3.216  loss_cls_stage0: 0.2798  loss_box_reg_stage0: 0.2852  loss_cls_stage1: 0.2764  loss_box_reg_stage1: 0.792  loss_cls_stage2: 0.2881  loss_box_reg_stage2: 1.033  loss_mask: 0.08832  loss_rpn_cls: 0.01538  loss_rpn_loc: 0.05922  time: 0.6261  data_time: 0.0045  lr: 0.00016  max_mem: 19679M
[07/29 14:09:28] d2.utils.events INFO:  eta: 9:55:38  iter: 10899  total_loss: 3.238  loss_cls_stage0: 0.2978  loss_box_reg_stage0: 0.2725  loss_cls_stage1: 0.343  loss_box_reg_stage1: 0.7651  loss_cls_stage2: 0.3277  loss_box_reg_stage2: 1.097  loss_mask: 0.08813  loss_rpn_cls: 0.01282  loss_rpn_loc: 0.04991  time: 0.6261  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:09:40] d2.utils.events INFO:  eta: 9:54:48  iter: 10919  total_loss: 3.296  loss_cls_stage0: 0.2922  loss_box_reg_stage0: 0.2777  loss_cls_stage1: 0.3167  loss_box_reg_stage1: 0.783  loss_cls_stage2: 0.3435  loss_box_reg_stage2: 0.9823  loss_mask: 0.07882  loss_rpn_cls: 0.01397  loss_rpn_loc: 0.04763  time: 0.6261  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:09:53] d2.utils.events INFO:  eta: 9:54:00  iter: 10939  total_loss: 3.243  loss_cls_stage0: 0.3174  loss_box_reg_stage0: 0.288  loss_cls_stage1: 0.3348  loss_box_reg_stage1: 0.7691  loss_cls_stage2: 0.318  loss_box_reg_stage2: 1.023  loss_mask: 0.08728  loss_rpn_cls: 0.01331  loss_rpn_loc: 0.04556  time: 0.6261  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:10:05] d2.utils.events INFO:  eta: 9:53:44  iter: 10959  total_loss: 3.043  loss_cls_stage0: 0.2826  loss_box_reg_stage0: 0.2797  loss_cls_stage1: 0.3286  loss_box_reg_stage1: 0.6829  loss_cls_stage2: 0.3112  loss_box_reg_stage2: 0.9126  loss_mask: 0.09602  loss_rpn_cls: 0.02884  loss_rpn_loc: 0.1203  time: 0.6261  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:10:18] d2.utils.events INFO:  eta: 9:53:33  iter: 10979  total_loss: 2.709  loss_cls_stage0: 0.2569  loss_box_reg_stage0: 0.2659  loss_cls_stage1: 0.2587  loss_box_reg_stage1: 0.6461  loss_cls_stage2: 0.2562  loss_box_reg_stage2: 0.8621  loss_mask: 0.07606  loss_rpn_cls: 0.02299  loss_rpn_loc: 0.04911  time: 0.6260  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:10:30] d2.utils.events INFO:  eta: 9:53:23  iter: 10999  total_loss: 3.054  loss_cls_stage0: 0.3009  loss_box_reg_stage0: 0.2789  loss_cls_stage1: 0.3446  loss_box_reg_stage1: 0.6782  loss_cls_stage2: 0.3094  loss_box_reg_stage2: 0.9039  loss_mask: 0.08294  loss_rpn_cls: 0.02218  loss_rpn_loc: 0.04828  time: 0.6261  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:10:43] d2.utils.events INFO:  eta: 9:53:45  iter: 11019  total_loss: 2.863  loss_cls_stage0: 0.2679  loss_box_reg_stage0: 0.2778  loss_cls_stage1: 0.2887  loss_box_reg_stage1: 0.7059  loss_cls_stage2: 0.2669  loss_box_reg_stage2: 0.9132  loss_mask: 0.08699  loss_rpn_cls: 0.01659  loss_rpn_loc: 0.04384  time: 0.6261  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:10:55] d2.utils.events INFO:  eta: 9:54:18  iter: 11039  total_loss: 2.804  loss_cls_stage0: 0.2779  loss_box_reg_stage0: 0.2919  loss_cls_stage1: 0.2925  loss_box_reg_stage1: 0.6856  loss_cls_stage2: 0.2811  loss_box_reg_stage2: 0.8584  loss_mask: 0.08841  loss_rpn_cls: 0.01849  loss_rpn_loc: 0.04042  time: 0.6260  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 14:11:08] d2.utils.events INFO:  eta: 9:53:57  iter: 11059  total_loss: 2.941  loss_cls_stage0: 0.2671  loss_box_reg_stage0: 0.2824  loss_cls_stage1: 0.2983  loss_box_reg_stage1: 0.6828  loss_cls_stage2: 0.3051  loss_box_reg_stage2: 0.9387  loss_mask: 0.08211  loss_rpn_cls: 0.01631  loss_rpn_loc: 0.05499  time: 0.6260  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 14:11:20] d2.utils.events INFO:  eta: 9:53:34  iter: 11079  total_loss: 3.645  loss_cls_stage0: 0.2914  loss_box_reg_stage0: 0.2974  loss_cls_stage1: 0.3717  loss_box_reg_stage1: 0.8951  loss_cls_stage2: 0.3801  loss_box_reg_stage2: 1.234  loss_mask: 0.08606  loss_rpn_cls: 0.01297  loss_rpn_loc: 0.04827  time: 0.6259  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 14:11:32] d2.utils.events INFO:  eta: 9:53:10  iter: 11099  total_loss: 3.399  loss_cls_stage0: 0.3074  loss_box_reg_stage0: 0.3079  loss_cls_stage1: 0.3423  loss_box_reg_stage1: 0.8279  loss_cls_stage2: 0.3246  loss_box_reg_stage2: 1.112  loss_mask: 0.09674  loss_rpn_cls: 0.01494  loss_rpn_loc: 0.05952  time: 0.6260  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:11:45] d2.utils.events INFO:  eta: 9:52:50  iter: 11119  total_loss: 3.131  loss_cls_stage0: 0.2794  loss_box_reg_stage0: 0.2736  loss_cls_stage1: 0.3026  loss_box_reg_stage1: 0.7241  loss_cls_stage2: 0.2956  loss_box_reg_stage2: 1.002  loss_mask: 0.07872  loss_rpn_cls: 0.009513  loss_rpn_loc: 0.0564  time: 0.6260  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:11:58] d2.utils.events INFO:  eta: 9:52:13  iter: 11139  total_loss: 3.311  loss_cls_stage0: 0.282  loss_box_reg_stage0: 0.2677  loss_cls_stage1: 0.3417  loss_box_reg_stage1: 0.7849  loss_cls_stage2: 0.3384  loss_box_reg_stage2: 0.9659  loss_mask: 0.07863  loss_rpn_cls: 0.0199  loss_rpn_loc: 0.06423  time: 0.6260  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:12:10] d2.utils.events INFO:  eta: 9:51:52  iter: 11159  total_loss: 2.974  loss_cls_stage0: 0.2924  loss_box_reg_stage0: 0.2913  loss_cls_stage1: 0.3039  loss_box_reg_stage1: 0.7286  loss_cls_stage2: 0.3026  loss_box_reg_stage2: 0.8841  loss_mask: 0.07828  loss_rpn_cls: 0.01539  loss_rpn_loc: 0.04787  time: 0.6260  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:12:22] d2.utils.events INFO:  eta: 9:51:29  iter: 11179  total_loss: 3.187  loss_cls_stage0: 0.2694  loss_box_reg_stage0: 0.2616  loss_cls_stage1: 0.2944  loss_box_reg_stage1: 0.774  loss_cls_stage2: 0.2682  loss_box_reg_stage2: 1.008  loss_mask: 0.0821  loss_rpn_cls: 0.0186  loss_rpn_loc: 0.05898  time: 0.6260  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:12:35] d2.utils.events INFO:  eta: 9:52:04  iter: 11199  total_loss: 3.154  loss_cls_stage0: 0.3033  loss_box_reg_stage0: 0.2889  loss_cls_stage1: 0.2956  loss_box_reg_stage1: 0.7118  loss_cls_stage2: 0.3106  loss_box_reg_stage2: 0.9549  loss_mask: 0.08481  loss_rpn_cls: 0.01345  loss_rpn_loc: 0.04684  time: 0.6260  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 14:12:48] d2.utils.events INFO:  eta: 9:52:16  iter: 11219  total_loss: 3.274  loss_cls_stage0: 0.2967  loss_box_reg_stage0: 0.2918  loss_cls_stage1: 0.3226  loss_box_reg_stage1: 0.7638  loss_cls_stage2: 0.3386  loss_box_reg_stage2: 1.019  loss_mask: 0.09266  loss_rpn_cls: 0.01528  loss_rpn_loc: 0.04952  time: 0.6260  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:13:00] d2.utils.events INFO:  eta: 9:51:27  iter: 11239  total_loss: 3.113  loss_cls_stage0: 0.2851  loss_box_reg_stage0: 0.2896  loss_cls_stage1: 0.2838  loss_box_reg_stage1: 0.7235  loss_cls_stage2: 0.2816  loss_box_reg_stage2: 1.066  loss_mask: 0.08857  loss_rpn_cls: 0.0178  loss_rpn_loc: 0.05826  time: 0.6260  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:13:12] d2.utils.events INFO:  eta: 9:50:35  iter: 11259  total_loss: 2.99  loss_cls_stage0: 0.2837  loss_box_reg_stage0: 0.274  loss_cls_stage1: 0.2808  loss_box_reg_stage1: 0.7157  loss_cls_stage2: 0.3122  loss_box_reg_stage2: 0.9572  loss_mask: 0.09308  loss_rpn_cls: 0.02229  loss_rpn_loc: 0.05268  time: 0.6258  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:13:24] d2.utils.events INFO:  eta: 9:50:01  iter: 11279  total_loss: 2.834  loss_cls_stage0: 0.2638  loss_box_reg_stage0: 0.2681  loss_cls_stage1: 0.2923  loss_box_reg_stage1: 0.6515  loss_cls_stage2: 0.2983  loss_box_reg_stage2: 0.8761  loss_mask: 0.07324  loss_rpn_cls: 0.01695  loss_rpn_loc: 0.0531  time: 0.6258  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:13:36] d2.utils.events INFO:  eta: 9:49:14  iter: 11299  total_loss: 3.179  loss_cls_stage0: 0.3099  loss_box_reg_stage0: 0.2912  loss_cls_stage1: 0.3532  loss_box_reg_stage1: 0.7233  loss_cls_stage2: 0.3422  loss_box_reg_stage2: 1.001  loss_mask: 0.08381  loss_rpn_cls: 0.01432  loss_rpn_loc: 0.04371  time: 0.6257  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:13:49] d2.utils.events INFO:  eta: 9:48:51  iter: 11319  total_loss: 3.156  loss_cls_stage0: 0.2798  loss_box_reg_stage0: 0.28  loss_cls_stage1: 0.3063  loss_box_reg_stage1: 0.73  loss_cls_stage2: 0.3035  loss_box_reg_stage2: 0.9625  loss_mask: 0.0793  loss_rpn_cls: 0.0129  loss_rpn_loc: 0.04602  time: 0.6257  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:14:01] d2.utils.events INFO:  eta: 9:48:36  iter: 11339  total_loss: 2.876  loss_cls_stage0: 0.2836  loss_box_reg_stage0: 0.267  loss_cls_stage1: 0.2989  loss_box_reg_stage1: 0.6693  loss_cls_stage2: 0.2888  loss_box_reg_stage2: 0.8779  loss_mask: 0.07673  loss_rpn_cls: 0.01556  loss_rpn_loc: 0.05229  time: 0.6257  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:14:14] d2.utils.events INFO:  eta: 9:48:16  iter: 11359  total_loss: 2.972  loss_cls_stage0: 0.2694  loss_box_reg_stage0: 0.2765  loss_cls_stage1: 0.279  loss_box_reg_stage1: 0.7034  loss_cls_stage2: 0.2783  loss_box_reg_stage2: 0.977  loss_mask: 0.08109  loss_rpn_cls: 0.01575  loss_rpn_loc: 0.04676  time: 0.6257  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:14:26] d2.utils.events INFO:  eta: 9:47:38  iter: 11379  total_loss: 3.39  loss_cls_stage0: 0.3466  loss_box_reg_stage0: 0.3024  loss_cls_stage1: 0.3845  loss_box_reg_stage1: 0.7748  loss_cls_stage2: 0.3867  loss_box_reg_stage2: 1.017  loss_mask: 0.09109  loss_rpn_cls: 0.01198  loss_rpn_loc: 0.04686  time: 0.6256  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:14:38] d2.utils.events INFO:  eta: 9:47:22  iter: 11399  total_loss: 3.013  loss_cls_stage0: 0.3093  loss_box_reg_stage0: 0.2821  loss_cls_stage1: 0.3231  loss_box_reg_stage1: 0.7211  loss_cls_stage2: 0.3096  loss_box_reg_stage2: 0.9534  loss_mask: 0.08604  loss_rpn_cls: 0.02064  loss_rpn_loc: 0.05465  time: 0.6256  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:14:51] d2.utils.events INFO:  eta: 9:47:15  iter: 11419  total_loss: 3.188  loss_cls_stage0: 0.3269  loss_box_reg_stage0: 0.3174  loss_cls_stage1: 0.345  loss_box_reg_stage1: 0.7514  loss_cls_stage2: 0.3405  loss_box_reg_stage2: 1.001  loss_mask: 0.09011  loss_rpn_cls: 0.01704  loss_rpn_loc: 0.04978  time: 0.6256  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:15:03] d2.utils.events INFO:  eta: 9:47:14  iter: 11439  total_loss: 3.011  loss_cls_stage0: 0.2856  loss_box_reg_stage0: 0.2793  loss_cls_stage1: 0.3001  loss_box_reg_stage1: 0.7082  loss_cls_stage2: 0.2901  loss_box_reg_stage2: 0.9006  loss_mask: 0.08732  loss_rpn_cls: 0.0164  loss_rpn_loc: 0.0423  time: 0.6256  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:15:16] d2.utils.events INFO:  eta: 9:46:50  iter: 11459  total_loss: 3.212  loss_cls_stage0: 0.2882  loss_box_reg_stage0: 0.2894  loss_cls_stage1: 0.3485  loss_box_reg_stage1: 0.719  loss_cls_stage2: 0.3381  loss_box_reg_stage2: 0.9549  loss_mask: 0.09386  loss_rpn_cls: 0.01867  loss_rpn_loc: 0.0547  time: 0.6256  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:15:29] d2.utils.events INFO:  eta: 9:46:23  iter: 11479  total_loss: 3.155  loss_cls_stage0: 0.2936  loss_box_reg_stage0: 0.2971  loss_cls_stage1: 0.318  loss_box_reg_stage1: 0.7956  loss_cls_stage2: 0.3098  loss_box_reg_stage2: 1.015  loss_mask: 0.09108  loss_rpn_cls: 0.01127  loss_rpn_loc: 0.05418  time: 0.6257  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:15:41] d2.utils.events INFO:  eta: 9:46:09  iter: 11499  total_loss: 3.496  loss_cls_stage0: 0.3418  loss_box_reg_stage0: 0.3085  loss_cls_stage1: 0.3516  loss_box_reg_stage1: 0.8021  loss_cls_stage2: 0.3328  loss_box_reg_stage2: 1.022  loss_mask: 0.09486  loss_rpn_cls: 0.0243  loss_rpn_loc: 0.05095  time: 0.6256  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:15:53] d2.utils.events INFO:  eta: 9:45:50  iter: 11519  total_loss: 2.974  loss_cls_stage0: 0.2645  loss_box_reg_stage0: 0.2651  loss_cls_stage1: 0.2983  loss_box_reg_stage1: 0.7199  loss_cls_stage2: 0.2894  loss_box_reg_stage2: 0.9862  loss_mask: 0.08285  loss_rpn_cls: 0.01822  loss_rpn_loc: 0.04438  time: 0.6256  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:16:06] d2.utils.events INFO:  eta: 9:45:31  iter: 11539  total_loss: 3.092  loss_cls_stage0: 0.3058  loss_box_reg_stage0: 0.2831  loss_cls_stage1: 0.3674  loss_box_reg_stage1: 0.7067  loss_cls_stage2: 0.3528  loss_box_reg_stage2: 0.9697  loss_mask: 0.08281  loss_rpn_cls: 0.01732  loss_rpn_loc: 0.06826  time: 0.6256  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:16:18] d2.utils.events INFO:  eta: 9:45:20  iter: 11559  total_loss: 3.253  loss_cls_stage0: 0.3254  loss_box_reg_stage0: 0.2981  loss_cls_stage1: 0.3411  loss_box_reg_stage1: 0.7735  loss_cls_stage2: 0.3419  loss_box_reg_stage2: 1.002  loss_mask: 0.09537  loss_rpn_cls: 0.01684  loss_rpn_loc: 0.04236  time: 0.6255  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 14:16:30] d2.utils.events INFO:  eta: 9:45:07  iter: 11579  total_loss: 2.948  loss_cls_stage0: 0.2914  loss_box_reg_stage0: 0.269  loss_cls_stage1: 0.3148  loss_box_reg_stage1: 0.7024  loss_cls_stage2: 0.2769  loss_box_reg_stage2: 0.9581  loss_mask: 0.09026  loss_rpn_cls: 0.0191  loss_rpn_loc: 0.04322  time: 0.6255  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 14:16:43] d2.utils.events INFO:  eta: 9:45:00  iter: 11599  total_loss: 2.871  loss_cls_stage0: 0.2509  loss_box_reg_stage0: 0.2549  loss_cls_stage1: 0.2713  loss_box_reg_stage1: 0.6749  loss_cls_stage2: 0.2653  loss_box_reg_stage2: 0.991  loss_mask: 0.0768  loss_rpn_cls: 0.01477  loss_rpn_loc: 0.05122  time: 0.6255  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:16:55] d2.utils.events INFO:  eta: 9:44:41  iter: 11619  total_loss: 2.896  loss_cls_stage0: 0.2641  loss_box_reg_stage0: 0.2581  loss_cls_stage1: 0.277  loss_box_reg_stage1: 0.7047  loss_cls_stage2: 0.2659  loss_box_reg_stage2: 0.9969  loss_mask: 0.08425  loss_rpn_cls: 0.01671  loss_rpn_loc: 0.05669  time: 0.6255  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:17:08] d2.utils.events INFO:  eta: 9:44:27  iter: 11639  total_loss: 2.971  loss_cls_stage0: 0.3068  loss_box_reg_stage0: 0.2792  loss_cls_stage1: 0.3362  loss_box_reg_stage1: 0.6851  loss_cls_stage2: 0.3194  loss_box_reg_stage2: 0.9077  loss_mask: 0.08711  loss_rpn_cls: 0.01723  loss_rpn_loc: 0.0603  time: 0.6254  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:17:20] d2.utils.events INFO:  eta: 9:44:00  iter: 11659  total_loss: 3.157  loss_cls_stage0: 0.3026  loss_box_reg_stage0: 0.2939  loss_cls_stage1: 0.3562  loss_box_reg_stage1: 0.7117  loss_cls_stage2: 0.3194  loss_box_reg_stage2: 0.94  loss_mask: 0.09189  loss_rpn_cls: 0.01435  loss_rpn_loc: 0.08153  time: 0.6254  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:17:32] d2.utils.events INFO:  eta: 9:43:57  iter: 11679  total_loss: 3.272  loss_cls_stage0: 0.3136  loss_box_reg_stage0: 0.2804  loss_cls_stage1: 0.3722  loss_box_reg_stage1: 0.801  loss_cls_stage2: 0.3648  loss_box_reg_stage2: 1.043  loss_mask: 0.09494  loss_rpn_cls: 0.01454  loss_rpn_loc: 0.05263  time: 0.6254  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:17:45] d2.utils.events INFO:  eta: 9:44:03  iter: 11699  total_loss: 2.944  loss_cls_stage0: 0.2976  loss_box_reg_stage0: 0.2912  loss_cls_stage1: 0.3474  loss_box_reg_stage1: 0.6873  loss_cls_stage2: 0.3234  loss_box_reg_stage2: 0.9203  loss_mask: 0.08704  loss_rpn_cls: 0.01291  loss_rpn_loc: 0.04214  time: 0.6255  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:17:58] d2.utils.events INFO:  eta: 9:43:41  iter: 11719  total_loss: 3.208  loss_cls_stage0: 0.317  loss_box_reg_stage0: 0.2938  loss_cls_stage1: 0.3523  loss_box_reg_stage1: 0.6915  loss_cls_stage2: 0.3405  loss_box_reg_stage2: 0.8996  loss_mask: 0.09078  loss_rpn_cls: 0.02126  loss_rpn_loc: 0.05791  time: 0.6255  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:18:11] d2.utils.events INFO:  eta: 9:43:38  iter: 11739  total_loss: 3.255  loss_cls_stage0: 0.297  loss_box_reg_stage0: 0.3082  loss_cls_stage1: 0.3175  loss_box_reg_stage1: 0.7943  loss_cls_stage2: 0.315  loss_box_reg_stage2: 1.043  loss_mask: 0.08487  loss_rpn_cls: 0.01728  loss_rpn_loc: 0.05137  time: 0.6256  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:18:23] d2.utils.events INFO:  eta: 9:43:28  iter: 11759  total_loss: 3.158  loss_cls_stage0: 0.2916  loss_box_reg_stage0: 0.2906  loss_cls_stage1: 0.3147  loss_box_reg_stage1: 0.7765  loss_cls_stage2: 0.307  loss_box_reg_stage2: 0.9859  loss_mask: 0.08824  loss_rpn_cls: 0.01843  loss_rpn_loc: 0.05298  time: 0.6256  data_time: 0.0045  lr: 0.00016  max_mem: 19679M
[07/29 14:18:36] d2.utils.events INFO:  eta: 9:43:01  iter: 11779  total_loss: 3.176  loss_cls_stage0: 0.2997  loss_box_reg_stage0: 0.2896  loss_cls_stage1: 0.2983  loss_box_reg_stage1: 0.7272  loss_cls_stage2: 0.2957  loss_box_reg_stage2: 0.9721  loss_mask: 0.09363  loss_rpn_cls: 0.01792  loss_rpn_loc: 0.04785  time: 0.6255  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:18:48] d2.utils.events INFO:  eta: 9:43:00  iter: 11799  total_loss: 3.092  loss_cls_stage0: 0.3007  loss_box_reg_stage0: 0.3009  loss_cls_stage1: 0.2916  loss_box_reg_stage1: 0.7119  loss_cls_stage2: 0.2976  loss_box_reg_stage2: 0.9589  loss_mask: 0.09178  loss_rpn_cls: 0.01822  loss_rpn_loc: 0.04952  time: 0.6255  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:19:01] d2.utils.events INFO:  eta: 9:42:59  iter: 11819  total_loss: 2.989  loss_cls_stage0: 0.279  loss_box_reg_stage0: 0.2868  loss_cls_stage1: 0.2826  loss_box_reg_stage1: 0.6969  loss_cls_stage2: 0.2695  loss_box_reg_stage2: 0.9676  loss_mask: 0.09096  loss_rpn_cls: 0.01949  loss_rpn_loc: 0.06481  time: 0.6256  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:19:14] d2.utils.events INFO:  eta: 9:43:14  iter: 11839  total_loss: 2.807  loss_cls_stage0: 0.2694  loss_box_reg_stage0: 0.2702  loss_cls_stage1: 0.2714  loss_box_reg_stage1: 0.6698  loss_cls_stage2: 0.2442  loss_box_reg_stage2: 0.9146  loss_mask: 0.086  loss_rpn_cls: 0.01811  loss_rpn_loc: 0.04922  time: 0.6256  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:19:26] d2.utils.events INFO:  eta: 9:43:02  iter: 11859  total_loss: 3.021  loss_cls_stage0: 0.2871  loss_box_reg_stage0: 0.2723  loss_cls_stage1: 0.3015  loss_box_reg_stage1: 0.714  loss_cls_stage2: 0.2936  loss_box_reg_stage2: 0.9259  loss_mask: 0.08497  loss_rpn_cls: 0.01917  loss_rpn_loc: 0.04776  time: 0.6256  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:19:39] d2.utils.events INFO:  eta: 9:42:34  iter: 11879  total_loss: 2.909  loss_cls_stage0: 0.2671  loss_box_reg_stage0: 0.2719  loss_cls_stage1: 0.2958  loss_box_reg_stage1: 0.6936  loss_cls_stage2: 0.2919  loss_box_reg_stage2: 0.8874  loss_mask: 0.08118  loss_rpn_cls: 0.01893  loss_rpn_loc: 0.06884  time: 0.6256  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:19:51] d2.utils.events INFO:  eta: 9:42:14  iter: 11899  total_loss: 2.787  loss_cls_stage0: 0.252  loss_box_reg_stage0: 0.2761  loss_cls_stage1: 0.2859  loss_box_reg_stage1: 0.6622  loss_cls_stage2: 0.2763  loss_box_reg_stage2: 0.9181  loss_mask: 0.08293  loss_rpn_cls: 0.01541  loss_rpn_loc: 0.05112  time: 0.6256  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:20:04] d2.utils.events INFO:  eta: 9:42:32  iter: 11919  total_loss: 3.345  loss_cls_stage0: 0.3293  loss_box_reg_stage0: 0.3193  loss_cls_stage1: 0.3533  loss_box_reg_stage1: 0.7722  loss_cls_stage2: 0.3452  loss_box_reg_stage2: 1.027  loss_mask: 0.09603  loss_rpn_cls: 0.0176  loss_rpn_loc: 0.06548  time: 0.6256  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:20:17] d2.utils.events INFO:  eta: 9:42:41  iter: 11939  total_loss: 2.917  loss_cls_stage0: 0.2814  loss_box_reg_stage0: 0.2752  loss_cls_stage1: 0.3068  loss_box_reg_stage1: 0.6967  loss_cls_stage2: 0.2935  loss_box_reg_stage2: 0.9284  loss_mask: 0.08077  loss_rpn_cls: 0.01383  loss_rpn_loc: 0.04382  time: 0.6256  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:20:29] d2.utils.events INFO:  eta: 9:42:44  iter: 11959  total_loss: 3.12  loss_cls_stage0: 0.2608  loss_box_reg_stage0: 0.2593  loss_cls_stage1: 0.2673  loss_box_reg_stage1: 0.7338  loss_cls_stage2: 0.2753  loss_box_reg_stage2: 1.037  loss_mask: 0.08271  loss_rpn_cls: 0.008509  loss_rpn_loc: 0.03637  time: 0.6256  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:20:42] d2.utils.events INFO:  eta: 9:42:57  iter: 11979  total_loss: 3.089  loss_cls_stage0: 0.2861  loss_box_reg_stage0: 0.2812  loss_cls_stage1: 0.3142  loss_box_reg_stage1: 0.7252  loss_cls_stage2: 0.294  loss_box_reg_stage2: 0.9829  loss_mask: 0.0863  loss_rpn_cls: 0.01321  loss_rpn_loc: 0.0455  time: 0.6257  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 14:20:55] d2.utils.events INFO:  eta: 9:43:01  iter: 11999  total_loss: 3.268  loss_cls_stage0: 0.3053  loss_box_reg_stage0: 0.2871  loss_cls_stage1: 0.3462  loss_box_reg_stage1: 0.7725  loss_cls_stage2: 0.3199  loss_box_reg_stage2: 1.031  loss_mask: 0.08642  loss_rpn_cls: 0.02483  loss_rpn_loc: 0.06045  time: 0.6257  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:21:08] d2.utils.events INFO:  eta: 9:43:12  iter: 12019  total_loss: 2.901  loss_cls_stage0: 0.2748  loss_box_reg_stage0: 0.2798  loss_cls_stage1: 0.2982  loss_box_reg_stage1: 0.7054  loss_cls_stage2: 0.2669  loss_box_reg_stage2: 0.9083  loss_mask: 0.07428  loss_rpn_cls: 0.02129  loss_rpn_loc: 0.05081  time: 0.6257  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:21:20] d2.utils.events INFO:  eta: 9:43:00  iter: 12039  total_loss: 3.138  loss_cls_stage0: 0.2729  loss_box_reg_stage0: 0.2839  loss_cls_stage1: 0.3091  loss_box_reg_stage1: 0.7022  loss_cls_stage2: 0.3168  loss_box_reg_stage2: 0.9435  loss_mask: 0.08165  loss_rpn_cls: 0.01752  loss_rpn_loc: 0.07378  time: 0.6258  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:21:32] d2.utils.events INFO:  eta: 9:42:47  iter: 12059  total_loss: 2.989  loss_cls_stage0: 0.3023  loss_box_reg_stage0: 0.31  loss_cls_stage1: 0.3221  loss_box_reg_stage1: 0.6965  loss_cls_stage2: 0.3038  loss_box_reg_stage2: 0.8601  loss_mask: 0.09361  loss_rpn_cls: 0.02766  loss_rpn_loc: 0.0514  time: 0.6257  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:21:45] d2.utils.events INFO:  eta: 9:42:53  iter: 12079  total_loss: 3.205  loss_cls_stage0: 0.2892  loss_box_reg_stage0: 0.3003  loss_cls_stage1: 0.2903  loss_box_reg_stage1: 0.7485  loss_cls_stage2: 0.2927  loss_box_reg_stage2: 0.9682  loss_mask: 0.093  loss_rpn_cls: 0.01371  loss_rpn_loc: 0.05337  time: 0.6257  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:21:57] d2.utils.events INFO:  eta: 9:42:41  iter: 12099  total_loss: 3.042  loss_cls_stage0: 0.2855  loss_box_reg_stage0: 0.2998  loss_cls_stage1: 0.2975  loss_box_reg_stage1: 0.7122  loss_cls_stage2: 0.2883  loss_box_reg_stage2: 0.9391  loss_mask: 0.08241  loss_rpn_cls: 0.0196  loss_rpn_loc: 0.05573  time: 0.6257  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:22:10] d2.utils.events INFO:  eta: 9:42:20  iter: 12119  total_loss: 2.984  loss_cls_stage0: 0.2733  loss_box_reg_stage0: 0.2627  loss_cls_stage1: 0.3135  loss_box_reg_stage1: 0.6526  loss_cls_stage2: 0.296  loss_box_reg_stage2: 0.8646  loss_mask: 0.08444  loss_rpn_cls: 0.01795  loss_rpn_loc: 0.06895  time: 0.6257  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:22:22] d2.utils.events INFO:  eta: 9:42:16  iter: 12139  total_loss: 3.106  loss_cls_stage0: 0.281  loss_box_reg_stage0: 0.2623  loss_cls_stage1: 0.3012  loss_box_reg_stage1: 0.7142  loss_cls_stage2: 0.3009  loss_box_reg_stage2: 0.9622  loss_mask: 0.07685  loss_rpn_cls: 0.01946  loss_rpn_loc: 0.05189  time: 0.6257  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:22:35] d2.utils.events INFO:  eta: 9:42:02  iter: 12159  total_loss: 3.26  loss_cls_stage0: 0.3044  loss_box_reg_stage0: 0.3004  loss_cls_stage1: 0.3227  loss_box_reg_stage1: 0.7771  loss_cls_stage2: 0.3064  loss_box_reg_stage2: 1.056  loss_mask: 0.08109  loss_rpn_cls: 0.01248  loss_rpn_loc: 0.04979  time: 0.6257  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:22:47] d2.utils.events INFO:  eta: 9:41:50  iter: 12179  total_loss: 3.022  loss_cls_stage0: 0.2741  loss_box_reg_stage0: 0.2798  loss_cls_stage1: 0.2958  loss_box_reg_stage1: 0.722  loss_cls_stage2: 0.2776  loss_box_reg_stage2: 0.9553  loss_mask: 0.08301  loss_rpn_cls: 0.02216  loss_rpn_loc: 0.04172  time: 0.6257  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:23:00] d2.utils.events INFO:  eta: 9:41:30  iter: 12199  total_loss: 2.85  loss_cls_stage0: 0.2484  loss_box_reg_stage0: 0.2659  loss_cls_stage1: 0.2686  loss_box_reg_stage1: 0.669  loss_cls_stage2: 0.2541  loss_box_reg_stage2: 0.8883  loss_mask: 0.07716  loss_rpn_cls: 0.01702  loss_rpn_loc: 0.04624  time: 0.6257  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 14:23:12] d2.utils.events INFO:  eta: 9:41:01  iter: 12219  total_loss: 3.047  loss_cls_stage0: 0.2771  loss_box_reg_stage0: 0.2944  loss_cls_stage1: 0.3008  loss_box_reg_stage1: 0.7083  loss_cls_stage2: 0.3101  loss_box_reg_stage2: 0.8999  loss_mask: 0.08829  loss_rpn_cls: 0.01328  loss_rpn_loc: 0.05379  time: 0.6257  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:23:25] d2.utils.events INFO:  eta: 9:40:59  iter: 12239  total_loss: 2.936  loss_cls_stage0: 0.2769  loss_box_reg_stage0: 0.2659  loss_cls_stage1: 0.3104  loss_box_reg_stage1: 0.6812  loss_cls_stage2: 0.2647  loss_box_reg_stage2: 0.935  loss_mask: 0.08434  loss_rpn_cls: 0.01604  loss_rpn_loc: 0.0526  time: 0.6256  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:23:38] d2.utils.events INFO:  eta: 9:41:38  iter: 12259  total_loss: 2.98  loss_cls_stage0: 0.3026  loss_box_reg_stage0: 0.2967  loss_cls_stage1: 0.3166  loss_box_reg_stage1: 0.7227  loss_cls_stage2: 0.2952  loss_box_reg_stage2: 0.9241  loss_mask: 0.08534  loss_rpn_cls: 0.01605  loss_rpn_loc: 0.05675  time: 0.6257  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:23:50] d2.utils.events INFO:  eta: 9:41:59  iter: 12279  total_loss: 3.115  loss_cls_stage0: 0.3243  loss_box_reg_stage0: 0.2937  loss_cls_stage1: 0.3562  loss_box_reg_stage1: 0.7245  loss_cls_stage2: 0.3255  loss_box_reg_stage2: 0.9776  loss_mask: 0.08931  loss_rpn_cls: 0.01529  loss_rpn_loc: 0.05032  time: 0.6257  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:24:03] d2.utils.events INFO:  eta: 9:41:56  iter: 12299  total_loss: 3.322  loss_cls_stage0: 0.306  loss_box_reg_stage0: 0.2962  loss_cls_stage1: 0.3242  loss_box_reg_stage1: 0.7793  loss_cls_stage2: 0.342  loss_box_reg_stage2: 1.013  loss_mask: 0.0908  loss_rpn_cls: 0.02333  loss_rpn_loc: 0.06091  time: 0.6257  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:24:15] d2.utils.events INFO:  eta: 9:41:25  iter: 12319  total_loss: 3.016  loss_cls_stage0: 0.2606  loss_box_reg_stage0: 0.2728  loss_cls_stage1: 0.2987  loss_box_reg_stage1: 0.7197  loss_cls_stage2: 0.3011  loss_box_reg_stage2: 0.9981  loss_mask: 0.08172  loss_rpn_cls: 0.0198  loss_rpn_loc: 0.04908  time: 0.6257  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 14:24:28] d2.utils.events INFO:  eta: 9:41:19  iter: 12339  total_loss: 3.059  loss_cls_stage0: 0.2874  loss_box_reg_stage0: 0.2851  loss_cls_stage1: 0.32  loss_box_reg_stage1: 0.6673  loss_cls_stage2: 0.2774  loss_box_reg_stage2: 0.8569  loss_mask: 0.07807  loss_rpn_cls: 0.01521  loss_rpn_loc: 0.04817  time: 0.6257  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:24:41] d2.utils.events INFO:  eta: 9:41:13  iter: 12359  total_loss: 3.058  loss_cls_stage0: 0.2886  loss_box_reg_stage0: 0.2958  loss_cls_stage1: 0.2983  loss_box_reg_stage1: 0.7227  loss_cls_stage2: 0.3166  loss_box_reg_stage2: 0.8982  loss_mask: 0.0873  loss_rpn_cls: 0.02339  loss_rpn_loc: 0.04692  time: 0.6257  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 14:24:53] d2.utils.events INFO:  eta: 9:41:18  iter: 12379  total_loss: 2.918  loss_cls_stage0: 0.2838  loss_box_reg_stage0: 0.2984  loss_cls_stage1: 0.2925  loss_box_reg_stage1: 0.6726  loss_cls_stage2: 0.2581  loss_box_reg_stage2: 0.874  loss_mask: 0.08771  loss_rpn_cls: 0.0187  loss_rpn_loc: 0.04789  time: 0.6257  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:25:05] d2.utils.events INFO:  eta: 9:41:00  iter: 12399  total_loss: 3.167  loss_cls_stage0: 0.3038  loss_box_reg_stage0: 0.2969  loss_cls_stage1: 0.3311  loss_box_reg_stage1: 0.7339  loss_cls_stage2: 0.311  loss_box_reg_stage2: 0.9341  loss_mask: 0.09609  loss_rpn_cls: 0.01343  loss_rpn_loc: 0.04814  time: 0.6256  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:25:17] d2.utils.events INFO:  eta: 9:40:35  iter: 12419  total_loss: 3.078  loss_cls_stage0: 0.2759  loss_box_reg_stage0: 0.3046  loss_cls_stage1: 0.3029  loss_box_reg_stage1: 0.7629  loss_cls_stage2: 0.2833  loss_box_reg_stage2: 0.956  loss_mask: 0.08749  loss_rpn_cls: 0.01428  loss_rpn_loc: 0.04973  time: 0.6256  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:25:30] d2.utils.events INFO:  eta: 9:40:34  iter: 12439  total_loss: 3.008  loss_cls_stage0: 0.3206  loss_box_reg_stage0: 0.2761  loss_cls_stage1: 0.3318  loss_box_reg_stage1: 0.6706  loss_cls_stage2: 0.3024  loss_box_reg_stage2: 0.8558  loss_mask: 0.09033  loss_rpn_cls: 0.01839  loss_rpn_loc: 0.05253  time: 0.6256  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:25:42] d2.utils.events INFO:  eta: 9:40:03  iter: 12459  total_loss: 3.261  loss_cls_stage0: 0.3061  loss_box_reg_stage0: 0.2935  loss_cls_stage1: 0.3416  loss_box_reg_stage1: 0.743  loss_cls_stage2: 0.3207  loss_box_reg_stage2: 0.9432  loss_mask: 0.09296  loss_rpn_cls: 0.01597  loss_rpn_loc: 0.05232  time: 0.6256  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:25:55] d2.utils.events INFO:  eta: 9:39:43  iter: 12479  total_loss: 3.047  loss_cls_stage0: 0.2929  loss_box_reg_stage0: 0.2982  loss_cls_stage1: 0.2948  loss_box_reg_stage1: 0.7125  loss_cls_stage2: 0.2879  loss_box_reg_stage2: 0.9483  loss_mask: 0.08843  loss_rpn_cls: 0.01922  loss_rpn_loc: 0.06614  time: 0.6256  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:26:07] d2.utils.events INFO:  eta: 9:39:19  iter: 12499  total_loss: 2.885  loss_cls_stage0: 0.2924  loss_box_reg_stage0: 0.2814  loss_cls_stage1: 0.3213  loss_box_reg_stage1: 0.617  loss_cls_stage2: 0.2839  loss_box_reg_stage2: 0.7689  loss_mask: 0.08399  loss_rpn_cls: 0.01807  loss_rpn_loc: 0.04598  time: 0.6256  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:26:20] d2.utils.events INFO:  eta: 9:39:18  iter: 12519  total_loss: 2.8  loss_cls_stage0: 0.2501  loss_box_reg_stage0: 0.2826  loss_cls_stage1: 0.2752  loss_box_reg_stage1: 0.6977  loss_cls_stage2: 0.2688  loss_box_reg_stage2: 0.8836  loss_mask: 0.08236  loss_rpn_cls: 0.01274  loss_rpn_loc: 0.04386  time: 0.6256  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:26:33] d2.utils.events INFO:  eta: 9:39:33  iter: 12539  total_loss: 2.784  loss_cls_stage0: 0.2485  loss_box_reg_stage0: 0.2729  loss_cls_stage1: 0.2748  loss_box_reg_stage1: 0.6392  loss_cls_stage2: 0.2853  loss_box_reg_stage2: 0.8358  loss_mask: 0.08584  loss_rpn_cls: 0.01905  loss_rpn_loc: 0.04845  time: 0.6256  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:26:45] d2.utils.events INFO:  eta: 9:39:26  iter: 12559  total_loss: 2.745  loss_cls_stage0: 0.2385  loss_box_reg_stage0: 0.2654  loss_cls_stage1: 0.2762  loss_box_reg_stage1: 0.6525  loss_cls_stage2: 0.2801  loss_box_reg_stage2: 0.883  loss_mask: 0.08554  loss_rpn_cls: 0.01979  loss_rpn_loc: 0.04608  time: 0.6256  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:26:58] d2.utils.events INFO:  eta: 9:40:09  iter: 12579  total_loss: 2.883  loss_cls_stage0: 0.2622  loss_box_reg_stage0: 0.2685  loss_cls_stage1: 0.27  loss_box_reg_stage1: 0.7225  loss_cls_stage2: 0.2564  loss_box_reg_stage2: 0.9578  loss_mask: 0.08525  loss_rpn_cls: 0.01202  loss_rpn_loc: 0.04741  time: 0.6256  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 14:27:10] d2.utils.events INFO:  eta: 9:39:08  iter: 12599  total_loss: 3.071  loss_cls_stage0: 0.2752  loss_box_reg_stage0: 0.3003  loss_cls_stage1: 0.2772  loss_box_reg_stage1: 0.7353  loss_cls_stage2: 0.2811  loss_box_reg_stage2: 0.9836  loss_mask: 0.07999  loss_rpn_cls: 0.01848  loss_rpn_loc: 0.04439  time: 0.6256  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:27:23] d2.utils.events INFO:  eta: 9:39:16  iter: 12619  total_loss: 3.356  loss_cls_stage0: 0.3209  loss_box_reg_stage0: 0.2896  loss_cls_stage1: 0.3499  loss_box_reg_stage1: 0.7498  loss_cls_stage2: 0.3248  loss_box_reg_stage2: 0.9524  loss_mask: 0.09169  loss_rpn_cls: 0.01742  loss_rpn_loc: 0.04782  time: 0.6256  data_time: 0.0045  lr: 0.00016  max_mem: 19679M
[07/29 14:27:35] d2.utils.events INFO:  eta: 9:38:36  iter: 12639  total_loss: 2.763  loss_cls_stage0: 0.24  loss_box_reg_stage0: 0.2547  loss_cls_stage1: 0.2645  loss_box_reg_stage1: 0.6604  loss_cls_stage2: 0.2527  loss_box_reg_stage2: 0.9213  loss_mask: 0.07408  loss_rpn_cls: 0.01822  loss_rpn_loc: 0.05426  time: 0.6256  data_time: 0.0045  lr: 0.00016  max_mem: 19679M
[07/29 14:27:47] d2.utils.events INFO:  eta: 9:38:23  iter: 12659  total_loss: 2.765  loss_cls_stage0: 0.2469  loss_box_reg_stage0: 0.2495  loss_cls_stage1: 0.2669  loss_box_reg_stage1: 0.6756  loss_cls_stage2: 0.2721  loss_box_reg_stage2: 0.8692  loss_mask: 0.0724  loss_rpn_cls: 0.01264  loss_rpn_loc: 0.03863  time: 0.6256  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 14:28:00] d2.utils.events INFO:  eta: 9:37:41  iter: 12679  total_loss: 3.094  loss_cls_stage0: 0.2997  loss_box_reg_stage0: 0.2852  loss_cls_stage1: 0.3428  loss_box_reg_stage1: 0.7307  loss_cls_stage2: 0.3245  loss_box_reg_stage2: 0.9155  loss_mask: 0.08943  loss_rpn_cls: 0.01355  loss_rpn_loc: 0.04236  time: 0.6255  data_time: 0.0043  lr: 0.00016  max_mem: 19679M
[07/29 14:28:13] d2.utils.events INFO:  eta: 9:37:38  iter: 12699  total_loss: 3.1  loss_cls_stage0: 0.2982  loss_box_reg_stage0: 0.2677  loss_cls_stage1: 0.3094  loss_box_reg_stage1: 0.6865  loss_cls_stage2: 0.3164  loss_box_reg_stage2: 0.9226  loss_mask: 0.08147  loss_rpn_cls: 0.0158  loss_rpn_loc: 0.04632  time: 0.6256  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 14:28:25] d2.utils.events INFO:  eta: 9:36:59  iter: 12719  total_loss: 2.827  loss_cls_stage0: 0.2888  loss_box_reg_stage0: 0.262  loss_cls_stage1: 0.3054  loss_box_reg_stage1: 0.6693  loss_cls_stage2: 0.2784  loss_box_reg_stage2: 0.9028  loss_mask: 0.07885  loss_rpn_cls: 0.01431  loss_rpn_loc: 0.04753  time: 0.6256  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:28:38] d2.utils.events INFO:  eta: 9:36:37  iter: 12739  total_loss: 3.117  loss_cls_stage0: 0.3191  loss_box_reg_stage0: 0.3029  loss_cls_stage1: 0.3378  loss_box_reg_stage1: 0.7218  loss_cls_stage2: 0.3154  loss_box_reg_stage2: 0.9412  loss_mask: 0.08733  loss_rpn_cls: 0.0192  loss_rpn_loc: 0.05612  time: 0.6256  data_time: 0.0045  lr: 0.00016  max_mem: 19679M
[07/29 14:28:50] d2.utils.events INFO:  eta: 9:35:57  iter: 12759  total_loss: 2.694  loss_cls_stage0: 0.2635  loss_box_reg_stage0: 0.2787  loss_cls_stage1: 0.2604  loss_box_reg_stage1: 0.6111  loss_cls_stage2: 0.2667  loss_box_reg_stage2: 0.7925  loss_mask: 0.08756  loss_rpn_cls: 0.01752  loss_rpn_loc: 0.03582  time: 0.6255  data_time: 0.0044  lr: 0.00016  max_mem: 19679M
[07/29 14:29:02] d2.utils.events INFO:  eta: 9:35:36  iter: 12779  total_loss: 3.092  loss_cls_stage0: 0.2906  loss_box_reg_stage0: 0.267  loss_cls_stage1: 0.3264  loss_box_reg_stage1: 0.6733  loss_cls_stage2: 0.3067  loss_box_reg_stage2: 0.9468  loss_mask: 0.08127  loss_rpn_cls: 0.01539  loss_rpn_loc: 0.05075  time: 0.6255  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 14:29:15] d2.utils.events INFO:  eta: 9:35:19  iter: 12799  total_loss: 2.977  loss_cls_stage0: 0.3065  loss_box_reg_stage0: 0.2843  loss_cls_stage1: 0.3103  loss_box_reg_stage1: 0.682  loss_cls_stage2: 0.2698  loss_box_reg_stage2: 0.9073  loss_mask: 0.08383  loss_rpn_cls: 0.01901  loss_rpn_loc: 0.0672  time: 0.6255  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:29:27] d2.utils.events INFO:  eta: 9:35:11  iter: 12819  total_loss: 3.385  loss_cls_stage0: 0.3402  loss_box_reg_stage0: 0.3144  loss_cls_stage1: 0.3527  loss_box_reg_stage1: 0.7606  loss_cls_stage2: 0.3376  loss_box_reg_stage2: 0.9952  loss_mask: 0.09184  loss_rpn_cls: 0.02121  loss_rpn_loc: 0.05216  time: 0.6255  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:29:40] d2.utils.events INFO:  eta: 9:34:40  iter: 12839  total_loss: 2.955  loss_cls_stage0: 0.2547  loss_box_reg_stage0: 0.283  loss_cls_stage1: 0.2761  loss_box_reg_stage1: 0.7244  loss_cls_stage2: 0.2715  loss_box_reg_stage2: 0.9728  loss_mask: 0.08243  loss_rpn_cls: 0.0162  loss_rpn_loc: 0.04321  time: 0.6255  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 14:29:52] d2.utils.events INFO:  eta: 9:34:30  iter: 12859  total_loss: 2.945  loss_cls_stage0: 0.3045  loss_box_reg_stage0: 0.2992  loss_cls_stage1: 0.292  loss_box_reg_stage1: 0.6993  loss_cls_stage2: 0.2815  loss_box_reg_stage2: 0.9528  loss_mask: 0.08038  loss_rpn_cls: 0.01079  loss_rpn_loc: 0.04122  time: 0.6255  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 14:30:05] d2.utils.events INFO:  eta: 9:34:31  iter: 12879  total_loss: 2.923  loss_cls_stage0: 0.2984  loss_box_reg_stage0: 0.2935  loss_cls_stage1: 0.286  loss_box_reg_stage1: 0.6891  loss_cls_stage2: 0.2874  loss_box_reg_stage2: 0.8973  loss_mask: 0.08548  loss_rpn_cls: 0.01388  loss_rpn_loc: 0.04335  time: 0.6255  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:30:18] d2.utils.events INFO:  eta: 9:34:21  iter: 12899  total_loss: 3.294  loss_cls_stage0: 0.2993  loss_box_reg_stage0: 0.2801  loss_cls_stage1: 0.3347  loss_box_reg_stage1: 0.6937  loss_cls_stage2: 0.334  loss_box_reg_stage2: 0.9211  loss_mask: 0.07917  loss_rpn_cls: 0.01664  loss_rpn_loc: 0.04691  time: 0.6255  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:30:30] d2.utils.events INFO:  eta: 9:34:04  iter: 12919  total_loss: 3.381  loss_cls_stage0: 0.3173  loss_box_reg_stage0: 0.2908  loss_cls_stage1: 0.3622  loss_box_reg_stage1: 0.8107  loss_cls_stage2: 0.3263  loss_box_reg_stage2: 1.019  loss_mask: 0.09735  loss_rpn_cls: 0.01897  loss_rpn_loc: 0.04401  time: 0.6255  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 14:30:43] d2.utils.events INFO:  eta: 9:33:41  iter: 12939  total_loss: 3.217  loss_cls_stage0: 0.2947  loss_box_reg_stage0: 0.3169  loss_cls_stage1: 0.3047  loss_box_reg_stage1: 0.7683  loss_cls_stage2: 0.3015  loss_box_reg_stage2: 0.9499  loss_mask: 0.08704  loss_rpn_cls: 0.0204  loss_rpn_loc: 0.0551  time: 0.6255  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 14:30:55] d2.utils.events INFO:  eta: 9:33:24  iter: 12959  total_loss: 2.943  loss_cls_stage0: 0.2642  loss_box_reg_stage0: 0.2686  loss_cls_stage1: 0.29  loss_box_reg_stage1: 0.7179  loss_cls_stage2: 0.2597  loss_box_reg_stage2: 0.9598  loss_mask: 0.08012  loss_rpn_cls: 0.0189  loss_rpn_loc: 0.0568  time: 0.6255  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:31:07] d2.utils.events INFO:  eta: 9:32:07  iter: 12979  total_loss: 3.099  loss_cls_stage0: 0.3179  loss_box_reg_stage0: 0.2877  loss_cls_stage1: 0.3626  loss_box_reg_stage1: 0.7376  loss_cls_stage2: 0.3295  loss_box_reg_stage2: 0.9657  loss_mask: 0.08318  loss_rpn_cls: 0.01964  loss_rpn_loc: 0.05003  time: 0.6255  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:31:20] d2.utils.events INFO:  eta: 9:31:19  iter: 12999  total_loss: 3.178  loss_cls_stage0: 0.31  loss_box_reg_stage0: 0.2999  loss_cls_stage1: 0.3233  loss_box_reg_stage1: 0.7567  loss_cls_stage2: 0.3215  loss_box_reg_stage2: 1.043  loss_mask: 0.08509  loss_rpn_cls: 0.01382  loss_rpn_loc: 0.04383  time: 0.6255  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:31:32] d2.utils.events INFO:  eta: 9:30:59  iter: 13019  total_loss: 3.159  loss_cls_stage0: 0.3171  loss_box_reg_stage0: 0.29  loss_cls_stage1: 0.3396  loss_box_reg_stage1: 0.7185  loss_cls_stage2: 0.3117  loss_box_reg_stage2: 0.9793  loss_mask: 0.09377  loss_rpn_cls: 0.01598  loss_rpn_loc: 0.04504  time: 0.6255  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:31:44] d2.utils.events INFO:  eta: 9:30:47  iter: 13039  total_loss: 2.712  loss_cls_stage0: 0.2576  loss_box_reg_stage0: 0.2699  loss_cls_stage1: 0.2875  loss_box_reg_stage1: 0.6221  loss_cls_stage2: 0.2795  loss_box_reg_stage2: 0.8564  loss_mask: 0.08076  loss_rpn_cls: 0.02166  loss_rpn_loc: 0.04541  time: 0.6254  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:31:57] d2.utils.events INFO:  eta: 9:30:41  iter: 13059  total_loss: 2.91  loss_cls_stage0: 0.2915  loss_box_reg_stage0: 0.2889  loss_cls_stage1: 0.2873  loss_box_reg_stage1: 0.6966  loss_cls_stage2: 0.2584  loss_box_reg_stage2: 0.86  loss_mask: 0.08764  loss_rpn_cls: 0.01676  loss_rpn_loc: 0.04554  time: 0.6254  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 14:32:10] d2.utils.events INFO:  eta: 9:30:13  iter: 13079  total_loss: 3.207  loss_cls_stage0: 0.3198  loss_box_reg_stage0: 0.3041  loss_cls_stage1: 0.3409  loss_box_reg_stage1: 0.7364  loss_cls_stage2: 0.3257  loss_box_reg_stage2: 0.9772  loss_mask: 0.09574  loss_rpn_cls: 0.02043  loss_rpn_loc: 0.06403  time: 0.6254  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:32:22] d2.utils.events INFO:  eta: 9:29:33  iter: 13099  total_loss: 3.028  loss_cls_stage0: 0.2816  loss_box_reg_stage0: 0.294  loss_cls_stage1: 0.3103  loss_box_reg_stage1: 0.7173  loss_cls_stage2: 0.306  loss_box_reg_stage2: 0.9049  loss_mask: 0.08758  loss_rpn_cls: 0.01882  loss_rpn_loc: 0.0458  time: 0.6254  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:32:35] d2.utils.events INFO:  eta: 9:29:34  iter: 13119  total_loss: 3.067  loss_cls_stage0: 0.3121  loss_box_reg_stage0: 0.2987  loss_cls_stage1: 0.349  loss_box_reg_stage1: 0.7011  loss_cls_stage2: 0.3288  loss_box_reg_stage2: 0.8956  loss_mask: 0.09241  loss_rpn_cls: 0.01817  loss_rpn_loc: 0.05393  time: 0.6254  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:32:47] d2.utils.events INFO:  eta: 9:29:16  iter: 13139  total_loss: 3.419  loss_cls_stage0: 0.334  loss_box_reg_stage0: 0.3117  loss_cls_stage1: 0.4135  loss_box_reg_stage1: 0.8018  loss_cls_stage2: 0.3619  loss_box_reg_stage2: 1.014  loss_mask: 0.08636  loss_rpn_cls: 0.01798  loss_rpn_loc: 0.04412  time: 0.6254  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:32:59] d2.utils.events INFO:  eta: 9:28:35  iter: 13159  total_loss: 3.141  loss_cls_stage0: 0.3133  loss_box_reg_stage0: 0.2843  loss_cls_stage1: 0.3322  loss_box_reg_stage1: 0.723  loss_cls_stage2: 0.3169  loss_box_reg_stage2: 0.9215  loss_mask: 0.08975  loss_rpn_cls: 0.01677  loss_rpn_loc: 0.05364  time: 0.6254  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 14:33:11] d2.utils.events INFO:  eta: 9:28:06  iter: 13179  total_loss: 2.745  loss_cls_stage0: 0.2467  loss_box_reg_stage0: 0.2699  loss_cls_stage1: 0.2636  loss_box_reg_stage1: 0.671  loss_cls_stage2: 0.2494  loss_box_reg_stage2: 0.9241  loss_mask: 0.07945  loss_rpn_cls: 0.01952  loss_rpn_loc: 0.07133  time: 0.6253  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 14:33:24] d2.utils.events INFO:  eta: 9:27:49  iter: 13199  total_loss: 2.762  loss_cls_stage0: 0.2875  loss_box_reg_stage0: 0.2996  loss_cls_stage1: 0.2583  loss_box_reg_stage1: 0.6775  loss_cls_stage2: 0.2328  loss_box_reg_stage2: 0.8918  loss_mask: 0.07862  loss_rpn_cls: 0.01998  loss_rpn_loc: 0.04623  time: 0.6253  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:33:36] d2.utils.events INFO:  eta: 9:27:32  iter: 13219  total_loss: 3.038  loss_cls_stage0: 0.3019  loss_box_reg_stage0: 0.2718  loss_cls_stage1: 0.3255  loss_box_reg_stage1: 0.7206  loss_cls_stage2: 0.3099  loss_box_reg_stage2: 0.9649  loss_mask: 0.08679  loss_rpn_cls: 0.01924  loss_rpn_loc: 0.04755  time: 0.6253  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 14:33:48] d2.utils.events INFO:  eta: 9:27:14  iter: 13239  total_loss: 2.913  loss_cls_stage0: 0.272  loss_box_reg_stage0: 0.2662  loss_cls_stage1: 0.2898  loss_box_reg_stage1: 0.6682  loss_cls_stage2: 0.275  loss_box_reg_stage2: 0.9321  loss_mask: 0.08225  loss_rpn_cls: 0.01597  loss_rpn_loc: 0.046  time: 0.6253  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:34:01] d2.utils.events INFO:  eta: 9:27:12  iter: 13259  total_loss: 3.131  loss_cls_stage0: 0.2862  loss_box_reg_stage0: 0.2594  loss_cls_stage1: 0.3364  loss_box_reg_stage1: 0.7419  loss_cls_stage2: 0.3325  loss_box_reg_stage2: 0.9602  loss_mask: 0.08037  loss_rpn_cls: 0.01371  loss_rpn_loc: 0.04209  time: 0.6253  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:34:14] d2.utils.events INFO:  eta: 9:27:07  iter: 13279  total_loss: 2.941  loss_cls_stage0: 0.2567  loss_box_reg_stage0: 0.2696  loss_cls_stage1: 0.3142  loss_box_reg_stage1: 0.6804  loss_cls_stage2: 0.3  loss_box_reg_stage2: 0.886  loss_mask: 0.08227  loss_rpn_cls: 0.01639  loss_rpn_loc: 0.05031  time: 0.6253  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:34:27] d2.utils.events INFO:  eta: 9:26:51  iter: 13299  total_loss: 2.863  loss_cls_stage0: 0.2712  loss_box_reg_stage0: 0.274  loss_cls_stage1: 0.2732  loss_box_reg_stage1: 0.6545  loss_cls_stage2: 0.2706  loss_box_reg_stage2: 0.9278  loss_mask: 0.07741  loss_rpn_cls: 0.01739  loss_rpn_loc: 0.04729  time: 0.6253  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:34:39] d2.utils.events INFO:  eta: 9:26:49  iter: 13319  total_loss: 2.892  loss_cls_stage0: 0.2939  loss_box_reg_stage0: 0.2668  loss_cls_stage1: 0.3372  loss_box_reg_stage1: 0.6772  loss_cls_stage2: 0.3189  loss_box_reg_stage2: 0.9468  loss_mask: 0.08524  loss_rpn_cls: 0.02258  loss_rpn_loc: 0.04337  time: 0.6253  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:34:52] d2.utils.events INFO:  eta: 9:26:32  iter: 13339  total_loss: 3.411  loss_cls_stage0: 0.28  loss_box_reg_stage0: 0.2687  loss_cls_stage1: 0.3619  loss_box_reg_stage1: 0.8123  loss_cls_stage2: 0.3735  loss_box_reg_stage2: 1.092  loss_mask: 0.08564  loss_rpn_cls: 0.0155  loss_rpn_loc: 0.04794  time: 0.6253  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:35:04] d2.utils.events INFO:  eta: 9:26:16  iter: 13359  total_loss: 3.226  loss_cls_stage0: 0.2989  loss_box_reg_stage0: 0.2968  loss_cls_stage1: 0.3352  loss_box_reg_stage1: 0.793  loss_cls_stage2: 0.3232  loss_box_reg_stage2: 1.051  loss_mask: 0.08633  loss_rpn_cls: 0.01277  loss_rpn_loc: 0.04849  time: 0.6253  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:35:17] d2.utils.events INFO:  eta: 9:26:13  iter: 13379  total_loss: 3.208  loss_cls_stage0: 0.2867  loss_box_reg_stage0: 0.2666  loss_cls_stage1: 0.3321  loss_box_reg_stage1: 0.7413  loss_cls_stage2: 0.3085  loss_box_reg_stage2: 0.9082  loss_mask: 0.07808  loss_rpn_cls: 0.01981  loss_rpn_loc: 0.05032  time: 0.6253  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 14:35:29] d2.utils.events INFO:  eta: 9:26:04  iter: 13399  total_loss: 3.452  loss_cls_stage0: 0.3333  loss_box_reg_stage0: 0.2985  loss_cls_stage1: 0.3679  loss_box_reg_stage1: 0.8316  loss_cls_stage2: 0.3349  loss_box_reg_stage2: 1.077  loss_mask: 0.09038  loss_rpn_cls: 0.01286  loss_rpn_loc: 0.04362  time: 0.6253  data_time: 0.0044  lr: 0.00016  max_mem: 19679M
[07/29 14:35:41] d2.utils.events INFO:  eta: 9:25:50  iter: 13419  total_loss: 3.048  loss_cls_stage0: 0.2707  loss_box_reg_stage0: 0.2568  loss_cls_stage1: 0.3052  loss_box_reg_stage1: 0.7428  loss_cls_stage2: 0.2931  loss_box_reg_stage2: 1.009  loss_mask: 0.07953  loss_rpn_cls: 0.01203  loss_rpn_loc: 0.0511  time: 0.6253  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 14:35:54] d2.utils.events INFO:  eta: 9:25:37  iter: 13439  total_loss: 2.917  loss_cls_stage0: 0.2723  loss_box_reg_stage0: 0.2571  loss_cls_stage1: 0.3163  loss_box_reg_stage1: 0.7153  loss_cls_stage2: 0.2821  loss_box_reg_stage2: 0.9316  loss_mask: 0.07714  loss_rpn_cls: 0.01594  loss_rpn_loc: 0.04587  time: 0.6253  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:36:07] d2.utils.events INFO:  eta: 9:25:48  iter: 13459  total_loss: 2.849  loss_cls_stage0: 0.2758  loss_box_reg_stage0: 0.2535  loss_cls_stage1: 0.2963  loss_box_reg_stage1: 0.6833  loss_cls_stage2: 0.2794  loss_box_reg_stage2: 0.901  loss_mask: 0.08574  loss_rpn_cls: 0.02181  loss_rpn_loc: 0.05285  time: 0.6254  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:36:20] d2.utils.events INFO:  eta: 9:25:57  iter: 13479  total_loss: 3.288  loss_cls_stage0: 0.3131  loss_box_reg_stage0: 0.3231  loss_cls_stage1: 0.3379  loss_box_reg_stage1: 0.7801  loss_cls_stage2: 0.3393  loss_box_reg_stage2: 1.053  loss_mask: 0.08644  loss_rpn_cls: 0.02409  loss_rpn_loc: 0.05064  time: 0.6254  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:36:32] d2.utils.events INFO:  eta: 9:25:48  iter: 13499  total_loss: 2.747  loss_cls_stage0: 0.2435  loss_box_reg_stage0: 0.2697  loss_cls_stage1: 0.2719  loss_box_reg_stage1: 0.6369  loss_cls_stage2: 0.2723  loss_box_reg_stage2: 0.8027  loss_mask: 0.08499  loss_rpn_cls: 0.02219  loss_rpn_loc: 0.05733  time: 0.6254  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:36:45] d2.utils.events INFO:  eta: 9:25:36  iter: 13519  total_loss: 3.191  loss_cls_stage0: 0.3097  loss_box_reg_stage0: 0.3065  loss_cls_stage1: 0.3236  loss_box_reg_stage1: 0.7718  loss_cls_stage2: 0.2947  loss_box_reg_stage2: 0.9773  loss_mask: 0.08044  loss_rpn_cls: 0.01934  loss_rpn_loc: 0.06127  time: 0.6254  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 14:36:58] d2.utils.events INFO:  eta: 9:25:35  iter: 13539  total_loss: 3.472  loss_cls_stage0: 0.3066  loss_box_reg_stage0: 0.2883  loss_cls_stage1: 0.3451  loss_box_reg_stage1: 0.8066  loss_cls_stage2: 0.3085  loss_box_reg_stage2: 1.05  loss_mask: 0.08789  loss_rpn_cls: 0.0137  loss_rpn_loc: 0.04579  time: 0.6254  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 14:37:10] d2.utils.events INFO:  eta: 9:25:24  iter: 13559  total_loss: 3.45  loss_cls_stage0: 0.3212  loss_box_reg_stage0: 0.2965  loss_cls_stage1: 0.3576  loss_box_reg_stage1: 0.8215  loss_cls_stage2: 0.3342  loss_box_reg_stage2: 1.14  loss_mask: 0.08919  loss_rpn_cls: 0.01345  loss_rpn_loc: 0.04953  time: 0.6254  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:37:23] d2.utils.events INFO:  eta: 9:25:06  iter: 13579  total_loss: 3.302  loss_cls_stage0: 0.3148  loss_box_reg_stage0: 0.2919  loss_cls_stage1: 0.3432  loss_box_reg_stage1: 0.8035  loss_cls_stage2: 0.3316  loss_box_reg_stage2: 1.013  loss_mask: 0.08229  loss_rpn_cls: 0.01521  loss_rpn_loc: 0.052  time: 0.6254  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:37:35] d2.utils.events INFO:  eta: 9:24:59  iter: 13599  total_loss: 2.898  loss_cls_stage0: 0.2925  loss_box_reg_stage0: 0.29  loss_cls_stage1: 0.3044  loss_box_reg_stage1: 0.6938  loss_cls_stage2: 0.2825  loss_box_reg_stage2: 0.8902  loss_mask: 0.07545  loss_rpn_cls: 0.02361  loss_rpn_loc: 0.05171  time: 0.6254  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:37:48] d2.utils.events INFO:  eta: 9:24:58  iter: 13619  total_loss: 2.84  loss_cls_stage0: 0.2872  loss_box_reg_stage0: 0.2785  loss_cls_stage1: 0.3065  loss_box_reg_stage1: 0.703  loss_cls_stage2: 0.2783  loss_box_reg_stage2: 0.9335  loss_mask: 0.08507  loss_rpn_cls: 0.0109  loss_rpn_loc: 0.04315  time: 0.6255  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:38:01] d2.utils.events INFO:  eta: 9:25:31  iter: 13639  total_loss: 2.96  loss_cls_stage0: 0.3073  loss_box_reg_stage0: 0.2948  loss_cls_stage1: 0.3096  loss_box_reg_stage1: 0.693  loss_cls_stage2: 0.2949  loss_box_reg_stage2: 0.8493  loss_mask: 0.09026  loss_rpn_cls: 0.01467  loss_rpn_loc: 0.04528  time: 0.6255  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:38:13] d2.utils.events INFO:  eta: 9:24:33  iter: 13659  total_loss: 3.198  loss_cls_stage0: 0.3063  loss_box_reg_stage0: 0.2984  loss_cls_stage1: 0.3198  loss_box_reg_stage1: 0.7482  loss_cls_stage2: 0.2897  loss_box_reg_stage2: 0.9228  loss_mask: 0.08507  loss_rpn_cls: 0.02014  loss_rpn_loc: 0.05957  time: 0.6254  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:38:25] d2.utils.events INFO:  eta: 9:25:10  iter: 13679  total_loss: 2.908  loss_cls_stage0: 0.2767  loss_box_reg_stage0: 0.2827  loss_cls_stage1: 0.2987  loss_box_reg_stage1: 0.6923  loss_cls_stage2: 0.2919  loss_box_reg_stage2: 0.8763  loss_mask: 0.08253  loss_rpn_cls: 0.01421  loss_rpn_loc: 0.04893  time: 0.6254  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 14:38:38] d2.utils.events INFO:  eta: 9:24:21  iter: 13699  total_loss: 3.028  loss_cls_stage0: 0.2822  loss_box_reg_stage0: 0.2763  loss_cls_stage1: 0.3183  loss_box_reg_stage1: 0.7119  loss_cls_stage2: 0.3008  loss_box_reg_stage2: 0.9522  loss_mask: 0.08568  loss_rpn_cls: 0.01636  loss_rpn_loc: 0.04994  time: 0.6254  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 14:38:50] d2.utils.events INFO:  eta: 9:24:08  iter: 13719  total_loss: 3.076  loss_cls_stage0: 0.2984  loss_box_reg_stage0: 0.2768  loss_cls_stage1: 0.3098  loss_box_reg_stage1: 0.7298  loss_cls_stage2: 0.2793  loss_box_reg_stage2: 0.9372  loss_mask: 0.0875  loss_rpn_cls: 0.01484  loss_rpn_loc: 0.0482  time: 0.6254  data_time: 0.0045  lr: 0.00016  max_mem: 19679M
[07/29 14:39:02] d2.utils.events INFO:  eta: 9:23:57  iter: 13739  total_loss: 3.092  loss_cls_stage0: 0.2976  loss_box_reg_stage0: 0.274  loss_cls_stage1: 0.3285  loss_box_reg_stage1: 0.7098  loss_cls_stage2: 0.2979  loss_box_reg_stage2: 0.9976  loss_mask: 0.07857  loss_rpn_cls: 0.0166  loss_rpn_loc: 0.04797  time: 0.6254  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:39:15] d2.utils.events INFO:  eta: 9:24:15  iter: 13759  total_loss: 3.061  loss_cls_stage0: 0.2703  loss_box_reg_stage0: 0.2683  loss_cls_stage1: 0.278  loss_box_reg_stage1: 0.7342  loss_cls_stage2: 0.287  loss_box_reg_stage2: 1.037  loss_mask: 0.08577  loss_rpn_cls: 0.01421  loss_rpn_loc: 0.04916  time: 0.6254  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:39:28] d2.utils.events INFO:  eta: 9:24:26  iter: 13779  total_loss: 2.94  loss_cls_stage0: 0.281  loss_box_reg_stage0: 0.2868  loss_cls_stage1: 0.2974  loss_box_reg_stage1: 0.6679  loss_cls_stage2: 0.2963  loss_box_reg_stage2: 0.9357  loss_mask: 0.08225  loss_rpn_cls: 0.01018  loss_rpn_loc: 0.04199  time: 0.6254  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:39:40] d2.utils.events INFO:  eta: 9:24:03  iter: 13799  total_loss: 2.807  loss_cls_stage0: 0.277  loss_box_reg_stage0: 0.2682  loss_cls_stage1: 0.2822  loss_box_reg_stage1: 0.6599  loss_cls_stage2: 0.278  loss_box_reg_stage2: 0.933  loss_mask: 0.07887  loss_rpn_cls: 0.01963  loss_rpn_loc: 0.05811  time: 0.6254  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:39:52] d2.utils.events INFO:  eta: 9:23:22  iter: 13819  total_loss: 3.112  loss_cls_stage0: 0.2761  loss_box_reg_stage0: 0.2842  loss_cls_stage1: 0.3256  loss_box_reg_stage1: 0.7242  loss_cls_stage2: 0.3222  loss_box_reg_stage2: 0.9841  loss_mask: 0.07941  loss_rpn_cls: 0.01773  loss_rpn_loc: 0.04207  time: 0.6254  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:40:05] d2.utils.events INFO:  eta: 9:23:39  iter: 13839  total_loss: 3.077  loss_cls_stage0: 0.2651  loss_box_reg_stage0: 0.265  loss_cls_stage1: 0.2937  loss_box_reg_stage1: 0.7287  loss_cls_stage2: 0.298  loss_box_reg_stage2: 1.063  loss_mask: 0.07692  loss_rpn_cls: 0.01819  loss_rpn_loc: 0.04651  time: 0.6254  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:40:17] d2.utils.events INFO:  eta: 9:23:25  iter: 13859  total_loss: 2.866  loss_cls_stage0: 0.2596  loss_box_reg_stage0: 0.3202  loss_cls_stage1: 0.2562  loss_box_reg_stage1: 0.6843  loss_cls_stage2: 0.2659  loss_box_reg_stage2: 0.8624  loss_mask: 0.08901  loss_rpn_cls: 0.01876  loss_rpn_loc: 0.05987  time: 0.6253  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:40:30] d2.utils.events INFO:  eta: 9:22:29  iter: 13879  total_loss: 2.956  loss_cls_stage0: 0.2548  loss_box_reg_stage0: 0.2717  loss_cls_stage1: 0.2874  loss_box_reg_stage1: 0.6692  loss_cls_stage2: 0.2931  loss_box_reg_stage2: 0.8816  loss_mask: 0.07762  loss_rpn_cls: 0.01191  loss_rpn_loc: 0.03815  time: 0.6253  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:40:42] d2.utils.events INFO:  eta: 9:22:10  iter: 13899  total_loss: 2.794  loss_cls_stage0: 0.2602  loss_box_reg_stage0: 0.2619  loss_cls_stage1: 0.2811  loss_box_reg_stage1: 0.6344  loss_cls_stage2: 0.2673  loss_box_reg_stage2: 0.8886  loss_mask: 0.07653  loss_rpn_cls: 0.01481  loss_rpn_loc: 0.04645  time: 0.6253  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 14:40:54] d2.utils.events INFO:  eta: 9:21:54  iter: 13919  total_loss: 2.799  loss_cls_stage0: 0.2622  loss_box_reg_stage0: 0.2575  loss_cls_stage1: 0.2773  loss_box_reg_stage1: 0.6831  loss_cls_stage2: 0.2567  loss_box_reg_stage2: 0.9321  loss_mask: 0.08546  loss_rpn_cls: 0.01171  loss_rpn_loc: 0.05337  time: 0.6253  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:41:07] d2.utils.events INFO:  eta: 9:21:50  iter: 13939  total_loss: 3.047  loss_cls_stage0: 0.2788  loss_box_reg_stage0: 0.2662  loss_cls_stage1: 0.2986  loss_box_reg_stage1: 0.7281  loss_cls_stage2: 0.2862  loss_box_reg_stage2: 1.002  loss_mask: 0.07187  loss_rpn_cls: 0.01475  loss_rpn_loc: 0.04508  time: 0.6253  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:41:19] d2.utils.events INFO:  eta: 9:22:22  iter: 13959  total_loss: 2.929  loss_cls_stage0: 0.2726  loss_box_reg_stage0: 0.2829  loss_cls_stage1: 0.3085  loss_box_reg_stage1: 0.7057  loss_cls_stage2: 0.2851  loss_box_reg_stage2: 0.9068  loss_mask: 0.0889  loss_rpn_cls: 0.0179  loss_rpn_loc: 0.04446  time: 0.6253  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 14:41:32] d2.utils.events INFO:  eta: 9:22:09  iter: 13979  total_loss: 2.835  loss_cls_stage0: 0.2657  loss_box_reg_stage0: 0.2661  loss_cls_stage1: 0.3081  loss_box_reg_stage1: 0.6625  loss_cls_stage2: 0.2804  loss_box_reg_stage2: 0.8827  loss_mask: 0.08267  loss_rpn_cls: 0.0127  loss_rpn_loc: 0.04862  time: 0.6253  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 14:41:44] d2.utils.events INFO:  eta: 9:21:58  iter: 13999  total_loss: 2.762  loss_cls_stage0: 0.253  loss_box_reg_stage0: 0.2707  loss_cls_stage1: 0.2597  loss_box_reg_stage1: 0.6815  loss_cls_stage2: 0.2523  loss_box_reg_stage2: 0.9465  loss_mask: 0.0825  loss_rpn_cls: 0.01765  loss_rpn_loc: 0.04728  time: 0.6253  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:41:57] d2.utils.events INFO:  eta: 9:22:13  iter: 14019  total_loss: 3.112  loss_cls_stage0: 0.2834  loss_box_reg_stage0: 0.2794  loss_cls_stage1: 0.327  loss_box_reg_stage1: 0.7546  loss_cls_stage2: 0.3349  loss_box_reg_stage2: 0.9734  loss_mask: 0.08385  loss_rpn_cls: 0.01869  loss_rpn_loc: 0.04671  time: 0.6253  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:42:10] d2.utils.events INFO:  eta: 9:21:57  iter: 14039  total_loss: 3.044  loss_cls_stage0: 0.2502  loss_box_reg_stage0: 0.2649  loss_cls_stage1: 0.3171  loss_box_reg_stage1: 0.7289  loss_cls_stage2: 0.285  loss_box_reg_stage2: 0.9803  loss_mask: 0.07783  loss_rpn_cls: 0.01451  loss_rpn_loc: 0.04612  time: 0.6253  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:42:22] d2.utils.events INFO:  eta: 9:21:48  iter: 14059  total_loss: 2.944  loss_cls_stage0: 0.2738  loss_box_reg_stage0: 0.2667  loss_cls_stage1: 0.2971  loss_box_reg_stage1: 0.6729  loss_cls_stage2: 0.2636  loss_box_reg_stage2: 0.9225  loss_mask: 0.0744  loss_rpn_cls: 0.01758  loss_rpn_loc: 0.0491  time: 0.6253  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:42:35] d2.utils.events INFO:  eta: 9:21:43  iter: 14079  total_loss: 3.092  loss_cls_stage0: 0.2979  loss_box_reg_stage0: 0.2819  loss_cls_stage1: 0.3195  loss_box_reg_stage1: 0.7373  loss_cls_stage2: 0.3109  loss_box_reg_stage2: 0.936  loss_mask: 0.08089  loss_rpn_cls: 0.01946  loss_rpn_loc: 0.04938  time: 0.6253  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:42:48] d2.utils.events INFO:  eta: 9:22:24  iter: 14099  total_loss: 3.252  loss_cls_stage0: 0.3259  loss_box_reg_stage0: 0.2934  loss_cls_stage1: 0.3487  loss_box_reg_stage1: 0.7308  loss_cls_stage2: 0.3183  loss_box_reg_stage2: 0.9305  loss_mask: 0.08117  loss_rpn_cls: 0.01968  loss_rpn_loc: 0.05439  time: 0.6253  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 14:43:01] d2.utils.events INFO:  eta: 9:22:20  iter: 14119  total_loss: 2.973  loss_cls_stage0: 0.2743  loss_box_reg_stage0: 0.2667  loss_cls_stage1: 0.3  loss_box_reg_stage1: 0.6954  loss_cls_stage2: 0.3093  loss_box_reg_stage2: 0.9834  loss_mask: 0.08319  loss_rpn_cls: 0.01609  loss_rpn_loc: 0.0406  time: 0.6254  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 14:43:14] d2.utils.events INFO:  eta: 9:22:35  iter: 14139  total_loss: 3.214  loss_cls_stage0: 0.2899  loss_box_reg_stage0: 0.2784  loss_cls_stage1: 0.3439  loss_box_reg_stage1: 0.7615  loss_cls_stage2: 0.3285  loss_box_reg_stage2: 1.057  loss_mask: 0.08935  loss_rpn_cls: 0.02577  loss_rpn_loc: 0.05814  time: 0.6254  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 14:43:26] d2.utils.events INFO:  eta: 9:22:27  iter: 14159  total_loss: 3.15  loss_cls_stage0: 0.3054  loss_box_reg_stage0: 0.2798  loss_cls_stage1: 0.3265  loss_box_reg_stage1: 0.716  loss_cls_stage2: 0.2965  loss_box_reg_stage2: 0.9662  loss_mask: 0.08282  loss_rpn_cls: 0.02209  loss_rpn_loc: 0.04795  time: 0.6254  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 14:43:39] d2.utils.events INFO:  eta: 9:22:55  iter: 14179  total_loss: 3.008  loss_cls_stage0: 0.2949  loss_box_reg_stage0: 0.2814  loss_cls_stage1: 0.3153  loss_box_reg_stage1: 0.7079  loss_cls_stage2: 0.2837  loss_box_reg_stage2: 0.9083  loss_mask: 0.08686  loss_rpn_cls: 0.01704  loss_rpn_loc: 0.05449  time: 0.6254  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 14:43:51] d2.utils.events INFO:  eta: 9:22:51  iter: 14199  total_loss: 2.672  loss_cls_stage0: 0.2535  loss_box_reg_stage0: 0.2873  loss_cls_stage1: 0.2863  loss_box_reg_stage1: 0.6205  loss_cls_stage2: 0.2793  loss_box_reg_stage2: 0.8228  loss_mask: 0.08319  loss_rpn_cls: 0.01648  loss_rpn_loc: 0.04681  time: 0.6254  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:44:03] d2.utils.events INFO:  eta: 9:22:31  iter: 14219  total_loss: 2.866  loss_cls_stage0: 0.2672  loss_box_reg_stage0: 0.2712  loss_cls_stage1: 0.2983  loss_box_reg_stage1: 0.6728  loss_cls_stage2: 0.2879  loss_box_reg_stage2: 0.8775  loss_mask: 0.08674  loss_rpn_cls: 0.02335  loss_rpn_loc: 0.04141  time: 0.6254  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:44:16] d2.utils.events INFO:  eta: 9:22:37  iter: 14239  total_loss: 2.587  loss_cls_stage0: 0.2239  loss_box_reg_stage0: 0.2631  loss_cls_stage1: 0.2326  loss_box_reg_stage1: 0.6397  loss_cls_stage2: 0.2216  loss_box_reg_stage2: 0.8237  loss_mask: 0.07283  loss_rpn_cls: 0.01369  loss_rpn_loc: 0.04271  time: 0.6254  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:44:28] d2.utils.events INFO:  eta: 9:21:27  iter: 14259  total_loss: 2.994  loss_cls_stage0: 0.2833  loss_box_reg_stage0: 0.2993  loss_cls_stage1: 0.2963  loss_box_reg_stage1: 0.7041  loss_cls_stage2: 0.2757  loss_box_reg_stage2: 0.8859  loss_mask: 0.08875  loss_rpn_cls: 0.0209  loss_rpn_loc: 0.06023  time: 0.6253  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:44:40] d2.utils.events INFO:  eta: 9:21:03  iter: 14279  total_loss: 3  loss_cls_stage0: 0.308  loss_box_reg_stage0: 0.2996  loss_cls_stage1: 0.3119  loss_box_reg_stage1: 0.6995  loss_cls_stage2: 0.2915  loss_box_reg_stage2: 0.8063  loss_mask: 0.08252  loss_rpn_cls: 0.01638  loss_rpn_loc: 0.05195  time: 0.6253  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:44:53] d2.utils.events INFO:  eta: 9:20:54  iter: 14299  total_loss: 2.983  loss_cls_stage0: 0.2715  loss_box_reg_stage0: 0.2929  loss_cls_stage1: 0.2822  loss_box_reg_stage1: 0.7223  loss_cls_stage2: 0.2816  loss_box_reg_stage2: 0.9919  loss_mask: 0.08501  loss_rpn_cls: 0.01503  loss_rpn_loc: 0.04475  time: 0.6253  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:45:05] d2.utils.events INFO:  eta: 9:20:32  iter: 14319  total_loss: 2.913  loss_cls_stage0: 0.2784  loss_box_reg_stage0: 0.282  loss_cls_stage1: 0.3243  loss_box_reg_stage1: 0.7127  loss_cls_stage2: 0.2801  loss_box_reg_stage2: 0.9203  loss_mask: 0.08805  loss_rpn_cls: 0.01386  loss_rpn_loc: 0.04821  time: 0.6253  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:45:18] d2.utils.events INFO:  eta: 9:20:19  iter: 14339  total_loss: 3.177  loss_cls_stage0: 0.31  loss_box_reg_stage0: 0.3153  loss_cls_stage1: 0.3437  loss_box_reg_stage1: 0.7039  loss_cls_stage2: 0.3312  loss_box_reg_stage2: 0.966  loss_mask: 0.0962  loss_rpn_cls: 0.02462  loss_rpn_loc: 0.05073  time: 0.6253  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:45:30] d2.utils.events INFO:  eta: 9:19:39  iter: 14359  total_loss: 2.72  loss_cls_stage0: 0.256  loss_box_reg_stage0: 0.2696  loss_cls_stage1: 0.2687  loss_box_reg_stage1: 0.6301  loss_cls_stage2: 0.2626  loss_box_reg_stage2: 0.8673  loss_mask: 0.08368  loss_rpn_cls: 0.01462  loss_rpn_loc: 0.04751  time: 0.6253  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:45:42] d2.utils.events INFO:  eta: 9:18:46  iter: 14379  total_loss: 2.934  loss_cls_stage0: 0.2726  loss_box_reg_stage0: 0.2842  loss_cls_stage1: 0.292  loss_box_reg_stage1: 0.7138  loss_cls_stage2: 0.2831  loss_box_reg_stage2: 0.8961  loss_mask: 0.08202  loss_rpn_cls: 0.01499  loss_rpn_loc: 0.04282  time: 0.6252  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:45:55] d2.utils.events INFO:  eta: 9:18:40  iter: 14399  total_loss: 2.908  loss_cls_stage0: 0.2968  loss_box_reg_stage0: 0.2803  loss_cls_stage1: 0.3239  loss_box_reg_stage1: 0.6761  loss_cls_stage2: 0.3035  loss_box_reg_stage2: 0.8976  loss_mask: 0.0807  loss_rpn_cls: 0.01257  loss_rpn_loc: 0.04005  time: 0.6252  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 14:46:07] d2.utils.events INFO:  eta: 9:18:37  iter: 14419  total_loss: 3.301  loss_cls_stage0: 0.2868  loss_box_reg_stage0: 0.3027  loss_cls_stage1: 0.3063  loss_box_reg_stage1: 0.7839  loss_cls_stage2: 0.3094  loss_box_reg_stage2: 0.9951  loss_mask: 0.07596  loss_rpn_cls: 0.009405  loss_rpn_loc: 0.0417  time: 0.6252  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:46:19] d2.utils.events INFO:  eta: 9:18:15  iter: 14439  total_loss: 3.371  loss_cls_stage0: 0.3084  loss_box_reg_stage0: 0.2908  loss_cls_stage1: 0.3524  loss_box_reg_stage1: 0.8167  loss_cls_stage2: 0.3236  loss_box_reg_stage2: 1.063  loss_mask: 0.07886  loss_rpn_cls: 0.01484  loss_rpn_loc: 0.04593  time: 0.6252  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 14:46:32] d2.utils.events INFO:  eta: 9:17:56  iter: 14459  total_loss: 2.985  loss_cls_stage0: 0.277  loss_box_reg_stage0: 0.2944  loss_cls_stage1: 0.2918  loss_box_reg_stage1: 0.6848  loss_cls_stage2: 0.2826  loss_box_reg_stage2: 0.9534  loss_mask: 0.08283  loss_rpn_cls: 0.0187  loss_rpn_loc: 0.04859  time: 0.6252  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:46:44] d2.utils.events INFO:  eta: 9:17:25  iter: 14479  total_loss: 2.759  loss_cls_stage0: 0.2543  loss_box_reg_stage0: 0.2645  loss_cls_stage1: 0.2709  loss_box_reg_stage1: 0.6393  loss_cls_stage2: 0.2482  loss_box_reg_stage2: 0.8725  loss_mask: 0.07866  loss_rpn_cls: 0.02889  loss_rpn_loc: 0.05634  time: 0.6252  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:46:57] d2.utils.events INFO:  eta: 9:16:59  iter: 14499  total_loss: 2.666  loss_cls_stage0: 0.2525  loss_box_reg_stage0: 0.2719  loss_cls_stage1: 0.2875  loss_box_reg_stage1: 0.6636  loss_cls_stage2: 0.2792  loss_box_reg_stage2: 0.8272  loss_mask: 0.07877  loss_rpn_cls: 0.01569  loss_rpn_loc: 0.04372  time: 0.6252  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:47:09] d2.utils.events INFO:  eta: 9:16:45  iter: 14519  total_loss: 3.019  loss_cls_stage0: 0.2975  loss_box_reg_stage0: 0.3008  loss_cls_stage1: 0.3081  loss_box_reg_stage1: 0.7274  loss_cls_stage2: 0.2896  loss_box_reg_stage2: 0.9604  loss_mask: 0.08759  loss_rpn_cls: 0.009278  loss_rpn_loc: 0.04854  time: 0.6252  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:47:22] d2.utils.events INFO:  eta: 9:16:31  iter: 14539  total_loss: 3.074  loss_cls_stage0: 0.2814  loss_box_reg_stage0: 0.2868  loss_cls_stage1: 0.2897  loss_box_reg_stage1: 0.7159  loss_cls_stage2: 0.2839  loss_box_reg_stage2: 0.9727  loss_mask: 0.08355  loss_rpn_cls: 0.01629  loss_rpn_loc: 0.04657  time: 0.6252  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:47:34] d2.utils.events INFO:  eta: 9:16:22  iter: 14559  total_loss: 2.952  loss_cls_stage0: 0.2943  loss_box_reg_stage0: 0.2808  loss_cls_stage1: 0.3077  loss_box_reg_stage1: 0.7025  loss_cls_stage2: 0.3154  loss_box_reg_stage2: 0.8998  loss_mask: 0.09767  loss_rpn_cls: 0.01508  loss_rpn_loc: 0.04384  time: 0.6252  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:47:47] d2.utils.events INFO:  eta: 9:15:49  iter: 14579  total_loss: 3.065  loss_cls_stage0: 0.286  loss_box_reg_stage0: 0.2961  loss_cls_stage1: 0.3036  loss_box_reg_stage1: 0.699  loss_cls_stage2: 0.2863  loss_box_reg_stage2: 0.9239  loss_mask: 0.08756  loss_rpn_cls: 0.01529  loss_rpn_loc: 0.05233  time: 0.6252  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:47:59] d2.utils.events INFO:  eta: 9:15:14  iter: 14599  total_loss: 2.789  loss_cls_stage0: 0.2563  loss_box_reg_stage0: 0.2746  loss_cls_stage1: 0.2667  loss_box_reg_stage1: 0.6476  loss_cls_stage2: 0.2546  loss_box_reg_stage2: 0.8495  loss_mask: 0.07922  loss_rpn_cls: 0.02129  loss_rpn_loc: 0.05035  time: 0.6252  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:48:12] d2.utils.events INFO:  eta: 9:14:22  iter: 14619  total_loss: 2.975  loss_cls_stage0: 0.2447  loss_box_reg_stage0: 0.273  loss_cls_stage1: 0.2867  loss_box_reg_stage1: 0.6832  loss_cls_stage2: 0.2823  loss_box_reg_stage2: 0.9003  loss_mask: 0.07887  loss_rpn_cls: 0.0193  loss_rpn_loc: 0.04748  time: 0.6252  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 14:48:24] d2.utils.events INFO:  eta: 9:13:51  iter: 14639  total_loss: 3.188  loss_cls_stage0: 0.3036  loss_box_reg_stage0: 0.292  loss_cls_stage1: 0.3443  loss_box_reg_stage1: 0.7608  loss_cls_stage2: 0.3145  loss_box_reg_stage2: 0.9414  loss_mask: 0.08446  loss_rpn_cls: 0.0149  loss_rpn_loc: 0.04567  time: 0.6251  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 14:48:36] d2.utils.events INFO:  eta: 9:14:21  iter: 14659  total_loss: 3.149  loss_cls_stage0: 0.3018  loss_box_reg_stage0: 0.2854  loss_cls_stage1: 0.3155  loss_box_reg_stage1: 0.7488  loss_cls_stage2: 0.2726  loss_box_reg_stage2: 0.9631  loss_mask: 0.08436  loss_rpn_cls: 0.0181  loss_rpn_loc: 0.06126  time: 0.6251  data_time: 0.0045  lr: 0.00016  max_mem: 19679M
[07/29 14:48:49] d2.utils.events INFO:  eta: 9:13:49  iter: 14679  total_loss: 3.014  loss_cls_stage0: 0.2459  loss_box_reg_stage0: 0.291  loss_cls_stage1: 0.2618  loss_box_reg_stage1: 0.7328  loss_cls_stage2: 0.2683  loss_box_reg_stage2: 0.9814  loss_mask: 0.08346  loss_rpn_cls: 0.01467  loss_rpn_loc: 0.04951  time: 0.6251  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 14:49:01] d2.utils.events INFO:  eta: 9:13:31  iter: 14699  total_loss: 3.049  loss_cls_stage0: 0.2814  loss_box_reg_stage0: 0.2917  loss_cls_stage1: 0.2989  loss_box_reg_stage1: 0.703  loss_cls_stage2: 0.3079  loss_box_reg_stage2: 0.9422  loss_mask: 0.09068  loss_rpn_cls: 0.02487  loss_rpn_loc: 0.0454  time: 0.6250  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 14:49:13] d2.utils.events INFO:  eta: 9:12:50  iter: 14719  total_loss: 2.671  loss_cls_stage0: 0.2564  loss_box_reg_stage0: 0.2639  loss_cls_stage1: 0.2726  loss_box_reg_stage1: 0.6286  loss_cls_stage2: 0.2429  loss_box_reg_stage2: 0.8418  loss_mask: 0.07985  loss_rpn_cls: 0.01948  loss_rpn_loc: 0.04518  time: 0.6250  data_time: 0.0045  lr: 0.00016  max_mem: 19679M
[07/29 14:49:25] d2.utils.events INFO:  eta: 9:12:38  iter: 14739  total_loss: 2.719  loss_cls_stage0: 0.2308  loss_box_reg_stage0: 0.2716  loss_cls_stage1: 0.2542  loss_box_reg_stage1: 0.6699  loss_cls_stage2: 0.2466  loss_box_reg_stage2: 0.9018  loss_mask: 0.07806  loss_rpn_cls: 0.01188  loss_rpn_loc: 0.04122  time: 0.6250  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:49:38] d2.utils.events INFO:  eta: 9:12:27  iter: 14759  total_loss: 3.116  loss_cls_stage0: 0.3217  loss_box_reg_stage0: 0.2921  loss_cls_stage1: 0.3597  loss_box_reg_stage1: 0.7747  loss_cls_stage2: 0.3352  loss_box_reg_stage2: 0.994  loss_mask: 0.09176  loss_rpn_cls: 0.01573  loss_rpn_loc: 0.0514  time: 0.6250  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 14:49:51] d2.utils.events INFO:  eta: 9:12:09  iter: 14779  total_loss: 3.062  loss_cls_stage0: 0.2829  loss_box_reg_stage0: 0.2797  loss_cls_stage1: 0.3005  loss_box_reg_stage1: 0.7428  loss_cls_stage2: 0.311  loss_box_reg_stage2: 0.9098  loss_mask: 0.08198  loss_rpn_cls: 0.01035  loss_rpn_loc: 0.05752  time: 0.6251  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:50:03] d2.utils.events INFO:  eta: 9:12:19  iter: 14799  total_loss: 3.345  loss_cls_stage0: 0.3256  loss_box_reg_stage0: 0.3158  loss_cls_stage1: 0.3711  loss_box_reg_stage1: 0.8001  loss_cls_stage2: 0.3325  loss_box_reg_stage2: 1.013  loss_mask: 0.09223  loss_rpn_cls: 0.02074  loss_rpn_loc: 0.05622  time: 0.6251  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:50:16] d2.utils.events INFO:  eta: 9:11:49  iter: 14819  total_loss: 2.761  loss_cls_stage0: 0.2943  loss_box_reg_stage0: 0.2695  loss_cls_stage1: 0.2678  loss_box_reg_stage1: 0.6506  loss_cls_stage2: 0.2599  loss_box_reg_stage2: 0.8307  loss_mask: 0.08204  loss_rpn_cls: 0.01891  loss_rpn_loc: 0.04762  time: 0.6251  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 14:50:28] d2.utils.events INFO:  eta: 9:11:22  iter: 14839  total_loss: 3.225  loss_cls_stage0: 0.29  loss_box_reg_stage0: 0.2965  loss_cls_stage1: 0.3359  loss_box_reg_stage1: 0.7774  loss_cls_stage2: 0.309  loss_box_reg_stage2: 0.9671  loss_mask: 0.08289  loss_rpn_cls: 0.01993  loss_rpn_loc: 0.04866  time: 0.6250  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 14:50:41] d2.utils.events INFO:  eta: 9:11:28  iter: 14859  total_loss: 3.34  loss_cls_stage0: 0.3211  loss_box_reg_stage0: 0.3197  loss_cls_stage1: 0.3559  loss_box_reg_stage1: 0.7802  loss_cls_stage2: 0.335  loss_box_reg_stage2: 0.9613  loss_mask: 0.09371  loss_rpn_cls: 0.01092  loss_rpn_loc: 0.04094  time: 0.6250  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 14:50:53] d2.utils.events INFO:  eta: 9:11:23  iter: 14879  total_loss: 2.919  loss_cls_stage0: 0.279  loss_box_reg_stage0: 0.2702  loss_cls_stage1: 0.3147  loss_box_reg_stage1: 0.6592  loss_cls_stage2: 0.3093  loss_box_reg_stage2: 0.9206  loss_mask: 0.08014  loss_rpn_cls: 0.01595  loss_rpn_loc: 0.04169  time: 0.6250  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:51:06] d2.utils.events INFO:  eta: 9:11:10  iter: 14899  total_loss: 2.957  loss_cls_stage0: 0.2808  loss_box_reg_stage0: 0.27  loss_cls_stage1: 0.3169  loss_box_reg_stage1: 0.7255  loss_cls_stage2: 0.2832  loss_box_reg_stage2: 0.9377  loss_mask: 0.08359  loss_rpn_cls: 0.01517  loss_rpn_loc: 0.04907  time: 0.6250  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 14:51:18] d2.utils.events INFO:  eta: 9:10:55  iter: 14919  total_loss: 2.817  loss_cls_stage0: 0.2894  loss_box_reg_stage0: 0.2849  loss_cls_stage1: 0.2699  loss_box_reg_stage1: 0.669  loss_cls_stage2: 0.2872  loss_box_reg_stage2: 0.9227  loss_mask: 0.07861  loss_rpn_cls: 0.01602  loss_rpn_loc: 0.04643  time: 0.6250  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 14:51:30] d2.utils.events INFO:  eta: 9:10:28  iter: 14939  total_loss: 3.173  loss_cls_stage0: 0.2786  loss_box_reg_stage0: 0.2796  loss_cls_stage1: 0.3249  loss_box_reg_stage1: 0.7333  loss_cls_stage2: 0.2977  loss_box_reg_stage2: 0.984  loss_mask: 0.08134  loss_rpn_cls: 0.01468  loss_rpn_loc: 0.04821  time: 0.6249  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 14:51:42] d2.utils.events INFO:  eta: 9:09:45  iter: 14959  total_loss: 3.079  loss_cls_stage0: 0.2744  loss_box_reg_stage0: 0.2727  loss_cls_stage1: 0.3186  loss_box_reg_stage1: 0.7261  loss_cls_stage2: 0.3139  loss_box_reg_stage2: 0.9325  loss_mask: 0.08166  loss_rpn_cls: 0.0132  loss_rpn_loc: 0.0558  time: 0.6249  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 14:51:54] d2.utils.events INFO:  eta: 9:09:42  iter: 14979  total_loss: 2.718  loss_cls_stage0: 0.2452  loss_box_reg_stage0: 0.2578  loss_cls_stage1: 0.2575  loss_box_reg_stage1: 0.6608  loss_cls_stage2: 0.2509  loss_box_reg_stage2: 0.911  loss_mask: 0.07907  loss_rpn_cls: 0.01712  loss_rpn_loc: 0.04827  time: 0.6249  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 14:52:06] fvcore.common.checkpoint INFO: Saving checkpoint to ./output/model_0014999.pth
[07/29 14:52:09] d2.data.datasets.coco INFO: Loading datasets/tless/tless_annotations_test.json takes 1.62 seconds.
[07/29 14:52:09] d2.data.datasets.coco INFO: Loaded 10080 images in COCO format from datasets/tless/tless_annotations_test.json
[07/29 14:52:09] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[07/29 14:52:09] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[07/29 14:52:09] d2.data.common INFO: Serializing 10080 elements to byte tensors and concatenating them all ...
[07/29 14:52:09] d2.data.common INFO: Serialized dataset takes 50.77 MiB
[07/29 14:52:11] d2.evaluation.evaluator INFO: Start inference on 10080 batches
[07/29 14:52:12] d2.evaluation.evaluator INFO: Inference done 11/10080. Dataloading: 0.0007 s/iter. Inference: 0.1020 s/iter. Eval: 0.0032 s/iter. Total: 0.1059 s/iter. ETA=0:17:45
[07/29 14:52:17] d2.evaluation.evaluator INFO: Inference done 59/10080. Dataloading: 0.0009 s/iter. Inference: 0.1020 s/iter. Eval: 0.0026 s/iter. Total: 0.1056 s/iter. ETA=0:17:38
[07/29 14:52:22] d2.evaluation.evaluator INFO: Inference done 107/10080. Dataloading: 0.0009 s/iter. Inference: 0.1020 s/iter. Eval: 0.0026 s/iter. Total: 0.1056 s/iter. ETA=0:17:33
[07/29 14:52:27] d2.evaluation.evaluator INFO: Inference done 155/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0027 s/iter. Total: 0.1057 s/iter. ETA=0:17:28
[07/29 14:52:32] d2.evaluation.evaluator INFO: Inference done 203/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0028 s/iter. Total: 0.1057 s/iter. ETA=0:17:24
[07/29 14:52:37] d2.evaluation.evaluator INFO: Inference done 251/10080. Dataloading: 0.0009 s/iter. Inference: 0.1020 s/iter. Eval: 0.0028 s/iter. Total: 0.1058 s/iter. ETA=0:17:20
[07/29 14:52:43] d2.evaluation.evaluator INFO: Inference done 299/10080. Dataloading: 0.0009 s/iter. Inference: 0.1020 s/iter. Eval: 0.0028 s/iter. Total: 0.1058 s/iter. ETA=0:17:14
[07/29 14:52:48] d2.evaluation.evaluator INFO: Inference done 348/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0027 s/iter. Total: 0.1055 s/iter. ETA=0:17:06
[07/29 14:52:53] d2.evaluation.evaluator INFO: Inference done 397/10080. Dataloading: 0.0009 s/iter. Inference: 0.1017 s/iter. Eval: 0.0026 s/iter. Total: 0.1053 s/iter. ETA=0:16:59
[07/29 14:52:58] d2.evaluation.evaluator INFO: Inference done 445/10080. Dataloading: 0.0009 s/iter. Inference: 0.1017 s/iter. Eval: 0.0025 s/iter. Total: 0.1052 s/iter. ETA=0:16:53
[07/29 14:53:03] d2.evaluation.evaluator INFO: Inference done 493/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0025 s/iter. Total: 0.1052 s/iter. ETA=0:16:48
[07/29 14:53:08] d2.evaluation.evaluator INFO: Inference done 542/10080. Dataloading: 0.0009 s/iter. Inference: 0.1017 s/iter. Eval: 0.0024 s/iter. Total: 0.1050 s/iter. ETA=0:16:41
[07/29 14:53:13] d2.evaluation.evaluator INFO: Inference done 591/10080. Dataloading: 0.0009 s/iter. Inference: 0.1016 s/iter. Eval: 0.0023 s/iter. Total: 0.1048 s/iter. ETA=0:16:34
[07/29 14:53:18] d2.evaluation.evaluator INFO: Inference done 640/10080. Dataloading: 0.0009 s/iter. Inference: 0.1016 s/iter. Eval: 0.0022 s/iter. Total: 0.1047 s/iter. ETA=0:16:28
[07/29 14:53:23] d2.evaluation.evaluator INFO: Inference done 688/10080. Dataloading: 0.0009 s/iter. Inference: 0.1016 s/iter. Eval: 0.0022 s/iter. Total: 0.1047 s/iter. ETA=0:16:23
[07/29 14:53:28] d2.evaluation.evaluator INFO: Inference done 736/10080. Dataloading: 0.0009 s/iter. Inference: 0.1016 s/iter. Eval: 0.0022 s/iter. Total: 0.1047 s/iter. ETA=0:16:18
[07/29 14:53:33] d2.evaluation.evaluator INFO: Inference done 784/10080. Dataloading: 0.0009 s/iter. Inference: 0.1015 s/iter. Eval: 0.0022 s/iter. Total: 0.1047 s/iter. ETA=0:16:13
[07/29 14:53:38] d2.evaluation.evaluator INFO: Inference done 832/10080. Dataloading: 0.0009 s/iter. Inference: 0.1016 s/iter. Eval: 0.0022 s/iter. Total: 0.1047 s/iter. ETA=0:16:08
[07/29 14:53:43] d2.evaluation.evaluator INFO: Inference done 881/10080. Dataloading: 0.0009 s/iter. Inference: 0.1015 s/iter. Eval: 0.0022 s/iter. Total: 0.1047 s/iter. ETA=0:16:02
[07/29 14:53:48] d2.evaluation.evaluator INFO: Inference done 930/10080. Dataloading: 0.0009 s/iter. Inference: 0.1015 s/iter. Eval: 0.0021 s/iter. Total: 0.1046 s/iter. ETA=0:15:57
[07/29 14:53:53] d2.evaluation.evaluator INFO: Inference done 979/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0020 s/iter. Total: 0.1046 s/iter. ETA=0:15:51
[07/29 14:53:58] d2.evaluation.evaluator INFO: Inference done 1027/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0020 s/iter. Total: 0.1046 s/iter. ETA=0:15:46
[07/29 14:54:03] d2.evaluation.evaluator INFO: Inference done 1075/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0020 s/iter. Total: 0.1046 s/iter. ETA=0:15:41
[07/29 14:54:08] d2.evaluation.evaluator INFO: Inference done 1123/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0020 s/iter. Total: 0.1047 s/iter. ETA=0:15:37
[07/29 14:54:14] d2.evaluation.evaluator INFO: Inference done 1171/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0020 s/iter. Total: 0.1047 s/iter. ETA=0:15:33
[07/29 14:54:19] d2.evaluation.evaluator INFO: Inference done 1218/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0020 s/iter. Total: 0.1048 s/iter. ETA=0:15:28
[07/29 14:54:24] d2.evaluation.evaluator INFO: Inference done 1265/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0021 s/iter. Total: 0.1049 s/iter. ETA=0:15:24
[07/29 14:54:29] d2.evaluation.evaluator INFO: Inference done 1313/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0021 s/iter. Total: 0.1049 s/iter. ETA=0:15:19
[07/29 14:54:34] d2.evaluation.evaluator INFO: Inference done 1361/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0021 s/iter. Total: 0.1049 s/iter. ETA=0:15:14
[07/29 14:54:39] d2.evaluation.evaluator INFO: Inference done 1409/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0021 s/iter. Total: 0.1049 s/iter. ETA=0:15:09
[07/29 14:54:44] d2.evaluation.evaluator INFO: Inference done 1457/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0020 s/iter. Total: 0.1049 s/iter. ETA=0:15:04
[07/29 14:54:49] d2.evaluation.evaluator INFO: Inference done 1506/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0020 s/iter. Total: 0.1048 s/iter. ETA=0:14:58
[07/29 14:54:54] d2.evaluation.evaluator INFO: Inference done 1555/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0020 s/iter. Total: 0.1048 s/iter. ETA=0:14:53
[07/29 14:54:59] d2.evaluation.evaluator INFO: Inference done 1603/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0020 s/iter. Total: 0.1048 s/iter. ETA=0:14:48
[07/29 14:55:04] d2.evaluation.evaluator INFO: Inference done 1651/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0020 s/iter. Total: 0.1048 s/iter. ETA=0:14:43
[07/29 14:55:09] d2.evaluation.evaluator INFO: Inference done 1699/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0021 s/iter. Total: 0.1048 s/iter. ETA=0:14:38
[07/29 14:55:14] d2.evaluation.evaluator INFO: Inference done 1747/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0021 s/iter. Total: 0.1048 s/iter. ETA=0:14:33
[07/29 14:55:19] d2.evaluation.evaluator INFO: Inference done 1795/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0021 s/iter. Total: 0.1049 s/iter. ETA=0:14:28
[07/29 14:55:24] d2.evaluation.evaluator INFO: Inference done 1843/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0021 s/iter. Total: 0.1049 s/iter. ETA=0:14:23
[07/29 14:55:29] d2.evaluation.evaluator INFO: Inference done 1891/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0021 s/iter. Total: 0.1049 s/iter. ETA=0:14:18
[07/29 14:55:34] d2.evaluation.evaluator INFO: Inference done 1939/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0021 s/iter. Total: 0.1048 s/iter. ETA=0:14:13
[07/29 14:55:39] d2.evaluation.evaluator INFO: Inference done 1987/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0021 s/iter. Total: 0.1048 s/iter. ETA=0:14:08
[07/29 14:55:44] d2.evaluation.evaluator INFO: Inference done 2035/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0021 s/iter. Total: 0.1048 s/iter. ETA=0:14:03
[07/29 14:55:49] d2.evaluation.evaluator INFO: Inference done 2082/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0022 s/iter. Total: 0.1049 s/iter. ETA=0:13:58
[07/29 14:55:54] d2.evaluation.evaluator INFO: Inference done 2129/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0022 s/iter. Total: 0.1049 s/iter. ETA=0:13:54
[07/29 14:55:59] d2.evaluation.evaluator INFO: Inference done 2177/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0022 s/iter. Total: 0.1049 s/iter. ETA=0:13:49
[07/29 14:56:04] d2.evaluation.evaluator INFO: Inference done 2225/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0022 s/iter. Total: 0.1050 s/iter. ETA=0:13:44
[07/29 14:56:09] d2.evaluation.evaluator INFO: Inference done 2272/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0022 s/iter. Total: 0.1050 s/iter. ETA=0:13:39
[07/29 14:56:15] d2.evaluation.evaluator INFO: Inference done 2320/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0023 s/iter. Total: 0.1050 s/iter. ETA=0:13:34
[07/29 14:56:20] d2.evaluation.evaluator INFO: Inference done 2367/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0023 s/iter. Total: 0.1050 s/iter. ETA=0:13:30
[07/29 14:56:25] d2.evaluation.evaluator INFO: Inference done 2414/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0023 s/iter. Total: 0.1051 s/iter. ETA=0:13:25
[07/29 14:56:30] d2.evaluation.evaluator INFO: Inference done 2462/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0023 s/iter. Total: 0.1051 s/iter. ETA=0:13:20
[07/29 14:56:35] d2.evaluation.evaluator INFO: Inference done 2509/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0023 s/iter. Total: 0.1051 s/iter. ETA=0:13:15
[07/29 14:56:40] d2.evaluation.evaluator INFO: Inference done 2557/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0023 s/iter. Total: 0.1051 s/iter. ETA=0:13:10
[07/29 14:56:45] d2.evaluation.evaluator INFO: Inference done 2605/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0023 s/iter. Total: 0.1051 s/iter. ETA=0:13:05
[07/29 14:56:50] d2.evaluation.evaluator INFO: Inference done 2653/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0023 s/iter. Total: 0.1051 s/iter. ETA=0:13:00
[07/29 14:56:55] d2.evaluation.evaluator INFO: Inference done 2701/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0023 s/iter. Total: 0.1051 s/iter. ETA=0:12:55
[07/29 14:57:00] d2.evaluation.evaluator INFO: Inference done 2749/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0023 s/iter. Total: 0.1051 s/iter. ETA=0:12:50
[07/29 14:57:05] d2.evaluation.evaluator INFO: Inference done 2797/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0023 s/iter. Total: 0.1051 s/iter. ETA=0:12:45
[07/29 14:57:10] d2.evaluation.evaluator INFO: Inference done 2845/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0023 s/iter. Total: 0.1051 s/iter. ETA=0:12:40
[07/29 14:57:15] d2.evaluation.evaluator INFO: Inference done 2893/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0024 s/iter. Total: 0.1051 s/iter. ETA=0:12:35
[07/29 14:57:20] d2.evaluation.evaluator INFO: Inference done 2941/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0023 s/iter. Total: 0.1051 s/iter. ETA=0:12:30
[07/29 14:57:25] d2.evaluation.evaluator INFO: Inference done 2989/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0023 s/iter. Total: 0.1051 s/iter. ETA=0:12:25
[07/29 14:57:30] d2.evaluation.evaluator INFO: Inference done 3037/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0023 s/iter. Total: 0.1051 s/iter. ETA=0:12:20
[07/29 14:57:35] d2.evaluation.evaluator INFO: Inference done 3083/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0024 s/iter. Total: 0.1052 s/iter. ETA=0:12:16
[07/29 14:57:40] d2.evaluation.evaluator INFO: Inference done 3129/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0024 s/iter. Total: 0.1053 s/iter. ETA=0:12:11
[07/29 14:57:45] d2.evaluation.evaluator INFO: Inference done 3175/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0025 s/iter. Total: 0.1053 s/iter. ETA=0:12:07
[07/29 14:57:50] d2.evaluation.evaluator INFO: Inference done 3221/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0025 s/iter. Total: 0.1054 s/iter. ETA=0:12:02
[07/29 14:57:55] d2.evaluation.evaluator INFO: Inference done 3267/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0026 s/iter. Total: 0.1054 s/iter. ETA=0:11:58
[07/29 14:58:00] d2.evaluation.evaluator INFO: Inference done 3314/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0026 s/iter. Total: 0.1055 s/iter. ETA=0:11:53
[07/29 14:58:06] d2.evaluation.evaluator INFO: Inference done 3361/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0026 s/iter. Total: 0.1055 s/iter. ETA=0:11:48
[07/29 14:58:11] d2.evaluation.evaluator INFO: Inference done 3408/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0026 s/iter. Total: 0.1055 s/iter. ETA=0:11:44
[07/29 14:58:16] d2.evaluation.evaluator INFO: Inference done 3454/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0027 s/iter. Total: 0.1056 s/iter. ETA=0:11:39
[07/29 14:58:21] d2.evaluation.evaluator INFO: Inference done 3502/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0027 s/iter. Total: 0.1056 s/iter. ETA=0:11:34
[07/29 14:58:26] d2.evaluation.evaluator INFO: Inference done 3549/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0027 s/iter. Total: 0.1056 s/iter. ETA=0:11:29
[07/29 14:58:31] d2.evaluation.evaluator INFO: Inference done 3596/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0027 s/iter. Total: 0.1056 s/iter. ETA=0:11:24
[07/29 14:58:36] d2.evaluation.evaluator INFO: Inference done 3643/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0027 s/iter. Total: 0.1057 s/iter. ETA=0:11:20
[07/29 14:58:41] d2.evaluation.evaluator INFO: Inference done 3690/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0028 s/iter. Total: 0.1057 s/iter. ETA=0:11:15
[07/29 14:58:46] d2.evaluation.evaluator INFO: Inference done 3737/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0028 s/iter. Total: 0.1057 s/iter. ETA=0:11:10
[07/29 14:58:51] d2.evaluation.evaluator INFO: Inference done 3784/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0028 s/iter. Total: 0.1058 s/iter. ETA=0:11:05
[07/29 14:58:56] d2.evaluation.evaluator INFO: Inference done 3831/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0029 s/iter. Total: 0.1058 s/iter. ETA=0:11:01
[07/29 14:59:01] d2.evaluation.evaluator INFO: Inference done 3878/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0029 s/iter. Total: 0.1058 s/iter. ETA=0:10:56
[07/29 14:59:06] d2.evaluation.evaluator INFO: Inference done 3925/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0029 s/iter. Total: 0.1059 s/iter. ETA=0:10:51
[07/29 14:59:11] d2.evaluation.evaluator INFO: Inference done 3968/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0030 s/iter. Total: 0.1060 s/iter. ETA=0:10:47
[07/29 14:59:16] d2.evaluation.evaluator INFO: Inference done 4015/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0030 s/iter. Total: 0.1060 s/iter. ETA=0:10:42
[07/29 14:59:21] d2.evaluation.evaluator INFO: Inference done 4061/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0030 s/iter. Total: 0.1060 s/iter. ETA=0:10:38
[07/29 14:59:26] d2.evaluation.evaluator INFO: Inference done 4106/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0031 s/iter. Total: 0.1061 s/iter. ETA=0:10:33
[07/29 14:59:32] d2.evaluation.evaluator INFO: Inference done 4152/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0031 s/iter. Total: 0.1061 s/iter. ETA=0:10:29
[07/29 14:59:37] d2.evaluation.evaluator INFO: Inference done 4198/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0031 s/iter. Total: 0.1062 s/iter. ETA=0:10:24
[07/29 14:59:42] d2.evaluation.evaluator INFO: Inference done 4244/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0032 s/iter. Total: 0.1062 s/iter. ETA=0:10:20
[07/29 14:59:47] d2.evaluation.evaluator INFO: Inference done 4290/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0032 s/iter. Total: 0.1063 s/iter. ETA=0:10:15
[07/29 14:59:52] d2.evaluation.evaluator INFO: Inference done 4337/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0032 s/iter. Total: 0.1063 s/iter. ETA=0:10:10
[07/29 14:59:57] d2.evaluation.evaluator INFO: Inference done 4384/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0032 s/iter. Total: 0.1063 s/iter. ETA=0:10:05
[07/29 15:00:02] d2.evaluation.evaluator INFO: Inference done 4431/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0033 s/iter. Total: 0.1063 s/iter. ETA=0:10:00
[07/29 15:00:07] d2.evaluation.evaluator INFO: Inference done 4478/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0033 s/iter. Total: 0.1063 s/iter. ETA=0:09:55
[07/29 15:00:12] d2.evaluation.evaluator INFO: Inference done 4525/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0033 s/iter. Total: 0.1063 s/iter. ETA=0:09:50
[07/29 15:00:17] d2.evaluation.evaluator INFO: Inference done 4572/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0033 s/iter. Total: 0.1063 s/iter. ETA=0:09:45
[07/29 15:00:22] d2.evaluation.evaluator INFO: Inference done 4619/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0033 s/iter. Total: 0.1064 s/iter. ETA=0:09:40
[07/29 15:00:27] d2.evaluation.evaluator INFO: Inference done 4666/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0033 s/iter. Total: 0.1064 s/iter. ETA=0:09:35
[07/29 15:00:32] d2.evaluation.evaluator INFO: Inference done 4713/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0034 s/iter. Total: 0.1064 s/iter. ETA=0:09:30
[07/29 15:00:37] d2.evaluation.evaluator INFO: Inference done 4760/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0034 s/iter. Total: 0.1064 s/iter. ETA=0:09:26
[07/29 15:00:42] d2.evaluation.evaluator INFO: Inference done 4807/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0034 s/iter. Total: 0.1064 s/iter. ETA=0:09:21
[07/29 15:00:48] d2.evaluation.evaluator INFO: Inference done 4855/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0034 s/iter. Total: 0.1064 s/iter. ETA=0:09:16
[07/29 15:00:53] d2.evaluation.evaluator INFO: Inference done 4902/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0034 s/iter. Total: 0.1064 s/iter. ETA=0:09:11
[07/29 15:00:58] d2.evaluation.evaluator INFO: Inference done 4950/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0034 s/iter. Total: 0.1064 s/iter. ETA=0:09:05
[07/29 15:01:03] d2.evaluation.evaluator INFO: Inference done 4997/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0034 s/iter. Total: 0.1064 s/iter. ETA=0:09:00
[07/29 15:01:08] d2.evaluation.evaluator INFO: Inference done 5044/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0034 s/iter. Total: 0.1064 s/iter. ETA=0:08:55
[07/29 15:01:13] d2.evaluation.evaluator INFO: Inference done 5092/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0034 s/iter. Total: 0.1064 s/iter. ETA=0:08:50
[07/29 15:01:18] d2.evaluation.evaluator INFO: Inference done 5139/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0034 s/iter. Total: 0.1064 s/iter. ETA=0:08:45
[07/29 15:01:23] d2.evaluation.evaluator INFO: Inference done 5186/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0034 s/iter. Total: 0.1064 s/iter. ETA=0:08:40
[07/29 15:01:28] d2.evaluation.evaluator INFO: Inference done 5233/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0034 s/iter. Total: 0.1064 s/iter. ETA=0:08:35
[07/29 15:01:33] d2.evaluation.evaluator INFO: Inference done 5280/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0034 s/iter. Total: 0.1064 s/iter. ETA=0:08:30
[07/29 15:01:38] d2.evaluation.evaluator INFO: Inference done 5327/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0035 s/iter. Total: 0.1064 s/iter. ETA=0:08:25
[07/29 15:01:43] d2.evaluation.evaluator INFO: Inference done 5375/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0035 s/iter. Total: 0.1064 s/iter. ETA=0:08:20
[07/29 15:01:48] d2.evaluation.evaluator INFO: Inference done 5423/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0035 s/iter. Total: 0.1064 s/iter. ETA=0:08:15
[07/29 15:01:53] d2.evaluation.evaluator INFO: Inference done 5471/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0034 s/iter. Total: 0.1064 s/iter. ETA=0:08:10
[07/29 15:01:58] d2.evaluation.evaluator INFO: Inference done 5520/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0034 s/iter. Total: 0.1064 s/iter. ETA=0:08:05
[07/29 15:02:03] d2.evaluation.evaluator INFO: Inference done 5568/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0034 s/iter. Total: 0.1064 s/iter. ETA=0:08:00
[07/29 15:02:08] d2.evaluation.evaluator INFO: Inference done 5615/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0034 s/iter. Total: 0.1064 s/iter. ETA=0:07:55
[07/29 15:02:13] d2.evaluation.evaluator INFO: Inference done 5662/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0034 s/iter. Total: 0.1064 s/iter. ETA=0:07:50
[07/29 15:02:19] d2.evaluation.evaluator INFO: Inference done 5709/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0034 s/iter. Total: 0.1064 s/iter. ETA=0:07:45
[07/29 15:02:24] d2.evaluation.evaluator INFO: Inference done 5756/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0034 s/iter. Total: 0.1064 s/iter. ETA=0:07:40
[07/29 15:02:29] d2.evaluation.evaluator INFO: Inference done 5803/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0035 s/iter. Total: 0.1065 s/iter. ETA=0:07:35
[07/29 15:02:34] d2.evaluation.evaluator INFO: Inference done 5850/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0035 s/iter. Total: 0.1065 s/iter. ETA=0:07:30
[07/29 15:02:39] d2.evaluation.evaluator INFO: Inference done 5897/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0035 s/iter. Total: 0.1065 s/iter. ETA=0:07:25
[07/29 15:02:44] d2.evaluation.evaluator INFO: Inference done 5944/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0035 s/iter. Total: 0.1065 s/iter. ETA=0:07:20
[07/29 15:02:49] d2.evaluation.evaluator INFO: Inference done 5992/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0034 s/iter. Total: 0.1065 s/iter. ETA=0:07:15
[07/29 15:02:54] d2.evaluation.evaluator INFO: Inference done 6039/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0034 s/iter. Total: 0.1065 s/iter. ETA=0:07:10
[07/29 15:02:59] d2.evaluation.evaluator INFO: Inference done 6086/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0034 s/iter. Total: 0.1065 s/iter. ETA=0:07:05
[07/29 15:03:04] d2.evaluation.evaluator INFO: Inference done 6133/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0034 s/iter. Total: 0.1065 s/iter. ETA=0:07:00
[07/29 15:03:09] d2.evaluation.evaluator INFO: Inference done 6180/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0035 s/iter. Total: 0.1065 s/iter. ETA=0:06:55
[07/29 15:03:14] d2.evaluation.evaluator INFO: Inference done 6226/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0035 s/iter. Total: 0.1065 s/iter. ETA=0:06:50
[07/29 15:03:19] d2.evaluation.evaluator INFO: Inference done 6273/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0035 s/iter. Total: 0.1065 s/iter. ETA=0:06:45
[07/29 15:03:24] d2.evaluation.evaluator INFO: Inference done 6320/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0035 s/iter. Total: 0.1065 s/iter. ETA=0:06:40
[07/29 15:03:29] d2.evaluation.evaluator INFO: Inference done 6367/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0035 s/iter. Total: 0.1065 s/iter. ETA=0:06:35
[07/29 15:03:34] d2.evaluation.evaluator INFO: Inference done 6414/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0035 s/iter. Total: 0.1065 s/iter. ETA=0:06:30
[07/29 15:03:39] d2.evaluation.evaluator INFO: Inference done 6461/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0035 s/iter. Total: 0.1065 s/iter. ETA=0:06:25
[07/29 15:03:44] d2.evaluation.evaluator INFO: Inference done 6509/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0035 s/iter. Total: 0.1065 s/iter. ETA=0:06:20
[07/29 15:03:49] d2.evaluation.evaluator INFO: Inference done 6556/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0035 s/iter. Total: 0.1065 s/iter. ETA=0:06:15
[07/29 15:03:54] d2.evaluation.evaluator INFO: Inference done 6602/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0035 s/iter. Total: 0.1066 s/iter. ETA=0:06:10
[07/29 15:03:59] d2.evaluation.evaluator INFO: Inference done 6648/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0035 s/iter. Total: 0.1066 s/iter. ETA=0:06:05
[07/29 15:04:05] d2.evaluation.evaluator INFO: Inference done 6694/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0036 s/iter. Total: 0.1066 s/iter. ETA=0:06:00
[07/29 15:04:10] d2.evaluation.evaluator INFO: Inference done 6740/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0036 s/iter. Total: 0.1066 s/iter. ETA=0:05:56
[07/29 15:04:15] d2.evaluation.evaluator INFO: Inference done 6787/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0036 s/iter. Total: 0.1066 s/iter. ETA=0:05:51
[07/29 15:04:20] d2.evaluation.evaluator INFO: Inference done 6833/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0036 s/iter. Total: 0.1067 s/iter. ETA=0:05:46
[07/29 15:04:25] d2.evaluation.evaluator INFO: Inference done 6880/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0036 s/iter. Total: 0.1067 s/iter. ETA=0:05:41
[07/29 15:04:30] d2.evaluation.evaluator INFO: Inference done 6928/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0036 s/iter. Total: 0.1067 s/iter. ETA=0:05:36
[07/29 15:04:35] d2.evaluation.evaluator INFO: Inference done 6976/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0036 s/iter. Total: 0.1067 s/iter. ETA=0:05:31
[07/29 15:04:40] d2.evaluation.evaluator INFO: Inference done 7023/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0036 s/iter. Total: 0.1067 s/iter. ETA=0:05:26
[07/29 15:04:45] d2.evaluation.evaluator INFO: Inference done 7071/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0036 s/iter. Total: 0.1066 s/iter. ETA=0:05:20
[07/29 15:04:50] d2.evaluation.evaluator INFO: Inference done 7118/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0036 s/iter. Total: 0.1067 s/iter. ETA=0:05:15
[07/29 15:04:55] d2.evaluation.evaluator INFO: Inference done 7165/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0036 s/iter. Total: 0.1067 s/iter. ETA=0:05:10
[07/29 15:05:00] d2.evaluation.evaluator INFO: Inference done 7212/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0036 s/iter. Total: 0.1067 s/iter. ETA=0:05:05
[07/29 15:05:05] d2.evaluation.evaluator INFO: Inference done 7259/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0036 s/iter. Total: 0.1067 s/iter. ETA=0:05:00
[07/29 15:05:10] d2.evaluation.evaluator INFO: Inference done 7306/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0036 s/iter. Total: 0.1067 s/iter. ETA=0:04:55
[07/29 15:05:15] d2.evaluation.evaluator INFO: Inference done 7353/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0036 s/iter. Total: 0.1067 s/iter. ETA=0:04:50
[07/29 15:05:20] d2.evaluation.evaluator INFO: Inference done 7400/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0036 s/iter. Total: 0.1067 s/iter. ETA=0:04:45
[07/29 15:05:25] d2.evaluation.evaluator INFO: Inference done 7447/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0036 s/iter. Total: 0.1067 s/iter. ETA=0:04:40
[07/29 15:05:30] d2.evaluation.evaluator INFO: Inference done 7495/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0036 s/iter. Total: 0.1067 s/iter. ETA=0:04:35
[07/29 15:05:35] d2.evaluation.evaluator INFO: Inference done 7543/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0036 s/iter. Total: 0.1067 s/iter. ETA=0:04:30
[07/29 15:05:40] d2.evaluation.evaluator INFO: Inference done 7590/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0036 s/iter. Total: 0.1067 s/iter. ETA=0:04:25
[07/29 15:05:45] d2.evaluation.evaluator INFO: Inference done 7637/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0036 s/iter. Total: 0.1067 s/iter. ETA=0:04:20
[07/29 15:05:51] d2.evaluation.evaluator INFO: Inference done 7684/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0036 s/iter. Total: 0.1067 s/iter. ETA=0:04:15
[07/29 15:05:56] d2.evaluation.evaluator INFO: Inference done 7730/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0037 s/iter. Total: 0.1067 s/iter. ETA=0:04:10
[07/29 15:06:01] d2.evaluation.evaluator INFO: Inference done 7776/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0037 s/iter. Total: 0.1067 s/iter. ETA=0:04:05
[07/29 15:06:06] d2.evaluation.evaluator INFO: Inference done 7822/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0037 s/iter. Total: 0.1067 s/iter. ETA=0:04:00
[07/29 15:06:11] d2.evaluation.evaluator INFO: Inference done 7868/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0037 s/iter. Total: 0.1067 s/iter. ETA=0:03:56
[07/29 15:06:16] d2.evaluation.evaluator INFO: Inference done 7915/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0037 s/iter. Total: 0.1067 s/iter. ETA=0:03:51
[07/29 15:06:21] d2.evaluation.evaluator INFO: Inference done 7962/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0037 s/iter. Total: 0.1068 s/iter. ETA=0:03:46
[07/29 15:06:26] d2.evaluation.evaluator INFO: Inference done 8009/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0037 s/iter. Total: 0.1068 s/iter. ETA=0:03:41
[07/29 15:06:31] d2.evaluation.evaluator INFO: Inference done 8057/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0037 s/iter. Total: 0.1067 s/iter. ETA=0:03:35
[07/29 15:06:36] d2.evaluation.evaluator INFO: Inference done 8104/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0037 s/iter. Total: 0.1067 s/iter. ETA=0:03:30
[07/29 15:06:41] d2.evaluation.evaluator INFO: Inference done 8151/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0037 s/iter. Total: 0.1067 s/iter. ETA=0:03:25
[07/29 15:06:46] d2.evaluation.evaluator INFO: Inference done 8197/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0037 s/iter. Total: 0.1068 s/iter. ETA=0:03:21
[07/29 15:06:51] d2.evaluation.evaluator INFO: Inference done 8243/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0037 s/iter. Total: 0.1068 s/iter. ETA=0:03:16
[07/29 15:06:56] d2.evaluation.evaluator INFO: Inference done 8289/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0037 s/iter. Total: 0.1068 s/iter. ETA=0:03:11
[07/29 15:07:01] d2.evaluation.evaluator INFO: Inference done 8334/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0038 s/iter. Total: 0.1068 s/iter. ETA=0:03:06
[07/29 15:07:06] d2.evaluation.evaluator INFO: Inference done 8380/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0038 s/iter. Total: 0.1068 s/iter. ETA=0:03:01
[07/29 15:07:11] d2.evaluation.evaluator INFO: Inference done 8426/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0038 s/iter. Total: 0.1069 s/iter. ETA=0:02:56
[07/29 15:07:16] d2.evaluation.evaluator INFO: Inference done 8472/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0038 s/iter. Total: 0.1069 s/iter. ETA=0:02:51
[07/29 15:07:21] d2.evaluation.evaluator INFO: Inference done 8519/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0038 s/iter. Total: 0.1069 s/iter. ETA=0:02:46
[07/29 15:07:26] d2.evaluation.evaluator INFO: Inference done 8566/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0038 s/iter. Total: 0.1069 s/iter. ETA=0:02:41
[07/29 15:07:32] d2.evaluation.evaluator INFO: Inference done 8613/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0038 s/iter. Total: 0.1069 s/iter. ETA=0:02:36
[07/29 15:07:37] d2.evaluation.evaluator INFO: Inference done 8660/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0038 s/iter. Total: 0.1069 s/iter. ETA=0:02:31
[07/29 15:07:42] d2.evaluation.evaluator INFO: Inference done 8706/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0038 s/iter. Total: 0.1069 s/iter. ETA=0:02:26
[07/29 15:07:47] d2.evaluation.evaluator INFO: Inference done 8752/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0038 s/iter. Total: 0.1069 s/iter. ETA=0:02:22
[07/29 15:07:52] d2.evaluation.evaluator INFO: Inference done 8797/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0038 s/iter. Total: 0.1070 s/iter. ETA=0:02:17
[07/29 15:07:57] d2.evaluation.evaluator INFO: Inference done 8843/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0039 s/iter. Total: 0.1070 s/iter. ETA=0:02:12
[07/29 15:08:02] d2.evaluation.evaluator INFO: Inference done 8889/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0039 s/iter. Total: 0.1070 s/iter. ETA=0:02:07
[07/29 15:08:07] d2.evaluation.evaluator INFO: Inference done 8935/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0039 s/iter. Total: 0.1070 s/iter. ETA=0:02:02
[07/29 15:08:12] d2.evaluation.evaluator INFO: Inference done 8981/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0039 s/iter. Total: 0.1070 s/iter. ETA=0:01:57
[07/29 15:08:17] d2.evaluation.evaluator INFO: Inference done 9028/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0039 s/iter. Total: 0.1070 s/iter. ETA=0:01:52
[07/29 15:08:22] d2.evaluation.evaluator INFO: Inference done 9075/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0039 s/iter. Total: 0.1070 s/iter. ETA=0:01:47
[07/29 15:08:27] d2.evaluation.evaluator INFO: Inference done 9122/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0039 s/iter. Total: 0.1070 s/iter. ETA=0:01:42
[07/29 15:08:32] d2.evaluation.evaluator INFO: Inference done 9168/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0039 s/iter. Total: 0.1070 s/iter. ETA=0:01:37
[07/29 15:08:37] d2.evaluation.evaluator INFO: Inference done 9214/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0039 s/iter. Total: 0.1070 s/iter. ETA=0:01:32
[07/29 15:08:42] d2.evaluation.evaluator INFO: Inference done 9261/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0039 s/iter. Total: 0.1070 s/iter. ETA=0:01:27
[07/29 15:08:47] d2.evaluation.evaluator INFO: Inference done 9308/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0039 s/iter. Total: 0.1071 s/iter. ETA=0:01:22
[07/29 15:08:52] d2.evaluation.evaluator INFO: Inference done 9355/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0039 s/iter. Total: 0.1071 s/iter. ETA=0:01:17
[07/29 15:08:58] d2.evaluation.evaluator INFO: Inference done 9402/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0039 s/iter. Total: 0.1071 s/iter. ETA=0:01:12
[07/29 15:09:03] d2.evaluation.evaluator INFO: Inference done 9449/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0039 s/iter. Total: 0.1071 s/iter. ETA=0:01:07
[07/29 15:09:08] d2.evaluation.evaluator INFO: Inference done 9496/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0039 s/iter. Total: 0.1071 s/iter. ETA=0:01:02
[07/29 15:09:13] d2.evaluation.evaluator INFO: Inference done 9541/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0039 s/iter. Total: 0.1071 s/iter. ETA=0:00:57
[07/29 15:09:18] d2.evaluation.evaluator INFO: Inference done 9588/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0039 s/iter. Total: 0.1071 s/iter. ETA=0:00:52
[07/29 15:09:23] d2.evaluation.evaluator INFO: Inference done 9631/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0040 s/iter. Total: 0.1072 s/iter. ETA=0:00:48
[07/29 15:09:28] d2.evaluation.evaluator INFO: Inference done 9675/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0040 s/iter. Total: 0.1072 s/iter. ETA=0:00:43
[07/29 15:09:33] d2.evaluation.evaluator INFO: Inference done 9719/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0040 s/iter. Total: 0.1072 s/iter. ETA=0:00:38
[07/29 15:09:38] d2.evaluation.evaluator INFO: Inference done 9763/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0040 s/iter. Total: 0.1073 s/iter. ETA=0:00:34
[07/29 15:09:43] d2.evaluation.evaluator INFO: Inference done 9808/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0041 s/iter. Total: 0.1073 s/iter. ETA=0:00:29
[07/29 15:09:48] d2.evaluation.evaluator INFO: Inference done 9853/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0041 s/iter. Total: 0.1073 s/iter. ETA=0:00:24
[07/29 15:09:53] d2.evaluation.evaluator INFO: Inference done 9899/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0041 s/iter. Total: 0.1073 s/iter. ETA=0:00:19
[07/29 15:09:58] d2.evaluation.evaluator INFO: Inference done 9944/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0041 s/iter. Total: 0.1073 s/iter. ETA=0:00:14
[07/29 15:10:03] d2.evaluation.evaluator INFO: Inference done 9990/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0041 s/iter. Total: 0.1074 s/iter. ETA=0:00:09
[07/29 15:10:08] d2.evaluation.evaluator INFO: Inference done 10037/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0041 s/iter. Total: 0.1074 s/iter. ETA=0:00:04
[07/29 15:10:13] d2.evaluation.evaluator INFO: Total inference time: 0:18:01.726085 (0.107367 s / iter per device, on 1 devices)
[07/29 15:10:13] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:17:09 (0.102217 s / iter per device, on 1 devices)
[07/29 15:10:13] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[07/29 15:10:13] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[07/29 15:10:13] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[07/29 15:10:18] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 4.46 seconds.
[07/29 15:10:18] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[07/29 15:10:18] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.37 seconds.
[07/29 15:10:18] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |
|:------:|:------:|:------:|:-----:|:------:|:------:|
| 32.908 | 43.925 | 36.853 | 4.097 | 30.951 | 32.618 |
[07/29 15:10:18] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP     | category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|:-----------|:-------|
| 1          | 40.546 | 2          | 17.206 | 3          | 24.326 |
| 4          | 6.004  | 5          | 10.093 | 6          | 5.253  |
| 7          | 0.018  | 8          | 5.928  | 9          | 54.873 |
| 10         | 51.677 | 11         | 54.508 | 12         | 52.726 |
| 13         | 30.708 | 14         | 16.354 | 15         | 42.841 |
| 16         | 47.284 | 17         | 53.803 | 18         | 63.101 |
| 19         | 42.078 | 20         | 26.137 | 21         | 27.987 |
| 22         | 32.105 | 23         | 20.147 | 24         | 61.144 |
| 25         | 53.916 | 26         | 35.162 | 27         | 19.842 |
| 28         | 51.866 | 29         | 34.106 | 30         | 5.507  |
[07/29 15:10:20] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *segm*
[07/29 15:10:28] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 7.72 seconds.
[07/29 15:10:28] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[07/29 15:10:28] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.40 seconds.
[07/29 15:10:28] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |
|:------:|:------:|:------:|:-----:|:------:|:------:|
| 18.177 | 30.821 | 19.045 | 0.168 | 14.757 | 18.716 |
[07/29 15:10:28] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|:-----------|:-------|
| 1          | 31.384 | 2          | 12.741 | 3          | 18.144 |
| 4          | 3.322  | 5          | 1.051  | 6          | 1.561  |
| 7          | 0.000  | 8          | 0.382  | 9          | 31.112 |
| 10         | 32.946 | 11         | 41.582 | 12         | 36.177 |
| 13         | 20.393 | 14         | 13.124 | 15         | 34.845 |
| 16         | 30.531 | 17         | 17.148 | 18         | 32.715 |
| 19         | 15.578 | 20         | 8.824  | 21         | 12.917 |
| 22         | 26.331 | 23         | 8.076  | 24         | 39.728 |
| 25         | 33.343 | 26         | 12.692 | 27         | 6.999  |
| 28         | 6.902  | 29         | 12.714 | 30         | 2.040  |
[07/29 15:10:29] d2.evaluation.testing INFO: copypaste: Task: bbox
[07/29 15:10:29] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/29 15:10:29] d2.evaluation.testing INFO: copypaste: 32.9082,43.9252,36.8527,4.0975,30.9514,32.6181
[07/29 15:10:29] d2.evaluation.testing INFO: copypaste: Task: segm
[07/29 15:10:29] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/29 15:10:29] d2.evaluation.testing INFO: copypaste: 18.1767,30.8206,19.0452,0.1681,14.7567,18.7155
[07/29 15:10:29] d2.utils.events INFO:  eta: 9:09:08  iter: 14999  total_loss: 2.997  loss_cls_stage0: 0.2876  loss_box_reg_stage0: 0.2815  loss_cls_stage1: 0.2975  loss_box_reg_stage1: 0.6926  loss_cls_stage2: 0.2922  loss_box_reg_stage2: 0.9759  loss_mask: 0.08455  loss_rpn_cls: 0.01889  loss_rpn_loc: 0.04935  time: 0.6248  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 15:10:41] d2.utils.events INFO:  eta: 9:07:51  iter: 15019  total_loss: 3.116  loss_cls_stage0: 0.3184  loss_box_reg_stage0: 0.2759  loss_cls_stage1: 0.3221  loss_box_reg_stage1: 0.7404  loss_cls_stage2: 0.312  loss_box_reg_stage2: 0.949  loss_mask: 0.08437  loss_rpn_cls: 0.01594  loss_rpn_loc: 0.05483  time: 0.6248  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:10:54] d2.utils.events INFO:  eta: 9:07:52  iter: 15039  total_loss: 3.19  loss_cls_stage0: 0.3049  loss_box_reg_stage0: 0.3061  loss_cls_stage1: 0.3474  loss_box_reg_stage1: 0.7256  loss_cls_stage2: 0.3382  loss_box_reg_stage2: 1.032  loss_mask: 0.08602  loss_rpn_cls: 0.01576  loss_rpn_loc: 0.04507  time: 0.6248  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:11:06] d2.utils.events INFO:  eta: 9:07:34  iter: 15059  total_loss: 2.829  loss_cls_stage0: 0.2569  loss_box_reg_stage0: 0.2464  loss_cls_stage1: 0.2946  loss_box_reg_stage1: 0.6457  loss_cls_stage2: 0.2932  loss_box_reg_stage2: 0.891  loss_mask: 0.08385  loss_rpn_cls: 0.01757  loss_rpn_loc: 0.04544  time: 0.6248  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:11:19] d2.utils.events INFO:  eta: 9:07:23  iter: 15079  total_loss: 3.257  loss_cls_stage0: 0.3046  loss_box_reg_stage0: 0.3229  loss_cls_stage1: 0.345  loss_box_reg_stage1: 0.7515  loss_cls_stage2: 0.3333  loss_box_reg_stage2: 1.011  loss_mask: 0.1022  loss_rpn_cls: 0.01612  loss_rpn_loc: 0.04579  time: 0.6248  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:11:31] d2.utils.events INFO:  eta: 9:07:01  iter: 15099  total_loss: 2.955  loss_cls_stage0: 0.2781  loss_box_reg_stage0: 0.2621  loss_cls_stage1: 0.3256  loss_box_reg_stage1: 0.6712  loss_cls_stage2: 0.3197  loss_box_reg_stage2: 0.9173  loss_mask: 0.08495  loss_rpn_cls: 0.01296  loss_rpn_loc: 0.0488  time: 0.6247  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:11:43] d2.utils.events INFO:  eta: 9:06:35  iter: 15119  total_loss: 3.279  loss_cls_stage0: 0.2957  loss_box_reg_stage0: 0.2982  loss_cls_stage1: 0.3658  loss_box_reg_stage1: 0.7714  loss_cls_stage2: 0.3366  loss_box_reg_stage2: 1.07  loss_mask: 0.08003  loss_rpn_cls: 0.01472  loss_rpn_loc: 0.05111  time: 0.6248  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 15:11:56] d2.utils.events INFO:  eta: 9:06:16  iter: 15139  total_loss: 3.186  loss_cls_stage0: 0.2788  loss_box_reg_stage0: 0.2751  loss_cls_stage1: 0.3241  loss_box_reg_stage1: 0.7416  loss_cls_stage2: 0.3283  loss_box_reg_stage2: 0.9891  loss_mask: 0.08061  loss_rpn_cls: 0.012  loss_rpn_loc: 0.04689  time: 0.6248  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:12:09] d2.utils.events INFO:  eta: 9:06:24  iter: 15159  total_loss: 3.447  loss_cls_stage0: 0.3092  loss_box_reg_stage0: 0.2801  loss_cls_stage1: 0.3839  loss_box_reg_stage1: 0.8175  loss_cls_stage2: 0.3809  loss_box_reg_stage2: 1.092  loss_mask: 0.09601  loss_rpn_cls: 0.01237  loss_rpn_loc: 0.05273  time: 0.6248  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:12:21] d2.utils.events INFO:  eta: 9:06:09  iter: 15179  total_loss: 3.106  loss_cls_stage0: 0.2716  loss_box_reg_stage0: 0.273  loss_cls_stage1: 0.2946  loss_box_reg_stage1: 0.7407  loss_cls_stage2: 0.2677  loss_box_reg_stage2: 1.022  loss_mask: 0.08068  loss_rpn_cls: 0.01553  loss_rpn_loc: 0.04905  time: 0.6248  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:12:34] d2.utils.events INFO:  eta: 9:05:57  iter: 15199  total_loss: 3.216  loss_cls_stage0: 0.2802  loss_box_reg_stage0: 0.2761  loss_cls_stage1: 0.32  loss_box_reg_stage1: 0.7861  loss_cls_stage2: 0.3459  loss_box_reg_stage2: 1.028  loss_mask: 0.08067  loss_rpn_cls: 0.009886  loss_rpn_loc: 0.04114  time: 0.6248  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:12:46] d2.utils.events INFO:  eta: 9:05:54  iter: 15219  total_loss: 3.055  loss_cls_stage0: 0.2857  loss_box_reg_stage0: 0.2836  loss_cls_stage1: 0.3245  loss_box_reg_stage1: 0.7155  loss_cls_stage2: 0.3154  loss_box_reg_stage2: 0.9113  loss_mask: 0.08145  loss_rpn_cls: 0.01833  loss_rpn_loc: 0.04225  time: 0.6248  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:12:59] d2.utils.events INFO:  eta: 9:05:43  iter: 15239  total_loss: 3.06  loss_cls_stage0: 0.3062  loss_box_reg_stage0: 0.3243  loss_cls_stage1: 0.3357  loss_box_reg_stage1: 0.7761  loss_cls_stage2: 0.3187  loss_box_reg_stage2: 0.9763  loss_mask: 0.09082  loss_rpn_cls: 0.01481  loss_rpn_loc: 0.04735  time: 0.6248  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:13:11] d2.utils.events INFO:  eta: 9:05:42  iter: 15259  total_loss: 3.325  loss_cls_stage0: 0.3086  loss_box_reg_stage0: 0.2765  loss_cls_stage1: 0.3557  loss_box_reg_stage1: 0.7617  loss_cls_stage2: 0.3607  loss_box_reg_stage2: 1.033  loss_mask: 0.08958  loss_rpn_cls: 0.008877  loss_rpn_loc: 0.04923  time: 0.6248  data_time: 0.0044  lr: 0.00016  max_mem: 19679M
[07/29 15:13:23] d2.utils.events INFO:  eta: 9:05:24  iter: 15279  total_loss: 3.16  loss_cls_stage0: 0.2983  loss_box_reg_stage0: 0.2918  loss_cls_stage1: 0.3218  loss_box_reg_stage1: 0.7452  loss_cls_stage2: 0.3169  loss_box_reg_stage2: 1.074  loss_mask: 0.08148  loss_rpn_cls: 0.01786  loss_rpn_loc: 0.05902  time: 0.6247  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 15:13:36] d2.utils.events INFO:  eta: 9:05:26  iter: 15299  total_loss: 3.489  loss_cls_stage0: 0.3257  loss_box_reg_stage0: 0.3214  loss_cls_stage1: 0.3629  loss_box_reg_stage1: 0.8465  loss_cls_stage2: 0.3596  loss_box_reg_stage2: 1.082  loss_mask: 0.08788  loss_rpn_cls: 0.01908  loss_rpn_loc: 0.05004  time: 0.6247  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:13:48] d2.utils.events INFO:  eta: 9:05:13  iter: 15319  total_loss: 3.206  loss_cls_stage0: 0.286  loss_box_reg_stage0: 0.2858  loss_cls_stage1: 0.3242  loss_box_reg_stage1: 0.7864  loss_cls_stage2: 0.2918  loss_box_reg_stage2: 1.078  loss_mask: 0.08002  loss_rpn_cls: 0.01232  loss_rpn_loc: 0.0407  time: 0.6247  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:14:01] d2.utils.events INFO:  eta: 9:05:22  iter: 15339  total_loss: 2.973  loss_cls_stage0: 0.2962  loss_box_reg_stage0: 0.2831  loss_cls_stage1: 0.3451  loss_box_reg_stage1: 0.6821  loss_cls_stage2: 0.3302  loss_box_reg_stage2: 0.936  loss_mask: 0.08022  loss_rpn_cls: 0.01724  loss_rpn_loc: 0.04996  time: 0.6248  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 15:14:14] d2.utils.events INFO:  eta: 9:05:36  iter: 15359  total_loss: 3.196  loss_cls_stage0: 0.2895  loss_box_reg_stage0: 0.2994  loss_cls_stage1: 0.2873  loss_box_reg_stage1: 0.7915  loss_cls_stage2: 0.2774  loss_box_reg_stage2: 1.053  loss_mask: 0.08855  loss_rpn_cls: 0.01673  loss_rpn_loc: 0.0481  time: 0.6248  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 15:14:26] d2.utils.events INFO:  eta: 9:05:49  iter: 15379  total_loss: 2.887  loss_cls_stage0: 0.2852  loss_box_reg_stage0: 0.2668  loss_cls_stage1: 0.2991  loss_box_reg_stage1: 0.6611  loss_cls_stage2: 0.2933  loss_box_reg_stage2: 0.8808  loss_mask: 0.08549  loss_rpn_cls: 0.01811  loss_rpn_loc: 0.05106  time: 0.6247  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:14:38] d2.utils.events INFO:  eta: 9:05:22  iter: 15399  total_loss: 3.15  loss_cls_stage0: 0.2804  loss_box_reg_stage0: 0.2775  loss_cls_stage1: 0.3265  loss_box_reg_stage1: 0.7336  loss_cls_stage2: 0.3309  loss_box_reg_stage2: 0.9762  loss_mask: 0.07885  loss_rpn_cls: 0.01696  loss_rpn_loc: 0.05605  time: 0.6247  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:14:51] d2.utils.events INFO:  eta: 9:04:57  iter: 15419  total_loss: 2.778  loss_cls_stage0: 0.2485  loss_box_reg_stage0: 0.2558  loss_cls_stage1: 0.2692  loss_box_reg_stage1: 0.6744  loss_cls_stage2: 0.2513  loss_box_reg_stage2: 0.9308  loss_mask: 0.07532  loss_rpn_cls: 0.01431  loss_rpn_loc: 0.04095  time: 0.6247  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:15:03] d2.utils.events INFO:  eta: 9:04:45  iter: 15439  total_loss: 2.868  loss_cls_stage0: 0.2623  loss_box_reg_stage0: 0.2735  loss_cls_stage1: 0.3009  loss_box_reg_stage1: 0.6774  loss_cls_stage2: 0.2659  loss_box_reg_stage2: 0.9596  loss_mask: 0.08088  loss_rpn_cls: 0.0216  loss_rpn_loc: 0.0494  time: 0.6247  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:15:15] d2.utils.events INFO:  eta: 9:03:36  iter: 15459  total_loss: 2.851  loss_cls_stage0: 0.2514  loss_box_reg_stage0: 0.2772  loss_cls_stage1: 0.2619  loss_box_reg_stage1: 0.644  loss_cls_stage2: 0.2408  loss_box_reg_stage2: 0.8986  loss_mask: 0.08087  loss_rpn_cls: 0.01363  loss_rpn_loc: 0.04425  time: 0.6247  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:15:28] d2.utils.events INFO:  eta: 9:04:20  iter: 15479  total_loss: 2.946  loss_cls_stage0: 0.2642  loss_box_reg_stage0: 0.2517  loss_cls_stage1: 0.2969  loss_box_reg_stage1: 0.7281  loss_cls_stage2: 0.3207  loss_box_reg_stage2: 1.001  loss_mask: 0.07333  loss_rpn_cls: 0.01437  loss_rpn_loc: 0.04895  time: 0.6247  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:15:40] d2.utils.events INFO:  eta: 9:03:52  iter: 15499  total_loss: 2.826  loss_cls_stage0: 0.279  loss_box_reg_stage0: 0.2683  loss_cls_stage1: 0.3048  loss_box_reg_stage1: 0.6561  loss_cls_stage2: 0.2735  loss_box_reg_stage2: 0.8834  loss_mask: 0.08956  loss_rpn_cls: 0.02134  loss_rpn_loc: 0.0756  time: 0.6247  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:15:52] d2.utils.events INFO:  eta: 9:03:21  iter: 15519  total_loss: 3.176  loss_cls_stage0: 0.3203  loss_box_reg_stage0: 0.3031  loss_cls_stage1: 0.3419  loss_box_reg_stage1: 0.7499  loss_cls_stage2: 0.3373  loss_box_reg_stage2: 1.025  loss_mask: 0.08521  loss_rpn_cls: 0.01742  loss_rpn_loc: 0.04654  time: 0.6246  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:16:05] d2.utils.events INFO:  eta: 9:03:48  iter: 15539  total_loss: 3.021  loss_cls_stage0: 0.3045  loss_box_reg_stage0: 0.3048  loss_cls_stage1: 0.3209  loss_box_reg_stage1: 0.7001  loss_cls_stage2: 0.2818  loss_box_reg_stage2: 0.8811  loss_mask: 0.08246  loss_rpn_cls: 0.0176  loss_rpn_loc: 0.05253  time: 0.6247  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 15:16:18] d2.utils.events INFO:  eta: 9:03:15  iter: 15559  total_loss: 3.347  loss_cls_stage0: 0.3183  loss_box_reg_stage0: 0.3131  loss_cls_stage1: 0.315  loss_box_reg_stage1: 0.7815  loss_cls_stage2: 0.34  loss_box_reg_stage2: 0.994  loss_mask: 0.09308  loss_rpn_cls: 0.01536  loss_rpn_loc: 0.05021  time: 0.6246  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:16:30] d2.utils.events INFO:  eta: 9:03:50  iter: 15579  total_loss: 3.113  loss_cls_stage0: 0.294  loss_box_reg_stage0: 0.2817  loss_cls_stage1: 0.3221  loss_box_reg_stage1: 0.7618  loss_cls_stage2: 0.2826  loss_box_reg_stage2: 1.015  loss_mask: 0.08268  loss_rpn_cls: 0.01845  loss_rpn_loc: 0.04623  time: 0.6246  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:16:43] d2.utils.events INFO:  eta: 9:03:55  iter: 15599  total_loss: 2.95  loss_cls_stage0: 0.3058  loss_box_reg_stage0: 0.2901  loss_cls_stage1: 0.3061  loss_box_reg_stage1: 0.6975  loss_cls_stage2: 0.312  loss_box_reg_stage2: 0.8928  loss_mask: 0.08329  loss_rpn_cls: 0.0201  loss_rpn_loc: 0.05273  time: 0.6247  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:16:55] d2.utils.events INFO:  eta: 9:03:53  iter: 15619  total_loss: 3.013  loss_cls_stage0: 0.2849  loss_box_reg_stage0: 0.2797  loss_cls_stage1: 0.3114  loss_box_reg_stage1: 0.6984  loss_cls_stage2: 0.2896  loss_box_reg_stage2: 0.9605  loss_mask: 0.07828  loss_rpn_cls: 0.01347  loss_rpn_loc: 0.06533  time: 0.6247  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 15:17:08] d2.utils.events INFO:  eta: 9:02:56  iter: 15639  total_loss: 3.129  loss_cls_stage0: 0.2569  loss_box_reg_stage0: 0.2768  loss_cls_stage1: 0.2916  loss_box_reg_stage1: 0.7399  loss_cls_stage2: 0.2773  loss_box_reg_stage2: 1.002  loss_mask: 0.08126  loss_rpn_cls: 0.01586  loss_rpn_loc: 0.04882  time: 0.6246  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:17:20] d2.utils.events INFO:  eta: 9:03:15  iter: 15659  total_loss: 3.022  loss_cls_stage0: 0.2756  loss_box_reg_stage0: 0.269  loss_cls_stage1: 0.3063  loss_box_reg_stage1: 0.7442  loss_cls_stage2: 0.2991  loss_box_reg_stage2: 1.005  loss_mask: 0.08552  loss_rpn_cls: 0.02007  loss_rpn_loc: 0.04999  time: 0.6247  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:17:33] d2.utils.events INFO:  eta: 9:03:04  iter: 15679  total_loss: 3.299  loss_cls_stage0: 0.2772  loss_box_reg_stage0: 0.2981  loss_cls_stage1: 0.3121  loss_box_reg_stage1: 0.7785  loss_cls_stage2: 0.3002  loss_box_reg_stage2: 1.071  loss_mask: 0.08667  loss_rpn_cls: 0.01521  loss_rpn_loc: 0.06314  time: 0.6247  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:17:46] d2.utils.events INFO:  eta: 9:03:34  iter: 15699  total_loss: 3.086  loss_cls_stage0: 0.2727  loss_box_reg_stage0: 0.275  loss_cls_stage1: 0.3172  loss_box_reg_stage1: 0.7237  loss_cls_stage2: 0.3404  loss_box_reg_stage2: 1.022  loss_mask: 0.07788  loss_rpn_cls: 0.008309  loss_rpn_loc: 0.04559  time: 0.6247  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:17:58] d2.utils.events INFO:  eta: 9:03:56  iter: 15719  total_loss: 2.977  loss_cls_stage0: 0.275  loss_box_reg_stage0: 0.2623  loss_cls_stage1: 0.2887  loss_box_reg_stage1: 0.7155  loss_cls_stage2: 0.299  loss_box_reg_stage2: 1.009  loss_mask: 0.08189  loss_rpn_cls: 0.01145  loss_rpn_loc: 0.0417  time: 0.6247  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:18:11] d2.utils.events INFO:  eta: 9:03:38  iter: 15739  total_loss: 3.286  loss_cls_stage0: 0.2962  loss_box_reg_stage0: 0.2887  loss_cls_stage1: 0.3442  loss_box_reg_stage1: 0.7487  loss_cls_stage2: 0.3239  loss_box_reg_stage2: 1.057  loss_mask: 0.09252  loss_rpn_cls: 0.01642  loss_rpn_loc: 0.05914  time: 0.6247  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:18:23] d2.utils.events INFO:  eta: 9:03:09  iter: 15759  total_loss: 2.831  loss_cls_stage0: 0.2567  loss_box_reg_stage0: 0.2584  loss_cls_stage1: 0.2713  loss_box_reg_stage1: 0.6524  loss_cls_stage2: 0.2634  loss_box_reg_stage2: 0.8975  loss_mask: 0.08087  loss_rpn_cls: 0.01394  loss_rpn_loc: 0.04406  time: 0.6247  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:18:36] d2.utils.events INFO:  eta: 9:03:13  iter: 15779  total_loss: 3.484  loss_cls_stage0: 0.3012  loss_box_reg_stage0: 0.3102  loss_cls_stage1: 0.3206  loss_box_reg_stage1: 0.8109  loss_cls_stage2: 0.3045  loss_box_reg_stage2: 1.076  loss_mask: 0.0882  loss_rpn_cls: 0.01345  loss_rpn_loc: 0.04025  time: 0.6247  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:18:48] d2.utils.events INFO:  eta: 9:02:44  iter: 15799  total_loss: 3.038  loss_cls_stage0: 0.2814  loss_box_reg_stage0: 0.2708  loss_cls_stage1: 0.3265  loss_box_reg_stage1: 0.7303  loss_cls_stage2: 0.3348  loss_box_reg_stage2: 0.9912  loss_mask: 0.08515  loss_rpn_cls: 0.01377  loss_rpn_loc: 0.03911  time: 0.6247  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:19:01] d2.utils.events INFO:  eta: 9:02:56  iter: 15819  total_loss: 3.243  loss_cls_stage0: 0.2982  loss_box_reg_stage0: 0.2921  loss_cls_stage1: 0.3492  loss_box_reg_stage1: 0.771  loss_cls_stage2: 0.3524  loss_box_reg_stage2: 1.057  loss_mask: 0.08784  loss_rpn_cls: 0.01906  loss_rpn_loc: 0.05188  time: 0.6247  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:19:14] d2.utils.events INFO:  eta: 9:03:36  iter: 15839  total_loss: 2.895  loss_cls_stage0: 0.2626  loss_box_reg_stage0: 0.2578  loss_cls_stage1: 0.2996  loss_box_reg_stage1: 0.6844  loss_cls_stage2: 0.2917  loss_box_reg_stage2: 0.9599  loss_mask: 0.07837  loss_rpn_cls: 0.01544  loss_rpn_loc: 0.04367  time: 0.6248  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:19:27] d2.utils.events INFO:  eta: 9:03:16  iter: 15859  total_loss: 3.344  loss_cls_stage0: 0.3281  loss_box_reg_stage0: 0.2947  loss_cls_stage1: 0.3643  loss_box_reg_stage1: 0.773  loss_cls_stage2: 0.3217  loss_box_reg_stage2: 1.013  loss_mask: 0.08267  loss_rpn_cls: 0.01565  loss_rpn_loc: 0.05364  time: 0.6248  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:19:39] d2.utils.events INFO:  eta: 9:03:01  iter: 15879  total_loss: 3.277  loss_cls_stage0: 0.2985  loss_box_reg_stage0: 0.2511  loss_cls_stage1: 0.3682  loss_box_reg_stage1: 0.7647  loss_cls_stage2: 0.3424  loss_box_reg_stage2: 1.062  loss_mask: 0.0763  loss_rpn_cls: 0.01102  loss_rpn_loc: 0.04491  time: 0.6248  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:19:52] d2.utils.events INFO:  eta: 9:02:51  iter: 15899  total_loss: 3.27  loss_cls_stage0: 0.3142  loss_box_reg_stage0: 0.2763  loss_cls_stage1: 0.3564  loss_box_reg_stage1: 0.7142  loss_cls_stage2: 0.3474  loss_box_reg_stage2: 0.9854  loss_mask: 0.08563  loss_rpn_cls: 0.01271  loss_rpn_loc: 0.05612  time: 0.6248  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:20:04] d2.utils.events INFO:  eta: 9:02:36  iter: 15919  total_loss: 3.183  loss_cls_stage0: 0.3239  loss_box_reg_stage0: 0.2916  loss_cls_stage1: 0.3527  loss_box_reg_stage1: 0.7556  loss_cls_stage2: 0.3157  loss_box_reg_stage2: 0.9557  loss_mask: 0.08183  loss_rpn_cls: 0.01541  loss_rpn_loc: 0.04583  time: 0.6248  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:20:18] d2.utils.events INFO:  eta: 9:03:02  iter: 15939  total_loss: 2.968  loss_cls_stage0: 0.2722  loss_box_reg_stage0: 0.2604  loss_cls_stage1: 0.3068  loss_box_reg_stage1: 0.7038  loss_cls_stage2: 0.2859  loss_box_reg_stage2: 0.9935  loss_mask: 0.08007  loss_rpn_cls: 0.01652  loss_rpn_loc: 0.05265  time: 0.6249  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 15:20:30] d2.utils.events INFO:  eta: 9:02:55  iter: 15959  total_loss: 3.044  loss_cls_stage0: 0.2704  loss_box_reg_stage0: 0.2899  loss_cls_stage1: 0.3203  loss_box_reg_stage1: 0.7422  loss_cls_stage2: 0.3065  loss_box_reg_stage2: 1.016  loss_mask: 0.08344  loss_rpn_cls: 0.01739  loss_rpn_loc: 0.05139  time: 0.6248  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 15:20:42] d2.utils.events INFO:  eta: 9:02:39  iter: 15979  total_loss: 2.828  loss_cls_stage0: 0.2583  loss_box_reg_stage0: 0.2651  loss_cls_stage1: 0.2875  loss_box_reg_stage1: 0.6721  loss_cls_stage2: 0.2732  loss_box_reg_stage2: 0.8438  loss_mask: 0.07897  loss_rpn_cls: 0.0139  loss_rpn_loc: 0.06294  time: 0.6248  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:20:55] d2.utils.events INFO:  eta: 9:03:08  iter: 15999  total_loss: 3.074  loss_cls_stage0: 0.3112  loss_box_reg_stage0: 0.2957  loss_cls_stage1: 0.3735  loss_box_reg_stage1: 0.7473  loss_cls_stage2: 0.3346  loss_box_reg_stage2: 0.9083  loss_mask: 0.07959  loss_rpn_cls: 0.01194  loss_rpn_loc: 0.04417  time: 0.6249  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:21:08] d2.utils.events INFO:  eta: 9:03:30  iter: 16019  total_loss: 3.172  loss_cls_stage0: 0.2964  loss_box_reg_stage0: 0.285  loss_cls_stage1: 0.3455  loss_box_reg_stage1: 0.7247  loss_cls_stage2: 0.3004  loss_box_reg_stage2: 1.017  loss_mask: 0.08237  loss_rpn_cls: 0.01454  loss_rpn_loc: 0.0431  time: 0.6249  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:21:20] d2.utils.events INFO:  eta: 9:02:34  iter: 16039  total_loss: 2.997  loss_cls_stage0: 0.2876  loss_box_reg_stage0: 0.2753  loss_cls_stage1: 0.3258  loss_box_reg_stage1: 0.6963  loss_cls_stage2: 0.29  loss_box_reg_stage2: 0.9305  loss_mask: 0.07862  loss_rpn_cls: 0.02155  loss_rpn_loc: 0.0462  time: 0.6248  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 15:21:32] d2.utils.events INFO:  eta: 9:02:22  iter: 16059  total_loss: 2.703  loss_cls_stage0: 0.2509  loss_box_reg_stage0: 0.261  loss_cls_stage1: 0.265  loss_box_reg_stage1: 0.6685  loss_cls_stage2: 0.2628  loss_box_reg_stage2: 0.8589  loss_mask: 0.07362  loss_rpn_cls: 0.02295  loss_rpn_loc: 0.0608  time: 0.6248  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:21:45] d2.utils.events INFO:  eta: 9:01:46  iter: 16079  total_loss: 3.321  loss_cls_stage0: 0.2826  loss_box_reg_stage0: 0.3015  loss_cls_stage1: 0.3171  loss_box_reg_stage1: 0.7715  loss_cls_stage2: 0.3037  loss_box_reg_stage2: 1.001  loss_mask: 0.08768  loss_rpn_cls: 0.01691  loss_rpn_loc: 0.04995  time: 0.6248  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:21:57] d2.utils.events INFO:  eta: 9:02:04  iter: 16099  total_loss: 2.717  loss_cls_stage0: 0.2478  loss_box_reg_stage0: 0.2646  loss_cls_stage1: 0.2716  loss_box_reg_stage1: 0.663  loss_cls_stage2: 0.2458  loss_box_reg_stage2: 0.8617  loss_mask: 0.06694  loss_rpn_cls: 0.01168  loss_rpn_loc: 0.04693  time: 0.6248  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:22:10] d2.utils.events INFO:  eta: 9:01:44  iter: 16119  total_loss: 2.999  loss_cls_stage0: 0.2669  loss_box_reg_stage0: 0.2837  loss_cls_stage1: 0.2755  loss_box_reg_stage1: 0.7164  loss_cls_stage2: 0.263  loss_box_reg_stage2: 0.9126  loss_mask: 0.08026  loss_rpn_cls: 0.02639  loss_rpn_loc: 0.0702  time: 0.6248  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 15:22:22] d2.utils.events INFO:  eta: 9:01:15  iter: 16139  total_loss: 2.813  loss_cls_stage0: 0.2769  loss_box_reg_stage0: 0.2729  loss_cls_stage1: 0.2731  loss_box_reg_stage1: 0.6873  loss_cls_stage2: 0.2605  loss_box_reg_stage2: 0.8981  loss_mask: 0.07575  loss_rpn_cls: 0.01687  loss_rpn_loc: 0.04753  time: 0.6248  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:22:34] d2.utils.events INFO:  eta: 9:00:40  iter: 16159  total_loss: 2.423  loss_cls_stage0: 0.2243  loss_box_reg_stage0: 0.2487  loss_cls_stage1: 0.2209  loss_box_reg_stage1: 0.6341  loss_cls_stage2: 0.2106  loss_box_reg_stage2: 0.8344  loss_mask: 0.0739  loss_rpn_cls: 0.02197  loss_rpn_loc: 0.0447  time: 0.6248  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:22:47] d2.utils.events INFO:  eta: 9:00:01  iter: 16179  total_loss: 2.801  loss_cls_stage0: 0.2619  loss_box_reg_stage0: 0.2599  loss_cls_stage1: 0.2875  loss_box_reg_stage1: 0.6498  loss_cls_stage2: 0.2783  loss_box_reg_stage2: 0.8464  loss_mask: 0.0733  loss_rpn_cls: 0.02075  loss_rpn_loc: 0.05508  time: 0.6248  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:22:59] d2.utils.events INFO:  eta: 8:59:41  iter: 16199  total_loss: 2.804  loss_cls_stage0: 0.2717  loss_box_reg_stage0: 0.2463  loss_cls_stage1: 0.2977  loss_box_reg_stage1: 0.5951  loss_cls_stage2: 0.2914  loss_box_reg_stage2: 0.8214  loss_mask: 0.07635  loss_rpn_cls: 0.01823  loss_rpn_loc: 0.07478  time: 0.6248  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 15:23:11] d2.utils.events INFO:  eta: 8:58:59  iter: 16219  total_loss: 3.283  loss_cls_stage0: 0.2959  loss_box_reg_stage0: 0.2973  loss_cls_stage1: 0.3468  loss_box_reg_stage1: 0.752  loss_cls_stage2: 0.3298  loss_box_reg_stage2: 0.9434  loss_mask: 0.0873  loss_rpn_cls: 0.01434  loss_rpn_loc: 0.05598  time: 0.6247  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 15:23:24] d2.utils.events INFO:  eta: 8:58:33  iter: 16239  total_loss: 3.098  loss_cls_stage0: 0.2815  loss_box_reg_stage0: 0.2771  loss_cls_stage1: 0.3171  loss_box_reg_stage1: 0.7613  loss_cls_stage2: 0.2997  loss_box_reg_stage2: 0.9428  loss_mask: 0.07902  loss_rpn_cls: 0.01265  loss_rpn_loc: 0.0561  time: 0.6247  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 15:23:36] d2.utils.events INFO:  eta: 8:58:20  iter: 16259  total_loss: 3.069  loss_cls_stage0: 0.2831  loss_box_reg_stage0: 0.2794  loss_cls_stage1: 0.2973  loss_box_reg_stage1: 0.7314  loss_cls_stage2: 0.2798  loss_box_reg_stage2: 0.9348  loss_mask: 0.08275  loss_rpn_cls: 0.01165  loss_rpn_loc: 0.0488  time: 0.6247  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:23:49] d2.utils.events INFO:  eta: 8:58:43  iter: 16279  total_loss: 2.65  loss_cls_stage0: 0.2614  loss_box_reg_stage0: 0.262  loss_cls_stage1: 0.2602  loss_box_reg_stage1: 0.621  loss_cls_stage2: 0.2523  loss_box_reg_stage2: 0.847  loss_mask: 0.06695  loss_rpn_cls: 0.01731  loss_rpn_loc: 0.04313  time: 0.6247  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:24:01] d2.utils.events INFO:  eta: 8:58:04  iter: 16299  total_loss: 2.79  loss_cls_stage0: 0.2259  loss_box_reg_stage0: 0.2379  loss_cls_stage1: 0.266  loss_box_reg_stage1: 0.6555  loss_cls_stage2: 0.2644  loss_box_reg_stage2: 0.8554  loss_mask: 0.07433  loss_rpn_cls: 0.01461  loss_rpn_loc: 0.05348  time: 0.6246  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 15:24:13] d2.utils.events INFO:  eta: 8:57:51  iter: 16319  total_loss: 2.788  loss_cls_stage0: 0.2357  loss_box_reg_stage0: 0.2548  loss_cls_stage1: 0.259  loss_box_reg_stage1: 0.6966  loss_cls_stage2: 0.2544  loss_box_reg_stage2: 0.9313  loss_mask: 0.07953  loss_rpn_cls: 0.01514  loss_rpn_loc: 0.0497  time: 0.6246  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:24:26] d2.utils.events INFO:  eta: 8:57:30  iter: 16339  total_loss: 3.124  loss_cls_stage0: 0.2951  loss_box_reg_stage0: 0.2717  loss_cls_stage1: 0.3282  loss_box_reg_stage1: 0.7699  loss_cls_stage2: 0.3266  loss_box_reg_stage2: 0.9846  loss_mask: 0.08797  loss_rpn_cls: 0.01133  loss_rpn_loc: 0.04761  time: 0.6246  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:24:38] d2.utils.events INFO:  eta: 8:56:41  iter: 16359  total_loss: 3.142  loss_cls_stage0: 0.2964  loss_box_reg_stage0: 0.2759  loss_cls_stage1: 0.346  loss_box_reg_stage1: 0.7423  loss_cls_stage2: 0.3465  loss_box_reg_stage2: 0.9536  loss_mask: 0.07973  loss_rpn_cls: 0.0134  loss_rpn_loc: 0.04926  time: 0.6246  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:24:50] d2.utils.events INFO:  eta: 8:56:28  iter: 16379  total_loss: 2.757  loss_cls_stage0: 0.2457  loss_box_reg_stage0: 0.2556  loss_cls_stage1: 0.2592  loss_box_reg_stage1: 0.6714  loss_cls_stage2: 0.258  loss_box_reg_stage2: 0.905  loss_mask: 0.07245  loss_rpn_cls: 0.01194  loss_rpn_loc: 0.04334  time: 0.6246  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 15:25:02] d2.utils.events INFO:  eta: 8:56:12  iter: 16399  total_loss: 3.153  loss_cls_stage0: 0.292  loss_box_reg_stage0: 0.2878  loss_cls_stage1: 0.2991  loss_box_reg_stage1: 0.7174  loss_cls_stage2: 0.292  loss_box_reg_stage2: 0.8496  loss_mask: 0.08251  loss_rpn_cls: 0.02647  loss_rpn_loc: 0.04798  time: 0.6246  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:25:14] d2.utils.events INFO:  eta: 8:55:58  iter: 16419  total_loss: 3.142  loss_cls_stage0: 0.3207  loss_box_reg_stage0: 0.2936  loss_cls_stage1: 0.3504  loss_box_reg_stage1: 0.7681  loss_cls_stage2: 0.3215  loss_box_reg_stage2: 1.027  loss_mask: 0.08445  loss_rpn_cls: 0.01695  loss_rpn_loc: 0.05764  time: 0.6245  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 15:25:27] d2.utils.events INFO:  eta: 8:55:31  iter: 16439  total_loss: 3.186  loss_cls_stage0: 0.3096  loss_box_reg_stage0: 0.312  loss_cls_stage1: 0.3305  loss_box_reg_stage1: 0.7319  loss_cls_stage2: 0.2863  loss_box_reg_stage2: 0.9956  loss_mask: 0.09058  loss_rpn_cls: 0.02589  loss_rpn_loc: 0.0491  time: 0.6245  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:25:39] d2.utils.events INFO:  eta: 8:55:29  iter: 16459  total_loss: 2.904  loss_cls_stage0: 0.2736  loss_box_reg_stage0: 0.2766  loss_cls_stage1: 0.2991  loss_box_reg_stage1: 0.6905  loss_cls_stage2: 0.2801  loss_box_reg_stage2: 0.8475  loss_mask: 0.08784  loss_rpn_cls: 0.01926  loss_rpn_loc: 0.04848  time: 0.6245  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:25:52] d2.utils.events INFO:  eta: 8:54:57  iter: 16479  total_loss: 2.926  loss_cls_stage0: 0.2469  loss_box_reg_stage0: 0.2848  loss_cls_stage1: 0.2539  loss_box_reg_stage1: 0.6972  loss_cls_stage2: 0.2452  loss_box_reg_stage2: 0.9408  loss_mask: 0.07982  loss_rpn_cls: 0.01571  loss_rpn_loc: 0.04149  time: 0.6245  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:26:04] d2.utils.events INFO:  eta: 8:54:39  iter: 16499  total_loss: 3.118  loss_cls_stage0: 0.3012  loss_box_reg_stage0: 0.2837  loss_cls_stage1: 0.3401  loss_box_reg_stage1: 0.6877  loss_cls_stage2: 0.3228  loss_box_reg_stage2: 0.9087  loss_mask: 0.08174  loss_rpn_cls: 0.01996  loss_rpn_loc: 0.04812  time: 0.6245  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 15:26:16] d2.utils.events INFO:  eta: 8:54:27  iter: 16519  total_loss: 2.868  loss_cls_stage0: 0.2595  loss_box_reg_stage0: 0.2759  loss_cls_stage1: 0.2748  loss_box_reg_stage1: 0.6689  loss_cls_stage2: 0.2876  loss_box_reg_stage2: 0.8649  loss_mask: 0.08263  loss_rpn_cls: 0.01728  loss_rpn_loc: 0.05299  time: 0.6245  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:26:29] d2.utils.events INFO:  eta: 8:54:00  iter: 16539  total_loss: 2.879  loss_cls_stage0: 0.2841  loss_box_reg_stage0: 0.289  loss_cls_stage1: 0.2742  loss_box_reg_stage1: 0.695  loss_cls_stage2: 0.2673  loss_box_reg_stage2: 0.8595  loss_mask: 0.08166  loss_rpn_cls: 0.01737  loss_rpn_loc: 0.05336  time: 0.6245  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 15:26:41] d2.utils.events INFO:  eta: 8:53:41  iter: 16559  total_loss: 2.915  loss_cls_stage0: 0.2656  loss_box_reg_stage0: 0.269  loss_cls_stage1: 0.2981  loss_box_reg_stage1: 0.6947  loss_cls_stage2: 0.3074  loss_box_reg_stage2: 0.9087  loss_mask: 0.08287  loss_rpn_cls: 0.01087  loss_rpn_loc: 0.04807  time: 0.6245  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:26:53] d2.utils.events INFO:  eta: 8:52:51  iter: 16579  total_loss: 2.712  loss_cls_stage0: 0.2414  loss_box_reg_stage0: 0.2413  loss_cls_stage1: 0.2694  loss_box_reg_stage1: 0.591  loss_cls_stage2: 0.2597  loss_box_reg_stage2: 0.8212  loss_mask: 0.07869  loss_rpn_cls: 0.01611  loss_rpn_loc: 0.04857  time: 0.6244  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 15:27:06] d2.utils.events INFO:  eta: 8:52:44  iter: 16599  total_loss: 2.911  loss_cls_stage0: 0.2858  loss_box_reg_stage0: 0.2681  loss_cls_stage1: 0.3029  loss_box_reg_stage1: 0.6634  loss_cls_stage2: 0.3131  loss_box_reg_stage2: 0.8653  loss_mask: 0.07569  loss_rpn_cls: 0.01296  loss_rpn_loc: 0.04741  time: 0.6244  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:27:18] d2.utils.events INFO:  eta: 8:52:27  iter: 16619  total_loss: 2.995  loss_cls_stage0: 0.2985  loss_box_reg_stage0: 0.2983  loss_cls_stage1: 0.3206  loss_box_reg_stage1: 0.721  loss_cls_stage2: 0.2856  loss_box_reg_stage2: 0.9477  loss_mask: 0.08332  loss_rpn_cls: 0.01415  loss_rpn_loc: 0.0585  time: 0.6244  data_time: 0.0055  lr: 0.00016  max_mem: 19679M
[07/29 15:27:30] d2.utils.events INFO:  eta: 8:52:19  iter: 16639  total_loss: 2.736  loss_cls_stage0: 0.2752  loss_box_reg_stage0: 0.2635  loss_cls_stage1: 0.3017  loss_box_reg_stage1: 0.6675  loss_cls_stage2: 0.2665  loss_box_reg_stage2: 0.8968  loss_mask: 0.07351  loss_rpn_cls: 0.01081  loss_rpn_loc: 0.04282  time: 0.6244  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:27:43] d2.utils.events INFO:  eta: 8:51:43  iter: 16659  total_loss: 3.331  loss_cls_stage0: 0.3044  loss_box_reg_stage0: 0.3037  loss_cls_stage1: 0.3272  loss_box_reg_stage1: 0.7757  loss_cls_stage2: 0.3255  loss_box_reg_stage2: 1.069  loss_mask: 0.0829  loss_rpn_cls: 0.009962  loss_rpn_loc: 0.04503  time: 0.6244  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:27:55] d2.utils.events INFO:  eta: 8:51:18  iter: 16679  total_loss: 2.627  loss_cls_stage0: 0.2605  loss_box_reg_stage0: 0.2811  loss_cls_stage1: 0.2624  loss_box_reg_stage1: 0.6342  loss_cls_stage2: 0.2567  loss_box_reg_stage2: 0.8387  loss_mask: 0.08334  loss_rpn_cls: 0.01246  loss_rpn_loc: 0.05133  time: 0.6244  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:28:07] d2.utils.events INFO:  eta: 8:50:07  iter: 16699  total_loss: 2.849  loss_cls_stage0: 0.2823  loss_box_reg_stage0: 0.2672  loss_cls_stage1: 0.2898  loss_box_reg_stage1: 0.6822  loss_cls_stage2: 0.2816  loss_box_reg_stage2: 0.8542  loss_mask: 0.07575  loss_rpn_cls: 0.01458  loss_rpn_loc: 0.04469  time: 0.6243  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 15:28:20] d2.utils.events INFO:  eta: 8:49:45  iter: 16719  total_loss: 2.784  loss_cls_stage0: 0.2997  loss_box_reg_stage0: 0.2853  loss_cls_stage1: 0.2741  loss_box_reg_stage1: 0.6542  loss_cls_stage2: 0.2546  loss_box_reg_stage2: 0.8575  loss_mask: 0.0804  loss_rpn_cls: 0.0229  loss_rpn_loc: 0.04967  time: 0.6243  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:28:32] d2.utils.events INFO:  eta: 8:49:46  iter: 16739  total_loss: 2.889  loss_cls_stage0: 0.3142  loss_box_reg_stage0: 0.2858  loss_cls_stage1: 0.317  loss_box_reg_stage1: 0.6241  loss_cls_stage2: 0.33  loss_box_reg_stage2: 0.8275  loss_mask: 0.0764  loss_rpn_cls: 0.01837  loss_rpn_loc: 0.05679  time: 0.6243  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:28:44] d2.utils.events INFO:  eta: 8:49:39  iter: 16759  total_loss: 2.813  loss_cls_stage0: 0.2858  loss_box_reg_stage0: 0.2896  loss_cls_stage1: 0.2841  loss_box_reg_stage1: 0.6748  loss_cls_stage2: 0.274  loss_box_reg_stage2: 0.8568  loss_mask: 0.08768  loss_rpn_cls: 0.01311  loss_rpn_loc: 0.04031  time: 0.6243  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:28:56] d2.utils.events INFO:  eta: 8:48:44  iter: 16779  total_loss: 2.88  loss_cls_stage0: 0.2631  loss_box_reg_stage0: 0.2686  loss_cls_stage1: 0.278  loss_box_reg_stage1: 0.7167  loss_cls_stage2: 0.2839  loss_box_reg_stage2: 0.9417  loss_mask: 0.07669  loss_rpn_cls: 0.0132  loss_rpn_loc: 0.04902  time: 0.6243  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:29:09] d2.utils.events INFO:  eta: 8:48:31  iter: 16799  total_loss: 2.787  loss_cls_stage0: 0.2745  loss_box_reg_stage0: 0.2645  loss_cls_stage1: 0.3188  loss_box_reg_stage1: 0.6639  loss_cls_stage2: 0.3142  loss_box_reg_stage2: 0.9277  loss_mask: 0.0779  loss_rpn_cls: 0.01518  loss_rpn_loc: 0.06215  time: 0.6243  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:29:21] d2.utils.events INFO:  eta: 8:48:19  iter: 16819  total_loss: 2.982  loss_cls_stage0: 0.2579  loss_box_reg_stage0: 0.2789  loss_cls_stage1: 0.3026  loss_box_reg_stage1: 0.716  loss_cls_stage2: 0.2665  loss_box_reg_stage2: 0.9635  loss_mask: 0.07755  loss_rpn_cls: 0.0134  loss_rpn_loc: 0.04324  time: 0.6243  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:29:34] d2.utils.events INFO:  eta: 8:47:34  iter: 16839  total_loss: 3.147  loss_cls_stage0: 0.2928  loss_box_reg_stage0: 0.2617  loss_cls_stage1: 0.3438  loss_box_reg_stage1: 0.6933  loss_cls_stage2: 0.3133  loss_box_reg_stage2: 0.9762  loss_mask: 0.07955  loss_rpn_cls: 0.0095  loss_rpn_loc: 0.0421  time: 0.6243  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:29:46] d2.utils.events INFO:  eta: 8:46:35  iter: 16859  total_loss: 3.019  loss_cls_stage0: 0.3041  loss_box_reg_stage0: 0.2824  loss_cls_stage1: 0.2977  loss_box_reg_stage1: 0.6785  loss_cls_stage2: 0.3072  loss_box_reg_stage2: 0.8916  loss_mask: 0.0786  loss_rpn_cls: 0.01381  loss_rpn_loc: 0.05669  time: 0.6242  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:29:58] d2.utils.events INFO:  eta: 8:46:28  iter: 16879  total_loss: 2.856  loss_cls_stage0: 0.256  loss_box_reg_stage0: 0.2733  loss_cls_stage1: 0.2855  loss_box_reg_stage1: 0.698  loss_cls_stage2: 0.2895  loss_box_reg_stage2: 0.9577  loss_mask: 0.08174  loss_rpn_cls: 0.01234  loss_rpn_loc: 0.04084  time: 0.6242  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:30:10] d2.utils.events INFO:  eta: 8:45:52  iter: 16899  total_loss: 2.791  loss_cls_stage0: 0.2322  loss_box_reg_stage0: 0.2502  loss_cls_stage1: 0.2706  loss_box_reg_stage1: 0.6436  loss_cls_stage2: 0.2702  loss_box_reg_stage2: 0.8662  loss_mask: 0.07644  loss_rpn_cls: 0.01388  loss_rpn_loc: 0.04394  time: 0.6242  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:30:23] d2.utils.events INFO:  eta: 8:46:03  iter: 16919  total_loss: 2.926  loss_cls_stage0: 0.2665  loss_box_reg_stage0: 0.257  loss_cls_stage1: 0.3097  loss_box_reg_stage1: 0.7119  loss_cls_stage2: 0.3237  loss_box_reg_stage2: 0.9138  loss_mask: 0.08281  loss_rpn_cls: 0.01089  loss_rpn_loc: 0.04546  time: 0.6242  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:30:35] d2.utils.events INFO:  eta: 8:44:56  iter: 16939  total_loss: 3.093  loss_cls_stage0: 0.2943  loss_box_reg_stage0: 0.2803  loss_cls_stage1: 0.318  loss_box_reg_stage1: 0.7336  loss_cls_stage2: 0.3188  loss_box_reg_stage2: 0.9461  loss_mask: 0.08562  loss_rpn_cls: 0.02433  loss_rpn_loc: 0.05924  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:30:47] d2.utils.events INFO:  eta: 8:44:41  iter: 16959  total_loss: 2.789  loss_cls_stage0: 0.2499  loss_box_reg_stage0: 0.2559  loss_cls_stage1: 0.2811  loss_box_reg_stage1: 0.6624  loss_cls_stage2: 0.2624  loss_box_reg_stage2: 0.9367  loss_mask: 0.08224  loss_rpn_cls: 0.01294  loss_rpn_loc: 0.04829  time: 0.6241  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 15:30:59] d2.utils.events INFO:  eta: 8:44:35  iter: 16979  total_loss: 2.861  loss_cls_stage0: 0.2536  loss_box_reg_stage0: 0.2712  loss_cls_stage1: 0.281  loss_box_reg_stage1: 0.7171  loss_cls_stage2: 0.2731  loss_box_reg_stage2: 0.9046  loss_mask: 0.07929  loss_rpn_cls: 0.01539  loss_rpn_loc: 0.04274  time: 0.6241  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 15:31:11] d2.utils.events INFO:  eta: 8:43:41  iter: 16999  total_loss: 2.515  loss_cls_stage0: 0.2262  loss_box_reg_stage0: 0.2313  loss_cls_stage1: 0.2682  loss_box_reg_stage1: 0.6345  loss_cls_stage2: 0.2632  loss_box_reg_stage2: 0.8556  loss_mask: 0.07244  loss_rpn_cls: 0.01578  loss_rpn_loc: 0.04092  time: 0.6241  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 15:31:24] d2.utils.events INFO:  eta: 8:42:44  iter: 17019  total_loss: 2.82  loss_cls_stage0: 0.2427  loss_box_reg_stage0: 0.2466  loss_cls_stage1: 0.2432  loss_box_reg_stage1: 0.6875  loss_cls_stage2: 0.2515  loss_box_reg_stage2: 0.9183  loss_mask: 0.07747  loss_rpn_cls: 0.01086  loss_rpn_loc: 0.039  time: 0.6240  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 15:31:36] d2.utils.events INFO:  eta: 8:42:58  iter: 17039  total_loss: 3.054  loss_cls_stage0: 0.3227  loss_box_reg_stage0: 0.3084  loss_cls_stage1: 0.3336  loss_box_reg_stage1: 0.7271  loss_cls_stage2: 0.3127  loss_box_reg_stage2: 0.9461  loss_mask: 0.09604  loss_rpn_cls: 0.01364  loss_rpn_loc: 0.04721  time: 0.6240  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:31:48] d2.utils.events INFO:  eta: 8:42:20  iter: 17059  total_loss: 3.039  loss_cls_stage0: 0.2651  loss_box_reg_stage0: 0.2634  loss_cls_stage1: 0.3217  loss_box_reg_stage1: 0.7506  loss_cls_stage2: 0.3361  loss_box_reg_stage2: 1.001  loss_mask: 0.0828  loss_rpn_cls: 0.01073  loss_rpn_loc: 0.04802  time: 0.6240  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:32:01] d2.utils.events INFO:  eta: 8:42:27  iter: 17079  total_loss: 3.083  loss_cls_stage0: 0.2822  loss_box_reg_stage0: 0.2897  loss_cls_stage1: 0.2928  loss_box_reg_stage1: 0.759  loss_cls_stage2: 0.2773  loss_box_reg_stage2: 0.9442  loss_mask: 0.08248  loss_rpn_cls: 0.01571  loss_rpn_loc: 0.05802  time: 0.6240  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:32:13] d2.utils.events INFO:  eta: 8:41:57  iter: 17099  total_loss: 2.896  loss_cls_stage0: 0.2892  loss_box_reg_stage0: 0.287  loss_cls_stage1: 0.294  loss_box_reg_stage1: 0.7011  loss_cls_stage2: 0.3078  loss_box_reg_stage2: 0.9399  loss_mask: 0.0827  loss_rpn_cls: 0.01378  loss_rpn_loc: 0.0459  time: 0.6240  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 15:32:26] d2.utils.events INFO:  eta: 8:41:50  iter: 17119  total_loss: 2.575  loss_cls_stage0: 0.2466  loss_box_reg_stage0: 0.2673  loss_cls_stage1: 0.2418  loss_box_reg_stage1: 0.6202  loss_cls_stage2: 0.2473  loss_box_reg_stage2: 0.8355  loss_mask: 0.08288  loss_rpn_cls: 0.01925  loss_rpn_loc: 0.03974  time: 0.6240  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:32:38] d2.utils.events INFO:  eta: 8:41:30  iter: 17139  total_loss: 2.775  loss_cls_stage0: 0.2782  loss_box_reg_stage0: 0.2693  loss_cls_stage1: 0.2598  loss_box_reg_stage1: 0.6525  loss_cls_stage2: 0.2464  loss_box_reg_stage2: 0.8704  loss_mask: 0.08266  loss_rpn_cls: 0.01507  loss_rpn_loc: 0.05503  time: 0.6240  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:32:51] d2.utils.events INFO:  eta: 8:41:58  iter: 17159  total_loss: 2.599  loss_cls_stage0: 0.2226  loss_box_reg_stage0: 0.2543  loss_cls_stage1: 0.2244  loss_box_reg_stage1: 0.565  loss_cls_stage2: 0.2171  loss_box_reg_stage2: 0.8118  loss_mask: 0.07197  loss_rpn_cls: 0.01909  loss_rpn_loc: 0.05462  time: 0.6240  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:33:03] d2.utils.events INFO:  eta: 8:41:24  iter: 17179  total_loss: 2.825  loss_cls_stage0: 0.2746  loss_box_reg_stage0: 0.274  loss_cls_stage1: 0.2643  loss_box_reg_stage1: 0.6451  loss_cls_stage2: 0.265  loss_box_reg_stage2: 0.8768  loss_mask: 0.07594  loss_rpn_cls: 0.01537  loss_rpn_loc: 0.03975  time: 0.6240  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 15:33:15] d2.utils.events INFO:  eta: 8:41:33  iter: 17199  total_loss: 2.909  loss_cls_stage0: 0.284  loss_box_reg_stage0: 0.2736  loss_cls_stage1: 0.3221  loss_box_reg_stage1: 0.6714  loss_cls_stage2: 0.3124  loss_box_reg_stage2: 0.9374  loss_mask: 0.08564  loss_rpn_cls: 0.01665  loss_rpn_loc: 0.04951  time: 0.6239  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 15:33:27] d2.utils.events INFO:  eta: 8:41:20  iter: 17219  total_loss: 3.051  loss_cls_stage0: 0.2855  loss_box_reg_stage0: 0.2756  loss_cls_stage1: 0.3291  loss_box_reg_stage1: 0.7029  loss_cls_stage2: 0.3065  loss_box_reg_stage2: 0.9134  loss_mask: 0.08965  loss_rpn_cls: 0.01652  loss_rpn_loc: 0.05061  time: 0.6239  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:33:40] d2.utils.events INFO:  eta: 8:41:19  iter: 17239  total_loss: 3.067  loss_cls_stage0: 0.2778  loss_box_reg_stage0: 0.2699  loss_cls_stage1: 0.3183  loss_box_reg_stage1: 0.6936  loss_cls_stage2: 0.2849  loss_box_reg_stage2: 0.9259  loss_mask: 0.08331  loss_rpn_cls: 0.0216  loss_rpn_loc: 0.04407  time: 0.6239  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:33:52] d2.utils.events INFO:  eta: 8:40:45  iter: 17259  total_loss: 2.864  loss_cls_stage0: 0.2808  loss_box_reg_stage0: 0.2959  loss_cls_stage1: 0.2934  loss_box_reg_stage1: 0.6613  loss_cls_stage2: 0.2711  loss_box_reg_stage2: 0.8682  loss_mask: 0.08192  loss_rpn_cls: 0.02122  loss_rpn_loc: 0.04619  time: 0.6239  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:34:04] d2.utils.events INFO:  eta: 8:40:38  iter: 17279  total_loss: 2.728  loss_cls_stage0: 0.2556  loss_box_reg_stage0: 0.2659  loss_cls_stage1: 0.2617  loss_box_reg_stage1: 0.6528  loss_cls_stage2: 0.2506  loss_box_reg_stage2: 0.8516  loss_mask: 0.07324  loss_rpn_cls: 0.01811  loss_rpn_loc: 0.04305  time: 0.6239  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:34:17] d2.utils.events INFO:  eta: 8:41:11  iter: 17299  total_loss: 3.005  loss_cls_stage0: 0.315  loss_box_reg_stage0: 0.3137  loss_cls_stage1: 0.3047  loss_box_reg_stage1: 0.6857  loss_cls_stage2: 0.2825  loss_box_reg_stage2: 0.8236  loss_mask: 0.09278  loss_rpn_cls: 0.01716  loss_rpn_loc: 0.05386  time: 0.6239  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:34:29] d2.utils.events INFO:  eta: 8:40:34  iter: 17319  total_loss: 2.494  loss_cls_stage0: 0.2257  loss_box_reg_stage0: 0.2438  loss_cls_stage1: 0.2361  loss_box_reg_stage1: 0.5952  loss_cls_stage2: 0.231  loss_box_reg_stage2: 0.7849  loss_mask: 0.07947  loss_rpn_cls: 0.01335  loss_rpn_loc: 0.05243  time: 0.6239  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:34:42] d2.utils.events INFO:  eta: 8:39:47  iter: 17339  total_loss: 2.893  loss_cls_stage0: 0.2576  loss_box_reg_stage0: 0.2826  loss_cls_stage1: 0.2894  loss_box_reg_stage1: 0.7158  loss_cls_stage2: 0.2719  loss_box_reg_stage2: 0.8685  loss_mask: 0.07713  loss_rpn_cls: 0.02114  loss_rpn_loc: 0.04878  time: 0.6239  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:34:54] d2.utils.events INFO:  eta: 8:39:29  iter: 17359  total_loss: 2.69  loss_cls_stage0: 0.2412  loss_box_reg_stage0: 0.2607  loss_cls_stage1: 0.2375  loss_box_reg_stage1: 0.6533  loss_cls_stage2: 0.2252  loss_box_reg_stage2: 0.9009  loss_mask: 0.07039  loss_rpn_cls: 0.01275  loss_rpn_loc: 0.04279  time: 0.6238  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:35:06] d2.utils.events INFO:  eta: 8:39:23  iter: 17379  total_loss: 3.199  loss_cls_stage0: 0.2865  loss_box_reg_stage0: 0.2591  loss_cls_stage1: 0.3277  loss_box_reg_stage1: 0.7261  loss_cls_stage2: 0.3098  loss_box_reg_stage2: 0.9573  loss_mask: 0.07471  loss_rpn_cls: 0.01774  loss_rpn_loc: 0.05167  time: 0.6238  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:35:18] d2.utils.events INFO:  eta: 8:39:14  iter: 17399  total_loss: 2.627  loss_cls_stage0: 0.2547  loss_box_reg_stage0: 0.2435  loss_cls_stage1: 0.2966  loss_box_reg_stage1: 0.6171  loss_cls_stage2: 0.272  loss_box_reg_stage2: 0.8045  loss_mask: 0.06941  loss_rpn_cls: 0.01357  loss_rpn_loc: 0.04748  time: 0.6238  data_time: 0.0045  lr: 0.00016  max_mem: 19679M
[07/29 15:35:30] d2.utils.events INFO:  eta: 8:38:52  iter: 17419  total_loss: 2.685  loss_cls_stage0: 0.2573  loss_box_reg_stage0: 0.2551  loss_cls_stage1: 0.3131  loss_box_reg_stage1: 0.6298  loss_cls_stage2: 0.3143  loss_box_reg_stage2: 0.9604  loss_mask: 0.08646  loss_rpn_cls: 0.01989  loss_rpn_loc: 0.05152  time: 0.6238  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:35:43] d2.utils.events INFO:  eta: 8:38:33  iter: 17439  total_loss: 2.946  loss_cls_stage0: 0.3055  loss_box_reg_stage0: 0.2893  loss_cls_stage1: 0.3166  loss_box_reg_stage1: 0.677  loss_cls_stage2: 0.2992  loss_box_reg_stage2: 0.8569  loss_mask: 0.08842  loss_rpn_cls: 0.01662  loss_rpn_loc: 0.04965  time: 0.6237  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:35:55] d2.utils.events INFO:  eta: 8:38:04  iter: 17459  total_loss: 2.897  loss_cls_stage0: 0.2773  loss_box_reg_stage0: 0.2919  loss_cls_stage1: 0.2884  loss_box_reg_stage1: 0.7036  loss_cls_stage2: 0.2573  loss_box_reg_stage2: 0.9328  loss_mask: 0.07654  loss_rpn_cls: 0.01723  loss_rpn_loc: 0.05197  time: 0.6237  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:36:07] d2.utils.events INFO:  eta: 8:37:38  iter: 17479  total_loss: 2.769  loss_cls_stage0: 0.2276  loss_box_reg_stage0: 0.2627  loss_cls_stage1: 0.2547  loss_box_reg_stage1: 0.6862  loss_cls_stage2: 0.2353  loss_box_reg_stage2: 0.8821  loss_mask: 0.07978  loss_rpn_cls: 0.02181  loss_rpn_loc: 0.05242  time: 0.6237  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:36:20] d2.utils.events INFO:  eta: 8:37:46  iter: 17499  total_loss: 2.683  loss_cls_stage0: 0.2591  loss_box_reg_stage0: 0.2552  loss_cls_stage1: 0.2814  loss_box_reg_stage1: 0.6342  loss_cls_stage2: 0.2854  loss_box_reg_stage2: 0.8609  loss_mask: 0.08118  loss_rpn_cls: 0.01389  loss_rpn_loc: 0.05387  time: 0.6237  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:36:32] d2.utils.events INFO:  eta: 8:37:34  iter: 17519  total_loss: 2.886  loss_cls_stage0: 0.2576  loss_box_reg_stage0: 0.2676  loss_cls_stage1: 0.2738  loss_box_reg_stage1: 0.7057  loss_cls_stage2: 0.2639  loss_box_reg_stage2: 0.9238  loss_mask: 0.08136  loss_rpn_cls: 0.01421  loss_rpn_loc: 0.04179  time: 0.6237  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:36:45] d2.utils.events INFO:  eta: 8:37:37  iter: 17539  total_loss: 3.047  loss_cls_stage0: 0.2954  loss_box_reg_stage0: 0.2695  loss_cls_stage1: 0.3309  loss_box_reg_stage1: 0.6991  loss_cls_stage2: 0.3128  loss_box_reg_stage2: 0.9402  loss_mask: 0.07581  loss_rpn_cls: 0.01486  loss_rpn_loc: 0.05736  time: 0.6237  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:36:57] d2.utils.events INFO:  eta: 8:37:42  iter: 17559  total_loss: 2.879  loss_cls_stage0: 0.2769  loss_box_reg_stage0: 0.2645  loss_cls_stage1: 0.2943  loss_box_reg_stage1: 0.7078  loss_cls_stage2: 0.2883  loss_box_reg_stage2: 0.953  loss_mask: 0.07216  loss_rpn_cls: 0.009743  loss_rpn_loc: 0.04466  time: 0.6237  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:37:09] d2.utils.events INFO:  eta: 8:37:32  iter: 17579  total_loss: 2.582  loss_cls_stage0: 0.2715  loss_box_reg_stage0: 0.2684  loss_cls_stage1: 0.2458  loss_box_reg_stage1: 0.6285  loss_cls_stage2: 0.2524  loss_box_reg_stage2: 0.8164  loss_mask: 0.08083  loss_rpn_cls: 0.02404  loss_rpn_loc: 0.04953  time: 0.6237  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:37:22] d2.utils.events INFO:  eta: 8:37:27  iter: 17599  total_loss: 2.454  loss_cls_stage0: 0.2273  loss_box_reg_stage0: 0.255  loss_cls_stage1: 0.2324  loss_box_reg_stage1: 0.5862  loss_cls_stage2: 0.212  loss_box_reg_stage2: 0.7716  loss_mask: 0.07214  loss_rpn_cls: 0.01721  loss_rpn_loc: 0.04448  time: 0.6237  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:37:35] d2.utils.events INFO:  eta: 8:37:48  iter: 17619  total_loss: 2.92  loss_cls_stage0: 0.2691  loss_box_reg_stage0: 0.2849  loss_cls_stage1: 0.2838  loss_box_reg_stage1: 0.6287  loss_cls_stage2: 0.2609  loss_box_reg_stage2: 0.8338  loss_mask: 0.08079  loss_rpn_cls: 0.01674  loss_rpn_loc: 0.04146  time: 0.6237  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:37:47] d2.utils.events INFO:  eta: 8:37:20  iter: 17639  total_loss: 2.732  loss_cls_stage0: 0.2532  loss_box_reg_stage0: 0.2737  loss_cls_stage1: 0.2559  loss_box_reg_stage1: 0.6438  loss_cls_stage2: 0.2715  loss_box_reg_stage2: 0.8404  loss_mask: 0.08203  loss_rpn_cls: 0.01792  loss_rpn_loc: 0.04602  time: 0.6237  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:38:00] d2.utils.events INFO:  eta: 8:37:25  iter: 17659  total_loss: 3.213  loss_cls_stage0: 0.3042  loss_box_reg_stage0: 0.2914  loss_cls_stage1: 0.2999  loss_box_reg_stage1: 0.7277  loss_cls_stage2: 0.2846  loss_box_reg_stage2: 0.9436  loss_mask: 0.08872  loss_rpn_cls: 0.02012  loss_rpn_loc: 0.04464  time: 0.6237  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:38:12] d2.utils.events INFO:  eta: 8:37:48  iter: 17679  total_loss: 3.076  loss_cls_stage0: 0.2845  loss_box_reg_stage0: 0.2844  loss_cls_stage1: 0.3136  loss_box_reg_stage1: 0.6898  loss_cls_stage2: 0.3053  loss_box_reg_stage2: 0.9092  loss_mask: 0.09913  loss_rpn_cls: 0.01959  loss_rpn_loc: 0.05013  time: 0.6237  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:38:25] d2.utils.events INFO:  eta: 8:38:11  iter: 17699  total_loss: 2.609  loss_cls_stage0: 0.2568  loss_box_reg_stage0: 0.2469  loss_cls_stage1: 0.2511  loss_box_reg_stage1: 0.5926  loss_cls_stage2: 0.246  loss_box_reg_stage2: 0.8246  loss_mask: 0.07517  loss_rpn_cls: 0.01486  loss_rpn_loc: 0.04044  time: 0.6237  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 15:38:38] d2.utils.events INFO:  eta: 8:38:15  iter: 17719  total_loss: 3.052  loss_cls_stage0: 0.2774  loss_box_reg_stage0: 0.291  loss_cls_stage1: 0.2773  loss_box_reg_stage1: 0.7208  loss_cls_stage2: 0.2679  loss_box_reg_stage2: 0.9745  loss_mask: 0.09439  loss_rpn_cls: 0.0126  loss_rpn_loc: 0.0555  time: 0.6238  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 15:38:50] d2.utils.events INFO:  eta: 8:38:12  iter: 17739  total_loss: 3.324  loss_cls_stage0: 0.3074  loss_box_reg_stage0: 0.2831  loss_cls_stage1: 0.3466  loss_box_reg_stage1: 0.7774  loss_cls_stage2: 0.3297  loss_box_reg_stage2: 1.023  loss_mask: 0.08963  loss_rpn_cls: 0.0161  loss_rpn_loc: 0.04505  time: 0.6238  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:39:03] d2.utils.events INFO:  eta: 8:38:08  iter: 17759  total_loss: 2.886  loss_cls_stage0: 0.2833  loss_box_reg_stage0: 0.2662  loss_cls_stage1: 0.2902  loss_box_reg_stage1: 0.6735  loss_cls_stage2: 0.3024  loss_box_reg_stage2: 0.9492  loss_mask: 0.07387  loss_rpn_cls: 0.0167  loss_rpn_loc: 0.05333  time: 0.6238  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:39:16] d2.utils.events INFO:  eta: 8:38:08  iter: 17779  total_loss: 2.811  loss_cls_stage0: 0.2608  loss_box_reg_stage0: 0.2594  loss_cls_stage1: 0.286  loss_box_reg_stage1: 0.6819  loss_cls_stage2: 0.2265  loss_box_reg_stage2: 0.8916  loss_mask: 0.07762  loss_rpn_cls: 0.02054  loss_rpn_loc: 0.06171  time: 0.6238  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:39:28] d2.utils.events INFO:  eta: 8:37:46  iter: 17799  total_loss: 2.927  loss_cls_stage0: 0.2845  loss_box_reg_stage0: 0.2781  loss_cls_stage1: 0.3077  loss_box_reg_stage1: 0.6863  loss_cls_stage2: 0.2945  loss_box_reg_stage2: 0.9062  loss_mask: 0.08613  loss_rpn_cls: 0.02191  loss_rpn_loc: 0.04635  time: 0.6238  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:39:41] d2.utils.events INFO:  eta: 8:37:31  iter: 17819  total_loss: 3.033  loss_cls_stage0: 0.2712  loss_box_reg_stage0: 0.2803  loss_cls_stage1: 0.3146  loss_box_reg_stage1: 0.7015  loss_cls_stage2: 0.3065  loss_box_reg_stage2: 0.9785  loss_mask: 0.08162  loss_rpn_cls: 0.01445  loss_rpn_loc: 0.04228  time: 0.6238  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 15:39:53] d2.utils.events INFO:  eta: 8:37:18  iter: 17839  total_loss: 2.996  loss_cls_stage0: 0.2641  loss_box_reg_stage0: 0.267  loss_cls_stage1: 0.2997  loss_box_reg_stage1: 0.6746  loss_cls_stage2: 0.2764  loss_box_reg_stage2: 0.9333  loss_mask: 0.07754  loss_rpn_cls: 0.02258  loss_rpn_loc: 0.06467  time: 0.6238  data_time: 0.0054  lr: 0.00016  max_mem: 19679M
[07/29 15:40:06] d2.utils.events INFO:  eta: 8:37:35  iter: 17859  total_loss: 2.904  loss_cls_stage0: 0.2714  loss_box_reg_stage0: 0.2525  loss_cls_stage1: 0.3079  loss_box_reg_stage1: 0.6828  loss_cls_stage2: 0.3014  loss_box_reg_stage2: 0.9306  loss_mask: 0.07011  loss_rpn_cls: 0.01645  loss_rpn_loc: 0.05155  time: 0.6238  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:40:19] d2.utils.events INFO:  eta: 8:37:25  iter: 17879  total_loss: 2.773  loss_cls_stage0: 0.2297  loss_box_reg_stage0: 0.2239  loss_cls_stage1: 0.2698  loss_box_reg_stage1: 0.6474  loss_cls_stage2: 0.2567  loss_box_reg_stage2: 0.9576  loss_mask: 0.06919  loss_rpn_cls: 0.01284  loss_rpn_loc: 0.04369  time: 0.6238  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:40:31] d2.utils.events INFO:  eta: 8:37:19  iter: 17899  total_loss: 2.753  loss_cls_stage0: 0.2747  loss_box_reg_stage0: 0.2581  loss_cls_stage1: 0.3018  loss_box_reg_stage1: 0.6721  loss_cls_stage2: 0.2853  loss_box_reg_stage2: 0.9519  loss_mask: 0.08469  loss_rpn_cls: 0.00851  loss_rpn_loc: 0.03976  time: 0.6238  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:40:43] d2.utils.events INFO:  eta: 8:37:06  iter: 17919  total_loss: 3.011  loss_cls_stage0: 0.2981  loss_box_reg_stage0: 0.2775  loss_cls_stage1: 0.3126  loss_box_reg_stage1: 0.7437  loss_cls_stage2: 0.2758  loss_box_reg_stage2: 0.9918  loss_mask: 0.08215  loss_rpn_cls: 0.02175  loss_rpn_loc: 0.04417  time: 0.6238  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 15:40:56] d2.utils.events INFO:  eta: 8:37:03  iter: 17939  total_loss: 2.852  loss_cls_stage0: 0.2684  loss_box_reg_stage0: 0.2548  loss_cls_stage1: 0.2989  loss_box_reg_stage1: 0.6856  loss_cls_stage2: 0.287  loss_box_reg_stage2: 0.8402  loss_mask: 0.06972  loss_rpn_cls: 0.01633  loss_rpn_loc: 0.05428  time: 0.6238  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:41:09] d2.utils.events INFO:  eta: 8:37:37  iter: 17959  total_loss: 2.988  loss_cls_stage0: 0.2728  loss_box_reg_stage0: 0.2635  loss_cls_stage1: 0.3015  loss_box_reg_stage1: 0.7077  loss_cls_stage2: 0.2996  loss_box_reg_stage2: 0.9712  loss_mask: 0.07452  loss_rpn_cls: 0.01249  loss_rpn_loc: 0.04284  time: 0.6238  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 15:41:21] d2.utils.events INFO:  eta: 8:36:50  iter: 17979  total_loss: 2.988  loss_cls_stage0: 0.2706  loss_box_reg_stage0: 0.2769  loss_cls_stage1: 0.2959  loss_box_reg_stage1: 0.698  loss_cls_stage2: 0.2682  loss_box_reg_stage2: 0.9319  loss_mask: 0.07931  loss_rpn_cls: 0.01224  loss_rpn_loc: 0.04475  time: 0.6238  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 15:41:34] d2.utils.events INFO:  eta: 8:36:44  iter: 17999  total_loss: 2.806  loss_cls_stage0: 0.2607  loss_box_reg_stage0: 0.2562  loss_cls_stage1: 0.2809  loss_box_reg_stage1: 0.6954  loss_cls_stage2: 0.281  loss_box_reg_stage2: 0.865  loss_mask: 0.08039  loss_rpn_cls: 0.0179  loss_rpn_loc: 0.04466  time: 0.6238  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:41:46] d2.utils.events INFO:  eta: 8:37:37  iter: 18019  total_loss: 2.571  loss_cls_stage0: 0.247  loss_box_reg_stage0: 0.2541  loss_cls_stage1: 0.2931  loss_box_reg_stage1: 0.6376  loss_cls_stage2: 0.2823  loss_box_reg_stage2: 0.8819  loss_mask: 0.07218  loss_rpn_cls: 0.01488  loss_rpn_loc: 0.04692  time: 0.6239  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 15:41:59] d2.utils.events INFO:  eta: 8:37:11  iter: 18039  total_loss: 2.762  loss_cls_stage0: 0.2579  loss_box_reg_stage0: 0.2649  loss_cls_stage1: 0.2721  loss_box_reg_stage1: 0.6815  loss_cls_stage2: 0.2671  loss_box_reg_stage2: 0.8621  loss_mask: 0.07401  loss_rpn_cls: 0.01394  loss_rpn_loc: 0.05279  time: 0.6239  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:42:12] d2.utils.events INFO:  eta: 8:37:21  iter: 18059  total_loss: 3.008  loss_cls_stage0: 0.2898  loss_box_reg_stage0: 0.2836  loss_cls_stage1: 0.3114  loss_box_reg_stage1: 0.7186  loss_cls_stage2: 0.3095  loss_box_reg_stage2: 0.9667  loss_mask: 0.07896  loss_rpn_cls: 0.02072  loss_rpn_loc: 0.0571  time: 0.6239  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:42:25] d2.utils.events INFO:  eta: 8:37:33  iter: 18079  total_loss: 3.06  loss_cls_stage0: 0.2893  loss_box_reg_stage0: 0.2655  loss_cls_stage1: 0.3004  loss_box_reg_stage1: 0.6713  loss_cls_stage2: 0.2754  loss_box_reg_stage2: 0.9568  loss_mask: 0.08384  loss_rpn_cls: 0.01535  loss_rpn_loc: 0.0496  time: 0.6239  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:42:37] d2.utils.events INFO:  eta: 8:37:33  iter: 18099  total_loss: 2.94  loss_cls_stage0: 0.2643  loss_box_reg_stage0: 0.273  loss_cls_stage1: 0.2846  loss_box_reg_stage1: 0.7153  loss_cls_stage2: 0.2874  loss_box_reg_stage2: 0.9516  loss_mask: 0.08111  loss_rpn_cls: 0.01329  loss_rpn_loc: 0.04609  time: 0.6239  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 15:42:49] d2.utils.events INFO:  eta: 8:37:23  iter: 18119  total_loss: 2.715  loss_cls_stage0: 0.2531  loss_box_reg_stage0: 0.2494  loss_cls_stage1: 0.2973  loss_box_reg_stage1: 0.6435  loss_cls_stage2: 0.269  loss_box_reg_stage2: 0.8528  loss_mask: 0.07509  loss_rpn_cls: 0.01665  loss_rpn_loc: 0.04933  time: 0.6239  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 15:43:02] d2.utils.events INFO:  eta: 8:37:31  iter: 18139  total_loss: 2.994  loss_cls_stage0: 0.2603  loss_box_reg_stage0: 0.2695  loss_cls_stage1: 0.2974  loss_box_reg_stage1: 0.717  loss_cls_stage2: 0.2873  loss_box_reg_stage2: 0.9266  loss_mask: 0.08591  loss_rpn_cls: 0.009748  loss_rpn_loc: 0.03834  time: 0.6239  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 15:43:14] d2.utils.events INFO:  eta: 8:36:56  iter: 18159  total_loss: 3.131  loss_cls_stage0: 0.2802  loss_box_reg_stage0: 0.2826  loss_cls_stage1: 0.318  loss_box_reg_stage1: 0.7884  loss_cls_stage2: 0.2885  loss_box_reg_stage2: 1.097  loss_mask: 0.0856  loss_rpn_cls: 0.01381  loss_rpn_loc: 0.04747  time: 0.6239  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:43:27] d2.utils.events INFO:  eta: 8:37:13  iter: 18179  total_loss: 3.343  loss_cls_stage0: 0.3267  loss_box_reg_stage0: 0.2984  loss_cls_stage1: 0.3434  loss_box_reg_stage1: 0.8224  loss_cls_stage2: 0.3239  loss_box_reg_stage2: 1.104  loss_mask: 0.08331  loss_rpn_cls: 0.01429  loss_rpn_loc: 0.05885  time: 0.6239  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:43:40] d2.utils.events INFO:  eta: 8:37:08  iter: 18199  total_loss: 3.192  loss_cls_stage0: 0.3055  loss_box_reg_stage0: 0.2804  loss_cls_stage1: 0.3526  loss_box_reg_stage1: 0.7647  loss_cls_stage2: 0.3238  loss_box_reg_stage2: 1.04  loss_mask: 0.08113  loss_rpn_cls: 0.01637  loss_rpn_loc: 0.05033  time: 0.6239  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:43:52] d2.utils.events INFO:  eta: 8:37:06  iter: 18219  total_loss: 3.026  loss_cls_stage0: 0.285  loss_box_reg_stage0: 0.2701  loss_cls_stage1: 0.3153  loss_box_reg_stage1: 0.7323  loss_cls_stage2: 0.3006  loss_box_reg_stage2: 0.9551  loss_mask: 0.08311  loss_rpn_cls: 0.01103  loss_rpn_loc: 0.05001  time: 0.6239  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:44:05] d2.utils.events INFO:  eta: 8:37:28  iter: 18239  total_loss: 2.924  loss_cls_stage0: 0.2681  loss_box_reg_stage0: 0.2573  loss_cls_stage1: 0.2878  loss_box_reg_stage1: 0.6813  loss_cls_stage2: 0.2806  loss_box_reg_stage2: 0.9491  loss_mask: 0.07638  loss_rpn_cls: 0.0136  loss_rpn_loc: 0.04635  time: 0.6239  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:44:17] d2.utils.events INFO:  eta: 8:37:35  iter: 18259  total_loss: 3.341  loss_cls_stage0: 0.3366  loss_box_reg_stage0: 0.2889  loss_cls_stage1: 0.3661  loss_box_reg_stage1: 0.7794  loss_cls_stage2: 0.3483  loss_box_reg_stage2: 1.075  loss_mask: 0.08759  loss_rpn_cls: 0.01405  loss_rpn_loc: 0.048  time: 0.6239  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:44:30] d2.utils.events INFO:  eta: 8:37:21  iter: 18279  total_loss: 2.969  loss_cls_stage0: 0.2506  loss_box_reg_stage0: 0.2528  loss_cls_stage1: 0.2914  loss_box_reg_stage1: 0.696  loss_cls_stage2: 0.2827  loss_box_reg_stage2: 0.9897  loss_mask: 0.07209  loss_rpn_cls: 0.01377  loss_rpn_loc: 0.05494  time: 0.6239  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:44:42] d2.utils.events INFO:  eta: 8:36:27  iter: 18299  total_loss: 2.814  loss_cls_stage0: 0.2651  loss_box_reg_stage0: 0.2537  loss_cls_stage1: 0.285  loss_box_reg_stage1: 0.6754  loss_cls_stage2: 0.2754  loss_box_reg_stage2: 0.9179  loss_mask: 0.0678  loss_rpn_cls: 0.01239  loss_rpn_loc: 0.05182  time: 0.6239  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:44:54] d2.utils.events INFO:  eta: 8:36:57  iter: 18319  total_loss: 2.995  loss_cls_stage0: 0.2819  loss_box_reg_stage0: 0.274  loss_cls_stage1: 0.322  loss_box_reg_stage1: 0.714  loss_cls_stage2: 0.2805  loss_box_reg_stage2: 0.9459  loss_mask: 0.06803  loss_rpn_cls: 0.01271  loss_rpn_loc: 0.04742  time: 0.6239  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:45:07] d2.utils.events INFO:  eta: 8:36:59  iter: 18339  total_loss: 2.925  loss_cls_stage0: 0.2677  loss_box_reg_stage0: 0.2644  loss_cls_stage1: 0.3017  loss_box_reg_stage1: 0.7079  loss_cls_stage2: 0.2849  loss_box_reg_stage2: 1.015  loss_mask: 0.07814  loss_rpn_cls: 0.01266  loss_rpn_loc: 0.04984  time: 0.6239  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:45:20] d2.utils.events INFO:  eta: 8:37:02  iter: 18359  total_loss: 2.624  loss_cls_stage0: 0.2267  loss_box_reg_stage0: 0.2408  loss_cls_stage1: 0.2442  loss_box_reg_stage1: 0.6184  loss_cls_stage2: 0.2449  loss_box_reg_stage2: 0.8503  loss_mask: 0.06882  loss_rpn_cls: 0.01275  loss_rpn_loc: 0.04436  time: 0.6239  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:45:32] d2.utils.events INFO:  eta: 8:36:45  iter: 18379  total_loss: 3.159  loss_cls_stage0: 0.3053  loss_box_reg_stage0: 0.309  loss_cls_stage1: 0.3479  loss_box_reg_stage1: 0.7402  loss_cls_stage2: 0.3136  loss_box_reg_stage2: 0.9705  loss_mask: 0.08127  loss_rpn_cls: 0.01209  loss_rpn_loc: 0.05249  time: 0.6239  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:45:44] d2.utils.events INFO:  eta: 8:36:52  iter: 18399  total_loss: 3.115  loss_cls_stage0: 0.289  loss_box_reg_stage0: 0.2926  loss_cls_stage1: 0.3139  loss_box_reg_stage1: 0.733  loss_cls_stage2: 0.2861  loss_box_reg_stage2: 1.025  loss_mask: 0.07471  loss_rpn_cls: 0.01495  loss_rpn_loc: 0.05637  time: 0.6239  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:45:57] d2.utils.events INFO:  eta: 8:36:43  iter: 18419  total_loss: 2.876  loss_cls_stage0: 0.2477  loss_box_reg_stage0: 0.2466  loss_cls_stage1: 0.2768  loss_box_reg_stage1: 0.6902  loss_cls_stage2: 0.2717  loss_box_reg_stage2: 0.8706  loss_mask: 0.07029  loss_rpn_cls: 0.01357  loss_rpn_loc: 0.04299  time: 0.6239  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 15:46:09] d2.utils.events INFO:  eta: 8:36:34  iter: 18439  total_loss: 3.304  loss_cls_stage0: 0.2998  loss_box_reg_stage0: 0.2971  loss_cls_stage1: 0.3199  loss_box_reg_stage1: 0.7887  loss_cls_stage2: 0.305  loss_box_reg_stage2: 1.09  loss_mask: 0.08534  loss_rpn_cls: 0.01074  loss_rpn_loc: 0.04255  time: 0.6239  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:46:22] d2.utils.events INFO:  eta: 8:36:46  iter: 18459  total_loss: 3.158  loss_cls_stage0: 0.2849  loss_box_reg_stage0: 0.2767  loss_cls_stage1: 0.3385  loss_box_reg_stage1: 0.7188  loss_cls_stage2: 0.3201  loss_box_reg_stage2: 0.9775  loss_mask: 0.09058  loss_rpn_cls: 0.01847  loss_rpn_loc: 0.05014  time: 0.6239  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 15:46:34] d2.utils.events INFO:  eta: 8:36:30  iter: 18479  total_loss: 3.146  loss_cls_stage0: 0.2772  loss_box_reg_stage0: 0.2807  loss_cls_stage1: 0.3079  loss_box_reg_stage1: 0.7815  loss_cls_stage2: 0.3156  loss_box_reg_stage2: 1.026  loss_mask: 0.08621  loss_rpn_cls: 0.01171  loss_rpn_loc: 0.05076  time: 0.6239  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:46:47] d2.utils.events INFO:  eta: 8:36:30  iter: 18499  total_loss: 2.918  loss_cls_stage0: 0.2678  loss_box_reg_stage0: 0.26  loss_cls_stage1: 0.2846  loss_box_reg_stage1: 0.6467  loss_cls_stage2: 0.2899  loss_box_reg_stage2: 0.885  loss_mask: 0.07163  loss_rpn_cls: 0.01528  loss_rpn_loc: 0.08033  time: 0.6239  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:46:59] d2.utils.events INFO:  eta: 8:36:14  iter: 18519  total_loss: 2.725  loss_cls_stage0: 0.2693  loss_box_reg_stage0: 0.2618  loss_cls_stage1: 0.2567  loss_box_reg_stage1: 0.633  loss_cls_stage2: 0.2619  loss_box_reg_stage2: 0.8347  loss_mask: 0.07886  loss_rpn_cls: 0.02485  loss_rpn_loc: 0.05386  time: 0.6239  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 15:47:12] d2.utils.events INFO:  eta: 8:35:55  iter: 18539  total_loss: 2.609  loss_cls_stage0: 0.2405  loss_box_reg_stage0: 0.2496  loss_cls_stage1: 0.2491  loss_box_reg_stage1: 0.629  loss_cls_stage2: 0.2377  loss_box_reg_stage2: 0.8059  loss_mask: 0.07553  loss_rpn_cls: 0.02738  loss_rpn_loc: 0.0496  time: 0.6239  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:47:24] d2.utils.events INFO:  eta: 8:35:25  iter: 18559  total_loss: 2.741  loss_cls_stage0: 0.2554  loss_box_reg_stage0: 0.2489  loss_cls_stage1: 0.2798  loss_box_reg_stage1: 0.6493  loss_cls_stage2: 0.2599  loss_box_reg_stage2: 0.9022  loss_mask: 0.06889  loss_rpn_cls: 0.02009  loss_rpn_loc: 0.0469  time: 0.6239  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:47:37] d2.utils.events INFO:  eta: 8:35:27  iter: 18579  total_loss: 2.553  loss_cls_stage0: 0.2591  loss_box_reg_stage0: 0.2445  loss_cls_stage1: 0.2501  loss_box_reg_stage1: 0.5814  loss_cls_stage2: 0.2303  loss_box_reg_stage2: 0.771  loss_mask: 0.06693  loss_rpn_cls: 0.006921  loss_rpn_loc: 0.03885  time: 0.6239  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:47:49] d2.utils.events INFO:  eta: 8:34:22  iter: 18599  total_loss: 2.841  loss_cls_stage0: 0.2644  loss_box_reg_stage0: 0.2807  loss_cls_stage1: 0.2912  loss_box_reg_stage1: 0.6595  loss_cls_stage2: 0.2855  loss_box_reg_stage2: 0.8738  loss_mask: 0.08091  loss_rpn_cls: 0.02018  loss_rpn_loc: 0.04849  time: 0.6238  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:48:01] d2.utils.events INFO:  eta: 8:33:50  iter: 18619  total_loss: 2.677  loss_cls_stage0: 0.2396  loss_box_reg_stage0: 0.2562  loss_cls_stage1: 0.2481  loss_box_reg_stage1: 0.6348  loss_cls_stage2: 0.2421  loss_box_reg_stage2: 0.9154  loss_mask: 0.07381  loss_rpn_cls: 0.0173  loss_rpn_loc: 0.05116  time: 0.6239  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:48:14] d2.utils.events INFO:  eta: 8:33:56  iter: 18639  total_loss: 2.993  loss_cls_stage0: 0.2471  loss_box_reg_stage0: 0.2623  loss_cls_stage1: 0.2866  loss_box_reg_stage1: 0.7141  loss_cls_stage2: 0.2836  loss_box_reg_stage2: 0.9809  loss_mask: 0.07974  loss_rpn_cls: 0.0174  loss_rpn_loc: 0.05153  time: 0.6239  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:48:27] d2.utils.events INFO:  eta: 8:33:45  iter: 18659  total_loss: 3.074  loss_cls_stage0: 0.2852  loss_box_reg_stage0: 0.2936  loss_cls_stage1: 0.3126  loss_box_reg_stage1: 0.7256  loss_cls_stage2: 0.2802  loss_box_reg_stage2: 0.9654  loss_mask: 0.07967  loss_rpn_cls: 0.0155  loss_rpn_loc: 0.04474  time: 0.6239  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:48:39] d2.utils.events INFO:  eta: 8:33:30  iter: 18679  total_loss: 3.016  loss_cls_stage0: 0.2715  loss_box_reg_stage0: 0.2711  loss_cls_stage1: 0.285  loss_box_reg_stage1: 0.7199  loss_cls_stage2: 0.2712  loss_box_reg_stage2: 0.9792  loss_mask: 0.08423  loss_rpn_cls: 0.01928  loss_rpn_loc: 0.04803  time: 0.6239  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:48:52] d2.utils.events INFO:  eta: 8:33:18  iter: 18699  total_loss: 2.977  loss_cls_stage0: 0.2726  loss_box_reg_stage0: 0.2739  loss_cls_stage1: 0.2941  loss_box_reg_stage1: 0.6947  loss_cls_stage2: 0.2825  loss_box_reg_stage2: 0.8928  loss_mask: 0.07632  loss_rpn_cls: 0.01329  loss_rpn_loc: 0.04742  time: 0.6239  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:49:04] d2.utils.events INFO:  eta: 8:33:03  iter: 18719  total_loss: 3.131  loss_cls_stage0: 0.2805  loss_box_reg_stage0: 0.2618  loss_cls_stage1: 0.3168  loss_box_reg_stage1: 0.7191  loss_cls_stage2: 0.3021  loss_box_reg_stage2: 0.9935  loss_mask: 0.07439  loss_rpn_cls: 0.01302  loss_rpn_loc: 0.04411  time: 0.6239  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 15:49:17] d2.utils.events INFO:  eta: 8:32:29  iter: 18739  total_loss: 3.242  loss_cls_stage0: 0.3292  loss_box_reg_stage0: 0.3031  loss_cls_stage1: 0.3471  loss_box_reg_stage1: 0.7452  loss_cls_stage2: 0.3029  loss_box_reg_stage2: 0.9723  loss_mask: 0.07692  loss_rpn_cls: 0.01925  loss_rpn_loc: 0.05593  time: 0.6239  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:49:29] d2.utils.events INFO:  eta: 8:32:02  iter: 18759  total_loss: 3.021  loss_cls_stage0: 0.2694  loss_box_reg_stage0: 0.2571  loss_cls_stage1: 0.2989  loss_box_reg_stage1: 0.7093  loss_cls_stage2: 0.3037  loss_box_reg_stage2: 0.9624  loss_mask: 0.08198  loss_rpn_cls: 0.01698  loss_rpn_loc: 0.05135  time: 0.6239  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:49:42] d2.utils.events INFO:  eta: 8:31:31  iter: 18779  total_loss: 2.971  loss_cls_stage0: 0.2755  loss_box_reg_stage0: 0.2807  loss_cls_stage1: 0.2933  loss_box_reg_stage1: 0.695  loss_cls_stage2: 0.2716  loss_box_reg_stage2: 0.9026  loss_mask: 0.09088  loss_rpn_cls: 0.01746  loss_rpn_loc: 0.05113  time: 0.6239  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:49:55] d2.utils.events INFO:  eta: 8:31:54  iter: 18799  total_loss: 2.966  loss_cls_stage0: 0.2832  loss_box_reg_stage0: 0.2673  loss_cls_stage1: 0.2957  loss_box_reg_stage1: 0.7035  loss_cls_stage2: 0.289  loss_box_reg_stage2: 0.9719  loss_mask: 0.08142  loss_rpn_cls: 0.01763  loss_rpn_loc: 0.05595  time: 0.6239  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:50:08] d2.utils.events INFO:  eta: 8:32:01  iter: 18819  total_loss: 3.215  loss_cls_stage0: 0.3158  loss_box_reg_stage0: 0.2813  loss_cls_stage1: 0.3387  loss_box_reg_stage1: 0.7191  loss_cls_stage2: 0.335  loss_box_reg_stage2: 0.9604  loss_mask: 0.09208  loss_rpn_cls: 0.01392  loss_rpn_loc: 0.04779  time: 0.6239  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:50:20] d2.utils.events INFO:  eta: 8:31:50  iter: 18839  total_loss: 2.995  loss_cls_stage0: 0.316  loss_box_reg_stage0: 0.2771  loss_cls_stage1: 0.3333  loss_box_reg_stage1: 0.6724  loss_cls_stage2: 0.3229  loss_box_reg_stage2: 0.8643  loss_mask: 0.07917  loss_rpn_cls: 0.02064  loss_rpn_loc: 0.0487  time: 0.6239  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 15:50:33] d2.utils.events INFO:  eta: 8:31:37  iter: 18859  total_loss: 2.927  loss_cls_stage0: 0.2803  loss_box_reg_stage0: 0.2769  loss_cls_stage1: 0.2871  loss_box_reg_stage1: 0.6539  loss_cls_stage2: 0.2919  loss_box_reg_stage2: 0.8588  loss_mask: 0.08108  loss_rpn_cls: 0.01979  loss_rpn_loc: 0.09122  time: 0.6239  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:50:44] d2.utils.events INFO:  eta: 8:30:08  iter: 18879  total_loss: 2.677  loss_cls_stage0: 0.2449  loss_box_reg_stage0: 0.2843  loss_cls_stage1: 0.2066  loss_box_reg_stage1: 0.643  loss_cls_stage2: 0.2137  loss_box_reg_stage2: 0.8297  loss_mask: 0.0765  loss_rpn_cls: 0.02875  loss_rpn_loc: 0.04225  time: 0.6239  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:50:56] d2.utils.events INFO:  eta: 8:29:46  iter: 18899  total_loss: 2.38  loss_cls_stage0: 0.1886  loss_box_reg_stage0: 0.2369  loss_cls_stage1: 0.1936  loss_box_reg_stage1: 0.613  loss_cls_stage2: 0.1945  loss_box_reg_stage2: 0.7824  loss_mask: 0.06824  loss_rpn_cls: 0.01861  loss_rpn_loc: 0.04317  time: 0.6238  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:51:09] d2.utils.events INFO:  eta: 8:29:55  iter: 18919  total_loss: 2.746  loss_cls_stage0: 0.262  loss_box_reg_stage0: 0.2491  loss_cls_stage1: 0.2659  loss_box_reg_stage1: 0.6362  loss_cls_stage2: 0.2654  loss_box_reg_stage2: 0.9276  loss_mask: 0.07815  loss_rpn_cls: 0.01208  loss_rpn_loc: 0.0446  time: 0.6238  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:51:21] d2.utils.events INFO:  eta: 8:30:13  iter: 18939  total_loss: 2.839  loss_cls_stage0: 0.2364  loss_box_reg_stage0: 0.257  loss_cls_stage1: 0.2576  loss_box_reg_stage1: 0.6896  loss_cls_stage2: 0.2585  loss_box_reg_stage2: 0.9895  loss_mask: 0.07722  loss_rpn_cls: 0.01342  loss_rpn_loc: 0.05178  time: 0.6239  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 15:51:34] d2.utils.events INFO:  eta: 8:30:01  iter: 18959  total_loss: 2.713  loss_cls_stage0: 0.2444  loss_box_reg_stage0: 0.2665  loss_cls_stage1: 0.2685  loss_box_reg_stage1: 0.6454  loss_cls_stage2: 0.2786  loss_box_reg_stage2: 0.834  loss_mask: 0.07906  loss_rpn_cls: 0.008269  loss_rpn_loc: 0.03869  time: 0.6239  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:51:47] d2.utils.events INFO:  eta: 8:30:12  iter: 18979  total_loss: 3.067  loss_cls_stage0: 0.2459  loss_box_reg_stage0: 0.2469  loss_cls_stage1: 0.3006  loss_box_reg_stage1: 0.7043  loss_cls_stage2: 0.2902  loss_box_reg_stage2: 0.9482  loss_mask: 0.08126  loss_rpn_cls: 0.01141  loss_rpn_loc: 0.04282  time: 0.6239  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:51:59] d2.utils.events INFO:  eta: 8:29:59  iter: 18999  total_loss: 2.965  loss_cls_stage0: 0.284  loss_box_reg_stage0: 0.2715  loss_cls_stage1: 0.305  loss_box_reg_stage1: 0.6736  loss_cls_stage2: 0.2744  loss_box_reg_stage2: 0.923  loss_mask: 0.08566  loss_rpn_cls: 0.02286  loss_rpn_loc: 0.05388  time: 0.6239  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:52:12] d2.utils.events INFO:  eta: 8:29:33  iter: 19019  total_loss: 2.827  loss_cls_stage0: 0.2581  loss_box_reg_stage0: 0.2823  loss_cls_stage1: 0.282  loss_box_reg_stage1: 0.6736  loss_cls_stage2: 0.2834  loss_box_reg_stage2: 0.883  loss_mask: 0.08841  loss_rpn_cls: 0.01889  loss_rpn_loc: 0.05091  time: 0.6239  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:52:24] d2.utils.events INFO:  eta: 8:28:54  iter: 19039  total_loss: 2.769  loss_cls_stage0: 0.2379  loss_box_reg_stage0: 0.2639  loss_cls_stage1: 0.2271  loss_box_reg_stage1: 0.6946  loss_cls_stage2: 0.2282  loss_box_reg_stage2: 0.9668  loss_mask: 0.07689  loss_rpn_cls: 0.01452  loss_rpn_loc: 0.04315  time: 0.6239  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:52:36] d2.utils.events INFO:  eta: 8:28:49  iter: 19059  total_loss: 2.788  loss_cls_stage0: 0.2622  loss_box_reg_stage0: 0.2851  loss_cls_stage1: 0.2635  loss_box_reg_stage1: 0.6318  loss_cls_stage2: 0.2589  loss_box_reg_stage2: 0.8988  loss_mask: 0.0843  loss_rpn_cls: 0.01735  loss_rpn_loc: 0.04565  time: 0.6238  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:52:49] d2.utils.events INFO:  eta: 8:28:05  iter: 19079  total_loss: 2.886  loss_cls_stage0: 0.2655  loss_box_reg_stage0: 0.2574  loss_cls_stage1: 0.256  loss_box_reg_stage1: 0.7001  loss_cls_stage2: 0.2535  loss_box_reg_stage2: 0.9218  loss_mask: 0.07848  loss_rpn_cls: 0.01779  loss_rpn_loc: 0.04701  time: 0.6238  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:53:01] d2.utils.events INFO:  eta: 8:27:34  iter: 19099  total_loss: 2.669  loss_cls_stage0: 0.258  loss_box_reg_stage0: 0.2464  loss_cls_stage1: 0.2666  loss_box_reg_stage1: 0.6185  loss_cls_stage2: 0.2602  loss_box_reg_stage2: 0.8691  loss_mask: 0.07051  loss_rpn_cls: 0.01687  loss_rpn_loc: 0.05389  time: 0.6238  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 15:53:14] d2.utils.events INFO:  eta: 8:27:22  iter: 19119  total_loss: 2.775  loss_cls_stage0: 0.2478  loss_box_reg_stage0: 0.235  loss_cls_stage1: 0.2905  loss_box_reg_stage1: 0.6414  loss_cls_stage2: 0.2717  loss_box_reg_stage2: 0.9104  loss_mask: 0.07664  loss_rpn_cls: 0.01483  loss_rpn_loc: 0.05043  time: 0.6239  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:53:26] d2.utils.events INFO:  eta: 8:27:05  iter: 19139  total_loss: 2.877  loss_cls_stage0: 0.2826  loss_box_reg_stage0: 0.2584  loss_cls_stage1: 0.2756  loss_box_reg_stage1: 0.6542  loss_cls_stage2: 0.2703  loss_box_reg_stage2: 0.9395  loss_mask: 0.07867  loss_rpn_cls: 0.0147  loss_rpn_loc: 0.04471  time: 0.6239  data_time: 0.0054  lr: 0.00016  max_mem: 19679M
[07/29 15:53:39] d2.utils.events INFO:  eta: 8:26:56  iter: 19159  total_loss: 2.907  loss_cls_stage0: 0.274  loss_box_reg_stage0: 0.2577  loss_cls_stage1: 0.3145  loss_box_reg_stage1: 0.6481  loss_cls_stage2: 0.2876  loss_box_reg_stage2: 0.8423  loss_mask: 0.07415  loss_rpn_cls: 0.02549  loss_rpn_loc: 0.06147  time: 0.6239  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:53:51] d2.utils.events INFO:  eta: 8:26:34  iter: 19179  total_loss: 2.831  loss_cls_stage0: 0.2587  loss_box_reg_stage0: 0.257  loss_cls_stage1: 0.2939  loss_box_reg_stage1: 0.6376  loss_cls_stage2: 0.2735  loss_box_reg_stage2: 0.9235  loss_mask: 0.0705  loss_rpn_cls: 0.01386  loss_rpn_loc: 0.05311  time: 0.6239  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:54:04] d2.utils.events INFO:  eta: 8:26:27  iter: 19199  total_loss: 2.985  loss_cls_stage0: 0.2778  loss_box_reg_stage0: 0.2775  loss_cls_stage1: 0.3049  loss_box_reg_stage1: 0.709  loss_cls_stage2: 0.3001  loss_box_reg_stage2: 0.903  loss_mask: 0.07603  loss_rpn_cls: 0.01858  loss_rpn_loc: 0.05193  time: 0.6239  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:54:17] d2.utils.events INFO:  eta: 8:26:17  iter: 19219  total_loss: 2.992  loss_cls_stage0: 0.3006  loss_box_reg_stage0: 0.2857  loss_cls_stage1: 0.3251  loss_box_reg_stage1: 0.702  loss_cls_stage2: 0.3017  loss_box_reg_stage2: 0.9576  loss_mask: 0.08645  loss_rpn_cls: 0.01614  loss_rpn_loc: 0.04067  time: 0.6239  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 15:54:30] d2.utils.events INFO:  eta: 8:26:02  iter: 19239  total_loss: 3.071  loss_cls_stage0: 0.2818  loss_box_reg_stage0: 0.2831  loss_cls_stage1: 0.3107  loss_box_reg_stage1: 0.798  loss_cls_stage2: 0.2967  loss_box_reg_stage2: 1.041  loss_mask: 0.07426  loss_rpn_cls: 0.01275  loss_rpn_loc: 0.04648  time: 0.6239  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 15:54:42] d2.utils.events INFO:  eta: 8:25:55  iter: 19259  total_loss: 2.903  loss_cls_stage0: 0.2438  loss_box_reg_stage0: 0.2463  loss_cls_stage1: 0.293  loss_box_reg_stage1: 0.6865  loss_cls_stage2: 0.3004  loss_box_reg_stage2: 0.9282  loss_mask: 0.07637  loss_rpn_cls: 0.01496  loss_rpn_loc: 0.07094  time: 0.6239  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:54:54] d2.utils.events INFO:  eta: 8:25:31  iter: 19279  total_loss: 2.663  loss_cls_stage0: 0.2595  loss_box_reg_stage0: 0.2853  loss_cls_stage1: 0.2484  loss_box_reg_stage1: 0.6457  loss_cls_stage2: 0.232  loss_box_reg_stage2: 0.8249  loss_mask: 0.08953  loss_rpn_cls: 0.02502  loss_rpn_loc: 0.05138  time: 0.6239  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:55:07] d2.utils.events INFO:  eta: 8:25:26  iter: 19299  total_loss: 2.734  loss_cls_stage0: 0.2593  loss_box_reg_stage0: 0.2576  loss_cls_stage1: 0.2679  loss_box_reg_stage1: 0.6439  loss_cls_stage2: 0.2617  loss_box_reg_stage2: 0.8771  loss_mask: 0.07364  loss_rpn_cls: 0.01697  loss_rpn_loc: 0.04422  time: 0.6239  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:55:20] d2.utils.events INFO:  eta: 8:25:17  iter: 19319  total_loss: 3.184  loss_cls_stage0: 0.2785  loss_box_reg_stage0: 0.2859  loss_cls_stage1: 0.3242  loss_box_reg_stage1: 0.7592  loss_cls_stage2: 0.3024  loss_box_reg_stage2: 1.045  loss_mask: 0.08543  loss_rpn_cls: 0.01133  loss_rpn_loc: 0.04225  time: 0.6239  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:55:32] d2.utils.events INFO:  eta: 8:24:53  iter: 19339  total_loss: 2.961  loss_cls_stage0: 0.2859  loss_box_reg_stage0: 0.2818  loss_cls_stage1: 0.284  loss_box_reg_stage1: 0.703  loss_cls_stage2: 0.2684  loss_box_reg_stage2: 0.9529  loss_mask: 0.07981  loss_rpn_cls: 0.01816  loss_rpn_loc: 0.04147  time: 0.6239  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:55:44] d2.utils.events INFO:  eta: 8:24:46  iter: 19359  total_loss: 2.726  loss_cls_stage0: 0.2787  loss_box_reg_stage0: 0.2481  loss_cls_stage1: 0.2829  loss_box_reg_stage1: 0.6177  loss_cls_stage2: 0.2879  loss_box_reg_stage2: 0.8404  loss_mask: 0.07427  loss_rpn_cls: 0.02412  loss_rpn_loc: 0.0619  time: 0.6239  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 15:55:57] d2.utils.events INFO:  eta: 8:24:42  iter: 19379  total_loss: 2.789  loss_cls_stage0: 0.2358  loss_box_reg_stage0: 0.2558  loss_cls_stage1: 0.2692  loss_box_reg_stage1: 0.6882  loss_cls_stage2: 0.2628  loss_box_reg_stage2: 0.9852  loss_mask: 0.07605  loss_rpn_cls: 0.01542  loss_rpn_loc: 0.04478  time: 0.6239  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 15:56:10] d2.utils.events INFO:  eta: 8:24:30  iter: 19399  total_loss: 2.887  loss_cls_stage0: 0.2738  loss_box_reg_stage0: 0.2765  loss_cls_stage1: 0.3075  loss_box_reg_stage1: 0.6908  loss_cls_stage2: 0.2726  loss_box_reg_stage2: 0.9836  loss_mask: 0.08005  loss_rpn_cls: 0.01631  loss_rpn_loc: 0.04711  time: 0.6239  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:56:23] d2.utils.events INFO:  eta: 8:25:11  iter: 19419  total_loss: 2.726  loss_cls_stage0: 0.2602  loss_box_reg_stage0: 0.2638  loss_cls_stage1: 0.2961  loss_box_reg_stage1: 0.6964  loss_cls_stage2: 0.2719  loss_box_reg_stage2: 0.9091  loss_mask: 0.07449  loss_rpn_cls: 0.01903  loss_rpn_loc: 0.06143  time: 0.6240  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:56:36] d2.utils.events INFO:  eta: 8:24:59  iter: 19439  total_loss: 3.174  loss_cls_stage0: 0.2948  loss_box_reg_stage0: 0.2995  loss_cls_stage1: 0.3222  loss_box_reg_stage1: 0.7983  loss_cls_stage2: 0.2945  loss_box_reg_stage2: 1.071  loss_mask: 0.06981  loss_rpn_cls: 0.01193  loss_rpn_loc: 0.04031  time: 0.6240  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:56:48] d2.utils.events INFO:  eta: 8:25:01  iter: 19459  total_loss: 2.886  loss_cls_stage0: 0.2477  loss_box_reg_stage0: 0.2597  loss_cls_stage1: 0.2905  loss_box_reg_stage1: 0.7254  loss_cls_stage2: 0.2819  loss_box_reg_stage2: 0.9328  loss_mask: 0.07899  loss_rpn_cls: 0.01587  loss_rpn_loc: 0.06678  time: 0.6240  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:57:01] d2.utils.events INFO:  eta: 8:25:07  iter: 19479  total_loss: 3.15  loss_cls_stage0: 0.3209  loss_box_reg_stage0: 0.2942  loss_cls_stage1: 0.34  loss_box_reg_stage1: 0.7487  loss_cls_stage2: 0.3268  loss_box_reg_stage2: 0.9963  loss_mask: 0.08025  loss_rpn_cls: 0.01743  loss_rpn_loc: 0.04643  time: 0.6240  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 15:57:13] d2.utils.events INFO:  eta: 8:24:38  iter: 19499  total_loss: 2.87  loss_cls_stage0: 0.249  loss_box_reg_stage0: 0.2485  loss_cls_stage1: 0.3012  loss_box_reg_stage1: 0.7199  loss_cls_stage2: 0.2793  loss_box_reg_stage2: 0.9378  loss_mask: 0.07204  loss_rpn_cls: 0.0164  loss_rpn_loc: 0.05207  time: 0.6240  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:57:26] d2.utils.events INFO:  eta: 8:25:03  iter: 19519  total_loss: 3.264  loss_cls_stage0: 0.2821  loss_box_reg_stage0: 0.2896  loss_cls_stage1: 0.3156  loss_box_reg_stage1: 0.7824  loss_cls_stage2: 0.3058  loss_box_reg_stage2: 0.9665  loss_mask: 0.08253  loss_rpn_cls: 0.01627  loss_rpn_loc: 0.04306  time: 0.6240  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:57:39] d2.utils.events INFO:  eta: 8:24:51  iter: 19539  total_loss: 3.03  loss_cls_stage0: 0.2817  loss_box_reg_stage0: 0.2646  loss_cls_stage1: 0.3216  loss_box_reg_stage1: 0.7079  loss_cls_stage2: 0.3132  loss_box_reg_stage2: 0.9388  loss_mask: 0.07849  loss_rpn_cls: 0.01072  loss_rpn_loc: 0.03583  time: 0.6240  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 15:57:52] d2.utils.events INFO:  eta: 8:25:07  iter: 19559  total_loss: 3.035  loss_cls_stage0: 0.2647  loss_box_reg_stage0: 0.2772  loss_cls_stage1: 0.3014  loss_box_reg_stage1: 0.7568  loss_cls_stage2: 0.2906  loss_box_reg_stage2: 1.062  loss_mask: 0.07789  loss_rpn_cls: 0.01073  loss_rpn_loc: 0.04157  time: 0.6241  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:58:04] d2.utils.events INFO:  eta: 8:24:47  iter: 19579  total_loss: 3.448  loss_cls_stage0: 0.3277  loss_box_reg_stage0: 0.3089  loss_cls_stage1: 0.3775  loss_box_reg_stage1: 0.8251  loss_cls_stage2: 0.3521  loss_box_reg_stage2: 1.097  loss_mask: 0.08289  loss_rpn_cls: 0.01454  loss_rpn_loc: 0.04892  time: 0.6240  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 15:58:17] d2.utils.events INFO:  eta: 8:24:46  iter: 19599  total_loss: 3.182  loss_cls_stage0: 0.2649  loss_box_reg_stage0: 0.2807  loss_cls_stage1: 0.3149  loss_box_reg_stage1: 0.7979  loss_cls_stage2: 0.2793  loss_box_reg_stage2: 1.081  loss_mask: 0.07353  loss_rpn_cls: 0.01432  loss_rpn_loc: 0.0424  time: 0.6241  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 15:58:29] d2.utils.events INFO:  eta: 8:24:38  iter: 19619  total_loss: 3.169  loss_cls_stage0: 0.2738  loss_box_reg_stage0: 0.2722  loss_cls_stage1: 0.3071  loss_box_reg_stage1: 0.7466  loss_cls_stage2: 0.3089  loss_box_reg_stage2: 0.9933  loss_mask: 0.07171  loss_rpn_cls: 0.0157  loss_rpn_loc: 0.04411  time: 0.6241  data_time: 0.0054  lr: 0.00016  max_mem: 19679M
[07/29 15:58:42] d2.utils.events INFO:  eta: 8:24:37  iter: 19639  total_loss: 2.843  loss_cls_stage0: 0.2439  loss_box_reg_stage0: 0.2468  loss_cls_stage1: 0.279  loss_box_reg_stage1: 0.669  loss_cls_stage2: 0.2934  loss_box_reg_stage2: 0.912  loss_mask: 0.07688  loss_rpn_cls: 0.01692  loss_rpn_loc: 0.03883  time: 0.6241  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 15:58:55] d2.utils.events INFO:  eta: 8:24:28  iter: 19659  total_loss: 2.763  loss_cls_stage0: 0.2565  loss_box_reg_stage0: 0.274  loss_cls_stage1: 0.2674  loss_box_reg_stage1: 0.6586  loss_cls_stage2: 0.2383  loss_box_reg_stage2: 0.9152  loss_mask: 0.08531  loss_rpn_cls: 0.01307  loss_rpn_loc: 0.03818  time: 0.6241  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:59:07] d2.utils.events INFO:  eta: 8:24:20  iter: 19679  total_loss: 2.881  loss_cls_stage0: 0.2743  loss_box_reg_stage0: 0.2658  loss_cls_stage1: 0.2926  loss_box_reg_stage1: 0.6904  loss_cls_stage2: 0.2748  loss_box_reg_stage2: 0.8704  loss_mask: 0.0776  loss_rpn_cls: 0.01633  loss_rpn_loc: 0.0588  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 15:59:20] d2.utils.events INFO:  eta: 8:23:59  iter: 19699  total_loss: 2.832  loss_cls_stage0: 0.264  loss_box_reg_stage0: 0.2539  loss_cls_stage1: 0.2973  loss_box_reg_stage1: 0.6707  loss_cls_stage2: 0.2787  loss_box_reg_stage2: 0.8635  loss_mask: 0.07687  loss_rpn_cls: 0.01336  loss_rpn_loc: 0.05448  time: 0.6241  data_time: 0.0054  lr: 0.00016  max_mem: 19679M
[07/29 15:59:32] d2.utils.events INFO:  eta: 8:23:54  iter: 19719  total_loss: 2.869  loss_cls_stage0: 0.2571  loss_box_reg_stage0: 0.2539  loss_cls_stage1: 0.2959  loss_box_reg_stage1: 0.6876  loss_cls_stage2: 0.3083  loss_box_reg_stage2: 0.902  loss_mask: 0.07855  loss_rpn_cls: 0.01387  loss_rpn_loc: 0.0404  time: 0.6241  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 15:59:45] d2.utils.events INFO:  eta: 8:24:07  iter: 19739  total_loss: 2.677  loss_cls_stage0: 0.2393  loss_box_reg_stage0: 0.2331  loss_cls_stage1: 0.2867  loss_box_reg_stage1: 0.6114  loss_cls_stage2: 0.272  loss_box_reg_stage2: 0.8975  loss_mask: 0.08274  loss_rpn_cls: 0.01147  loss_rpn_loc: 0.03705  time: 0.6241  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 15:59:57] d2.utils.events INFO:  eta: 8:23:57  iter: 19759  total_loss: 2.983  loss_cls_stage0: 0.2884  loss_box_reg_stage0: 0.2808  loss_cls_stage1: 0.3136  loss_box_reg_stage1: 0.7035  loss_cls_stage2: 0.2977  loss_box_reg_stage2: 0.9267  loss_mask: 0.07829  loss_rpn_cls: 0.02057  loss_rpn_loc: 0.04053  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:00:10] d2.utils.events INFO:  eta: 8:23:45  iter: 19779  total_loss: 2.739  loss_cls_stage0: 0.236  loss_box_reg_stage0: 0.2688  loss_cls_stage1: 0.2373  loss_box_reg_stage1: 0.6378  loss_cls_stage2: 0.2247  loss_box_reg_stage2: 0.8942  loss_mask: 0.07888  loss_rpn_cls: 0.01791  loss_rpn_loc: 0.04568  time: 0.6241  data_time: 0.0055  lr: 0.00016  max_mem: 19679M
[07/29 16:00:22] d2.utils.events INFO:  eta: 8:23:34  iter: 19799  total_loss: 2.562  loss_cls_stage0: 0.2208  loss_box_reg_stage0: 0.2538  loss_cls_stage1: 0.2342  loss_box_reg_stage1: 0.6137  loss_cls_stage2: 0.2389  loss_box_reg_stage2: 0.7655  loss_mask: 0.07028  loss_rpn_cls: 0.01872  loss_rpn_loc: 0.04514  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:00:35] d2.utils.events INFO:  eta: 8:23:21  iter: 19819  total_loss: 2.558  loss_cls_stage0: 0.2231  loss_box_reg_stage0: 0.2359  loss_cls_stage1: 0.2137  loss_box_reg_stage1: 0.6253  loss_cls_stage2: 0.2093  loss_box_reg_stage2: 0.9095  loss_mask: 0.07332  loss_rpn_cls: 0.01523  loss_rpn_loc: 0.04032  time: 0.6241  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:00:48] d2.utils.events INFO:  eta: 8:23:08  iter: 19839  total_loss: 2.956  loss_cls_stage0: 0.2801  loss_box_reg_stage0: 0.2771  loss_cls_stage1: 0.2822  loss_box_reg_stage1: 0.7479  loss_cls_stage2: 0.2678  loss_box_reg_stage2: 1.007  loss_mask: 0.08742  loss_rpn_cls: 0.01165  loss_rpn_loc: 0.04935  time: 0.6241  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 16:01:00] d2.utils.events INFO:  eta: 8:22:50  iter: 19859  total_loss: 2.919  loss_cls_stage0: 0.2698  loss_box_reg_stage0: 0.2645  loss_cls_stage1: 0.3011  loss_box_reg_stage1: 0.6868  loss_cls_stage2: 0.2783  loss_box_reg_stage2: 0.9314  loss_mask: 0.0744  loss_rpn_cls: 0.01794  loss_rpn_loc: 0.04765  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:01:13] d2.utils.events INFO:  eta: 8:23:00  iter: 19879  total_loss: 2.824  loss_cls_stage0: 0.267  loss_box_reg_stage0: 0.2414  loss_cls_stage1: 0.2838  loss_box_reg_stage1: 0.6747  loss_cls_stage2: 0.2723  loss_box_reg_stage2: 0.8946  loss_mask: 0.07523  loss_rpn_cls: 0.01547  loss_rpn_loc: 0.03883  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:01:25] d2.utils.events INFO:  eta: 8:23:16  iter: 19899  total_loss: 2.931  loss_cls_stage0: 0.2732  loss_box_reg_stage0: 0.2775  loss_cls_stage1: 0.2971  loss_box_reg_stage1: 0.7222  loss_cls_stage2: 0.2748  loss_box_reg_stage2: 0.9779  loss_mask: 0.07604  loss_rpn_cls: 0.01088  loss_rpn_loc: 0.04481  time: 0.6241  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:01:37] d2.utils.events INFO:  eta: 8:23:03  iter: 19919  total_loss: 3.06  loss_cls_stage0: 0.299  loss_box_reg_stage0: 0.2734  loss_cls_stage1: 0.3264  loss_box_reg_stage1: 0.6885  loss_cls_stage2: 0.3047  loss_box_reg_stage2: 0.9156  loss_mask: 0.08133  loss_rpn_cls: 0.01086  loss_rpn_loc: 0.04284  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:01:50] d2.utils.events INFO:  eta: 8:22:50  iter: 19939  total_loss: 2.662  loss_cls_stage0: 0.2567  loss_box_reg_stage0: 0.2479  loss_cls_stage1: 0.2608  loss_box_reg_stage1: 0.5719  loss_cls_stage2: 0.2403  loss_box_reg_stage2: 0.8213  loss_mask: 0.07616  loss_rpn_cls: 0.01857  loss_rpn_loc: 0.04841  time: 0.6241  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 16:02:02] d2.utils.events INFO:  eta: 8:22:34  iter: 19959  total_loss: 2.567  loss_cls_stage0: 0.2498  loss_box_reg_stage0: 0.2663  loss_cls_stage1: 0.2489  loss_box_reg_stage1: 0.6376  loss_cls_stage2: 0.2127  loss_box_reg_stage2: 0.8196  loss_mask: 0.08295  loss_rpn_cls: 0.02223  loss_rpn_loc: 0.05802  time: 0.6241  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 16:02:15] d2.utils.events INFO:  eta: 8:21:47  iter: 19979  total_loss: 2.608  loss_cls_stage0: 0.2518  loss_box_reg_stage0: 0.261  loss_cls_stage1: 0.2695  loss_box_reg_stage1: 0.6633  loss_cls_stage2: 0.2529  loss_box_reg_stage2: 0.8132  loss_mask: 0.08002  loss_rpn_cls: 0.01423  loss_rpn_loc: 0.04414  time: 0.6241  data_time: 0.0054  lr: 0.00016  max_mem: 19679M
[07/29 16:02:27] fvcore.common.checkpoint INFO: Saving checkpoint to ./output/model_0019999.pth
[07/29 16:02:29] d2.data.datasets.coco INFO: Loading datasets/tless/tless_annotations_test.json takes 1.39 seconds.
[07/29 16:02:29] d2.data.datasets.coco INFO: Loaded 10080 images in COCO format from datasets/tless/tless_annotations_test.json
[07/29 16:02:30] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[07/29 16:02:30] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[07/29 16:02:30] d2.data.common INFO: Serializing 10080 elements to byte tensors and concatenating them all ...
[07/29 16:02:30] d2.data.common INFO: Serialized dataset takes 50.77 MiB
[07/29 16:02:31] d2.evaluation.evaluator INFO: Start inference on 10080 batches
[07/29 16:02:33] d2.evaluation.evaluator INFO: Inference done 11/10080. Dataloading: 0.0006 s/iter. Inference: 0.1024 s/iter. Eval: 0.0029 s/iter. Total: 0.1059 s/iter. ETA=0:17:46
[07/29 16:02:38] d2.evaluation.evaluator INFO: Inference done 59/10080. Dataloading: 0.0009 s/iter. Inference: 0.1027 s/iter. Eval: 0.0027 s/iter. Total: 0.1063 s/iter. ETA=0:17:45
[07/29 16:02:43] d2.evaluation.evaluator INFO: Inference done 107/10080. Dataloading: 0.0009 s/iter. Inference: 0.1023 s/iter. Eval: 0.0026 s/iter. Total: 0.1059 s/iter. ETA=0:17:36
[07/29 16:02:48] d2.evaluation.evaluator INFO: Inference done 155/10080. Dataloading: 0.0009 s/iter. Inference: 0.1022 s/iter. Eval: 0.0026 s/iter. Total: 0.1057 s/iter. ETA=0:17:28
[07/29 16:02:53] d2.evaluation.evaluator INFO: Inference done 203/10080. Dataloading: 0.0009 s/iter. Inference: 0.1021 s/iter. Eval: 0.0025 s/iter. Total: 0.1056 s/iter. ETA=0:17:22
[07/29 16:02:58] d2.evaluation.evaluator INFO: Inference done 251/10080. Dataloading: 0.0009 s/iter. Inference: 0.1021 s/iter. Eval: 0.0024 s/iter. Total: 0.1054 s/iter. ETA=0:17:16
[07/29 16:03:03] d2.evaluation.evaluator INFO: Inference done 299/10080. Dataloading: 0.0009 s/iter. Inference: 0.1020 s/iter. Eval: 0.0023 s/iter. Total: 0.1053 s/iter. ETA=0:17:10
[07/29 16:03:08] d2.evaluation.evaluator INFO: Inference done 347/10080. Dataloading: 0.0009 s/iter. Inference: 0.1020 s/iter. Eval: 0.0023 s/iter. Total: 0.1053 s/iter. ETA=0:17:05
[07/29 16:03:13] d2.evaluation.evaluator INFO: Inference done 395/10080. Dataloading: 0.0009 s/iter. Inference: 0.1020 s/iter. Eval: 0.0024 s/iter. Total: 0.1054 s/iter. ETA=0:17:00
[07/29 16:03:18] d2.evaluation.evaluator INFO: Inference done 443/10080. Dataloading: 0.0009 s/iter. Inference: 0.1020 s/iter. Eval: 0.0025 s/iter. Total: 0.1054 s/iter. ETA=0:16:55
[07/29 16:03:23] d2.evaluation.evaluator INFO: Inference done 491/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0025 s/iter. Total: 0.1054 s/iter. ETA=0:16:50
[07/29 16:03:29] d2.evaluation.evaluator INFO: Inference done 540/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0024 s/iter. Total: 0.1053 s/iter. ETA=0:16:44
[07/29 16:03:34] d2.evaluation.evaluator INFO: Inference done 589/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0023 s/iter. Total: 0.1051 s/iter. ETA=0:16:37
[07/29 16:03:39] d2.evaluation.evaluator INFO: Inference done 638/10080. Dataloading: 0.0009 s/iter. Inference: 0.1017 s/iter. Eval: 0.0022 s/iter. Total: 0.1050 s/iter. ETA=0:16:31
[07/29 16:03:44] d2.evaluation.evaluator INFO: Inference done 687/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0022 s/iter. Total: 0.1049 s/iter. ETA=0:16:25
[07/29 16:03:49] d2.evaluation.evaluator INFO: Inference done 736/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0022 s/iter. Total: 0.1048 s/iter. ETA=0:16:19
[07/29 16:03:54] d2.evaluation.evaluator INFO: Inference done 784/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0021 s/iter. Total: 0.1048 s/iter. ETA=0:16:14
[07/29 16:03:59] d2.evaluation.evaluator INFO: Inference done 832/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0021 s/iter. Total: 0.1048 s/iter. ETA=0:16:09
[07/29 16:04:04] d2.evaluation.evaluator INFO: Inference done 880/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0021 s/iter. Total: 0.1048 s/iter. ETA=0:16:03
[07/29 16:04:09] d2.evaluation.evaluator INFO: Inference done 928/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0021 s/iter. Total: 0.1048 s/iter. ETA=0:15:58
[07/29 16:04:14] d2.evaluation.evaluator INFO: Inference done 976/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0022 s/iter. Total: 0.1048 s/iter. ETA=0:15:54
[07/29 16:04:19] d2.evaluation.evaluator INFO: Inference done 1024/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0022 s/iter. Total: 0.1048 s/iter. ETA=0:15:49
[07/29 16:04:24] d2.evaluation.evaluator INFO: Inference done 1072/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0022 s/iter. Total: 0.1048 s/iter. ETA=0:15:43
[07/29 16:04:29] d2.evaluation.evaluator INFO: Inference done 1120/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0022 s/iter. Total: 0.1048 s/iter. ETA=0:15:39
[07/29 16:04:34] d2.evaluation.evaluator INFO: Inference done 1168/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0022 s/iter. Total: 0.1048 s/iter. ETA=0:15:34
[07/29 16:04:39] d2.evaluation.evaluator INFO: Inference done 1216/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0022 s/iter. Total: 0.1048 s/iter. ETA=0:15:29
[07/29 16:04:44] d2.evaluation.evaluator INFO: Inference done 1264/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0022 s/iter. Total: 0.1048 s/iter. ETA=0:15:24
[07/29 16:04:49] d2.evaluation.evaluator INFO: Inference done 1313/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0022 s/iter. Total: 0.1048 s/iter. ETA=0:15:18
[07/29 16:04:54] d2.evaluation.evaluator INFO: Inference done 1362/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0022 s/iter. Total: 0.1047 s/iter. ETA=0:15:12
[07/29 16:04:59] d2.evaluation.evaluator INFO: Inference done 1410/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0022 s/iter. Total: 0.1047 s/iter. ETA=0:15:07
[07/29 16:05:04] d2.evaluation.evaluator INFO: Inference done 1458/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0023 s/iter. Total: 0.1048 s/iter. ETA=0:15:03
[07/29 16:05:10] d2.evaluation.evaluator INFO: Inference done 1506/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0023 s/iter. Total: 0.1048 s/iter. ETA=0:14:58
[07/29 16:05:15] d2.evaluation.evaluator INFO: Inference done 1554/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0023 s/iter. Total: 0.1048 s/iter. ETA=0:14:53
[07/29 16:05:20] d2.evaluation.evaluator INFO: Inference done 1602/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0023 s/iter. Total: 0.1048 s/iter. ETA=0:14:48
[07/29 16:05:25] d2.evaluation.evaluator INFO: Inference done 1650/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0023 s/iter. Total: 0.1048 s/iter. ETA=0:14:43
[07/29 16:05:30] d2.evaluation.evaluator INFO: Inference done 1698/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0023 s/iter. Total: 0.1048 s/iter. ETA=0:14:38
[07/29 16:05:35] d2.evaluation.evaluator INFO: Inference done 1746/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0023 s/iter. Total: 0.1048 s/iter. ETA=0:14:33
[07/29 16:05:40] d2.evaluation.evaluator INFO: Inference done 1794/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0023 s/iter. Total: 0.1048 s/iter. ETA=0:14:28
[07/29 16:05:45] d2.evaluation.evaluator INFO: Inference done 1842/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0023 s/iter. Total: 0.1048 s/iter. ETA=0:14:23
[07/29 16:05:50] d2.evaluation.evaluator INFO: Inference done 1890/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0023 s/iter. Total: 0.1048 s/iter. ETA=0:14:18
[07/29 16:05:55] d2.evaluation.evaluator INFO: Inference done 1938/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0024 s/iter. Total: 0.1049 s/iter. ETA=0:14:13
[07/29 16:06:00] d2.evaluation.evaluator INFO: Inference done 1986/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0024 s/iter. Total: 0.1049 s/iter. ETA=0:14:08
[07/29 16:06:05] d2.evaluation.evaluator INFO: Inference done 2034/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0024 s/iter. Total: 0.1049 s/iter. ETA=0:14:04
[07/29 16:06:10] d2.evaluation.evaluator INFO: Inference done 2081/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0024 s/iter. Total: 0.1050 s/iter. ETA=0:13:59
[07/29 16:06:15] d2.evaluation.evaluator INFO: Inference done 2129/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0024 s/iter. Total: 0.1050 s/iter. ETA=0:13:54
[07/29 16:06:20] d2.evaluation.evaluator INFO: Inference done 2176/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0025 s/iter. Total: 0.1050 s/iter. ETA=0:13:50
[07/29 16:06:25] d2.evaluation.evaluator INFO: Inference done 2224/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0025 s/iter. Total: 0.1050 s/iter. ETA=0:13:45
[07/29 16:06:30] d2.evaluation.evaluator INFO: Inference done 2272/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0025 s/iter. Total: 0.1051 s/iter. ETA=0:13:40
[07/29 16:06:35] d2.evaluation.evaluator INFO: Inference done 2319/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0025 s/iter. Total: 0.1051 s/iter. ETA=0:13:35
[07/29 16:06:40] d2.evaluation.evaluator INFO: Inference done 2366/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0025 s/iter. Total: 0.1051 s/iter. ETA=0:13:30
[07/29 16:06:45] d2.evaluation.evaluator INFO: Inference done 2413/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0025 s/iter. Total: 0.1051 s/iter. ETA=0:13:26
[07/29 16:06:51] d2.evaluation.evaluator INFO: Inference done 2460/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0026 s/iter. Total: 0.1052 s/iter. ETA=0:13:21
[07/29 16:06:56] d2.evaluation.evaluator INFO: Inference done 2507/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0026 s/iter. Total: 0.1052 s/iter. ETA=0:13:16
[07/29 16:07:01] d2.evaluation.evaluator INFO: Inference done 2554/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0026 s/iter. Total: 0.1053 s/iter. ETA=0:13:12
[07/29 16:07:06] d2.evaluation.evaluator INFO: Inference done 2602/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0026 s/iter. Total: 0.1053 s/iter. ETA=0:13:07
[07/29 16:07:11] d2.evaluation.evaluator INFO: Inference done 2650/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0026 s/iter. Total: 0.1053 s/iter. ETA=0:13:02
[07/29 16:07:16] d2.evaluation.evaluator INFO: Inference done 2698/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0026 s/iter. Total: 0.1053 s/iter. ETA=0:12:57
[07/29 16:07:21] d2.evaluation.evaluator INFO: Inference done 2746/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0026 s/iter. Total: 0.1053 s/iter. ETA=0:12:51
[07/29 16:07:26] d2.evaluation.evaluator INFO: Inference done 2793/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0027 s/iter. Total: 0.1053 s/iter. ETA=0:12:47
[07/29 16:07:31] d2.evaluation.evaluator INFO: Inference done 2841/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0027 s/iter. Total: 0.1053 s/iter. ETA=0:12:42
[07/29 16:07:36] d2.evaluation.evaluator INFO: Inference done 2889/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0027 s/iter. Total: 0.1053 s/iter. ETA=0:12:37
[07/29 16:07:41] d2.evaluation.evaluator INFO: Inference done 2936/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0027 s/iter. Total: 0.1053 s/iter. ETA=0:12:32
[07/29 16:07:46] d2.evaluation.evaluator INFO: Inference done 2984/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0027 s/iter. Total: 0.1053 s/iter. ETA=0:12:27
[07/29 16:07:51] d2.evaluation.evaluator INFO: Inference done 3031/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0027 s/iter. Total: 0.1054 s/iter. ETA=0:12:22
[07/29 16:07:56] d2.evaluation.evaluator INFO: Inference done 3077/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0028 s/iter. Total: 0.1054 s/iter. ETA=0:12:18
[07/29 16:08:01] d2.evaluation.evaluator INFO: Inference done 3123/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0028 s/iter. Total: 0.1055 s/iter. ETA=0:12:13
[07/29 16:08:06] d2.evaluation.evaluator INFO: Inference done 3169/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0029 s/iter. Total: 0.1055 s/iter. ETA=0:12:09
[07/29 16:08:11] d2.evaluation.evaluator INFO: Inference done 3216/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0029 s/iter. Total: 0.1056 s/iter. ETA=0:12:04
[07/29 16:08:16] d2.evaluation.evaluator INFO: Inference done 3263/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0029 s/iter. Total: 0.1056 s/iter. ETA=0:11:59
[07/29 16:08:21] d2.evaluation.evaluator INFO: Inference done 3311/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0029 s/iter. Total: 0.1056 s/iter. ETA=0:11:54
[07/29 16:08:26] d2.evaluation.evaluator INFO: Inference done 3359/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0029 s/iter. Total: 0.1056 s/iter. ETA=0:11:49
[07/29 16:08:32] d2.evaluation.evaluator INFO: Inference done 3406/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0030 s/iter. Total: 0.1056 s/iter. ETA=0:11:45
[07/29 16:08:37] d2.evaluation.evaluator INFO: Inference done 3452/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0030 s/iter. Total: 0.1057 s/iter. ETA=0:11:40
[07/29 16:08:42] d2.evaluation.evaluator INFO: Inference done 3497/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0031 s/iter. Total: 0.1058 s/iter. ETA=0:11:36
[07/29 16:08:47] d2.evaluation.evaluator INFO: Inference done 3543/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0031 s/iter. Total: 0.1058 s/iter. ETA=0:11:31
[07/29 16:08:52] d2.evaluation.evaluator INFO: Inference done 3589/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0032 s/iter. Total: 0.1059 s/iter. ETA=0:11:27
[07/29 16:08:57] d2.evaluation.evaluator INFO: Inference done 3637/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0032 s/iter. Total: 0.1059 s/iter. ETA=0:11:22
[07/29 16:09:02] d2.evaluation.evaluator INFO: Inference done 3685/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0032 s/iter. Total: 0.1059 s/iter. ETA=0:11:17
[07/29 16:09:07] d2.evaluation.evaluator INFO: Inference done 3731/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0032 s/iter. Total: 0.1059 s/iter. ETA=0:11:12
[07/29 16:09:12] d2.evaluation.evaluator INFO: Inference done 3774/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0033 s/iter. Total: 0.1061 s/iter. ETA=0:11:08
[07/29 16:09:17] d2.evaluation.evaluator INFO: Inference done 3821/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0033 s/iter. Total: 0.1061 s/iter. ETA=0:11:04
[07/29 16:09:22] d2.evaluation.evaluator INFO: Inference done 3867/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0033 s/iter. Total: 0.1061 s/iter. ETA=0:10:59
[07/29 16:09:27] d2.evaluation.evaluator INFO: Inference done 3913/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0034 s/iter. Total: 0.1062 s/iter. ETA=0:10:54
[07/29 16:09:32] d2.evaluation.evaluator INFO: Inference done 3959/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0034 s/iter. Total: 0.1062 s/iter. ETA=0:10:50
[07/29 16:09:37] d2.evaluation.evaluator INFO: Inference done 4005/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0035 s/iter. Total: 0.1063 s/iter. ETA=0:10:45
[07/29 16:09:42] d2.evaluation.evaluator INFO: Inference done 4051/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0035 s/iter. Total: 0.1063 s/iter. ETA=0:10:41
[07/29 16:09:47] d2.evaluation.evaluator INFO: Inference done 4097/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0035 s/iter. Total: 0.1064 s/iter. ETA=0:10:36
[07/29 16:09:53] d2.evaluation.evaluator INFO: Inference done 4143/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0036 s/iter. Total: 0.1064 s/iter. ETA=0:10:31
[07/29 16:09:58] d2.evaluation.evaluator INFO: Inference done 4189/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0036 s/iter. Total: 0.1064 s/iter. ETA=0:10:27
[07/29 16:10:03] d2.evaluation.evaluator INFO: Inference done 4235/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0036 s/iter. Total: 0.1065 s/iter. ETA=0:10:22
[07/29 16:10:08] d2.evaluation.evaluator INFO: Inference done 4281/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0036 s/iter. Total: 0.1065 s/iter. ETA=0:10:17
[07/29 16:10:13] d2.evaluation.evaluator INFO: Inference done 4327/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0037 s/iter. Total: 0.1066 s/iter. ETA=0:10:13
[07/29 16:10:18] d2.evaluation.evaluator INFO: Inference done 4372/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0037 s/iter. Total: 0.1066 s/iter. ETA=0:10:08
[07/29 16:10:23] d2.evaluation.evaluator INFO: Inference done 4417/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0038 s/iter. Total: 0.1067 s/iter. ETA=0:10:04
[07/29 16:10:28] d2.evaluation.evaluator INFO: Inference done 4462/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0039 s/iter. Total: 0.1068 s/iter. ETA=0:09:59
[07/29 16:10:33] d2.evaluation.evaluator INFO: Inference done 4507/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0039 s/iter. Total: 0.1068 s/iter. ETA=0:09:55
[07/29 16:10:38] d2.evaluation.evaluator INFO: Inference done 4553/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0039 s/iter. Total: 0.1069 s/iter. ETA=0:09:50
[07/29 16:10:43] d2.evaluation.evaluator INFO: Inference done 4600/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0039 s/iter. Total: 0.1069 s/iter. ETA=0:09:45
[07/29 16:10:48] d2.evaluation.evaluator INFO: Inference done 4647/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0039 s/iter. Total: 0.1069 s/iter. ETA=0:09:40
[07/29 16:10:53] d2.evaluation.evaluator INFO: Inference done 4694/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0039 s/iter. Total: 0.1069 s/iter. ETA=0:09:35
[07/29 16:10:58] d2.evaluation.evaluator INFO: Inference done 4741/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0039 s/iter. Total: 0.1069 s/iter. ETA=0:09:30
[07/29 16:11:03] d2.evaluation.evaluator INFO: Inference done 4787/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0040 s/iter. Total: 0.1069 s/iter. ETA=0:09:25
[07/29 16:11:09] d2.evaluation.evaluator INFO: Inference done 4834/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0040 s/iter. Total: 0.1069 s/iter. ETA=0:09:20
[07/29 16:11:14] d2.evaluation.evaluator INFO: Inference done 4881/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0040 s/iter. Total: 0.1069 s/iter. ETA=0:09:15
[07/29 16:11:19] d2.evaluation.evaluator INFO: Inference done 4927/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0040 s/iter. Total: 0.1070 s/iter. ETA=0:09:11
[07/29 16:11:24] d2.evaluation.evaluator INFO: Inference done 4973/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0040 s/iter. Total: 0.1070 s/iter. ETA=0:09:06
[07/29 16:11:29] d2.evaluation.evaluator INFO: Inference done 5018/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0041 s/iter. Total: 0.1070 s/iter. ETA=0:09:01
[07/29 16:11:34] d2.evaluation.evaluator INFO: Inference done 5065/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0041 s/iter. Total: 0.1070 s/iter. ETA=0:08:56
[07/29 16:11:39] d2.evaluation.evaluator INFO: Inference done 5114/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0041 s/iter. Total: 0.1070 s/iter. ETA=0:08:51
[07/29 16:11:44] d2.evaluation.evaluator INFO: Inference done 5162/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0040 s/iter. Total: 0.1070 s/iter. ETA=0:08:46
[07/29 16:11:49] d2.evaluation.evaluator INFO: Inference done 5210/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0040 s/iter. Total: 0.1070 s/iter. ETA=0:08:40
[07/29 16:11:54] d2.evaluation.evaluator INFO: Inference done 5258/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0040 s/iter. Total: 0.1070 s/iter. ETA=0:08:35
[07/29 16:11:59] d2.evaluation.evaluator INFO: Inference done 5306/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0040 s/iter. Total: 0.1069 s/iter. ETA=0:08:30
[07/29 16:12:04] d2.evaluation.evaluator INFO: Inference done 5355/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0040 s/iter. Total: 0.1069 s/iter. ETA=0:08:25
[07/29 16:12:09] d2.evaluation.evaluator INFO: Inference done 5403/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0040 s/iter. Total: 0.1069 s/iter. ETA=0:08:19
[07/29 16:12:14] d2.evaluation.evaluator INFO: Inference done 5451/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0040 s/iter. Total: 0.1069 s/iter. ETA=0:08:14
[07/29 16:12:19] d2.evaluation.evaluator INFO: Inference done 5499/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0039 s/iter. Total: 0.1069 s/iter. ETA=0:08:09
[07/29 16:12:24] d2.evaluation.evaluator INFO: Inference done 5546/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0039 s/iter. Total: 0.1069 s/iter. ETA=0:08:04
[07/29 16:12:30] d2.evaluation.evaluator INFO: Inference done 5593/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0039 s/iter. Total: 0.1069 s/iter. ETA=0:07:59
[07/29 16:12:35] d2.evaluation.evaluator INFO: Inference done 5640/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0039 s/iter. Total: 0.1069 s/iter. ETA=0:07:54
[07/29 16:12:40] d2.evaluation.evaluator INFO: Inference done 5687/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0039 s/iter. Total: 0.1069 s/iter. ETA=0:07:49
[07/29 16:12:45] d2.evaluation.evaluator INFO: Inference done 5734/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0039 s/iter. Total: 0.1069 s/iter. ETA=0:07:44
[07/29 16:12:50] d2.evaluation.evaluator INFO: Inference done 5781/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0039 s/iter. Total: 0.1069 s/iter. ETA=0:07:39
[07/29 16:12:55] d2.evaluation.evaluator INFO: Inference done 5828/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0039 s/iter. Total: 0.1069 s/iter. ETA=0:07:34
[07/29 16:13:00] d2.evaluation.evaluator INFO: Inference done 5874/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0040 s/iter. Total: 0.1069 s/iter. ETA=0:07:29
[07/29 16:13:05] d2.evaluation.evaluator INFO: Inference done 5920/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0040 s/iter. Total: 0.1070 s/iter. ETA=0:07:24
[07/29 16:13:10] d2.evaluation.evaluator INFO: Inference done 5966/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0040 s/iter. Total: 0.1070 s/iter. ETA=0:07:20
[07/29 16:13:15] d2.evaluation.evaluator INFO: Inference done 6013/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0040 s/iter. Total: 0.1070 s/iter. ETA=0:07:15
[07/29 16:13:20] d2.evaluation.evaluator INFO: Inference done 6059/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0040 s/iter. Total: 0.1070 s/iter. ETA=0:07:10
[07/29 16:13:25] d2.evaluation.evaluator INFO: Inference done 6106/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0040 s/iter. Total: 0.1070 s/iter. ETA=0:07:05
[07/29 16:13:30] d2.evaluation.evaluator INFO: Inference done 6152/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0040 s/iter. Total: 0.1070 s/iter. ETA=0:07:00
[07/29 16:13:35] d2.evaluation.evaluator INFO: Inference done 6197/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0041 s/iter. Total: 0.1071 s/iter. ETA=0:06:55
[07/29 16:13:40] d2.evaluation.evaluator INFO: Inference done 6243/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0041 s/iter. Total: 0.1071 s/iter. ETA=0:06:50
[07/29 16:13:45] d2.evaluation.evaluator INFO: Inference done 6288/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0041 s/iter. Total: 0.1071 s/iter. ETA=0:06:46
[07/29 16:13:50] d2.evaluation.evaluator INFO: Inference done 6333/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0042 s/iter. Total: 0.1072 s/iter. ETA=0:06:41
[07/29 16:13:55] d2.evaluation.evaluator INFO: Inference done 6379/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0042 s/iter. Total: 0.1072 s/iter. ETA=0:06:36
[07/29 16:14:00] d2.evaluation.evaluator INFO: Inference done 6425/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0042 s/iter. Total: 0.1072 s/iter. ETA=0:06:31
[07/29 16:14:06] d2.evaluation.evaluator INFO: Inference done 6470/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0042 s/iter. Total: 0.1072 s/iter. ETA=0:06:27
[07/29 16:14:11] d2.evaluation.evaluator INFO: Inference done 6516/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0042 s/iter. Total: 0.1073 s/iter. ETA=0:06:22
[07/29 16:14:16] d2.evaluation.evaluator INFO: Inference done 6561/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0043 s/iter. Total: 0.1073 s/iter. ETA=0:06:17
[07/29 16:14:21] d2.evaluation.evaluator INFO: Inference done 6608/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0043 s/iter. Total: 0.1073 s/iter. ETA=0:06:12
[07/29 16:14:26] d2.evaluation.evaluator INFO: Inference done 6655/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0043 s/iter. Total: 0.1073 s/iter. ETA=0:06:07
[07/29 16:14:31] d2.evaluation.evaluator INFO: Inference done 6701/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0043 s/iter. Total: 0.1073 s/iter. ETA=0:06:02
[07/29 16:14:36] d2.evaluation.evaluator INFO: Inference done 6748/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0043 s/iter. Total: 0.1073 s/iter. ETA=0:05:57
[07/29 16:14:41] d2.evaluation.evaluator INFO: Inference done 6794/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0043 s/iter. Total: 0.1073 s/iter. ETA=0:05:52
[07/29 16:14:46] d2.evaluation.evaluator INFO: Inference done 6840/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0043 s/iter. Total: 0.1074 s/iter. ETA=0:05:47
[07/29 16:14:51] d2.evaluation.evaluator INFO: Inference done 6886/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0043 s/iter. Total: 0.1074 s/iter. ETA=0:05:42
[07/29 16:14:56] d2.evaluation.evaluator INFO: Inference done 6932/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1074 s/iter. ETA=0:05:38
[07/29 16:15:01] d2.evaluation.evaluator INFO: Inference done 6978/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1074 s/iter. ETA=0:05:33
[07/29 16:15:06] d2.evaluation.evaluator INFO: Inference done 7024/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1074 s/iter. ETA=0:05:28
[07/29 16:15:11] d2.evaluation.evaluator INFO: Inference done 7070/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1074 s/iter. ETA=0:05:23
[07/29 16:15:16] d2.evaluation.evaluator INFO: Inference done 7117/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1074 s/iter. ETA=0:05:18
[07/29 16:15:21] d2.evaluation.evaluator INFO: Inference done 7164/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1074 s/iter. ETA=0:05:13
[07/29 16:15:26] d2.evaluation.evaluator INFO: Inference done 7212/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1074 s/iter. ETA=0:05:08
[07/29 16:15:31] d2.evaluation.evaluator INFO: Inference done 7260/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1074 s/iter. ETA=0:05:02
[07/29 16:15:36] d2.evaluation.evaluator INFO: Inference done 7307/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1074 s/iter. ETA=0:04:57
[07/29 16:15:42] d2.evaluation.evaluator INFO: Inference done 7354/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1074 s/iter. ETA=0:04:52
[07/29 16:15:47] d2.evaluation.evaluator INFO: Inference done 7401/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1074 s/iter. ETA=0:04:47
[07/29 16:15:52] d2.evaluation.evaluator INFO: Inference done 7448/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1074 s/iter. ETA=0:04:42
[07/29 16:15:57] d2.evaluation.evaluator INFO: Inference done 7495/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1074 s/iter. ETA=0:04:37
[07/29 16:16:02] d2.evaluation.evaluator INFO: Inference done 7542/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1074 s/iter. ETA=0:04:32
[07/29 16:16:07] d2.evaluation.evaluator INFO: Inference done 7589/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1074 s/iter. ETA=0:04:27
[07/29 16:16:12] d2.evaluation.evaluator INFO: Inference done 7635/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1074 s/iter. ETA=0:04:22
[07/29 16:16:17] d2.evaluation.evaluator INFO: Inference done 7682/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1074 s/iter. ETA=0:04:17
[07/29 16:16:22] d2.evaluation.evaluator INFO: Inference done 7729/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1074 s/iter. ETA=0:04:12
[07/29 16:16:27] d2.evaluation.evaluator INFO: Inference done 7776/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1074 s/iter. ETA=0:04:07
[07/29 16:16:32] d2.evaluation.evaluator INFO: Inference done 7823/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1074 s/iter. ETA=0:04:02
[07/29 16:16:37] d2.evaluation.evaluator INFO: Inference done 7869/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1074 s/iter. ETA=0:03:57
[07/29 16:16:42] d2.evaluation.evaluator INFO: Inference done 7916/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1074 s/iter. ETA=0:03:52
[07/29 16:16:47] d2.evaluation.evaluator INFO: Inference done 7962/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1074 s/iter. ETA=0:03:47
[07/29 16:16:52] d2.evaluation.evaluator INFO: Inference done 8008/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1075 s/iter. ETA=0:03:42
[07/29 16:16:57] d2.evaluation.evaluator INFO: Inference done 8054/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1075 s/iter. ETA=0:03:37
[07/29 16:17:02] d2.evaluation.evaluator INFO: Inference done 8100/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1075 s/iter. ETA=0:03:32
[07/29 16:17:07] d2.evaluation.evaluator INFO: Inference done 8147/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1075 s/iter. ETA=0:03:27
[07/29 16:17:12] d2.evaluation.evaluator INFO: Inference done 8193/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1075 s/iter. ETA=0:03:22
[07/29 16:17:17] d2.evaluation.evaluator INFO: Inference done 8239/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1075 s/iter. ETA=0:03:17
[07/29 16:17:22] d2.evaluation.evaluator INFO: Inference done 8285/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1075 s/iter. ETA=0:03:12
[07/29 16:17:28] d2.evaluation.evaluator INFO: Inference done 8331/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1075 s/iter. ETA=0:03:08
[07/29 16:17:33] d2.evaluation.evaluator INFO: Inference done 8377/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0045 s/iter. Total: 0.1075 s/iter. ETA=0:03:03
[07/29 16:17:38] d2.evaluation.evaluator INFO: Inference done 8423/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0045 s/iter. Total: 0.1076 s/iter. ETA=0:02:58
[07/29 16:17:43] d2.evaluation.evaluator INFO: Inference done 8468/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0045 s/iter. Total: 0.1076 s/iter. ETA=0:02:53
[07/29 16:17:48] d2.evaluation.evaluator INFO: Inference done 8514/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0045 s/iter. Total: 0.1076 s/iter. ETA=0:02:48
[07/29 16:17:53] d2.evaluation.evaluator INFO: Inference done 8556/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0045 s/iter. Total: 0.1077 s/iter. ETA=0:02:44
[07/29 16:17:58] d2.evaluation.evaluator INFO: Inference done 8602/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0045 s/iter. Total: 0.1077 s/iter. ETA=0:02:39
[07/29 16:18:03] d2.evaluation.evaluator INFO: Inference done 8647/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0045 s/iter. Total: 0.1077 s/iter. ETA=0:02:34
[07/29 16:18:08] d2.evaluation.evaluator INFO: Inference done 8692/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0045 s/iter. Total: 0.1077 s/iter. ETA=0:02:29
[07/29 16:18:13] d2.evaluation.evaluator INFO: Inference done 8737/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0046 s/iter. Total: 0.1077 s/iter. ETA=0:02:24
[07/29 16:18:18] d2.evaluation.evaluator INFO: Inference done 8782/10080. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0046 s/iter. Total: 0.1078 s/iter. ETA=0:02:19
[07/29 16:18:23] d2.evaluation.evaluator INFO: Inference done 8827/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0046 s/iter. Total: 0.1078 s/iter. ETA=0:02:15
[07/29 16:18:28] d2.evaluation.evaluator INFO: Inference done 8872/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0046 s/iter. Total: 0.1078 s/iter. ETA=0:02:10
[07/29 16:18:33] d2.evaluation.evaluator INFO: Inference done 8918/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0046 s/iter. Total: 0.1078 s/iter. ETA=0:02:05
[07/29 16:18:38] d2.evaluation.evaluator INFO: Inference done 8963/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0046 s/iter. Total: 0.1078 s/iter. ETA=0:02:00
[07/29 16:18:43] d2.evaluation.evaluator INFO: Inference done 9008/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0047 s/iter. Total: 0.1079 s/iter. ETA=0:01:55
[07/29 16:18:49] d2.evaluation.evaluator INFO: Inference done 9054/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0047 s/iter. Total: 0.1079 s/iter. ETA=0:01:50
[07/29 16:18:54] d2.evaluation.evaluator INFO: Inference done 9100/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0047 s/iter. Total: 0.1079 s/iter. ETA=0:01:45
[07/29 16:18:59] d2.evaluation.evaluator INFO: Inference done 9146/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0047 s/iter. Total: 0.1079 s/iter. ETA=0:01:40
[07/29 16:19:04] d2.evaluation.evaluator INFO: Inference done 9193/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0047 s/iter. Total: 0.1079 s/iter. ETA=0:01:35
[07/29 16:19:09] d2.evaluation.evaluator INFO: Inference done 9240/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0047 s/iter. Total: 0.1079 s/iter. ETA=0:01:30
[07/29 16:19:14] d2.evaluation.evaluator INFO: Inference done 9287/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0047 s/iter. Total: 0.1079 s/iter. ETA=0:01:25
[07/29 16:19:19] d2.evaluation.evaluator INFO: Inference done 9334/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0047 s/iter. Total: 0.1079 s/iter. ETA=0:01:20
[07/29 16:19:24] d2.evaluation.evaluator INFO: Inference done 9381/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0047 s/iter. Total: 0.1079 s/iter. ETA=0:01:15
[07/29 16:19:29] d2.evaluation.evaluator INFO: Inference done 9428/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0047 s/iter. Total: 0.1079 s/iter. ETA=0:01:10
[07/29 16:19:34] d2.evaluation.evaluator INFO: Inference done 9473/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0047 s/iter. Total: 0.1079 s/iter. ETA=0:01:05
[07/29 16:19:39] d2.evaluation.evaluator INFO: Inference done 9519/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0047 s/iter. Total: 0.1079 s/iter. ETA=0:01:00
[07/29 16:19:44] d2.evaluation.evaluator INFO: Inference done 9566/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0047 s/iter. Total: 0.1079 s/iter. ETA=0:00:55
[07/29 16:19:49] d2.evaluation.evaluator INFO: Inference done 9610/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0047 s/iter. Total: 0.1079 s/iter. ETA=0:00:50
[07/29 16:19:54] d2.evaluation.evaluator INFO: Inference done 9654/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0048 s/iter. Total: 0.1080 s/iter. ETA=0:00:45
[07/29 16:19:59] d2.evaluation.evaluator INFO: Inference done 9698/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0048 s/iter. Total: 0.1080 s/iter. ETA=0:00:41
[07/29 16:20:04] d2.evaluation.evaluator INFO: Inference done 9742/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0048 s/iter. Total: 0.1080 s/iter. ETA=0:00:36
[07/29 16:20:09] d2.evaluation.evaluator INFO: Inference done 9787/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0048 s/iter. Total: 0.1081 s/iter. ETA=0:00:31
[07/29 16:20:14] d2.evaluation.evaluator INFO: Inference done 9833/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0048 s/iter. Total: 0.1081 s/iter. ETA=0:00:26
[07/29 16:20:19] d2.evaluation.evaluator INFO: Inference done 9878/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0049 s/iter. Total: 0.1081 s/iter. ETA=0:00:21
[07/29 16:20:25] d2.evaluation.evaluator INFO: Inference done 9923/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0049 s/iter. Total: 0.1081 s/iter. ETA=0:00:16
[07/29 16:20:30] d2.evaluation.evaluator INFO: Inference done 9967/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0049 s/iter. Total: 0.1081 s/iter. ETA=0:00:12
[07/29 16:20:35] d2.evaluation.evaluator INFO: Inference done 10011/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0049 s/iter. Total: 0.1082 s/iter. ETA=0:00:07
[07/29 16:20:40] d2.evaluation.evaluator INFO: Inference done 10056/10080. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0049 s/iter. Total: 0.1082 s/iter. ETA=0:00:02
[07/29 16:20:43] d2.evaluation.evaluator INFO: Total inference time: 0:18:10.253763 (0.108214 s / iter per device, on 1 devices)
[07/29 16:20:43] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:17:09 (0.102222 s / iter per device, on 1 devices)
[07/29 16:20:43] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[07/29 16:20:43] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[07/29 16:20:43] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[07/29 16:20:47] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 4.75 seconds.
[07/29 16:20:47] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[07/29 16:20:48] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.44 seconds.
[07/29 16:20:48] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |
|:------:|:------:|:------:|:-----:|:------:|:------:|
| 54.939 | 67.899 | 61.722 | 5.316 | 50.163 | 56.283 |
[07/29 16:20:48] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP     | category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|:-----------|:-------|
| 1          | 47.709 | 2          | 30.682 | 3          | 28.160 |
| 4          | 24.510 | 5          | 32.507 | 6          | 38.694 |
| 7          | 18.217 | 8          | 41.000 | 9          | 77.371 |
| 10         | 75.444 | 11         | 66.116 | 12         | 66.745 |
| 13         | 56.255 | 14         | 56.632 | 15         | 62.955 |
| 16         | 68.010 | 17         | 63.238 | 18         | 75.709 |
| 19         | 59.722 | 20         | 49.043 | 21         | 43.821 |
| 22         | 48.674 | 23         | 73.403 | 24         | 76.735 |
| 25         | 75.403 | 26         | 68.830 | 27         | 56.834 |
| 28         | 71.007 | 29         | 75.177 | 30         | 19.574 |
[07/29 16:20:51] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *segm*
[07/29 16:20:58] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 7.56 seconds.
[07/29 16:20:58] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[07/29 16:20:59] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.45 seconds.
[07/29 16:20:59] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |
|:------:|:------:|:------:|:-----:|:------:|:------:|
| 48.417 | 64.782 | 56.031 | 0.387 | 36.827 | 55.924 |
[07/29 16:20:59] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|:-----------|:-------|
| 1          | 42.091 | 2          | 29.024 | 3          | 25.417 |
| 4          | 20.415 | 5          | 23.333 | 6          | 35.312 |
| 7          | 12.252 | 8          | 36.219 | 9          | 69.169 |
| 10         | 70.412 | 11         | 61.550 | 12         | 60.146 |
| 13         | 50.663 | 14         | 51.997 | 15         | 56.360 |
| 16         | 64.795 | 17         | 55.676 | 18         | 72.123 |
| 19         | 53.238 | 20         | 38.790 | 21         | 37.612 |
| 22         | 42.810 | 23         | 67.919 | 24         | 70.023 |
| 25         | 68.944 | 26         | 59.230 | 27         | 35.185 |
| 28         | 61.292 | 29         | 65.341 | 30         | 15.185 |
[07/29 16:21:00] d2.evaluation.testing INFO: copypaste: Task: bbox
[07/29 16:21:00] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/29 16:21:00] d2.evaluation.testing INFO: copypaste: 54.9394,67.8994,61.7217,5.3156,50.1626,56.2829
[07/29 16:21:00] d2.evaluation.testing INFO: copypaste: Task: segm
[07/29 16:21:00] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/29 16:21:00] d2.evaluation.testing INFO: copypaste: 48.4175,64.7822,56.0315,0.3866,36.8267,55.9239
[07/29 16:21:00] d2.utils.events INFO:  eta: 8:21:34  iter: 19999  total_loss: 2.885  loss_cls_stage0: 0.2927  loss_box_reg_stage0: 0.2754  loss_cls_stage1: 0.2979  loss_box_reg_stage1: 0.6727  loss_cls_stage2: 0.2878  loss_box_reg_stage2: 0.9051  loss_mask: 0.08699  loss_rpn_cls: 0.02209  loss_rpn_loc: 0.05243  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:21:12] d2.utils.events INFO:  eta: 8:21:21  iter: 20019  total_loss: 2.836  loss_cls_stage0: 0.2366  loss_box_reg_stage0: 0.2643  loss_cls_stage1: 0.2528  loss_box_reg_stage1: 0.7004  loss_cls_stage2: 0.2485  loss_box_reg_stage2: 0.9463  loss_mask: 0.07272  loss_rpn_cls: 0.01815  loss_rpn_loc: 0.05718  time: 0.6241  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:21:25] d2.utils.events INFO:  eta: 8:21:46  iter: 20039  total_loss: 2.835  loss_cls_stage0: 0.2492  loss_box_reg_stage0: 0.2545  loss_cls_stage1: 0.2806  loss_box_reg_stage1: 0.6698  loss_cls_stage2: 0.2877  loss_box_reg_stage2: 0.959  loss_mask: 0.07818  loss_rpn_cls: 0.01449  loss_rpn_loc: 0.04894  time: 0.6241  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 16:21:37] d2.utils.events INFO:  eta: 8:21:17  iter: 20059  total_loss: 3.026  loss_cls_stage0: 0.3016  loss_box_reg_stage0: 0.2796  loss_cls_stage1: 0.3076  loss_box_reg_stage1: 0.6532  loss_cls_stage2: 0.2851  loss_box_reg_stage2: 0.8579  loss_mask: 0.07155  loss_rpn_cls: 0.01566  loss_rpn_loc: 0.05074  time: 0.6241  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 16:21:50] d2.utils.events INFO:  eta: 8:21:21  iter: 20079  total_loss: 2.881  loss_cls_stage0: 0.2677  loss_box_reg_stage0: 0.2907  loss_cls_stage1: 0.2899  loss_box_reg_stage1: 0.7038  loss_cls_stage2: 0.2634  loss_box_reg_stage2: 0.926  loss_mask: 0.07577  loss_rpn_cls: 0.01778  loss_rpn_loc: 0.07454  time: 0.6241  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 16:22:02] d2.utils.events INFO:  eta: 8:21:05  iter: 20099  total_loss: 2.902  loss_cls_stage0: 0.2534  loss_box_reg_stage0: 0.2375  loss_cls_stage1: 0.2674  loss_box_reg_stage1: 0.705  loss_cls_stage2: 0.2698  loss_box_reg_stage2: 0.9569  loss_mask: 0.08092  loss_rpn_cls: 0.01745  loss_rpn_loc: 0.04262  time: 0.6241  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 16:22:15] d2.utils.events INFO:  eta: 8:20:25  iter: 20119  total_loss: 2.873  loss_cls_stage0: 0.2703  loss_box_reg_stage0: 0.2517  loss_cls_stage1: 0.3138  loss_box_reg_stage1: 0.6917  loss_cls_stage2: 0.2737  loss_box_reg_stage2: 0.9164  loss_mask: 0.07611  loss_rpn_cls: 0.01392  loss_rpn_loc: 0.04415  time: 0.6241  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 16:22:27] d2.utils.events INFO:  eta: 8:19:58  iter: 20139  total_loss: 2.686  loss_cls_stage0: 0.2384  loss_box_reg_stage0: 0.2509  loss_cls_stage1: 0.2419  loss_box_reg_stage1: 0.6247  loss_cls_stage2: 0.2357  loss_box_reg_stage2: 0.9241  loss_mask: 0.0851  loss_rpn_cls: 0.0183  loss_rpn_loc: 0.0423  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:22:39] d2.utils.events INFO:  eta: 8:19:20  iter: 20159  total_loss: 2.717  loss_cls_stage0: 0.2296  loss_box_reg_stage0: 0.2555  loss_cls_stage1: 0.2395  loss_box_reg_stage1: 0.668  loss_cls_stage2: 0.2389  loss_box_reg_stage2: 0.9045  loss_mask: 0.07174  loss_rpn_cls: 0.01593  loss_rpn_loc: 0.048  time: 0.6240  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 16:22:52] d2.utils.events INFO:  eta: 8:19:18  iter: 20179  total_loss: 3.106  loss_cls_stage0: 0.3017  loss_box_reg_stage0: 0.2981  loss_cls_stage1: 0.2999  loss_box_reg_stage1: 0.7236  loss_cls_stage2: 0.2777  loss_box_reg_stage2: 0.9656  loss_mask: 0.08279  loss_rpn_cls: 0.01331  loss_rpn_loc: 0.04914  time: 0.6240  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 16:23:04] d2.utils.events INFO:  eta: 8:19:12  iter: 20199  total_loss: 2.883  loss_cls_stage0: 0.2439  loss_box_reg_stage0: 0.2761  loss_cls_stage1: 0.2614  loss_box_reg_stage1: 0.6958  loss_cls_stage2: 0.2541  loss_box_reg_stage2: 0.9455  loss_mask: 0.07631  loss_rpn_cls: 0.01446  loss_rpn_loc: 0.04604  time: 0.6240  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 16:23:17] d2.utils.events INFO:  eta: 8:18:52  iter: 20219  total_loss: 2.913  loss_cls_stage0: 0.2602  loss_box_reg_stage0: 0.2399  loss_cls_stage1: 0.3058  loss_box_reg_stage1: 0.6279  loss_cls_stage2: 0.2961  loss_box_reg_stage2: 0.8963  loss_mask: 0.07572  loss_rpn_cls: 0.01329  loss_rpn_loc: 0.04912  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:23:30] d2.utils.events INFO:  eta: 8:18:54  iter: 20239  total_loss: 3.176  loss_cls_stage0: 0.2837  loss_box_reg_stage0: 0.2798  loss_cls_stage1: 0.3393  loss_box_reg_stage1: 0.7654  loss_cls_stage2: 0.326  loss_box_reg_stage2: 1.04  loss_mask: 0.08554  loss_rpn_cls: 0.01492  loss_rpn_loc: 0.04527  time: 0.6241  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:23:42] d2.utils.events INFO:  eta: 8:18:35  iter: 20259  total_loss: 2.985  loss_cls_stage0: 0.2931  loss_box_reg_stage0: 0.2608  loss_cls_stage1: 0.328  loss_box_reg_stage1: 0.689  loss_cls_stage2: 0.3045  loss_box_reg_stage2: 0.9633  loss_mask: 0.07313  loss_rpn_cls: 0.0242  loss_rpn_loc: 0.05957  time: 0.6241  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 16:23:54] d2.utils.events INFO:  eta: 8:18:29  iter: 20279  total_loss: 2.723  loss_cls_stage0: 0.2628  loss_box_reg_stage0: 0.2662  loss_cls_stage1: 0.2872  loss_box_reg_stage1: 0.6448  loss_cls_stage2: 0.2816  loss_box_reg_stage2: 0.8352  loss_mask: 0.07542  loss_rpn_cls: 0.02554  loss_rpn_loc: 0.06575  time: 0.6240  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 16:24:07] d2.utils.events INFO:  eta: 8:18:10  iter: 20299  total_loss: 2.618  loss_cls_stage0: 0.2476  loss_box_reg_stage0: 0.2633  loss_cls_stage1: 0.2539  loss_box_reg_stage1: 0.622  loss_cls_stage2: 0.2422  loss_box_reg_stage2: 0.7979  loss_mask: 0.07474  loss_rpn_cls: 0.02107  loss_rpn_loc: 0.0463  time: 0.6240  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 16:24:20] d2.utils.events INFO:  eta: 8:18:04  iter: 20319  total_loss: 2.905  loss_cls_stage0: 0.2583  loss_box_reg_stage0: 0.2547  loss_cls_stage1: 0.3044  loss_box_reg_stage1: 0.6746  loss_cls_stage2: 0.2984  loss_box_reg_stage2: 0.893  loss_mask: 0.07689  loss_rpn_cls: 0.01611  loss_rpn_loc: 0.05448  time: 0.6241  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:24:32] d2.utils.events INFO:  eta: 8:18:06  iter: 20339  total_loss: 3.204  loss_cls_stage0: 0.3205  loss_box_reg_stage0: 0.2862  loss_cls_stage1: 0.3299  loss_box_reg_stage1: 0.7519  loss_cls_stage2: 0.3198  loss_box_reg_stage2: 0.9935  loss_mask: 0.08272  loss_rpn_cls: 0.01795  loss_rpn_loc: 0.04981  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:24:45] d2.utils.events INFO:  eta: 8:17:59  iter: 20359  total_loss: 2.618  loss_cls_stage0: 0.2302  loss_box_reg_stage0: 0.2432  loss_cls_stage1: 0.2496  loss_box_reg_stage1: 0.5846  loss_cls_stage2: 0.2589  loss_box_reg_stage2: 0.8783  loss_mask: 0.07719  loss_rpn_cls: 0.01516  loss_rpn_loc: 0.06875  time: 0.6241  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 16:24:57] d2.utils.events INFO:  eta: 8:17:59  iter: 20379  total_loss: 3.02  loss_cls_stage0: 0.2723  loss_box_reg_stage0: 0.2905  loss_cls_stage1: 0.2865  loss_box_reg_stage1: 0.7466  loss_cls_stage2: 0.2695  loss_box_reg_stage2: 0.9929  loss_mask: 0.08161  loss_rpn_cls: 0.01475  loss_rpn_loc: 0.0535  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:25:10] d2.utils.events INFO:  eta: 8:17:15  iter: 20399  total_loss: 3.009  loss_cls_stage0: 0.263  loss_box_reg_stage0: 0.2606  loss_cls_stage1: 0.2857  loss_box_reg_stage1: 0.7293  loss_cls_stage2: 0.2845  loss_box_reg_stage2: 1.037  loss_mask: 0.08039  loss_rpn_cls: 0.0115  loss_rpn_loc: 0.0465  time: 0.6241  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:25:23] d2.utils.events INFO:  eta: 8:16:35  iter: 20419  total_loss: 2.661  loss_cls_stage0: 0.2193  loss_box_reg_stage0: 0.2352  loss_cls_stage1: 0.2772  loss_box_reg_stage1: 0.6234  loss_cls_stage2: 0.2803  loss_box_reg_stage2: 0.866  loss_mask: 0.0701  loss_rpn_cls: 0.01091  loss_rpn_loc: 0.04072  time: 0.6241  data_time: 0.0054  lr: 0.00016  max_mem: 19679M
[07/29 16:25:35] d2.utils.events INFO:  eta: 8:16:24  iter: 20439  total_loss: 2.963  loss_cls_stage0: 0.3095  loss_box_reg_stage0: 0.2832  loss_cls_stage1: 0.3403  loss_box_reg_stage1: 0.6833  loss_cls_stage2: 0.3249  loss_box_reg_stage2: 0.9507  loss_mask: 0.07888  loss_rpn_cls: 0.01686  loss_rpn_loc: 0.04439  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:25:48] d2.utils.events INFO:  eta: 8:16:13  iter: 20459  total_loss: 2.912  loss_cls_stage0: 0.2787  loss_box_reg_stage0: 0.2836  loss_cls_stage1: 0.2932  loss_box_reg_stage1: 0.6805  loss_cls_stage2: 0.2798  loss_box_reg_stage2: 0.8927  loss_mask: 0.0793  loss_rpn_cls: 0.01402  loss_rpn_loc: 0.05076  time: 0.6241  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 16:26:00] d2.utils.events INFO:  eta: 8:15:52  iter: 20479  total_loss: 3.119  loss_cls_stage0: 0.2935  loss_box_reg_stage0: 0.2896  loss_cls_stage1: 0.3155  loss_box_reg_stage1: 0.7275  loss_cls_stage2: 0.3169  loss_box_reg_stage2: 0.9592  loss_mask: 0.08482  loss_rpn_cls: 0.01317  loss_rpn_loc: 0.05245  time: 0.6241  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 16:26:13] d2.utils.events INFO:  eta: 8:15:43  iter: 20499  total_loss: 3.145  loss_cls_stage0: 0.2763  loss_box_reg_stage0: 0.2724  loss_cls_stage1: 0.3137  loss_box_reg_stage1: 0.7479  loss_cls_stage2: 0.3061  loss_box_reg_stage2: 0.9481  loss_mask: 0.07727  loss_rpn_cls: 0.01414  loss_rpn_loc: 0.05847  time: 0.6241  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 16:26:25] d2.utils.events INFO:  eta: 8:15:16  iter: 20519  total_loss: 2.963  loss_cls_stage0: 0.2686  loss_box_reg_stage0: 0.2675  loss_cls_stage1: 0.282  loss_box_reg_stage1: 0.6984  loss_cls_stage2: 0.2839  loss_box_reg_stage2: 0.9222  loss_mask: 0.07701  loss_rpn_cls: 0.01829  loss_rpn_loc: 0.03949  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:26:37] d2.utils.events INFO:  eta: 8:15:05  iter: 20539  total_loss: 2.925  loss_cls_stage0: 0.2654  loss_box_reg_stage0: 0.2751  loss_cls_stage1: 0.2871  loss_box_reg_stage1: 0.7138  loss_cls_stage2: 0.2798  loss_box_reg_stage2: 0.9548  loss_mask: 0.07257  loss_rpn_cls: 0.0237  loss_rpn_loc: 0.05084  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:26:50] d2.utils.events INFO:  eta: 8:14:37  iter: 20559  total_loss: 2.836  loss_cls_stage0: 0.2644  loss_box_reg_stage0: 0.2658  loss_cls_stage1: 0.2722  loss_box_reg_stage1: 0.6808  loss_cls_stage2: 0.2656  loss_box_reg_stage2: 0.908  loss_mask: 0.07941  loss_rpn_cls: 0.01624  loss_rpn_loc: 0.04635  time: 0.6241  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 16:27:03] d2.utils.events INFO:  eta: 8:14:42  iter: 20579  total_loss: 2.898  loss_cls_stage0: 0.2412  loss_box_reg_stage0: 0.2358  loss_cls_stage1: 0.2941  loss_box_reg_stage1: 0.722  loss_cls_stage2: 0.2664  loss_box_reg_stage2: 0.9309  loss_mask: 0.07717  loss_rpn_cls: 0.0172  loss_rpn_loc: 0.05161  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:27:16] d2.utils.events INFO:  eta: 8:14:35  iter: 20599  total_loss: 2.916  loss_cls_stage0: 0.2695  loss_box_reg_stage0: 0.2625  loss_cls_stage1: 0.311  loss_box_reg_stage1: 0.711  loss_cls_stage2: 0.2999  loss_box_reg_stage2: 0.9774  loss_mask: 0.07935  loss_rpn_cls: 0.01351  loss_rpn_loc: 0.0479  time: 0.6241  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 16:27:28] d2.utils.events INFO:  eta: 8:14:22  iter: 20619  total_loss: 2.841  loss_cls_stage0: 0.27  loss_box_reg_stage0: 0.2837  loss_cls_stage1: 0.2777  loss_box_reg_stage1: 0.6621  loss_cls_stage2: 0.2752  loss_box_reg_stage2: 0.8941  loss_mask: 0.07392  loss_rpn_cls: 0.01643  loss_rpn_loc: 0.05352  time: 0.6242  data_time: 0.0054  lr: 0.00016  max_mem: 19679M
[07/29 16:27:41] d2.utils.events INFO:  eta: 8:14:06  iter: 20639  total_loss: 2.644  loss_cls_stage0: 0.2334  loss_box_reg_stage0: 0.247  loss_cls_stage1: 0.2545  loss_box_reg_stage1: 0.6575  loss_cls_stage2: 0.2266  loss_box_reg_stage2: 0.9236  loss_mask: 0.07845  loss_rpn_cls: 0.01169  loss_rpn_loc: 0.04466  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:27:53] d2.utils.events INFO:  eta: 8:13:50  iter: 20659  total_loss: 2.93  loss_cls_stage0: 0.2907  loss_box_reg_stage0: 0.2806  loss_cls_stage1: 0.3035  loss_box_reg_stage1: 0.6977  loss_cls_stage2: 0.2988  loss_box_reg_stage2: 0.9607  loss_mask: 0.08651  loss_rpn_cls: 0.01809  loss_rpn_loc: 0.05614  time: 0.6241  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 16:28:06] d2.utils.events INFO:  eta: 8:13:40  iter: 20679  total_loss: 2.702  loss_cls_stage0: 0.2736  loss_box_reg_stage0: 0.2583  loss_cls_stage1: 0.2767  loss_box_reg_stage1: 0.6739  loss_cls_stage2: 0.2347  loss_box_reg_stage2: 0.8697  loss_mask: 0.07422  loss_rpn_cls: 0.01114  loss_rpn_loc: 0.06421  time: 0.6241  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 16:28:18] d2.utils.events INFO:  eta: 8:13:27  iter: 20699  total_loss: 2.817  loss_cls_stage0: 0.2558  loss_box_reg_stage0: 0.2679  loss_cls_stage1: 0.2561  loss_box_reg_stage1: 0.6619  loss_cls_stage2: 0.2642  loss_box_reg_stage2: 0.8678  loss_mask: 0.07061  loss_rpn_cls: 0.0118  loss_rpn_loc: 0.05335  time: 0.6241  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 16:28:31] d2.utils.events INFO:  eta: 8:13:12  iter: 20719  total_loss: 2.559  loss_cls_stage0: 0.2596  loss_box_reg_stage0: 0.2703  loss_cls_stage1: 0.2639  loss_box_reg_stage1: 0.6228  loss_cls_stage2: 0.2535  loss_box_reg_stage2: 0.8467  loss_mask: 0.0755  loss_rpn_cls: 0.02274  loss_rpn_loc: 0.06402  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:28:43] d2.utils.events INFO:  eta: 8:12:39  iter: 20739  total_loss: 2.677  loss_cls_stage0: 0.2549  loss_box_reg_stage0: 0.2728  loss_cls_stage1: 0.2474  loss_box_reg_stage1: 0.6315  loss_cls_stage2: 0.2275  loss_box_reg_stage2: 0.8502  loss_mask: 0.0854  loss_rpn_cls: 0.01812  loss_rpn_loc: 0.04145  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:28:56] d2.utils.events INFO:  eta: 8:12:31  iter: 20759  total_loss: 2.635  loss_cls_stage0: 0.2499  loss_box_reg_stage0: 0.2571  loss_cls_stage1: 0.2591  loss_box_reg_stage1: 0.6422  loss_cls_stage2: 0.2584  loss_box_reg_stage2: 0.8617  loss_mask: 0.07365  loss_rpn_cls: 0.0158  loss_rpn_loc: 0.04358  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:29:09] d2.utils.events INFO:  eta: 8:12:29  iter: 20779  total_loss: 2.659  loss_cls_stage0: 0.2394  loss_box_reg_stage0: 0.2615  loss_cls_stage1: 0.2539  loss_box_reg_stage1: 0.6218  loss_cls_stage2: 0.2546  loss_box_reg_stage2: 0.8571  loss_mask: 0.07629  loss_rpn_cls: 0.02038  loss_rpn_loc: 0.05782  time: 0.6242  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:29:21] d2.utils.events INFO:  eta: 8:12:02  iter: 20799  total_loss: 2.597  loss_cls_stage0: 0.2212  loss_box_reg_stage0: 0.2261  loss_cls_stage1: 0.2514  loss_box_reg_stage1: 0.5844  loss_cls_stage2: 0.233  loss_box_reg_stage2: 0.8589  loss_mask: 0.06532  loss_rpn_cls: 0.01482  loss_rpn_loc: 0.04729  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:29:33] d2.utils.events INFO:  eta: 8:10:55  iter: 20819  total_loss: 2.648  loss_cls_stage0: 0.2429  loss_box_reg_stage0: 0.2543  loss_cls_stage1: 0.2558  loss_box_reg_stage1: 0.6573  loss_cls_stage2: 0.2216  loss_box_reg_stage2: 0.8372  loss_mask: 0.07788  loss_rpn_cls: 0.01092  loss_rpn_loc: 0.03796  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:29:45] d2.utils.events INFO:  eta: 8:10:33  iter: 20839  total_loss: 2.799  loss_cls_stage0: 0.2427  loss_box_reg_stage0: 0.2742  loss_cls_stage1: 0.2485  loss_box_reg_stage1: 0.6791  loss_cls_stage2: 0.2618  loss_box_reg_stage2: 0.9143  loss_mask: 0.07262  loss_rpn_cls: 0.01005  loss_rpn_loc: 0.04361  time: 0.6241  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:29:58] d2.utils.events INFO:  eta: 8:10:12  iter: 20859  total_loss: 2.641  loss_cls_stage0: 0.2259  loss_box_reg_stage0: 0.2431  loss_cls_stage1: 0.2506  loss_box_reg_stage1: 0.6184  loss_cls_stage2: 0.247  loss_box_reg_stage2: 0.8196  loss_mask: 0.0772  loss_rpn_cls: 0.0156  loss_rpn_loc: 0.05074  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:30:10] d2.utils.events INFO:  eta: 8:09:51  iter: 20879  total_loss: 2.954  loss_cls_stage0: 0.2614  loss_box_reg_stage0: 0.2604  loss_cls_stage1: 0.2884  loss_box_reg_stage1: 0.6983  loss_cls_stage2: 0.2858  loss_box_reg_stage2: 0.9572  loss_mask: 0.0742  loss_rpn_cls: 0.01302  loss_rpn_loc: 0.0483  time: 0.6241  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 16:30:22] d2.utils.events INFO:  eta: 8:09:29  iter: 20899  total_loss: 3.098  loss_cls_stage0: 0.2912  loss_box_reg_stage0: 0.2798  loss_cls_stage1: 0.3058  loss_box_reg_stage1: 0.768  loss_cls_stage2: 0.3266  loss_box_reg_stage2: 1.065  loss_mask: 0.08003  loss_rpn_cls: 0.01392  loss_rpn_loc: 0.04434  time: 0.6241  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:30:35] d2.utils.events INFO:  eta: 8:09:20  iter: 20919  total_loss: 3.086  loss_cls_stage0: 0.2931  loss_box_reg_stage0: 0.2792  loss_cls_stage1: 0.3163  loss_box_reg_stage1: 0.739  loss_cls_stage2: 0.2943  loss_box_reg_stage2: 0.9733  loss_mask: 0.07929  loss_rpn_cls: 0.01552  loss_rpn_loc: 0.05049  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:30:47] d2.utils.events INFO:  eta: 8:09:03  iter: 20939  total_loss: 2.92  loss_cls_stage0: 0.2744  loss_box_reg_stage0: 0.2655  loss_cls_stage1: 0.3148  loss_box_reg_stage1: 0.7129  loss_cls_stage2: 0.2935  loss_box_reg_stage2: 0.8966  loss_mask: 0.08106  loss_rpn_cls: 0.01512  loss_rpn_loc: 0.06416  time: 0.6240  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:31:00] d2.utils.events INFO:  eta: 8:08:54  iter: 20959  total_loss: 2.882  loss_cls_stage0: 0.2825  loss_box_reg_stage0: 0.26  loss_cls_stage1: 0.285  loss_box_reg_stage1: 0.6667  loss_cls_stage2: 0.2833  loss_box_reg_stage2: 0.9365  loss_mask: 0.08176  loss_rpn_cls: 0.0188  loss_rpn_loc: 0.08126  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:31:12] d2.utils.events INFO:  eta: 8:08:48  iter: 20979  total_loss: 2.663  loss_cls_stage0: 0.2598  loss_box_reg_stage0: 0.2446  loss_cls_stage1: 0.2905  loss_box_reg_stage1: 0.6264  loss_cls_stage2: 0.2619  loss_box_reg_stage2: 0.8531  loss_mask: 0.07071  loss_rpn_cls: 0.01433  loss_rpn_loc: 0.05488  time: 0.6241  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 16:31:25] d2.utils.events INFO:  eta: 8:08:45  iter: 20999  total_loss: 2.686  loss_cls_stage0: 0.2276  loss_box_reg_stage0: 0.2403  loss_cls_stage1: 0.247  loss_box_reg_stage1: 0.6401  loss_cls_stage2: 0.272  loss_box_reg_stage2: 0.9152  loss_mask: 0.06503  loss_rpn_cls: 0.01563  loss_rpn_loc: 0.04266  time: 0.6241  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:31:37] d2.utils.events INFO:  eta: 8:08:33  iter: 21019  total_loss: 2.766  loss_cls_stage0: 0.2273  loss_box_reg_stage0: 0.2542  loss_cls_stage1: 0.2848  loss_box_reg_stage1: 0.6882  loss_cls_stage2: 0.2925  loss_box_reg_stage2: 0.8702  loss_mask: 0.07367  loss_rpn_cls: 0.01373  loss_rpn_loc: 0.03782  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:31:50] d2.utils.events INFO:  eta: 8:08:26  iter: 21039  total_loss: 3.052  loss_cls_stage0: 0.3019  loss_box_reg_stage0: 0.2859  loss_cls_stage1: 0.3308  loss_box_reg_stage1: 0.7195  loss_cls_stage2: 0.3023  loss_box_reg_stage2: 0.934  loss_mask: 0.08489  loss_rpn_cls: 0.01078  loss_rpn_loc: 0.04586  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:32:02] d2.utils.events INFO:  eta: 8:08:21  iter: 21059  total_loss: 3.056  loss_cls_stage0: 0.2687  loss_box_reg_stage0: 0.2911  loss_cls_stage1: 0.2927  loss_box_reg_stage1: 0.7397  loss_cls_stage2: 0.2871  loss_box_reg_stage2: 1.003  loss_mask: 0.08121  loss_rpn_cls: 0.01461  loss_rpn_loc: 0.05398  time: 0.6241  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 16:32:15] d2.utils.events INFO:  eta: 8:07:45  iter: 21079  total_loss: 2.834  loss_cls_stage0: 0.2829  loss_box_reg_stage0: 0.2729  loss_cls_stage1: 0.2881  loss_box_reg_stage1: 0.6928  loss_cls_stage2: 0.277  loss_box_reg_stage2: 0.9342  loss_mask: 0.0772  loss_rpn_cls: 0.01805  loss_rpn_loc: 0.04094  time: 0.6241  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 16:32:27] d2.utils.events INFO:  eta: 8:07:49  iter: 21099  total_loss: 2.893  loss_cls_stage0: 0.2676  loss_box_reg_stage0: 0.2574  loss_cls_stage1: 0.2925  loss_box_reg_stage1: 0.6783  loss_cls_stage2: 0.2851  loss_box_reg_stage2: 0.9115  loss_mask: 0.08504  loss_rpn_cls: 0.01402  loss_rpn_loc: 0.04271  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:32:40] d2.utils.events INFO:  eta: 8:07:26  iter: 21119  total_loss: 3.049  loss_cls_stage0: 0.2969  loss_box_reg_stage0: 0.2816  loss_cls_stage1: 0.3115  loss_box_reg_stage1: 0.7227  loss_cls_stage2: 0.2867  loss_box_reg_stage2: 0.8896  loss_mask: 0.07909  loss_rpn_cls: 0.0221  loss_rpn_loc: 0.0681  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:32:52] d2.utils.events INFO:  eta: 8:07:13  iter: 21139  total_loss: 2.955  loss_cls_stage0: 0.285  loss_box_reg_stage0: 0.268  loss_cls_stage1: 0.3232  loss_box_reg_stage1: 0.71  loss_cls_stage2: 0.2994  loss_box_reg_stage2: 0.9294  loss_mask: 0.07872  loss_rpn_cls: 0.01608  loss_rpn_loc: 0.04519  time: 0.6240  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:33:04] d2.utils.events INFO:  eta: 8:07:40  iter: 21159  total_loss: 2.97  loss_cls_stage0: 0.2823  loss_box_reg_stage0: 0.2611  loss_cls_stage1: 0.2882  loss_box_reg_stage1: 0.7028  loss_cls_stage2: 0.2704  loss_box_reg_stage2: 0.964  loss_mask: 0.07147  loss_rpn_cls: 0.01349  loss_rpn_loc: 0.06652  time: 0.6241  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:33:17] d2.utils.events INFO:  eta: 8:07:28  iter: 21179  total_loss: 2.704  loss_cls_stage0: 0.2565  loss_box_reg_stage0: 0.258  loss_cls_stage1: 0.265  loss_box_reg_stage1: 0.6541  loss_cls_stage2: 0.2551  loss_box_reg_stage2: 0.8924  loss_mask: 0.07461  loss_rpn_cls: 0.01502  loss_rpn_loc: 0.04482  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:33:29] d2.utils.events INFO:  eta: 8:07:08  iter: 21199  total_loss: 2.931  loss_cls_stage0: 0.2608  loss_box_reg_stage0: 0.2664  loss_cls_stage1: 0.2778  loss_box_reg_stage1: 0.6928  loss_cls_stage2: 0.2505  loss_box_reg_stage2: 0.951  loss_mask: 0.07412  loss_rpn_cls: 0.01437  loss_rpn_loc: 0.04749  time: 0.6240  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 16:33:42] d2.utils.events INFO:  eta: 8:06:56  iter: 21219  total_loss: 3.216  loss_cls_stage0: 0.2787  loss_box_reg_stage0: 0.2669  loss_cls_stage1: 0.3026  loss_box_reg_stage1: 0.7381  loss_cls_stage2: 0.2895  loss_box_reg_stage2: 1.063  loss_mask: 0.0753  loss_rpn_cls: 0.01859  loss_rpn_loc: 0.04856  time: 0.6241  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:33:54] d2.utils.events INFO:  eta: 8:06:08  iter: 21239  total_loss: 2.897  loss_cls_stage0: 0.2934  loss_box_reg_stage0: 0.2752  loss_cls_stage1: 0.3103  loss_box_reg_stage1: 0.6814  loss_cls_stage2: 0.3055  loss_box_reg_stage2: 0.9186  loss_mask: 0.07924  loss_rpn_cls: 0.01913  loss_rpn_loc: 0.05161  time: 0.6240  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 16:34:07] d2.utils.events INFO:  eta: 8:06:04  iter: 21259  total_loss: 3.165  loss_cls_stage0: 0.2808  loss_box_reg_stage0: 0.2823  loss_cls_stage1: 0.293  loss_box_reg_stage1: 0.7448  loss_cls_stage2: 0.303  loss_box_reg_stage2: 1.034  loss_mask: 0.07224  loss_rpn_cls: 0.01921  loss_rpn_loc: 0.04478  time: 0.6241  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 16:34:20] d2.utils.events INFO:  eta: 8:05:50  iter: 21279  total_loss: 2.817  loss_cls_stage0: 0.2782  loss_box_reg_stage0: 0.2646  loss_cls_stage1: 0.2885  loss_box_reg_stage1: 0.6421  loss_cls_stage2: 0.2719  loss_box_reg_stage2: 0.8906  loss_mask: 0.08402  loss_rpn_cls: 0.01561  loss_rpn_loc: 0.04587  time: 0.6241  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 16:34:33] d2.utils.events INFO:  eta: 8:05:33  iter: 21299  total_loss: 3.339  loss_cls_stage0: 0.2966  loss_box_reg_stage0: 0.3312  loss_cls_stage1: 0.3081  loss_box_reg_stage1: 0.778  loss_cls_stage2: 0.3035  loss_box_reg_stage2: 1.033  loss_mask: 0.09215  loss_rpn_cls: 0.01632  loss_rpn_loc: 0.05598  time: 0.6241  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 16:34:45] d2.utils.events INFO:  eta: 8:05:01  iter: 21319  total_loss: 3.044  loss_cls_stage0: 0.2723  loss_box_reg_stage0: 0.2633  loss_cls_stage1: 0.2819  loss_box_reg_stage1: 0.7197  loss_cls_stage2: 0.2733  loss_box_reg_stage2: 0.966  loss_mask: 0.07823  loss_rpn_cls: 0.01139  loss_rpn_loc: 0.03901  time: 0.6241  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:34:58] d2.utils.events INFO:  eta: 8:05:08  iter: 21339  total_loss: 3.227  loss_cls_stage0: 0.2805  loss_box_reg_stage0: 0.2611  loss_cls_stage1: 0.3319  loss_box_reg_stage1: 0.7661  loss_cls_stage2: 0.3283  loss_box_reg_stage2: 1.09  loss_mask: 0.07362  loss_rpn_cls: 0.01063  loss_rpn_loc: 0.04347  time: 0.6241  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 16:35:10] d2.utils.events INFO:  eta: 8:05:03  iter: 21359  total_loss: 3.164  loss_cls_stage0: 0.2889  loss_box_reg_stage0: 0.2825  loss_cls_stage1: 0.3149  loss_box_reg_stage1: 0.7286  loss_cls_stage2: 0.3076  loss_box_reg_stage2: 1.015  loss_mask: 0.08361  loss_rpn_cls: 0.01787  loss_rpn_loc: 0.06136  time: 0.6241  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:35:23] d2.utils.events INFO:  eta: 8:04:51  iter: 21379  total_loss: 2.875  loss_cls_stage0: 0.2654  loss_box_reg_stage0: 0.2561  loss_cls_stage1: 0.2913  loss_box_reg_stage1: 0.6555  loss_cls_stage2: 0.2876  loss_box_reg_stage2: 0.9069  loss_mask: 0.0803  loss_rpn_cls: 0.02188  loss_rpn_loc: 0.08278  time: 0.6241  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:35:36] d2.utils.events INFO:  eta: 8:04:41  iter: 21399  total_loss: 2.49  loss_cls_stage0: 0.2152  loss_box_reg_stage0: 0.2384  loss_cls_stage1: 0.2422  loss_box_reg_stage1: 0.5762  loss_cls_stage2: 0.2475  loss_box_reg_stage2: 0.8744  loss_mask: 0.06915  loss_rpn_cls: 0.01622  loss_rpn_loc: 0.03958  time: 0.6241  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 16:35:48] d2.utils.events INFO:  eta: 8:04:28  iter: 21419  total_loss: 2.956  loss_cls_stage0: 0.2763  loss_box_reg_stage0: 0.2442  loss_cls_stage1: 0.327  loss_box_reg_stage1: 0.7006  loss_cls_stage2: 0.3152  loss_box_reg_stage2: 0.9179  loss_mask: 0.0774  loss_rpn_cls: 0.01659  loss_rpn_loc: 0.06033  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:36:01] d2.utils.events INFO:  eta: 8:04:14  iter: 21439  total_loss: 2.781  loss_cls_stage0: 0.2959  loss_box_reg_stage0: 0.2718  loss_cls_stage1: 0.3147  loss_box_reg_stage1: 0.6862  loss_cls_stage2: 0.2923  loss_box_reg_stage2: 0.9071  loss_mask: 0.08106  loss_rpn_cls: 0.01485  loss_rpn_loc: 0.04421  time: 0.6241  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:36:13] d2.utils.events INFO:  eta: 8:03:57  iter: 21459  total_loss: 2.857  loss_cls_stage0: 0.2457  loss_box_reg_stage0: 0.2589  loss_cls_stage1: 0.2855  loss_box_reg_stage1: 0.729  loss_cls_stage2: 0.2747  loss_box_reg_stage2: 0.8972  loss_mask: 0.07367  loss_rpn_cls: 0.01473  loss_rpn_loc: 0.03875  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:36:26] d2.utils.events INFO:  eta: 8:04:13  iter: 21479  total_loss: 2.706  loss_cls_stage0: 0.2591  loss_box_reg_stage0: 0.2618  loss_cls_stage1: 0.2872  loss_box_reg_stage1: 0.7004  loss_cls_stage2: 0.2582  loss_box_reg_stage2: 0.9439  loss_mask: 0.0752  loss_rpn_cls: 0.01675  loss_rpn_loc: 0.05432  time: 0.6242  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 16:36:39] d2.utils.events INFO:  eta: 8:04:04  iter: 21499  total_loss: 3.036  loss_cls_stage0: 0.2917  loss_box_reg_stage0: 0.2755  loss_cls_stage1: 0.3311  loss_box_reg_stage1: 0.7011  loss_cls_stage2: 0.3085  loss_box_reg_stage2: 0.982  loss_mask: 0.07683  loss_rpn_cls: 0.01926  loss_rpn_loc: 0.05769  time: 0.6242  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:36:51] d2.utils.events INFO:  eta: 8:03:48  iter: 21519  total_loss: 3.027  loss_cls_stage0: 0.2994  loss_box_reg_stage0: 0.2722  loss_cls_stage1: 0.3156  loss_box_reg_stage1: 0.7188  loss_cls_stage2: 0.3002  loss_box_reg_stage2: 1.008  loss_mask: 0.08443  loss_rpn_cls: 0.01841  loss_rpn_loc: 0.06605  time: 0.6242  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:37:04] d2.utils.events INFO:  eta: 8:03:25  iter: 21539  total_loss: 2.956  loss_cls_stage0: 0.2515  loss_box_reg_stage0: 0.2727  loss_cls_stage1: 0.2957  loss_box_reg_stage1: 0.7122  loss_cls_stage2: 0.2793  loss_box_reg_stage2: 0.9489  loss_mask: 0.08178  loss_rpn_cls: 0.01077  loss_rpn_loc: 0.04862  time: 0.6241  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 16:37:16] d2.utils.events INFO:  eta: 8:03:21  iter: 21559  total_loss: 3.071  loss_cls_stage0: 0.277  loss_box_reg_stage0: 0.2802  loss_cls_stage1: 0.276  loss_box_reg_stage1: 0.6914  loss_cls_stage2: 0.262  loss_box_reg_stage2: 0.8629  loss_mask: 0.08278  loss_rpn_cls: 0.01317  loss_rpn_loc: 0.04684  time: 0.6242  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:37:29] d2.utils.events INFO:  eta: 8:02:52  iter: 21579  total_loss: 2.671  loss_cls_stage0: 0.2586  loss_box_reg_stage0: 0.2545  loss_cls_stage1: 0.2597  loss_box_reg_stage1: 0.6597  loss_cls_stage2: 0.2577  loss_box_reg_stage2: 0.8773  loss_mask: 0.07199  loss_rpn_cls: 0.01312  loss_rpn_loc: 0.03997  time: 0.6242  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:37:41] d2.utils.events INFO:  eta: 8:02:33  iter: 21599  total_loss: 2.881  loss_cls_stage0: 0.2534  loss_box_reg_stage0: 0.2482  loss_cls_stage1: 0.3013  loss_box_reg_stage1: 0.7062  loss_cls_stage2: 0.2958  loss_box_reg_stage2: 0.9378  loss_mask: 0.07136  loss_rpn_cls: 0.009923  loss_rpn_loc: 0.04114  time: 0.6242  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 16:37:54] d2.utils.events INFO:  eta: 8:02:20  iter: 21619  total_loss: 2.713  loss_cls_stage0: 0.2603  loss_box_reg_stage0: 0.2544  loss_cls_stage1: 0.2685  loss_box_reg_stage1: 0.6569  loss_cls_stage2: 0.2566  loss_box_reg_stage2: 0.9153  loss_mask: 0.07744  loss_rpn_cls: 0.01156  loss_rpn_loc: 0.04312  time: 0.6242  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:38:06] d2.utils.events INFO:  eta: 8:01:59  iter: 21639  total_loss: 2.612  loss_cls_stage0: 0.2251  loss_box_reg_stage0: 0.2304  loss_cls_stage1: 0.2554  loss_box_reg_stage1: 0.651  loss_cls_stage2: 0.2533  loss_box_reg_stage2: 0.8572  loss_mask: 0.07255  loss_rpn_cls: 0.01149  loss_rpn_loc: 0.04873  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:38:18] d2.utils.events INFO:  eta: 8:01:50  iter: 21659  total_loss: 2.987  loss_cls_stage0: 0.2756  loss_box_reg_stage0: 0.2838  loss_cls_stage1: 0.2896  loss_box_reg_stage1: 0.7353  loss_cls_stage2: 0.2781  loss_box_reg_stage2: 0.9339  loss_mask: 0.08127  loss_rpn_cls: 0.01417  loss_rpn_loc: 0.04707  time: 0.6241  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 16:38:31] d2.utils.events INFO:  eta: 8:01:10  iter: 21679  total_loss: 2.623  loss_cls_stage0: 0.2338  loss_box_reg_stage0: 0.263  loss_cls_stage1: 0.2461  loss_box_reg_stage1: 0.6215  loss_cls_stage2: 0.2553  loss_box_reg_stage2: 0.8216  loss_mask: 0.07943  loss_rpn_cls: 0.01509  loss_rpn_loc: 0.03931  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:38:43] d2.utils.events INFO:  eta: 8:01:22  iter: 21699  total_loss: 2.978  loss_cls_stage0: 0.2548  loss_box_reg_stage0: 0.2632  loss_cls_stage1: 0.2794  loss_box_reg_stage1: 0.7204  loss_cls_stage2: 0.2745  loss_box_reg_stage2: 0.9499  loss_mask: 0.07024  loss_rpn_cls: 0.007676  loss_rpn_loc: 0.03671  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:38:56] d2.utils.events INFO:  eta: 8:01:15  iter: 21719  total_loss: 3.015  loss_cls_stage0: 0.276  loss_box_reg_stage0: 0.2764  loss_cls_stage1: 0.3085  loss_box_reg_stage1: 0.6989  loss_cls_stage2: 0.2839  loss_box_reg_stage2: 0.9691  loss_mask: 0.08336  loss_rpn_cls: 0.01262  loss_rpn_loc: 0.04777  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:39:08] d2.utils.events INFO:  eta: 8:00:55  iter: 21739  total_loss: 2.841  loss_cls_stage0: 0.2731  loss_box_reg_stage0: 0.2545  loss_cls_stage1: 0.2904  loss_box_reg_stage1: 0.6681  loss_cls_stage2: 0.2478  loss_box_reg_stage2: 0.8981  loss_mask: 0.07686  loss_rpn_cls: 0.01917  loss_rpn_loc: 0.04928  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:39:21] d2.utils.events INFO:  eta: 8:00:19  iter: 21759  total_loss: 2.97  loss_cls_stage0: 0.2728  loss_box_reg_stage0: 0.2832  loss_cls_stage1: 0.2906  loss_box_reg_stage1: 0.6999  loss_cls_stage2: 0.2921  loss_box_reg_stage2: 0.9771  loss_mask: 0.07452  loss_rpn_cls: 0.0203  loss_rpn_loc: 0.04651  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:39:33] d2.utils.events INFO:  eta: 8:00:05  iter: 21779  total_loss: 2.628  loss_cls_stage0: 0.2458  loss_box_reg_stage0: 0.238  loss_cls_stage1: 0.2338  loss_box_reg_stage1: 0.6445  loss_cls_stage2: 0.2271  loss_box_reg_stage2: 0.8488  loss_mask: 0.07098  loss_rpn_cls: 0.01616  loss_rpn_loc: 0.05499  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:39:46] d2.utils.events INFO:  eta: 8:00:03  iter: 21799  total_loss: 2.88  loss_cls_stage0: 0.2541  loss_box_reg_stage0: 0.2752  loss_cls_stage1: 0.2442  loss_box_reg_stage1: 0.6788  loss_cls_stage2: 0.2542  loss_box_reg_stage2: 0.9248  loss_mask: 0.08132  loss_rpn_cls: 0.01741  loss_rpn_loc: 0.06951  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:39:58] d2.utils.events INFO:  eta: 7:59:50  iter: 21819  total_loss: 3.219  loss_cls_stage0: 0.2832  loss_box_reg_stage0: 0.2885  loss_cls_stage1: 0.3349  loss_box_reg_stage1: 0.7627  loss_cls_stage2: 0.3316  loss_box_reg_stage2: 1.051  loss_mask: 0.08488  loss_rpn_cls: 0.01893  loss_rpn_loc: 0.05456  time: 0.6241  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 16:40:11] d2.utils.events INFO:  eta: 7:59:52  iter: 21839  total_loss: 2.904  loss_cls_stage0: 0.2646  loss_box_reg_stage0: 0.2592  loss_cls_stage1: 0.2708  loss_box_reg_stage1: 0.7127  loss_cls_stage2: 0.2717  loss_box_reg_stage2: 0.9556  loss_mask: 0.07702  loss_rpn_cls: 0.01121  loss_rpn_loc: 0.05468  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:40:23] d2.utils.events INFO:  eta: 7:59:40  iter: 21859  total_loss: 2.859  loss_cls_stage0: 0.2445  loss_box_reg_stage0: 0.2717  loss_cls_stage1: 0.2867  loss_box_reg_stage1: 0.6138  loss_cls_stage2: 0.2958  loss_box_reg_stage2: 0.8593  loss_mask: 0.07154  loss_rpn_cls: 0.02225  loss_rpn_loc: 0.07454  time: 0.6241  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:40:36] d2.utils.events INFO:  eta: 7:59:27  iter: 21879  total_loss: 2.746  loss_cls_stage0: 0.2582  loss_box_reg_stage0: 0.2485  loss_cls_stage1: 0.2481  loss_box_reg_stage1: 0.6614  loss_cls_stage2: 0.236  loss_box_reg_stage2: 0.8473  loss_mask: 0.07266  loss_rpn_cls: 0.01473  loss_rpn_loc: 0.05107  time: 0.6241  data_time: 0.0055  lr: 0.00016  max_mem: 19679M
[07/29 16:40:48] d2.utils.events INFO:  eta: 7:59:16  iter: 21899  total_loss: 2.579  loss_cls_stage0: 0.2361  loss_box_reg_stage0: 0.2424  loss_cls_stage1: 0.252  loss_box_reg_stage1: 0.6708  loss_cls_stage2: 0.2389  loss_box_reg_stage2: 0.8903  loss_mask: 0.07259  loss_rpn_cls: 0.01112  loss_rpn_loc: 0.04508  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:41:00] d2.utils.events INFO:  eta: 7:58:46  iter: 21919  total_loss: 2.931  loss_cls_stage0: 0.2771  loss_box_reg_stage0: 0.2786  loss_cls_stage1: 0.2958  loss_box_reg_stage1: 0.6983  loss_cls_stage2: 0.2984  loss_box_reg_stage2: 0.9204  loss_mask: 0.0723  loss_rpn_cls: 0.01156  loss_rpn_loc: 0.04187  time: 0.6241  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:41:13] d2.utils.events INFO:  eta: 7:58:53  iter: 21939  total_loss: 3.064  loss_cls_stage0: 0.2791  loss_box_reg_stage0: 0.2785  loss_cls_stage1: 0.3143  loss_box_reg_stage1: 0.6989  loss_cls_stage2: 0.2988  loss_box_reg_stage2: 0.8919  loss_mask: 0.07424  loss_rpn_cls: 0.01192  loss_rpn_loc: 0.05912  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:41:26] d2.utils.events INFO:  eta: 7:58:30  iter: 21959  total_loss: 2.942  loss_cls_stage0: 0.3146  loss_box_reg_stage0: 0.2932  loss_cls_stage1: 0.3056  loss_box_reg_stage1: 0.6578  loss_cls_stage2: 0.2799  loss_box_reg_stage2: 0.8918  loss_mask: 0.07811  loss_rpn_cls: 0.01618  loss_rpn_loc: 0.04995  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:41:39] d2.utils.events INFO:  eta: 7:58:24  iter: 21979  total_loss: 2.705  loss_cls_stage0: 0.2524  loss_box_reg_stage0: 0.2415  loss_cls_stage1: 0.2679  loss_box_reg_stage1: 0.6509  loss_cls_stage2: 0.2571  loss_box_reg_stage2: 0.9054  loss_mask: 0.07427  loss_rpn_cls: 0.0136  loss_rpn_loc: 0.04022  time: 0.6241  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:41:52] d2.utils.events INFO:  eta: 7:58:20  iter: 21999  total_loss: 2.794  loss_cls_stage0: 0.2527  loss_box_reg_stage0: 0.2464  loss_cls_stage1: 0.2667  loss_box_reg_stage1: 0.6616  loss_cls_stage2: 0.2656  loss_box_reg_stage2: 0.918  loss_mask: 0.07397  loss_rpn_cls: 0.01436  loss_rpn_loc: 0.03764  time: 0.6242  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 16:42:04] d2.utils.events INFO:  eta: 7:58:09  iter: 22019  total_loss: 2.845  loss_cls_stage0: 0.2663  loss_box_reg_stage0: 0.268  loss_cls_stage1: 0.293  loss_box_reg_stage1: 0.7091  loss_cls_stage2: 0.2686  loss_box_reg_stage2: 0.9273  loss_mask: 0.07699  loss_rpn_cls: 0.01572  loss_rpn_loc: 0.05659  time: 0.6242  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 16:42:17] d2.utils.events INFO:  eta: 7:58:02  iter: 22039  total_loss: 2.797  loss_cls_stage0: 0.2566  loss_box_reg_stage0: 0.2562  loss_cls_stage1: 0.2809  loss_box_reg_stage1: 0.6792  loss_cls_stage2: 0.243  loss_box_reg_stage2: 0.906  loss_mask: 0.07246  loss_rpn_cls: 0.01429  loss_rpn_loc: 0.04974  time: 0.6242  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:42:30] d2.utils.events INFO:  eta: 7:57:55  iter: 22059  total_loss: 2.911  loss_cls_stage0: 0.2828  loss_box_reg_stage0: 0.2679  loss_cls_stage1: 0.2835  loss_box_reg_stage1: 0.7112  loss_cls_stage2: 0.227  loss_box_reg_stage2: 0.9168  loss_mask: 0.07396  loss_rpn_cls: 0.01426  loss_rpn_loc: 0.05277  time: 0.6242  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 16:42:42] d2.utils.events INFO:  eta: 7:58:19  iter: 22079  total_loss: 2.669  loss_cls_stage0: 0.2336  loss_box_reg_stage0: 0.2317  loss_cls_stage1: 0.2617  loss_box_reg_stage1: 0.6112  loss_cls_stage2: 0.2574  loss_box_reg_stage2: 0.9119  loss_mask: 0.07158  loss_rpn_cls: 0.01189  loss_rpn_loc: 0.04062  time: 0.6242  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:42:55] d2.utils.events INFO:  eta: 7:58:07  iter: 22099  total_loss: 3.186  loss_cls_stage0: 0.2779  loss_box_reg_stage0: 0.2625  loss_cls_stage1: 0.2944  loss_box_reg_stage1: 0.7316  loss_cls_stage2: 0.2822  loss_box_reg_stage2: 1.027  loss_mask: 0.07624  loss_rpn_cls: 0.01371  loss_rpn_loc: 0.05377  time: 0.6242  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:43:08] d2.utils.events INFO:  eta: 7:58:25  iter: 22119  total_loss: 2.871  loss_cls_stage0: 0.2484  loss_box_reg_stage0: 0.2491  loss_cls_stage1: 0.2699  loss_box_reg_stage1: 0.7001  loss_cls_stage2: 0.2685  loss_box_reg_stage2: 0.9819  loss_mask: 0.07581  loss_rpn_cls: 0.01357  loss_rpn_loc: 0.04399  time: 0.6242  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 16:43:20] d2.utils.events INFO:  eta: 7:58:54  iter: 22139  total_loss: 3.156  loss_cls_stage0: 0.2825  loss_box_reg_stage0: 0.2689  loss_cls_stage1: 0.3072  loss_box_reg_stage1: 0.7595  loss_cls_stage2: 0.3106  loss_box_reg_stage2: 0.9911  loss_mask: 0.07456  loss_rpn_cls: 0.01364  loss_rpn_loc: 0.04481  time: 0.6242  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 16:43:33] d2.utils.events INFO:  eta: 7:58:05  iter: 22159  total_loss: 2.905  loss_cls_stage0: 0.2758  loss_box_reg_stage0: 0.2742  loss_cls_stage1: 0.3026  loss_box_reg_stage1: 0.6589  loss_cls_stage2: 0.2913  loss_box_reg_stage2: 0.8953  loss_mask: 0.07222  loss_rpn_cls: 0.01868  loss_rpn_loc: 0.03972  time: 0.6242  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 16:43:45] d2.utils.events INFO:  eta: 7:57:29  iter: 22179  total_loss: 3.105  loss_cls_stage0: 0.2803  loss_box_reg_stage0: 0.2824  loss_cls_stage1: 0.3124  loss_box_reg_stage1: 0.7537  loss_cls_stage2: 0.2938  loss_box_reg_stage2: 0.973  loss_mask: 0.08129  loss_rpn_cls: 0.01855  loss_rpn_loc: 0.05016  time: 0.6242  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 16:43:57] d2.utils.events INFO:  eta: 7:57:04  iter: 22199  total_loss: 2.734  loss_cls_stage0: 0.2269  loss_box_reg_stage0: 0.26  loss_cls_stage1: 0.2581  loss_box_reg_stage1: 0.71  loss_cls_stage2: 0.2566  loss_box_reg_stage2: 0.9051  loss_mask: 0.07272  loss_rpn_cls: 0.01867  loss_rpn_loc: 0.04386  time: 0.6242  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 16:44:10] d2.utils.events INFO:  eta: 7:56:32  iter: 22219  total_loss: 2.963  loss_cls_stage0: 0.256  loss_box_reg_stage0: 0.2594  loss_cls_stage1: 0.2739  loss_box_reg_stage1: 0.6831  loss_cls_stage2: 0.2699  loss_box_reg_stage2: 0.9441  loss_mask: 0.06756  loss_rpn_cls: 0.01381  loss_rpn_loc: 0.04936  time: 0.6242  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:44:22] d2.utils.events INFO:  eta: 7:56:39  iter: 22239  total_loss: 2.648  loss_cls_stage0: 0.2222  loss_box_reg_stage0: 0.2622  loss_cls_stage1: 0.2614  loss_box_reg_stage1: 0.6662  loss_cls_stage2: 0.2664  loss_box_reg_stage2: 0.8971  loss_mask: 0.07737  loss_rpn_cls: 0.01609  loss_rpn_loc: 0.0489  time: 0.6242  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:44:34] d2.utils.events INFO:  eta: 7:55:52  iter: 22259  total_loss: 2.648  loss_cls_stage0: 0.2343  loss_box_reg_stage0: 0.2427  loss_cls_stage1: 0.2704  loss_box_reg_stage1: 0.6303  loss_cls_stage2: 0.2435  loss_box_reg_stage2: 0.7943  loss_mask: 0.06949  loss_rpn_cls: 0.01632  loss_rpn_loc: 0.05694  time: 0.6242  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:44:46] d2.utils.events INFO:  eta: 7:55:36  iter: 22279  total_loss: 2.979  loss_cls_stage0: 0.2753  loss_box_reg_stage0: 0.2974  loss_cls_stage1: 0.2923  loss_box_reg_stage1: 0.7031  loss_cls_stage2: 0.2762  loss_box_reg_stage2: 0.9157  loss_mask: 0.08618  loss_rpn_cls: 0.01696  loss_rpn_loc: 0.05283  time: 0.6242  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:44:59] d2.utils.events INFO:  eta: 7:55:19  iter: 22299  total_loss: 3.112  loss_cls_stage0: 0.332  loss_box_reg_stage0: 0.2933  loss_cls_stage1: 0.3566  loss_box_reg_stage1: 0.7236  loss_cls_stage2: 0.3365  loss_box_reg_stage2: 0.9085  loss_mask: 0.07797  loss_rpn_cls: 0.01675  loss_rpn_loc: 0.04721  time: 0.6241  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:45:11] d2.utils.events INFO:  eta: 7:54:58  iter: 22319  total_loss: 2.857  loss_cls_stage0: 0.249  loss_box_reg_stage0: 0.248  loss_cls_stage1: 0.2746  loss_box_reg_stage1: 0.6971  loss_cls_stage2: 0.2837  loss_box_reg_stage2: 0.9437  loss_mask: 0.06963  loss_rpn_cls: 0.008775  loss_rpn_loc: 0.04718  time: 0.6241  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 16:45:24] d2.utils.events INFO:  eta: 7:54:35  iter: 22339  total_loss: 3.175  loss_cls_stage0: 0.259  loss_box_reg_stage0: 0.2546  loss_cls_stage1: 0.3139  loss_box_reg_stage1: 0.7715  loss_cls_stage2: 0.3135  loss_box_reg_stage2: 1.046  loss_mask: 0.07408  loss_rpn_cls: 0.01384  loss_rpn_loc: 0.05168  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:45:36] d2.utils.events INFO:  eta: 7:54:18  iter: 22359  total_loss: 2.737  loss_cls_stage0: 0.2383  loss_box_reg_stage0: 0.2532  loss_cls_stage1: 0.2618  loss_box_reg_stage1: 0.6833  loss_cls_stage2: 0.246  loss_box_reg_stage2: 0.9915  loss_mask: 0.06953  loss_rpn_cls: 0.01822  loss_rpn_loc: 0.04389  time: 0.6242  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 16:45:49] d2.utils.events INFO:  eta: 7:53:47  iter: 22379  total_loss: 2.907  loss_cls_stage0: 0.2695  loss_box_reg_stage0: 0.254  loss_cls_stage1: 0.3267  loss_box_reg_stage1: 0.6568  loss_cls_stage2: 0.3133  loss_box_reg_stage2: 0.9191  loss_mask: 0.0741  loss_rpn_cls: 0.01391  loss_rpn_loc: 0.05947  time: 0.6241  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 16:46:01] d2.utils.events INFO:  eta: 7:53:44  iter: 22399  total_loss: 2.605  loss_cls_stage0: 0.2426  loss_box_reg_stage0: 0.2529  loss_cls_stage1: 0.2741  loss_box_reg_stage1: 0.5991  loss_cls_stage2: 0.2631  loss_box_reg_stage2: 0.8824  loss_mask: 0.07064  loss_rpn_cls: 0.01762  loss_rpn_loc: 0.04599  time: 0.6242  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:46:14] d2.utils.events INFO:  eta: 7:53:45  iter: 22419  total_loss: 2.776  loss_cls_stage0: 0.2413  loss_box_reg_stage0: 0.2695  loss_cls_stage1: 0.2557  loss_box_reg_stage1: 0.7201  loss_cls_stage2: 0.2399  loss_box_reg_stage2: 0.9346  loss_mask: 0.08133  loss_rpn_cls: 0.01104  loss_rpn_loc: 0.04975  time: 0.6242  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 16:46:27] d2.utils.events INFO:  eta: 7:53:37  iter: 22439  total_loss: 3.07  loss_cls_stage0: 0.2746  loss_box_reg_stage0: 0.2925  loss_cls_stage1: 0.3033  loss_box_reg_stage1: 0.7125  loss_cls_stage2: 0.3083  loss_box_reg_stage2: 0.9859  loss_mask: 0.0785  loss_rpn_cls: 0.0141  loss_rpn_loc: 0.05155  time: 0.6242  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 16:46:39] d2.utils.events INFO:  eta: 7:53:08  iter: 22459  total_loss: 2.985  loss_cls_stage0: 0.271  loss_box_reg_stage0: 0.2623  loss_cls_stage1: 0.3026  loss_box_reg_stage1: 0.722  loss_cls_stage2: 0.2859  loss_box_reg_stage2: 0.9369  loss_mask: 0.0776  loss_rpn_cls: 0.01316  loss_rpn_loc: 0.04666  time: 0.6242  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:46:52] d2.utils.events INFO:  eta: 7:52:42  iter: 22479  total_loss: 2.86  loss_cls_stage0: 0.2794  loss_box_reg_stage0: 0.2775  loss_cls_stage1: 0.2838  loss_box_reg_stage1: 0.6765  loss_cls_stage2: 0.2764  loss_box_reg_stage2: 0.9121  loss_mask: 0.07683  loss_rpn_cls: 0.01357  loss_rpn_loc: 0.03892  time: 0.6242  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:47:04] d2.utils.events INFO:  eta: 7:52:02  iter: 22499  total_loss: 3.059  loss_cls_stage0: 0.2575  loss_box_reg_stage0: 0.2614  loss_cls_stage1: 0.284  loss_box_reg_stage1: 0.7508  loss_cls_stage2: 0.2917  loss_box_reg_stage2: 1.004  loss_mask: 0.07259  loss_rpn_cls: 0.01686  loss_rpn_loc: 0.05187  time: 0.6242  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:47:16] d2.utils.events INFO:  eta: 7:51:49  iter: 22519  total_loss: 2.742  loss_cls_stage0: 0.2421  loss_box_reg_stage0: 0.2681  loss_cls_stage1: 0.2834  loss_box_reg_stage1: 0.631  loss_cls_stage2: 0.2484  loss_box_reg_stage2: 0.8491  loss_mask: 0.08259  loss_rpn_cls: 0.02397  loss_rpn_loc: 0.05574  time: 0.6242  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 16:47:29] d2.utils.events INFO:  eta: 7:52:00  iter: 22539  total_loss: 2.617  loss_cls_stage0: 0.234  loss_box_reg_stage0: 0.2476  loss_cls_stage1: 0.2442  loss_box_reg_stage1: 0.6162  loss_cls_stage2: 0.2135  loss_box_reg_stage2: 0.7825  loss_mask: 0.06858  loss_rpn_cls: 0.01708  loss_rpn_loc: 0.08513  time: 0.6242  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 16:47:41] d2.utils.events INFO:  eta: 7:50:58  iter: 22559  total_loss: 2.696  loss_cls_stage0: 0.2581  loss_box_reg_stage0: 0.275  loss_cls_stage1: 0.2581  loss_box_reg_stage1: 0.674  loss_cls_stage2: 0.2315  loss_box_reg_stage2: 0.7917  loss_mask: 0.07419  loss_rpn_cls: 0.02582  loss_rpn_loc: 0.05024  time: 0.6241  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 16:47:53] d2.utils.events INFO:  eta: 7:50:45  iter: 22579  total_loss: 2.403  loss_cls_stage0: 0.2314  loss_box_reg_stage0: 0.2389  loss_cls_stage1: 0.234  loss_box_reg_stage1: 0.5809  loss_cls_stage2: 0.2234  loss_box_reg_stage2: 0.8129  loss_mask: 0.07401  loss_rpn_cls: 0.01319  loss_rpn_loc: 0.04683  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:48:06] d2.utils.events INFO:  eta: 7:50:36  iter: 22599  total_loss: 2.742  loss_cls_stage0: 0.2559  loss_box_reg_stage0: 0.2598  loss_cls_stage1: 0.2762  loss_box_reg_stage1: 0.6649  loss_cls_stage2: 0.2628  loss_box_reg_stage2: 0.8816  loss_mask: 0.0848  loss_rpn_cls: 0.01553  loss_rpn_loc: 0.03781  time: 0.6241  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:48:18] d2.utils.events INFO:  eta: 7:50:21  iter: 22619  total_loss: 2.696  loss_cls_stage0: 0.2409  loss_box_reg_stage0: 0.2546  loss_cls_stage1: 0.2561  loss_box_reg_stage1: 0.6745  loss_cls_stage2: 0.2482  loss_box_reg_stage2: 0.8653  loss_mask: 0.07889  loss_rpn_cls: 0.01589  loss_rpn_loc: 0.04749  time: 0.6241  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:48:31] d2.utils.events INFO:  eta: 7:50:38  iter: 22639  total_loss: 2.442  loss_cls_stage0: 0.2076  loss_box_reg_stage0: 0.239  loss_cls_stage1: 0.2017  loss_box_reg_stage1: 0.5946  loss_cls_stage2: 0.2091  loss_box_reg_stage2: 0.7886  loss_mask: 0.07276  loss_rpn_cls: 0.008235  loss_rpn_loc: 0.03801  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:48:43] d2.utils.events INFO:  eta: 7:50:37  iter: 22659  total_loss: 2.85  loss_cls_stage0: 0.2617  loss_box_reg_stage0: 0.2712  loss_cls_stage1: 0.2911  loss_box_reg_stage1: 0.6453  loss_cls_stage2: 0.2921  loss_box_reg_stage2: 0.8889  loss_mask: 0.07752  loss_rpn_cls: 0.01379  loss_rpn_loc: 0.05437  time: 0.6241  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 16:48:56] d2.utils.events INFO:  eta: 7:50:32  iter: 22679  total_loss: 2.636  loss_cls_stage0: 0.2401  loss_box_reg_stage0: 0.2339  loss_cls_stage1: 0.2377  loss_box_reg_stage1: 0.6453  loss_cls_stage2: 0.2354  loss_box_reg_stage2: 0.8942  loss_mask: 0.07417  loss_rpn_cls: 0.02175  loss_rpn_loc: 0.04771  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:49:09] d2.utils.events INFO:  eta: 7:50:25  iter: 22699  total_loss: 2.721  loss_cls_stage0: 0.2494  loss_box_reg_stage0: 0.2542  loss_cls_stage1: 0.2757  loss_box_reg_stage1: 0.644  loss_cls_stage2: 0.2401  loss_box_reg_stage2: 0.8187  loss_mask: 0.07087  loss_rpn_cls: 0.01575  loss_rpn_loc: 0.06075  time: 0.6242  data_time: 0.0056  lr: 0.00016  max_mem: 19679M
[07/29 16:49:21] d2.utils.events INFO:  eta: 7:50:06  iter: 22719  total_loss: 2.896  loss_cls_stage0: 0.281  loss_box_reg_stage0: 0.261  loss_cls_stage1: 0.2729  loss_box_reg_stage1: 0.6591  loss_cls_stage2: 0.2727  loss_box_reg_stage2: 0.8507  loss_mask: 0.0696  loss_rpn_cls: 0.01903  loss_rpn_loc: 0.05638  time: 0.6242  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:49:34] d2.utils.events INFO:  eta: 7:49:58  iter: 22739  total_loss: 2.779  loss_cls_stage0: 0.255  loss_box_reg_stage0: 0.2532  loss_cls_stage1: 0.2623  loss_box_reg_stage1: 0.6632  loss_cls_stage2: 0.2512  loss_box_reg_stage2: 0.9151  loss_mask: 0.07741  loss_rpn_cls: 0.01069  loss_rpn_loc: 0.04634  time: 0.6242  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:49:47] d2.utils.events INFO:  eta: 7:49:46  iter: 22759  total_loss: 2.343  loss_cls_stage0: 0.2419  loss_box_reg_stage0: 0.2376  loss_cls_stage1: 0.2397  loss_box_reg_stage1: 0.5936  loss_cls_stage2: 0.242  loss_box_reg_stage2: 0.7697  loss_mask: 0.07409  loss_rpn_cls: 0.01736  loss_rpn_loc: 0.04584  time: 0.6242  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:49:58] d2.utils.events INFO:  eta: 7:49:10  iter: 22779  total_loss: 2.694  loss_cls_stage0: 0.2781  loss_box_reg_stage0: 0.252  loss_cls_stage1: 0.3089  loss_box_reg_stage1: 0.6128  loss_cls_stage2: 0.2886  loss_box_reg_stage2: 0.783  loss_mask: 0.07468  loss_rpn_cls: 0.01767  loss_rpn_loc: 0.05234  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:50:11] d2.utils.events INFO:  eta: 7:48:57  iter: 22799  total_loss: 2.723  loss_cls_stage0: 0.2498  loss_box_reg_stage0: 0.2531  loss_cls_stage1: 0.2743  loss_box_reg_stage1: 0.6803  loss_cls_stage2: 0.2731  loss_box_reg_stage2: 0.8824  loss_mask: 0.08009  loss_rpn_cls: 0.01609  loss_rpn_loc: 0.04412  time: 0.6241  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 16:50:23] d2.utils.events INFO:  eta: 7:48:45  iter: 22819  total_loss: 2.763  loss_cls_stage0: 0.2363  loss_box_reg_stage0: 0.2573  loss_cls_stage1: 0.2663  loss_box_reg_stage1: 0.6809  loss_cls_stage2: 0.2419  loss_box_reg_stage2: 0.9294  loss_mask: 0.08295  loss_rpn_cls: 0.01106  loss_rpn_loc: 0.04418  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:50:35] d2.utils.events INFO:  eta: 7:48:35  iter: 22839  total_loss: 2.999  loss_cls_stage0: 0.2601  loss_box_reg_stage0: 0.2741  loss_cls_stage1: 0.2797  loss_box_reg_stage1: 0.7059  loss_cls_stage2: 0.2916  loss_box_reg_stage2: 0.9476  loss_mask: 0.08718  loss_rpn_cls: 0.01479  loss_rpn_loc: 0.0489  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:50:48] d2.utils.events INFO:  eta: 7:48:31  iter: 22859  total_loss: 2.989  loss_cls_stage0: 0.2898  loss_box_reg_stage0: 0.2724  loss_cls_stage1: 0.3098  loss_box_reg_stage1: 0.6591  loss_cls_stage2: 0.2793  loss_box_reg_stage2: 0.927  loss_mask: 0.07883  loss_rpn_cls: 0.01672  loss_rpn_loc: 0.06937  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:51:01] d2.utils.events INFO:  eta: 7:48:29  iter: 22879  total_loss: 2.799  loss_cls_stage0: 0.2389  loss_box_reg_stage0: 0.2418  loss_cls_stage1: 0.2588  loss_box_reg_stage1: 0.6368  loss_cls_stage2: 0.2726  loss_box_reg_stage2: 0.8309  loss_mask: 0.08156  loss_rpn_cls: 0.01635  loss_rpn_loc: 0.04937  time: 0.6241  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 16:51:13] d2.utils.events INFO:  eta: 7:48:16  iter: 22899  total_loss: 2.817  loss_cls_stage0: 0.2537  loss_box_reg_stage0: 0.2484  loss_cls_stage1: 0.2658  loss_box_reg_stage1: 0.6621  loss_cls_stage2: 0.2535  loss_box_reg_stage2: 0.8654  loss_mask: 0.08502  loss_rpn_cls: 0.01548  loss_rpn_loc: 0.03627  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:51:26] d2.utils.events INFO:  eta: 7:48:05  iter: 22919  total_loss: 2.584  loss_cls_stage0: 0.2281  loss_box_reg_stage0: 0.2265  loss_cls_stage1: 0.2797  loss_box_reg_stage1: 0.6067  loss_cls_stage2: 0.2651  loss_box_reg_stage2: 0.8349  loss_mask: 0.06867  loss_rpn_cls: 0.01319  loss_rpn_loc: 0.03973  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:51:38] d2.utils.events INFO:  eta: 7:47:41  iter: 22939  total_loss: 2.774  loss_cls_stage0: 0.2288  loss_box_reg_stage0: 0.2345  loss_cls_stage1: 0.2664  loss_box_reg_stage1: 0.6335  loss_cls_stage2: 0.2709  loss_box_reg_stage2: 0.9414  loss_mask: 0.07053  loss_rpn_cls: 0.01203  loss_rpn_loc: 0.04004  time: 0.6241  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 16:51:51] d2.utils.events INFO:  eta: 7:47:35  iter: 22959  total_loss: 2.644  loss_cls_stage0: 0.2371  loss_box_reg_stage0: 0.2587  loss_cls_stage1: 0.2292  loss_box_reg_stage1: 0.6204  loss_cls_stage2: 0.2185  loss_box_reg_stage2: 0.8239  loss_mask: 0.07178  loss_rpn_cls: 0.01883  loss_rpn_loc: 0.05659  time: 0.6241  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 16:52:02] d2.utils.events INFO:  eta: 7:46:37  iter: 22979  total_loss: 2.647  loss_cls_stage0: 0.2511  loss_box_reg_stage0: 0.2562  loss_cls_stage1: 0.2426  loss_box_reg_stage1: 0.6469  loss_cls_stage2: 0.2419  loss_box_reg_stage2: 0.7976  loss_mask: 0.07679  loss_rpn_cls: 0.01908  loss_rpn_loc: 0.05293  time: 0.6241  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 16:52:15] d2.utils.events INFO:  eta: 7:46:21  iter: 22999  total_loss: 2.932  loss_cls_stage0: 0.2739  loss_box_reg_stage0: 0.2797  loss_cls_stage1: 0.2914  loss_box_reg_stage1: 0.6961  loss_cls_stage2: 0.2748  loss_box_reg_stage2: 0.9439  loss_mask: 0.07628  loss_rpn_cls: 0.01459  loss_rpn_loc: 0.04472  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:52:27] d2.utils.events INFO:  eta: 7:46:08  iter: 23019  total_loss: 2.853  loss_cls_stage0: 0.2398  loss_box_reg_stage0: 0.2641  loss_cls_stage1: 0.2785  loss_box_reg_stage1: 0.6795  loss_cls_stage2: 0.2652  loss_box_reg_stage2: 0.9309  loss_mask: 0.07585  loss_rpn_cls: 0.01727  loss_rpn_loc: 0.06917  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:52:40] d2.utils.events INFO:  eta: 7:45:31  iter: 23039  total_loss: 2.87  loss_cls_stage0: 0.2577  loss_box_reg_stage0: 0.2599  loss_cls_stage1: 0.2612  loss_box_reg_stage1: 0.7195  loss_cls_stage2: 0.225  loss_box_reg_stage2: 0.9223  loss_mask: 0.0695  loss_rpn_cls: 0.01689  loss_rpn_loc: 0.05392  time: 0.6241  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 16:52:52] d2.utils.events INFO:  eta: 7:45:06  iter: 23059  total_loss: 2.907  loss_cls_stage0: 0.2726  loss_box_reg_stage0: 0.2808  loss_cls_stage1: 0.2855  loss_box_reg_stage1: 0.6676  loss_cls_stage2: 0.2588  loss_box_reg_stage2: 0.8479  loss_mask: 0.07743  loss_rpn_cls: 0.01899  loss_rpn_loc: 0.05178  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:53:05] d2.utils.events INFO:  eta: 7:44:33  iter: 23079  total_loss: 2.667  loss_cls_stage0: 0.2438  loss_box_reg_stage0: 0.2698  loss_cls_stage1: 0.2573  loss_box_reg_stage1: 0.6621  loss_cls_stage2: 0.2505  loss_box_reg_stage2: 0.871  loss_mask: 0.07391  loss_rpn_cls: 0.01925  loss_rpn_loc: 0.04634  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:53:17] d2.utils.events INFO:  eta: 7:44:20  iter: 23099  total_loss: 2.792  loss_cls_stage0: 0.248  loss_box_reg_stage0: 0.2616  loss_cls_stage1: 0.2736  loss_box_reg_stage1: 0.6626  loss_cls_stage2: 0.255  loss_box_reg_stage2: 0.8562  loss_mask: 0.07342  loss_rpn_cls: 0.01585  loss_rpn_loc: 0.04561  time: 0.6241  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 16:53:30] d2.utils.events INFO:  eta: 7:44:12  iter: 23119  total_loss: 2.76  loss_cls_stage0: 0.2546  loss_box_reg_stage0: 0.2445  loss_cls_stage1: 0.2766  loss_box_reg_stage1: 0.6406  loss_cls_stage2: 0.2758  loss_box_reg_stage2: 0.8949  loss_mask: 0.08209  loss_rpn_cls: 0.0132  loss_rpn_loc: 0.04075  time: 0.6241  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 16:53:43] d2.utils.events INFO:  eta: 7:43:56  iter: 23139  total_loss: 2.833  loss_cls_stage0: 0.246  loss_box_reg_stage0: 0.2836  loss_cls_stage1: 0.256  loss_box_reg_stage1: 0.6591  loss_cls_stage2: 0.2581  loss_box_reg_stage2: 0.8736  loss_mask: 0.08825  loss_rpn_cls: 0.02194  loss_rpn_loc: 0.04453  time: 0.6241  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:53:55] d2.utils.events INFO:  eta: 7:43:44  iter: 23159  total_loss: 2.519  loss_cls_stage0: 0.2041  loss_box_reg_stage0: 0.2355  loss_cls_stage1: 0.2297  loss_box_reg_stage1: 0.5812  loss_cls_stage2: 0.2197  loss_box_reg_stage2: 0.7785  loss_mask: 0.06867  loss_rpn_cls: 0.01287  loss_rpn_loc: 0.04477  time: 0.6241  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:54:07] d2.utils.events INFO:  eta: 7:43:30  iter: 23179  total_loss: 2.98  loss_cls_stage0: 0.2771  loss_box_reg_stage0: 0.2684  loss_cls_stage1: 0.2882  loss_box_reg_stage1: 0.7022  loss_cls_stage2: 0.2667  loss_box_reg_stage2: 0.8748  loss_mask: 0.07458  loss_rpn_cls: 0.01947  loss_rpn_loc: 0.07407  time: 0.6241  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:54:20] d2.utils.events INFO:  eta: 7:43:19  iter: 23199  total_loss: 2.538  loss_cls_stage0: 0.2604  loss_box_reg_stage0: 0.241  loss_cls_stage1: 0.2707  loss_box_reg_stage1: 0.6067  loss_cls_stage2: 0.2674  loss_box_reg_stage2: 0.8245  loss_mask: 0.0745  loss_rpn_cls: 0.01745  loss_rpn_loc: 0.04341  time: 0.6240  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 16:54:32] d2.utils.events INFO:  eta: 7:43:10  iter: 23219  total_loss: 2.685  loss_cls_stage0: 0.2701  loss_box_reg_stage0: 0.2517  loss_cls_stage1: 0.288  loss_box_reg_stage1: 0.6196  loss_cls_stage2: 0.2747  loss_box_reg_stage2: 0.8825  loss_mask: 0.07613  loss_rpn_cls: 0.01631  loss_rpn_loc: 0.04181  time: 0.6240  data_time: 0.0054  lr: 0.00016  max_mem: 19679M
[07/29 16:54:44] d2.utils.events INFO:  eta: 7:42:52  iter: 23239  total_loss: 2.739  loss_cls_stage0: 0.2476  loss_box_reg_stage0: 0.2569  loss_cls_stage1: 0.2814  loss_box_reg_stage1: 0.5963  loss_cls_stage2: 0.248  loss_box_reg_stage2: 0.8142  loss_mask: 0.0707  loss_rpn_cls: 0.0246  loss_rpn_loc: 0.05026  time: 0.6240  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 16:54:56] d2.utils.events INFO:  eta: 7:42:45  iter: 23259  total_loss: 2.879  loss_cls_stage0: 0.2855  loss_box_reg_stage0: 0.2784  loss_cls_stage1: 0.2795  loss_box_reg_stage1: 0.681  loss_cls_stage2: 0.2688  loss_box_reg_stage2: 0.8563  loss_mask: 0.08263  loss_rpn_cls: 0.0175  loss_rpn_loc: 0.05427  time: 0.6240  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 16:55:09] d2.utils.events INFO:  eta: 7:42:32  iter: 23279  total_loss: 2.543  loss_cls_stage0: 0.2183  loss_box_reg_stage0: 0.2678  loss_cls_stage1: 0.2399  loss_box_reg_stage1: 0.618  loss_cls_stage2: 0.2337  loss_box_reg_stage2: 0.8327  loss_mask: 0.08183  loss_rpn_cls: 0.009444  loss_rpn_loc: 0.03872  time: 0.6240  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 16:55:21] d2.utils.events INFO:  eta: 7:42:24  iter: 23299  total_loss: 2.815  loss_cls_stage0: 0.2668  loss_box_reg_stage0: 0.2621  loss_cls_stage1: 0.2854  loss_box_reg_stage1: 0.6422  loss_cls_stage2: 0.2824  loss_box_reg_stage2: 0.8358  loss_mask: 0.07566  loss_rpn_cls: 0.01723  loss_rpn_loc: 0.05139  time: 0.6240  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 16:55:33] d2.utils.events INFO:  eta: 7:42:11  iter: 23319  total_loss: 2.731  loss_cls_stage0: 0.2685  loss_box_reg_stage0: 0.2654  loss_cls_stage1: 0.2695  loss_box_reg_stage1: 0.6933  loss_cls_stage2: 0.269  loss_box_reg_stage2: 0.8525  loss_mask: 0.07944  loss_rpn_cls: 0.01786  loss_rpn_loc: 0.04164  time: 0.6240  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:55:46] d2.utils.events INFO:  eta: 7:42:18  iter: 23339  total_loss: 2.75  loss_cls_stage0: 0.2802  loss_box_reg_stage0: 0.2754  loss_cls_stage1: 0.2773  loss_box_reg_stage1: 0.6324  loss_cls_stage2: 0.24  loss_box_reg_stage2: 0.8758  loss_mask: 0.08609  loss_rpn_cls: 0.01726  loss_rpn_loc: 0.05335  time: 0.6240  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 16:55:58] d2.utils.events INFO:  eta: 7:41:37  iter: 23359  total_loss: 2.574  loss_cls_stage0: 0.2373  loss_box_reg_stage0: 0.2364  loss_cls_stage1: 0.2359  loss_box_reg_stage1: 0.6013  loss_cls_stage2: 0.2293  loss_box_reg_stage2: 0.8737  loss_mask: 0.06852  loss_rpn_cls: 0.01774  loss_rpn_loc: 0.05161  time: 0.6240  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 16:56:10] d2.utils.events INFO:  eta: 7:41:22  iter: 23379  total_loss: 2.722  loss_cls_stage0: 0.2299  loss_box_reg_stage0: 0.2574  loss_cls_stage1: 0.2598  loss_box_reg_stage1: 0.6564  loss_cls_stage2: 0.2608  loss_box_reg_stage2: 0.9317  loss_mask: 0.08003  loss_rpn_cls: 0.01345  loss_rpn_loc: 0.04539  time: 0.6240  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:56:23] d2.utils.events INFO:  eta: 7:41:08  iter: 23399  total_loss: 2.87  loss_cls_stage0: 0.2667  loss_box_reg_stage0: 0.2722  loss_cls_stage1: 0.2581  loss_box_reg_stage1: 0.7056  loss_cls_stage2: 0.2575  loss_box_reg_stage2: 0.8951  loss_mask: 0.07411  loss_rpn_cls: 0.0192  loss_rpn_loc: 0.05527  time: 0.6240  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 16:56:36] d2.utils.events INFO:  eta: 7:40:49  iter: 23419  total_loss: 2.518  loss_cls_stage0: 0.2529  loss_box_reg_stage0: 0.2677  loss_cls_stage1: 0.2602  loss_box_reg_stage1: 0.6156  loss_cls_stage2: 0.2196  loss_box_reg_stage2: 0.8171  loss_mask: 0.07325  loss_rpn_cls: 0.0177  loss_rpn_loc: 0.05555  time: 0.6240  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:56:48] d2.utils.events INFO:  eta: 7:40:40  iter: 23439  total_loss: 2.88  loss_cls_stage0: 0.25  loss_box_reg_stage0: 0.2588  loss_cls_stage1: 0.2486  loss_box_reg_stage1: 0.7059  loss_cls_stage2: 0.2659  loss_box_reg_stage2: 0.9349  loss_mask: 0.08916  loss_rpn_cls: 0.01727  loss_rpn_loc: 0.0646  time: 0.6240  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 16:57:01] d2.utils.events INFO:  eta: 7:40:36  iter: 23459  total_loss: 3.004  loss_cls_stage0: 0.2534  loss_box_reg_stage0: 0.2973  loss_cls_stage1: 0.2794  loss_box_reg_stage1: 0.7378  loss_cls_stage2: 0.2547  loss_box_reg_stage2: 0.9518  loss_mask: 0.08468  loss_rpn_cls: 0.01725  loss_rpn_loc: 0.05006  time: 0.6240  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:57:13] d2.utils.events INFO:  eta: 7:40:23  iter: 23479  total_loss: 2.691  loss_cls_stage0: 0.2449  loss_box_reg_stage0: 0.241  loss_cls_stage1: 0.2656  loss_box_reg_stage1: 0.6425  loss_cls_stage2: 0.254  loss_box_reg_stage2: 0.854  loss_mask: 0.0741  loss_rpn_cls: 0.01227  loss_rpn_loc: 0.04165  time: 0.6240  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 16:57:26] d2.utils.events INFO:  eta: 7:40:11  iter: 23499  total_loss: 2.843  loss_cls_stage0: 0.2569  loss_box_reg_stage0: 0.2786  loss_cls_stage1: 0.2842  loss_box_reg_stage1: 0.6885  loss_cls_stage2: 0.2538  loss_box_reg_stage2: 0.9378  loss_mask: 0.08109  loss_rpn_cls: 0.01045  loss_rpn_loc: 0.04126  time: 0.6240  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:57:38] d2.utils.events INFO:  eta: 7:39:56  iter: 23519  total_loss: 2.956  loss_cls_stage0: 0.2876  loss_box_reg_stage0: 0.2716  loss_cls_stage1: 0.3086  loss_box_reg_stage1: 0.7072  loss_cls_stage2: 0.2765  loss_box_reg_stage2: 0.9572  loss_mask: 0.08014  loss_rpn_cls: 0.01593  loss_rpn_loc: 0.04192  time: 0.6240  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 16:57:51] d2.utils.events INFO:  eta: 7:39:45  iter: 23539  total_loss: 2.485  loss_cls_stage0: 0.2302  loss_box_reg_stage0: 0.2418  loss_cls_stage1: 0.2438  loss_box_reg_stage1: 0.5923  loss_cls_stage2: 0.2201  loss_box_reg_stage2: 0.8495  loss_mask: 0.06997  loss_rpn_cls: 0.01309  loss_rpn_loc: 0.04287  time: 0.6240  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:58:03] d2.utils.events INFO:  eta: 7:39:50  iter: 23559  total_loss: 2.916  loss_cls_stage0: 0.2922  loss_box_reg_stage0: 0.2789  loss_cls_stage1: 0.2738  loss_box_reg_stage1: 0.6636  loss_cls_stage2: 0.2578  loss_box_reg_stage2: 0.8238  loss_mask: 0.08918  loss_rpn_cls: 0.02886  loss_rpn_loc: 0.07314  time: 0.6240  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:58:16] d2.utils.events INFO:  eta: 7:39:50  iter: 23579  total_loss: 2.815  loss_cls_stage0: 0.2812  loss_box_reg_stage0: 0.2707  loss_cls_stage1: 0.3094  loss_box_reg_stage1: 0.6729  loss_cls_stage2: 0.2725  loss_box_reg_stage2: 0.8413  loss_mask: 0.0862  loss_rpn_cls: 0.02136  loss_rpn_loc: 0.0505  time: 0.6240  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 16:58:28] d2.utils.events INFO:  eta: 7:39:34  iter: 23599  total_loss: 2.856  loss_cls_stage0: 0.2658  loss_box_reg_stage0: 0.2902  loss_cls_stage1: 0.2683  loss_box_reg_stage1: 0.6807  loss_cls_stage2: 0.2661  loss_box_reg_stage2: 0.8743  loss_mask: 0.08753  loss_rpn_cls: 0.01819  loss_rpn_loc: 0.05204  time: 0.6240  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:58:40] d2.utils.events INFO:  eta: 7:39:25  iter: 23619  total_loss: 2.925  loss_cls_stage0: 0.2814  loss_box_reg_stage0: 0.2905  loss_cls_stage1: 0.2707  loss_box_reg_stage1: 0.6806  loss_cls_stage2: 0.2609  loss_box_reg_stage2: 0.9051  loss_mask: 0.0961  loss_rpn_cls: 0.0138  loss_rpn_loc: 0.04692  time: 0.6240  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:58:53] d2.utils.events INFO:  eta: 7:38:53  iter: 23639  total_loss: 3.085  loss_cls_stage0: 0.2864  loss_box_reg_stage0: 0.2631  loss_cls_stage1: 0.3172  loss_box_reg_stage1: 0.6914  loss_cls_stage2: 0.3138  loss_box_reg_stage2: 0.949  loss_mask: 0.08389  loss_rpn_cls: 0.0152  loss_rpn_loc: 0.04745  time: 0.6240  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:59:05] d2.utils.events INFO:  eta: 7:38:35  iter: 23659  total_loss: 2.777  loss_cls_stage0: 0.2395  loss_box_reg_stage0: 0.2358  loss_cls_stage1: 0.2719  loss_box_reg_stage1: 0.6837  loss_cls_stage2: 0.2419  loss_box_reg_stage2: 0.9253  loss_mask: 0.07027  loss_rpn_cls: 0.013  loss_rpn_loc: 0.04366  time: 0.6239  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 16:59:18] d2.utils.events INFO:  eta: 7:38:35  iter: 23679  total_loss: 2.863  loss_cls_stage0: 0.2538  loss_box_reg_stage0: 0.2513  loss_cls_stage1: 0.2782  loss_box_reg_stage1: 0.6853  loss_cls_stage2: 0.2877  loss_box_reg_stage2: 0.9564  loss_mask: 0.0739  loss_rpn_cls: 0.01561  loss_rpn_loc: 0.04557  time: 0.6240  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 16:59:30] d2.utils.events INFO:  eta: 7:38:06  iter: 23699  total_loss: 2.832  loss_cls_stage0: 0.2536  loss_box_reg_stage0: 0.266  loss_cls_stage1: 0.2787  loss_box_reg_stage1: 0.6672  loss_cls_stage2: 0.2807  loss_box_reg_stage2: 0.9082  loss_mask: 0.07204  loss_rpn_cls: 0.015  loss_rpn_loc: 0.04716  time: 0.6240  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 16:59:43] d2.utils.events INFO:  eta: 7:37:58  iter: 23719  total_loss: 2.711  loss_cls_stage0: 0.2527  loss_box_reg_stage0: 0.262  loss_cls_stage1: 0.264  loss_box_reg_stage1: 0.6222  loss_cls_stage2: 0.2436  loss_box_reg_stage2: 0.8393  loss_mask: 0.07699  loss_rpn_cls: 0.01568  loss_rpn_loc: 0.06107  time: 0.6239  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 16:59:55] d2.utils.events INFO:  eta: 7:37:33  iter: 23739  total_loss: 2.747  loss_cls_stage0: 0.2604  loss_box_reg_stage0: 0.2654  loss_cls_stage1: 0.2676  loss_box_reg_stage1: 0.6338  loss_cls_stage2: 0.267  loss_box_reg_stage2: 0.8649  loss_mask: 0.07501  loss_rpn_cls: 0.02235  loss_rpn_loc: 0.04465  time: 0.6239  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:00:07] d2.utils.events INFO:  eta: 7:37:08  iter: 23759  total_loss: 2.767  loss_cls_stage0: 0.2466  loss_box_reg_stage0: 0.2591  loss_cls_stage1: 0.2566  loss_box_reg_stage1: 0.6235  loss_cls_stage2: 0.2553  loss_box_reg_stage2: 0.8765  loss_mask: 0.07409  loss_rpn_cls: 0.01447  loss_rpn_loc: 0.04943  time: 0.6239  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 17:00:19] d2.utils.events INFO:  eta: 7:36:55  iter: 23779  total_loss: 2.766  loss_cls_stage0: 0.2279  loss_box_reg_stage0: 0.2548  loss_cls_stage1: 0.2449  loss_box_reg_stage1: 0.6771  loss_cls_stage2: 0.2303  loss_box_reg_stage2: 0.8258  loss_mask: 0.07241  loss_rpn_cls: 0.01759  loss_rpn_loc: 0.04987  time: 0.6239  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 17:00:31] d2.utils.events INFO:  eta: 7:36:16  iter: 23799  total_loss: 2.38  loss_cls_stage0: 0.2245  loss_box_reg_stage0: 0.2274  loss_cls_stage1: 0.2372  loss_box_reg_stage1: 0.6045  loss_cls_stage2: 0.2446  loss_box_reg_stage2: 0.812  loss_mask: 0.07262  loss_rpn_cls: 0.01282  loss_rpn_loc: 0.03883  time: 0.6239  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:00:44] d2.utils.events INFO:  eta: 7:36:24  iter: 23819  total_loss: 2.832  loss_cls_stage0: 0.2759  loss_box_reg_stage0: 0.2639  loss_cls_stage1: 0.3028  loss_box_reg_stage1: 0.7221  loss_cls_stage2: 0.2843  loss_box_reg_stage2: 0.9372  loss_mask: 0.07921  loss_rpn_cls: 0.01303  loss_rpn_loc: 0.04579  time: 0.6239  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:00:56] d2.utils.events INFO:  eta: 7:36:19  iter: 23839  total_loss: 2.828  loss_cls_stage0: 0.2902  loss_box_reg_stage0: 0.2684  loss_cls_stage1: 0.3047  loss_box_reg_stage1: 0.6541  loss_cls_stage2: 0.2714  loss_box_reg_stage2: 0.8335  loss_mask: 0.0788  loss_rpn_cls: 0.01929  loss_rpn_loc: 0.04939  time: 0.6239  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:01:09] d2.utils.events INFO:  eta: 7:36:18  iter: 23859  total_loss: 2.66  loss_cls_stage0: 0.2511  loss_box_reg_stage0: 0.2303  loss_cls_stage1: 0.2613  loss_box_reg_stage1: 0.6207  loss_cls_stage2: 0.2657  loss_box_reg_stage2: 0.9213  loss_mask: 0.071  loss_rpn_cls: 0.01453  loss_rpn_loc: 0.04143  time: 0.6239  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:01:21] d2.utils.events INFO:  eta: 7:35:54  iter: 23879  total_loss: 2.785  loss_cls_stage0: 0.2727  loss_box_reg_stage0: 0.2737  loss_cls_stage1: 0.2734  loss_box_reg_stage1: 0.6685  loss_cls_stage2: 0.2674  loss_box_reg_stage2: 0.8786  loss_mask: 0.07682  loss_rpn_cls: 0.02484  loss_rpn_loc: 0.04508  time: 0.6239  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:01:34] d2.utils.events INFO:  eta: 7:35:52  iter: 23899  total_loss: 3.007  loss_cls_stage0: 0.2863  loss_box_reg_stage0: 0.2858  loss_cls_stage1: 0.2947  loss_box_reg_stage1: 0.673  loss_cls_stage2: 0.2545  loss_box_reg_stage2: 0.9184  loss_mask: 0.08091  loss_rpn_cls: 0.01681  loss_rpn_loc: 0.07214  time: 0.6239  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:01:46] d2.utils.events INFO:  eta: 7:35:29  iter: 23919  total_loss: 2.627  loss_cls_stage0: 0.2575  loss_box_reg_stage0: 0.2725  loss_cls_stage1: 0.2829  loss_box_reg_stage1: 0.6346  loss_cls_stage2: 0.2595  loss_box_reg_stage2: 0.7912  loss_mask: 0.07923  loss_rpn_cls: 0.02006  loss_rpn_loc: 0.04056  time: 0.6239  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:01:59] d2.utils.events INFO:  eta: 7:35:27  iter: 23939  total_loss: 2.577  loss_cls_stage0: 0.2617  loss_box_reg_stage0: 0.2523  loss_cls_stage1: 0.2581  loss_box_reg_stage1: 0.5956  loss_cls_stage2: 0.2384  loss_box_reg_stage2: 0.7962  loss_mask: 0.07476  loss_rpn_cls: 0.0136  loss_rpn_loc: 0.04042  time: 0.6239  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 17:02:11] d2.utils.events INFO:  eta: 7:35:04  iter: 23959  total_loss: 2.361  loss_cls_stage0: 0.212  loss_box_reg_stage0: 0.2235  loss_cls_stage1: 0.214  loss_box_reg_stage1: 0.5994  loss_cls_stage2: 0.2164  loss_box_reg_stage2: 0.7717  loss_mask: 0.07168  loss_rpn_cls: 0.0124  loss_rpn_loc: 0.04806  time: 0.6238  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:02:23] d2.utils.events INFO:  eta: 7:35:32  iter: 23979  total_loss: 2.434  loss_cls_stage0: 0.2162  loss_box_reg_stage0: 0.2365  loss_cls_stage1: 0.2601  loss_box_reg_stage1: 0.5909  loss_cls_stage2: 0.2443  loss_box_reg_stage2: 0.7475  loss_mask: 0.06984  loss_rpn_cls: 0.01827  loss_rpn_loc: 0.0519  time: 0.6238  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:02:35] d2.utils.events INFO:  eta: 7:34:50  iter: 23999  total_loss: 2.526  loss_cls_stage0: 0.214  loss_box_reg_stage0: 0.2485  loss_cls_stage1: 0.2397  loss_box_reg_stage1: 0.6215  loss_cls_stage2: 0.2165  loss_box_reg_stage2: 0.778  loss_mask: 0.07392  loss_rpn_cls: 0.01438  loss_rpn_loc: 0.04376  time: 0.6238  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:02:48] d2.utils.events INFO:  eta: 7:35:10  iter: 24019  total_loss: 2.537  loss_cls_stage0: 0.2221  loss_box_reg_stage0: 0.2382  loss_cls_stage1: 0.2344  loss_box_reg_stage1: 0.5646  loss_cls_stage2: 0.2258  loss_box_reg_stage2: 0.8182  loss_mask: 0.07015  loss_rpn_cls: 0.01367  loss_rpn_loc: 0.04179  time: 0.6238  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:03:01] d2.utils.events INFO:  eta: 7:34:49  iter: 24039  total_loss: 3.287  loss_cls_stage0: 0.3098  loss_box_reg_stage0: 0.2866  loss_cls_stage1: 0.3423  loss_box_reg_stage1: 0.7339  loss_cls_stage2: 0.3212  loss_box_reg_stage2: 0.976  loss_mask: 0.08138  loss_rpn_cls: 0.01588  loss_rpn_loc: 0.04824  time: 0.6238  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:03:13] d2.utils.events INFO:  eta: 7:34:23  iter: 24059  total_loss: 2.793  loss_cls_stage0: 0.2359  loss_box_reg_stage0: 0.2454  loss_cls_stage1: 0.2609  loss_box_reg_stage1: 0.6487  loss_cls_stage2: 0.259  loss_box_reg_stage2: 0.8759  loss_mask: 0.07116  loss_rpn_cls: 0.01704  loss_rpn_loc: 0.05202  time: 0.6238  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:03:25] d2.utils.events INFO:  eta: 7:33:55  iter: 24079  total_loss: 2.882  loss_cls_stage0: 0.2675  loss_box_reg_stage0: 0.2699  loss_cls_stage1: 0.2897  loss_box_reg_stage1: 0.674  loss_cls_stage2: 0.2801  loss_box_reg_stage2: 0.8768  loss_mask: 0.07657  loss_rpn_cls: 0.01468  loss_rpn_loc: 0.04479  time: 0.6238  data_time: 0.0043  lr: 0.00016  max_mem: 19679M
[07/29 17:03:37] d2.utils.events INFO:  eta: 7:33:35  iter: 24099  total_loss: 2.786  loss_cls_stage0: 0.2664  loss_box_reg_stage0: 0.2603  loss_cls_stage1: 0.2795  loss_box_reg_stage1: 0.688  loss_cls_stage2: 0.2649  loss_box_reg_stage2: 0.9273  loss_mask: 0.07126  loss_rpn_cls: 0.01554  loss_rpn_loc: 0.04184  time: 0.6238  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 17:03:50] d2.utils.events INFO:  eta: 7:32:44  iter: 24119  total_loss: 3.005  loss_cls_stage0: 0.2876  loss_box_reg_stage0: 0.2664  loss_cls_stage1: 0.281  loss_box_reg_stage1: 0.6841  loss_cls_stage2: 0.2514  loss_box_reg_stage2: 0.9119  loss_mask: 0.07436  loss_rpn_cls: 0.01782  loss_rpn_loc: 0.05676  time: 0.6238  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:04:03] d2.utils.events INFO:  eta: 7:32:22  iter: 24139  total_loss: 2.618  loss_cls_stage0: 0.2437  loss_box_reg_stage0: 0.2433  loss_cls_stage1: 0.2286  loss_box_reg_stage1: 0.6371  loss_cls_stage2: 0.2284  loss_box_reg_stage2: 0.929  loss_mask: 0.06865  loss_rpn_cls: 0.01458  loss_rpn_loc: 0.0431  time: 0.6238  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:04:15] d2.utils.events INFO:  eta: 7:32:29  iter: 24159  total_loss: 2.768  loss_cls_stage0: 0.2563  loss_box_reg_stage0: 0.2616  loss_cls_stage1: 0.2579  loss_box_reg_stage1: 0.6509  loss_cls_stage2: 0.2565  loss_box_reg_stage2: 0.8713  loss_mask: 0.07196  loss_rpn_cls: 0.01478  loss_rpn_loc: 0.04232  time: 0.6238  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:04:28] d2.utils.events INFO:  eta: 7:32:35  iter: 24179  total_loss: 2.775  loss_cls_stage0: 0.236  loss_box_reg_stage0: 0.2472  loss_cls_stage1: 0.2602  loss_box_reg_stage1: 0.6336  loss_cls_stage2: 0.2513  loss_box_reg_stage2: 0.8266  loss_mask: 0.06752  loss_rpn_cls: 0.01966  loss_rpn_loc: 0.05066  time: 0.6238  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:04:40] d2.utils.events INFO:  eta: 7:32:23  iter: 24199  total_loss: 2.743  loss_cls_stage0: 0.237  loss_box_reg_stage0: 0.2654  loss_cls_stage1: 0.2405  loss_box_reg_stage1: 0.6419  loss_cls_stage2: 0.2326  loss_box_reg_stage2: 0.8748  loss_mask: 0.07984  loss_rpn_cls: 0.01279  loss_rpn_loc: 0.0402  time: 0.6238  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:04:53] d2.utils.events INFO:  eta: 7:32:10  iter: 24219  total_loss: 2.591  loss_cls_stage0: 0.2194  loss_box_reg_stage0: 0.2513  loss_cls_stage1: 0.2294  loss_box_reg_stage1: 0.6426  loss_cls_stage2: 0.2096  loss_box_reg_stage2: 0.8827  loss_mask: 0.07586  loss_rpn_cls: 0.01077  loss_rpn_loc: 0.04096  time: 0.6238  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 17:05:05] d2.utils.events INFO:  eta: 7:32:17  iter: 24239  total_loss: 2.782  loss_cls_stage0: 0.2571  loss_box_reg_stage0: 0.2742  loss_cls_stage1: 0.2564  loss_box_reg_stage1: 0.6838  loss_cls_stage2: 0.2511  loss_box_reg_stage2: 0.8493  loss_mask: 0.07633  loss_rpn_cls: 0.01296  loss_rpn_loc: 0.04481  time: 0.6238  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:05:18] d2.utils.events INFO:  eta: 7:31:57  iter: 24259  total_loss: 2.889  loss_cls_stage0: 0.2575  loss_box_reg_stage0: 0.2677  loss_cls_stage1: 0.254  loss_box_reg_stage1: 0.6705  loss_cls_stage2: 0.2798  loss_box_reg_stage2: 0.8991  loss_mask: 0.06974  loss_rpn_cls: 0.01748  loss_rpn_loc: 0.06083  time: 0.6238  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 17:05:30] d2.utils.events INFO:  eta: 7:31:06  iter: 24279  total_loss: 2.53  loss_cls_stage0: 0.2176  loss_box_reg_stage0: 0.2515  loss_cls_stage1: 0.2478  loss_box_reg_stage1: 0.6228  loss_cls_stage2: 0.223  loss_box_reg_stage2: 0.8729  loss_mask: 0.06897  loss_rpn_cls: 0.01383  loss_rpn_loc: 0.04576  time: 0.6238  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:05:42] d2.utils.events INFO:  eta: 7:31:20  iter: 24299  total_loss: 2.709  loss_cls_stage0: 0.2656  loss_box_reg_stage0: 0.2547  loss_cls_stage1: 0.2785  loss_box_reg_stage1: 0.6404  loss_cls_stage2: 0.258  loss_box_reg_stage2: 0.8426  loss_mask: 0.07401  loss_rpn_cls: 0.01516  loss_rpn_loc: 0.05032  time: 0.6238  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 17:05:55] d2.utils.events INFO:  eta: 7:31:28  iter: 24319  total_loss: 2.908  loss_cls_stage0: 0.2886  loss_box_reg_stage0: 0.2944  loss_cls_stage1: 0.2703  loss_box_reg_stage1: 0.6863  loss_cls_stage2: 0.261  loss_box_reg_stage2: 0.9094  loss_mask: 0.08051  loss_rpn_cls: 0.01759  loss_rpn_loc: 0.05195  time: 0.6238  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:06:07] d2.utils.events INFO:  eta: 7:30:28  iter: 24339  total_loss: 2.437  loss_cls_stage0: 0.2132  loss_box_reg_stage0: 0.2257  loss_cls_stage1: 0.2316  loss_box_reg_stage1: 0.5828  loss_cls_stage2: 0.2461  loss_box_reg_stage2: 0.7906  loss_mask: 0.07186  loss_rpn_cls: 0.01477  loss_rpn_loc: 0.03732  time: 0.6238  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:06:19] d2.utils.events INFO:  eta: 7:30:54  iter: 24359  total_loss: 2.842  loss_cls_stage0: 0.2663  loss_box_reg_stage0: 0.2598  loss_cls_stage1: 0.2901  loss_box_reg_stage1: 0.6579  loss_cls_stage2: 0.2378  loss_box_reg_stage2: 0.8124  loss_mask: 0.07502  loss_rpn_cls: 0.01281  loss_rpn_loc: 0.05624  time: 0.6238  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:06:32] d2.utils.events INFO:  eta: 7:31:08  iter: 24379  total_loss: 2.654  loss_cls_stage0: 0.2363  loss_box_reg_stage0: 0.2447  loss_cls_stage1: 0.2674  loss_box_reg_stage1: 0.6361  loss_cls_stage2: 0.2736  loss_box_reg_stage2: 0.8803  loss_mask: 0.08296  loss_rpn_cls: 0.01516  loss_rpn_loc: 0.04121  time: 0.6238  data_time: 0.0054  lr: 0.00016  max_mem: 19679M
[07/29 17:06:44] d2.utils.events INFO:  eta: 7:30:29  iter: 24399  total_loss: 2.884  loss_cls_stage0: 0.2457  loss_box_reg_stage0: 0.2577  loss_cls_stage1: 0.266  loss_box_reg_stage1: 0.693  loss_cls_stage2: 0.2578  loss_box_reg_stage2: 0.8946  loss_mask: 0.07227  loss_rpn_cls: 0.01762  loss_rpn_loc: 0.04938  time: 0.6238  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:06:57] d2.utils.events INFO:  eta: 7:30:05  iter: 24419  total_loss: 2.873  loss_cls_stage0: 0.2535  loss_box_reg_stage0: 0.2595  loss_cls_stage1: 0.2692  loss_box_reg_stage1: 0.6673  loss_cls_stage2: 0.2554  loss_box_reg_stage2: 0.8882  loss_mask: 0.07231  loss_rpn_cls: 0.01488  loss_rpn_loc: 0.04389  time: 0.6238  data_time: 0.0054  lr: 0.00016  max_mem: 19679M
[07/29 17:07:09] d2.utils.events INFO:  eta: 7:29:17  iter: 24439  total_loss: 2.763  loss_cls_stage0: 0.2656  loss_box_reg_stage0: 0.2486  loss_cls_stage1: 0.2717  loss_box_reg_stage1: 0.6148  loss_cls_stage2: 0.2675  loss_box_reg_stage2: 0.8157  loss_mask: 0.07363  loss_rpn_cls: 0.02454  loss_rpn_loc: 0.05522  time: 0.6238  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:07:21] d2.utils.events INFO:  eta: 7:29:12  iter: 24459  total_loss: 2.545  loss_cls_stage0: 0.2471  loss_box_reg_stage0: 0.2559  loss_cls_stage1: 0.2466  loss_box_reg_stage1: 0.6189  loss_cls_stage2: 0.2471  loss_box_reg_stage2: 0.8364  loss_mask: 0.07814  loss_rpn_cls: 0.01437  loss_rpn_loc: 0.04398  time: 0.6237  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:07:34] d2.utils.events INFO:  eta: 7:28:40  iter: 24479  total_loss: 2.966  loss_cls_stage0: 0.2593  loss_box_reg_stage0: 0.2518  loss_cls_stage1: 0.2815  loss_box_reg_stage1: 0.7052  loss_cls_stage2: 0.2663  loss_box_reg_stage2: 0.975  loss_mask: 0.07473  loss_rpn_cls: 0.0107  loss_rpn_loc: 0.04231  time: 0.6237  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:07:46] d2.utils.events INFO:  eta: 7:28:45  iter: 24499  total_loss: 2.635  loss_cls_stage0: 0.2485  loss_box_reg_stage0: 0.2417  loss_cls_stage1: 0.2568  loss_box_reg_stage1: 0.6329  loss_cls_stage2: 0.2712  loss_box_reg_stage2: 0.8966  loss_mask: 0.06345  loss_rpn_cls: 0.0105  loss_rpn_loc: 0.03802  time: 0.6237  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:07:59] d2.utils.events INFO:  eta: 7:28:26  iter: 24519  total_loss: 2.673  loss_cls_stage0: 0.2586  loss_box_reg_stage0: 0.26  loss_cls_stage1: 0.2496  loss_box_reg_stage1: 0.6482  loss_cls_stage2: 0.2326  loss_box_reg_stage2: 0.8888  loss_mask: 0.07036  loss_rpn_cls: 0.01025  loss_rpn_loc: 0.0403  time: 0.6237  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 17:08:11] d2.utils.events INFO:  eta: 7:27:46  iter: 24539  total_loss: 2.973  loss_cls_stage0: 0.2821  loss_box_reg_stage0: 0.2568  loss_cls_stage1: 0.3  loss_box_reg_stage1: 0.6969  loss_cls_stage2: 0.276  loss_box_reg_stage2: 0.9262  loss_mask: 0.0713  loss_rpn_cls: 0.01548  loss_rpn_loc: 0.04691  time: 0.6237  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:08:24] d2.utils.events INFO:  eta: 7:27:40  iter: 24559  total_loss: 2.874  loss_cls_stage0: 0.2591  loss_box_reg_stage0: 0.2639  loss_cls_stage1: 0.2868  loss_box_reg_stage1: 0.6918  loss_cls_stage2: 0.2573  loss_box_reg_stage2: 0.9398  loss_mask: 0.08769  loss_rpn_cls: 0.01392  loss_rpn_loc: 0.04322  time: 0.6238  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:08:36] d2.utils.events INFO:  eta: 7:27:25  iter: 24579  total_loss: 2.846  loss_cls_stage0: 0.2626  loss_box_reg_stage0: 0.2636  loss_cls_stage1: 0.2861  loss_box_reg_stage1: 0.6525  loss_cls_stage2: 0.2748  loss_box_reg_stage2: 0.9047  loss_mask: 0.07761  loss_rpn_cls: 0.009507  loss_rpn_loc: 0.04117  time: 0.6237  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:08:49] d2.utils.events INFO:  eta: 7:27:42  iter: 24599  total_loss: 2.477  loss_cls_stage0: 0.2145  loss_box_reg_stage0: 0.2331  loss_cls_stage1: 0.2416  loss_box_reg_stage1: 0.6157  loss_cls_stage2: 0.2291  loss_box_reg_stage2: 0.8375  loss_mask: 0.06733  loss_rpn_cls: 0.01193  loss_rpn_loc: 0.0402  time: 0.6238  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:09:02] d2.utils.events INFO:  eta: 7:27:42  iter: 24619  total_loss: 2.973  loss_cls_stage0: 0.2732  loss_box_reg_stage0: 0.2574  loss_cls_stage1: 0.3001  loss_box_reg_stage1: 0.6922  loss_cls_stage2: 0.2799  loss_box_reg_stage2: 0.9315  loss_mask: 0.06995  loss_rpn_cls: 0.01513  loss_rpn_loc: 0.04207  time: 0.6238  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:09:14] d2.utils.events INFO:  eta: 7:27:51  iter: 24639  total_loss: 2.853  loss_cls_stage0: 0.2385  loss_box_reg_stage0: 0.2386  loss_cls_stage1: 0.2931  loss_box_reg_stage1: 0.6418  loss_cls_stage2: 0.2609  loss_box_reg_stage2: 0.833  loss_mask: 0.06971  loss_rpn_cls: 0.01638  loss_rpn_loc: 0.05238  time: 0.6238  data_time: 0.0045  lr: 0.00016  max_mem: 19679M
[07/29 17:09:27] d2.utils.events INFO:  eta: 7:27:54  iter: 24659  total_loss: 2.808  loss_cls_stage0: 0.254  loss_box_reg_stage0: 0.2409  loss_cls_stage1: 0.2854  loss_box_reg_stage1: 0.6367  loss_cls_stage2: 0.2644  loss_box_reg_stage2: 0.8906  loss_mask: 0.07681  loss_rpn_cls: 0.02013  loss_rpn_loc: 0.06097  time: 0.6238  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:09:39] d2.utils.events INFO:  eta: 7:27:05  iter: 24679  total_loss: 3.026  loss_cls_stage0: 0.276  loss_box_reg_stage0: 0.2774  loss_cls_stage1: 0.2947  loss_box_reg_stage1: 0.707  loss_cls_stage2: 0.2816  loss_box_reg_stage2: 0.9407  loss_mask: 0.08027  loss_rpn_cls: 0.01432  loss_rpn_loc: 0.0468  time: 0.6238  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:09:52] d2.utils.events INFO:  eta: 7:26:52  iter: 24699  total_loss: 2.929  loss_cls_stage0: 0.2851  loss_box_reg_stage0: 0.2687  loss_cls_stage1: 0.3053  loss_box_reg_stage1: 0.7066  loss_cls_stage2: 0.3054  loss_box_reg_stage2: 0.9406  loss_mask: 0.07474  loss_rpn_cls: 0.01802  loss_rpn_loc: 0.05461  time: 0.6238  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:10:05] d2.utils.events INFO:  eta: 7:27:17  iter: 24719  total_loss: 2.868  loss_cls_stage0: 0.2704  loss_box_reg_stage0: 0.2599  loss_cls_stage1: 0.2785  loss_box_reg_stage1: 0.6606  loss_cls_stage2: 0.2731  loss_box_reg_stage2: 0.9067  loss_mask: 0.07144  loss_rpn_cls: 0.01198  loss_rpn_loc: 0.04101  time: 0.6238  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:10:17] d2.utils.events INFO:  eta: 7:26:48  iter: 24739  total_loss: 2.512  loss_cls_stage0: 0.2384  loss_box_reg_stage0: 0.2617  loss_cls_stage1: 0.2512  loss_box_reg_stage1: 0.5957  loss_cls_stage2: 0.2171  loss_box_reg_stage2: 0.7667  loss_mask: 0.07973  loss_rpn_cls: 0.02063  loss_rpn_loc: 0.04135  time: 0.6238  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:10:29] d2.utils.events INFO:  eta: 7:26:55  iter: 24759  total_loss: 2.559  loss_cls_stage0: 0.24  loss_box_reg_stage0: 0.259  loss_cls_stage1: 0.2313  loss_box_reg_stage1: 0.5857  loss_cls_stage2: 0.2138  loss_box_reg_stage2: 0.7439  loss_mask: 0.07611  loss_rpn_cls: 0.01532  loss_rpn_loc: 0.04909  time: 0.6238  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:10:41] d2.utils.events INFO:  eta: 7:26:39  iter: 24779  total_loss: 2.418  loss_cls_stage0: 0.2175  loss_box_reg_stage0: 0.2419  loss_cls_stage1: 0.2334  loss_box_reg_stage1: 0.5662  loss_cls_stage2: 0.2214  loss_box_reg_stage2: 0.7655  loss_mask: 0.06706  loss_rpn_cls: 0.02016  loss_rpn_loc: 0.0585  time: 0.6237  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:10:53] d2.utils.events INFO:  eta: 7:25:41  iter: 24799  total_loss: 2.774  loss_cls_stage0: 0.2646  loss_box_reg_stage0: 0.2796  loss_cls_stage1: 0.2656  loss_box_reg_stage1: 0.6629  loss_cls_stage2: 0.2419  loss_box_reg_stage2: 0.8136  loss_mask: 0.07832  loss_rpn_cls: 0.01812  loss_rpn_loc: 0.05288  time: 0.6237  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:11:05] d2.utils.events INFO:  eta: 7:25:11  iter: 24819  total_loss: 2.533  loss_cls_stage0: 0.2307  loss_box_reg_stage0: 0.2593  loss_cls_stage1: 0.2514  loss_box_reg_stage1: 0.6143  loss_cls_stage2: 0.2364  loss_box_reg_stage2: 0.8142  loss_mask: 0.06733  loss_rpn_cls: 0.02072  loss_rpn_loc: 0.04873  time: 0.6237  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:11:18] d2.utils.events INFO:  eta: 7:25:02  iter: 24839  total_loss: 2.67  loss_cls_stage0: 0.2426  loss_box_reg_stage0: 0.2599  loss_cls_stage1: 0.2479  loss_box_reg_stage1: 0.6442  loss_cls_stage2: 0.2179  loss_box_reg_stage2: 0.8289  loss_mask: 0.07816  loss_rpn_cls: 0.02113  loss_rpn_loc: 0.04797  time: 0.6237  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:11:30] d2.utils.events INFO:  eta: 7:24:46  iter: 24859  total_loss: 2.656  loss_cls_stage0: 0.2536  loss_box_reg_stage0: 0.2557  loss_cls_stage1: 0.2614  loss_box_reg_stage1: 0.6392  loss_cls_stage2: 0.2433  loss_box_reg_stage2: 0.8497  loss_mask: 0.07687  loss_rpn_cls: 0.01363  loss_rpn_loc: 0.04318  time: 0.6237  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:11:43] d2.utils.events INFO:  eta: 7:24:47  iter: 24879  total_loss: 2.627  loss_cls_stage0: 0.2269  loss_box_reg_stage0: 0.2419  loss_cls_stage1: 0.2493  loss_box_reg_stage1: 0.6102  loss_cls_stage2: 0.2482  loss_box_reg_stage2: 0.8037  loss_mask: 0.06969  loss_rpn_cls: 0.01118  loss_rpn_loc: 0.03604  time: 0.6237  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:11:56] d2.utils.events INFO:  eta: 7:24:29  iter: 24899  total_loss: 2.748  loss_cls_stage0: 0.2535  loss_box_reg_stage0: 0.2517  loss_cls_stage1: 0.253  loss_box_reg_stage1: 0.6419  loss_cls_stage2: 0.2357  loss_box_reg_stage2: 0.9088  loss_mask: 0.0715  loss_rpn_cls: 0.01668  loss_rpn_loc: 0.05012  time: 0.6237  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:12:08] d2.utils.events INFO:  eta: 7:25:11  iter: 24919  total_loss: 2.526  loss_cls_stage0: 0.2224  loss_box_reg_stage0: 0.249  loss_cls_stage1: 0.2334  loss_box_reg_stage1: 0.5786  loss_cls_stage2: 0.2174  loss_box_reg_stage2: 0.7805  loss_mask: 0.06641  loss_rpn_cls: 0.02589  loss_rpn_loc: 0.04311  time: 0.6237  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:12:20] d2.utils.events INFO:  eta: 7:24:04  iter: 24939  total_loss: 2.475  loss_cls_stage0: 0.2153  loss_box_reg_stage0: 0.2398  loss_cls_stage1: 0.2373  loss_box_reg_stage1: 0.5994  loss_cls_stage2: 0.2247  loss_box_reg_stage2: 0.8181  loss_mask: 0.07406  loss_rpn_cls: 0.01792  loss_rpn_loc: 0.05063  time: 0.6237  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:12:33] d2.utils.events INFO:  eta: 7:24:09  iter: 24959  total_loss: 3.162  loss_cls_stage0: 0.274  loss_box_reg_stage0: 0.2789  loss_cls_stage1: 0.2859  loss_box_reg_stage1: 0.756  loss_cls_stage2: 0.2946  loss_box_reg_stage2: 0.9598  loss_mask: 0.08077  loss_rpn_cls: 0.01159  loss_rpn_loc: 0.04217  time: 0.6237  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 17:12:45] d2.utils.events INFO:  eta: 7:24:12  iter: 24979  total_loss: 2.593  loss_cls_stage0: 0.2359  loss_box_reg_stage0: 0.2533  loss_cls_stage1: 0.2492  loss_box_reg_stage1: 0.6405  loss_cls_stage2: 0.2558  loss_box_reg_stage2: 0.9221  loss_mask: 0.07282  loss_rpn_cls: 0.01415  loss_rpn_loc: 0.04479  time: 0.6237  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:12:58] fvcore.common.checkpoint INFO: Saving checkpoint to ./output/model_0024999.pth
[07/29 17:13:00] d2.data.datasets.coco INFO: Loading datasets/tless/tless_annotations_test.json takes 1.33 seconds.
[07/29 17:13:00] d2.data.datasets.coco INFO: Loaded 10080 images in COCO format from datasets/tless/tless_annotations_test.json
[07/29 17:13:01] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[07/29 17:13:01] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[07/29 17:13:01] d2.data.common INFO: Serializing 10080 elements to byte tensors and concatenating them all ...
[07/29 17:13:01] d2.data.common INFO: Serialized dataset takes 50.77 MiB
[07/29 17:13:03] d2.evaluation.evaluator INFO: Start inference on 10080 batches
[07/29 17:13:04] d2.evaluation.evaluator INFO: Inference done 11/10080. Dataloading: 0.0006 s/iter. Inference: 0.1018 s/iter. Eval: 0.0022 s/iter. Total: 0.1047 s/iter. ETA=0:17:33
[07/29 17:13:09] d2.evaluation.evaluator INFO: Inference done 59/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0023 s/iter. Total: 0.1051 s/iter. ETA=0:17:33
[07/29 17:13:14] d2.evaluation.evaluator INFO: Inference done 107/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0022 s/iter. Total: 0.1051 s/iter. ETA=0:17:28
[07/29 17:13:19] d2.evaluation.evaluator INFO: Inference done 155/10080. Dataloading: 0.0009 s/iter. Inference: 0.1020 s/iter. Eval: 0.0023 s/iter. Total: 0.1053 s/iter. ETA=0:17:24
[07/29 17:13:24] d2.evaluation.evaluator INFO: Inference done 203/10080. Dataloading: 0.0009 s/iter. Inference: 0.1021 s/iter. Eval: 0.0023 s/iter. Total: 0.1054 s/iter. ETA=0:17:20
[07/29 17:13:29] d2.evaluation.evaluator INFO: Inference done 251/10080. Dataloading: 0.0009 s/iter. Inference: 0.1021 s/iter. Eval: 0.0023 s/iter. Total: 0.1053 s/iter. ETA=0:17:15
[07/29 17:13:34] d2.evaluation.evaluator INFO: Inference done 300/10080. Dataloading: 0.0009 s/iter. Inference: 0.1018 s/iter. Eval: 0.0023 s/iter. Total: 0.1050 s/iter. ETA=0:17:06
[07/29 17:13:39] d2.evaluation.evaluator INFO: Inference done 349/10080. Dataloading: 0.0008 s/iter. Inference: 0.1015 s/iter. Eval: 0.0023 s/iter. Total: 0.1047 s/iter. ETA=0:16:58
[07/29 17:13:45] d2.evaluation.evaluator INFO: Inference done 398/10080. Dataloading: 0.0008 s/iter. Inference: 0.1014 s/iter. Eval: 0.0023 s/iter. Total: 0.1046 s/iter. ETA=0:16:52
[07/29 17:13:50] d2.evaluation.evaluator INFO: Inference done 446/10080. Dataloading: 0.0008 s/iter. Inference: 0.1014 s/iter. Eval: 0.0024 s/iter. Total: 0.1047 s/iter. ETA=0:16:48
[07/29 17:13:55] d2.evaluation.evaluator INFO: Inference done 494/10080. Dataloading: 0.0008 s/iter. Inference: 0.1015 s/iter. Eval: 0.0024 s/iter. Total: 0.1048 s/iter. ETA=0:16:44
[07/29 17:14:00] d2.evaluation.evaluator INFO: Inference done 543/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0024 s/iter. Total: 0.1047 s/iter. ETA=0:16:38
[07/29 17:14:05] d2.evaluation.evaluator INFO: Inference done 592/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0023 s/iter. Total: 0.1046 s/iter. ETA=0:16:32
[07/29 17:14:10] d2.evaluation.evaluator INFO: Inference done 640/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0023 s/iter. Total: 0.1046 s/iter. ETA=0:16:27
[07/29 17:14:15] d2.evaluation.evaluator INFO: Inference done 688/10080. Dataloading: 0.0009 s/iter. Inference: 0.1013 s/iter. Eval: 0.0023 s/iter. Total: 0.1046 s/iter. ETA=0:16:22
[07/29 17:14:20] d2.evaluation.evaluator INFO: Inference done 736/10080. Dataloading: 0.0009 s/iter. Inference: 0.1013 s/iter. Eval: 0.0024 s/iter. Total: 0.1046 s/iter. ETA=0:16:17
[07/29 17:14:25] d2.evaluation.evaluator INFO: Inference done 784/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0024 s/iter. Total: 0.1047 s/iter. ETA=0:16:13
[07/29 17:14:30] d2.evaluation.evaluator INFO: Inference done 832/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0024 s/iter. Total: 0.1047 s/iter. ETA=0:16:08
[07/29 17:14:35] d2.evaluation.evaluator INFO: Inference done 880/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0025 s/iter. Total: 0.1048 s/iter. ETA=0:16:04
[07/29 17:14:40] d2.evaluation.evaluator INFO: Inference done 928/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0025 s/iter. Total: 0.1048 s/iter. ETA=0:15:58
[07/29 17:14:45] d2.evaluation.evaluator INFO: Inference done 976/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0024 s/iter. Total: 0.1048 s/iter. ETA=0:15:53
[07/29 17:14:50] d2.evaluation.evaluator INFO: Inference done 1025/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0024 s/iter. Total: 0.1047 s/iter. ETA=0:15:48
[07/29 17:14:55] d2.evaluation.evaluator INFO: Inference done 1074/10080. Dataloading: 0.0009 s/iter. Inference: 0.1014 s/iter. Eval: 0.0024 s/iter. Total: 0.1047 s/iter. ETA=0:15:42
[07/29 17:15:00] d2.evaluation.evaluator INFO: Inference done 1123/10080. Dataloading: 0.0009 s/iter. Inference: 0.1013 s/iter. Eval: 0.0023 s/iter. Total: 0.1046 s/iter. ETA=0:15:37
[07/29 17:15:05] d2.evaluation.evaluator INFO: Inference done 1172/10080. Dataloading: 0.0009 s/iter. Inference: 0.1013 s/iter. Eval: 0.0023 s/iter. Total: 0.1046 s/iter. ETA=0:15:31
[07/29 17:15:11] d2.evaluation.evaluator INFO: Inference done 1221/10080. Dataloading: 0.0009 s/iter. Inference: 0.1013 s/iter. Eval: 0.0023 s/iter. Total: 0.1046 s/iter. ETA=0:15:26
[07/29 17:15:16] d2.evaluation.evaluator INFO: Inference done 1270/10080. Dataloading: 0.0009 s/iter. Inference: 0.1013 s/iter. Eval: 0.0023 s/iter. Total: 0.1046 s/iter. ETA=0:15:21
[07/29 17:15:21] d2.evaluation.evaluator INFO: Inference done 1319/10080. Dataloading: 0.0009 s/iter. Inference: 0.1013 s/iter. Eval: 0.0022 s/iter. Total: 0.1045 s/iter. ETA=0:15:15
[07/29 17:15:26] d2.evaluation.evaluator INFO: Inference done 1368/10080. Dataloading: 0.0009 s/iter. Inference: 0.1013 s/iter. Eval: 0.0022 s/iter. Total: 0.1045 s/iter. ETA=0:15:10
[07/29 17:15:31] d2.evaluation.evaluator INFO: Inference done 1417/10080. Dataloading: 0.0009 s/iter. Inference: 0.1013 s/iter. Eval: 0.0022 s/iter. Total: 0.1045 s/iter. ETA=0:15:05
[07/29 17:15:36] d2.evaluation.evaluator INFO: Inference done 1465/10080. Dataloading: 0.0009 s/iter. Inference: 0.1013 s/iter. Eval: 0.0022 s/iter. Total: 0.1045 s/iter. ETA=0:15:00
[07/29 17:15:41] d2.evaluation.evaluator INFO: Inference done 1513/10080. Dataloading: 0.0009 s/iter. Inference: 0.1013 s/iter. Eval: 0.0022 s/iter. Total: 0.1045 s/iter. ETA=0:14:55
[07/29 17:15:46] d2.evaluation.evaluator INFO: Inference done 1562/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0022 s/iter. Total: 0.1045 s/iter. ETA=0:14:49
[07/29 17:15:51] d2.evaluation.evaluator INFO: Inference done 1610/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0022 s/iter. Total: 0.1045 s/iter. ETA=0:14:45
[07/29 17:15:56] d2.evaluation.evaluator INFO: Inference done 1658/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0022 s/iter. Total: 0.1045 s/iter. ETA=0:14:40
[07/29 17:16:01] d2.evaluation.evaluator INFO: Inference done 1706/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0022 s/iter. Total: 0.1045 s/iter. ETA=0:14:35
[07/29 17:16:06] d2.evaluation.evaluator INFO: Inference done 1754/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0022 s/iter. Total: 0.1045 s/iter. ETA=0:14:30
[07/29 17:16:11] d2.evaluation.evaluator INFO: Inference done 1802/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:14:25
[07/29 17:16:16] d2.evaluation.evaluator INFO: Inference done 1850/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:14:20
[07/29 17:16:21] d2.evaluation.evaluator INFO: Inference done 1898/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:14:15
[07/29 17:16:26] d2.evaluation.evaluator INFO: Inference done 1946/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:14:10
[07/29 17:16:31] d2.evaluation.evaluator INFO: Inference done 1994/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1045 s/iter. ETA=0:14:05
[07/29 17:16:36] d2.evaluation.evaluator INFO: Inference done 2042/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:14:00
[07/29 17:16:41] d2.evaluation.evaluator INFO: Inference done 2090/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:13:55
[07/29 17:16:47] d2.evaluation.evaluator INFO: Inference done 2138/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:13:50
[07/29 17:16:52] d2.evaluation.evaluator INFO: Inference done 2186/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:13:45
[07/29 17:16:57] d2.evaluation.evaluator INFO: Inference done 2234/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1047 s/iter. ETA=0:13:41
[07/29 17:17:02] d2.evaluation.evaluator INFO: Inference done 2282/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0022 s/iter. Total: 0.1047 s/iter. ETA=0:13:36
[07/29 17:17:07] d2.evaluation.evaluator INFO: Inference done 2330/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0022 s/iter. Total: 0.1047 s/iter. ETA=0:13:31
[07/29 17:17:12] d2.evaluation.evaluator INFO: Inference done 2378/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0022 s/iter. Total: 0.1047 s/iter. ETA=0:13:26
[07/29 17:17:17] d2.evaluation.evaluator INFO: Inference done 2425/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0022 s/iter. Total: 0.1048 s/iter. ETA=0:13:22
[07/29 17:17:22] d2.evaluation.evaluator INFO: Inference done 2473/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0023 s/iter. Total: 0.1048 s/iter. ETA=0:13:17
[07/29 17:17:27] d2.evaluation.evaluator INFO: Inference done 2521/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0023 s/iter. Total: 0.1048 s/iter. ETA=0:13:12
[07/29 17:17:32] d2.evaluation.evaluator INFO: Inference done 2570/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0023 s/iter. Total: 0.1047 s/iter. ETA=0:13:06
[07/29 17:17:37] d2.evaluation.evaluator INFO: Inference done 2619/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0023 s/iter. Total: 0.1047 s/iter. ETA=0:13:01
[07/29 17:17:42] d2.evaluation.evaluator INFO: Inference done 2668/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0023 s/iter. Total: 0.1047 s/iter. ETA=0:12:55
[07/29 17:17:47] d2.evaluation.evaluator INFO: Inference done 2716/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1047 s/iter. ETA=0:12:50
[07/29 17:17:52] d2.evaluation.evaluator INFO: Inference done 2765/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1047 s/iter. ETA=0:12:45
[07/29 17:17:57] d2.evaluation.evaluator INFO: Inference done 2813/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1047 s/iter. ETA=0:12:40
[07/29 17:18:02] d2.evaluation.evaluator INFO: Inference done 2861/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1047 s/iter. ETA=0:12:35
[07/29 17:18:07] d2.evaluation.evaluator INFO: Inference done 2909/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1047 s/iter. ETA=0:12:30
[07/29 17:18:12] d2.evaluation.evaluator INFO: Inference done 2957/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0023 s/iter. Total: 0.1047 s/iter. ETA=0:12:25
[07/29 17:18:17] d2.evaluation.evaluator INFO: Inference done 3005/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0023 s/iter. Total: 0.1047 s/iter. ETA=0:12:20
[07/29 17:18:23] d2.evaluation.evaluator INFO: Inference done 3052/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0023 s/iter. Total: 0.1048 s/iter. ETA=0:12:16
[07/29 17:18:28] d2.evaluation.evaluator INFO: Inference done 3099/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0023 s/iter. Total: 0.1048 s/iter. ETA=0:12:11
[07/29 17:18:33] d2.evaluation.evaluator INFO: Inference done 3146/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0024 s/iter. Total: 0.1048 s/iter. ETA=0:12:06
[07/29 17:18:38] d2.evaluation.evaluator INFO: Inference done 3193/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0024 s/iter. Total: 0.1049 s/iter. ETA=0:12:02
[07/29 17:18:43] d2.evaluation.evaluator INFO: Inference done 3240/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0024 s/iter. Total: 0.1049 s/iter. ETA=0:11:57
[07/29 17:18:48] d2.evaluation.evaluator INFO: Inference done 3287/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0024 s/iter. Total: 0.1049 s/iter. ETA=0:11:52
[07/29 17:18:53] d2.evaluation.evaluator INFO: Inference done 3334/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0025 s/iter. Total: 0.1050 s/iter. ETA=0:11:48
[07/29 17:18:58] d2.evaluation.evaluator INFO: Inference done 3381/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0025 s/iter. Total: 0.1050 s/iter. ETA=0:11:43
[07/29 17:19:03] d2.evaluation.evaluator INFO: Inference done 3428/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0025 s/iter. Total: 0.1050 s/iter. ETA=0:11:38
[07/29 17:19:08] d2.evaluation.evaluator INFO: Inference done 3475/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0026 s/iter. Total: 0.1051 s/iter. ETA=0:11:34
[07/29 17:19:13] d2.evaluation.evaluator INFO: Inference done 3522/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0026 s/iter. Total: 0.1051 s/iter. ETA=0:11:29
[07/29 17:19:18] d2.evaluation.evaluator INFO: Inference done 3569/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0026 s/iter. Total: 0.1052 s/iter. ETA=0:11:24
[07/29 17:19:23] d2.evaluation.evaluator INFO: Inference done 3616/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0026 s/iter. Total: 0.1052 s/iter. ETA=0:11:20
[07/29 17:19:28] d2.evaluation.evaluator INFO: Inference done 3663/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0027 s/iter. Total: 0.1052 s/iter. ETA=0:11:15
[07/29 17:19:33] d2.evaluation.evaluator INFO: Inference done 3710/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0027 s/iter. Total: 0.1053 s/iter. ETA=0:11:10
[07/29 17:19:38] d2.evaluation.evaluator INFO: Inference done 3757/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0027 s/iter. Total: 0.1053 s/iter. ETA=0:11:05
[07/29 17:19:43] d2.evaluation.evaluator INFO: Inference done 3804/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0027 s/iter. Total: 0.1053 s/iter. ETA=0:11:00
[07/29 17:19:48] d2.evaluation.evaluator INFO: Inference done 3851/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0027 s/iter. Total: 0.1053 s/iter. ETA=0:10:56
[07/29 17:19:53] d2.evaluation.evaluator INFO: Inference done 3898/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0028 s/iter. Total: 0.1053 s/iter. ETA=0:10:51
[07/29 17:19:59] d2.evaluation.evaluator INFO: Inference done 3945/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0028 s/iter. Total: 0.1054 s/iter. ETA=0:10:46
[07/29 17:20:04] d2.evaluation.evaluator INFO: Inference done 3992/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0028 s/iter. Total: 0.1054 s/iter. ETA=0:10:41
[07/29 17:20:09] d2.evaluation.evaluator INFO: Inference done 4039/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0028 s/iter. Total: 0.1054 s/iter. ETA=0:10:36
[07/29 17:20:14] d2.evaluation.evaluator INFO: Inference done 4085/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0029 s/iter. Total: 0.1055 s/iter. ETA=0:10:32
[07/29 17:20:19] d2.evaluation.evaluator INFO: Inference done 4131/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0029 s/iter. Total: 0.1055 s/iter. ETA=0:10:27
[07/29 17:20:24] d2.evaluation.evaluator INFO: Inference done 4177/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0030 s/iter. Total: 0.1056 s/iter. ETA=0:10:23
[07/29 17:20:29] d2.evaluation.evaluator INFO: Inference done 4223/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0030 s/iter. Total: 0.1056 s/iter. ETA=0:10:18
[07/29 17:20:34] d2.evaluation.evaluator INFO: Inference done 4268/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0030 s/iter. Total: 0.1057 s/iter. ETA=0:10:14
[07/29 17:20:39] d2.evaluation.evaluator INFO: Inference done 4313/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0031 s/iter. Total: 0.1058 s/iter. ETA=0:10:09
[07/29 17:20:44] d2.evaluation.evaluator INFO: Inference done 4358/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0031 s/iter. Total: 0.1058 s/iter. ETA=0:10:05
[07/29 17:20:49] d2.evaluation.evaluator INFO: Inference done 4403/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0032 s/iter. Total: 0.1059 s/iter. ETA=0:10:01
[07/29 17:20:54] d2.evaluation.evaluator INFO: Inference done 4448/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0032 s/iter. Total: 0.1059 s/iter. ETA=0:09:56
[07/29 17:20:59] d2.evaluation.evaluator INFO: Inference done 4493/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0033 s/iter. Total: 0.1060 s/iter. ETA=0:09:52
[07/29 17:21:04] d2.evaluation.evaluator INFO: Inference done 4538/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0033 s/iter. Total: 0.1061 s/iter. ETA=0:09:47
[07/29 17:21:09] d2.evaluation.evaluator INFO: Inference done 4585/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0033 s/iter. Total: 0.1061 s/iter. ETA=0:09:42
[07/29 17:21:14] d2.evaluation.evaluator INFO: Inference done 4632/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0034 s/iter. Total: 0.1061 s/iter. ETA=0:09:38
[07/29 17:21:19] d2.evaluation.evaluator INFO: Inference done 4679/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0034 s/iter. Total: 0.1061 s/iter. ETA=0:09:33
[07/29 17:21:24] d2.evaluation.evaluator INFO: Inference done 4726/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0034 s/iter. Total: 0.1061 s/iter. ETA=0:09:28
[07/29 17:21:29] d2.evaluation.evaluator INFO: Inference done 4773/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0034 s/iter. Total: 0.1061 s/iter. ETA=0:09:23
[07/29 17:21:34] d2.evaluation.evaluator INFO: Inference done 4820/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0034 s/iter. Total: 0.1061 s/iter. ETA=0:09:18
[07/29 17:21:40] d2.evaluation.evaluator INFO: Inference done 4868/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0034 s/iter. Total: 0.1061 s/iter. ETA=0:09:13
[07/29 17:21:45] d2.evaluation.evaluator INFO: Inference done 4915/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0034 s/iter. Total: 0.1062 s/iter. ETA=0:09:08
[07/29 17:21:50] d2.evaluation.evaluator INFO: Inference done 4962/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0035 s/iter. Total: 0.1062 s/iter. ETA=0:09:03
[07/29 17:21:55] d2.evaluation.evaluator INFO: Inference done 5008/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0035 s/iter. Total: 0.1062 s/iter. ETA=0:08:58
[07/29 17:22:00] d2.evaluation.evaluator INFO: Inference done 5055/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0035 s/iter. Total: 0.1062 s/iter. ETA=0:08:53
[07/29 17:22:05] d2.evaluation.evaluator INFO: Inference done 5103/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0035 s/iter. Total: 0.1062 s/iter. ETA=0:08:48
[07/29 17:22:10] d2.evaluation.evaluator INFO: Inference done 5151/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0035 s/iter. Total: 0.1062 s/iter. ETA=0:08:43
[07/29 17:22:15] d2.evaluation.evaluator INFO: Inference done 5199/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0035 s/iter. Total: 0.1062 s/iter. ETA=0:08:38
[07/29 17:22:20] d2.evaluation.evaluator INFO: Inference done 5247/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0035 s/iter. Total: 0.1062 s/iter. ETA=0:08:33
[07/29 17:22:25] d2.evaluation.evaluator INFO: Inference done 5295/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0035 s/iter. Total: 0.1062 s/iter. ETA=0:08:28
[07/29 17:22:30] d2.evaluation.evaluator INFO: Inference done 5343/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0035 s/iter. Total: 0.1062 s/iter. ETA=0:08:23
[07/29 17:22:35] d2.evaluation.evaluator INFO: Inference done 5391/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0035 s/iter. Total: 0.1062 s/iter. ETA=0:08:17
[07/29 17:22:40] d2.evaluation.evaluator INFO: Inference done 5439/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0035 s/iter. Total: 0.1062 s/iter. ETA=0:08:12
[07/29 17:22:45] d2.evaluation.evaluator INFO: Inference done 5487/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0035 s/iter. Total: 0.1062 s/iter. ETA=0:08:07
[07/29 17:22:51] d2.evaluation.evaluator INFO: Inference done 5535/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0035 s/iter. Total: 0.1062 s/iter. ETA=0:08:02
[07/29 17:22:56] d2.evaluation.evaluator INFO: Inference done 5582/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0035 s/iter. Total: 0.1062 s/iter. ETA=0:07:57
[07/29 17:23:01] d2.evaluation.evaluator INFO: Inference done 5629/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0035 s/iter. Total: 0.1062 s/iter. ETA=0:07:52
[07/29 17:23:06] d2.evaluation.evaluator INFO: Inference done 5676/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0035 s/iter. Total: 0.1062 s/iter. ETA=0:07:47
[07/29 17:23:11] d2.evaluation.evaluator INFO: Inference done 5723/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0035 s/iter. Total: 0.1062 s/iter. ETA=0:07:42
[07/29 17:23:16] d2.evaluation.evaluator INFO: Inference done 5770/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0035 s/iter. Total: 0.1062 s/iter. ETA=0:07:37
[07/29 17:23:21] d2.evaluation.evaluator INFO: Inference done 5817/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0035 s/iter. Total: 0.1063 s/iter. ETA=0:07:32
[07/29 17:23:26] d2.evaluation.evaluator INFO: Inference done 5864/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0035 s/iter. Total: 0.1063 s/iter. ETA=0:07:27
[07/29 17:23:31] d2.evaluation.evaluator INFO: Inference done 5911/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0035 s/iter. Total: 0.1063 s/iter. ETA=0:07:23
[07/29 17:23:36] d2.evaluation.evaluator INFO: Inference done 5959/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0036 s/iter. Total: 0.1063 s/iter. ETA=0:07:17
[07/29 17:23:41] d2.evaluation.evaluator INFO: Inference done 6007/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0036 s/iter. Total: 0.1063 s/iter. ETA=0:07:12
[07/29 17:23:46] d2.evaluation.evaluator INFO: Inference done 6054/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0036 s/iter. Total: 0.1063 s/iter. ETA=0:07:07
[07/29 17:23:51] d2.evaluation.evaluator INFO: Inference done 6098/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0036 s/iter. Total: 0.1063 s/iter. ETA=0:07:03
[07/29 17:23:56] d2.evaluation.evaluator INFO: Inference done 6145/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0036 s/iter. Total: 0.1064 s/iter. ETA=0:06:58
[07/29 17:24:01] d2.evaluation.evaluator INFO: Inference done 6192/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0036 s/iter. Total: 0.1064 s/iter. ETA=0:06:53
[07/29 17:24:06] d2.evaluation.evaluator INFO: Inference done 6238/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0036 s/iter. Total: 0.1064 s/iter. ETA=0:06:48
[07/29 17:24:12] d2.evaluation.evaluator INFO: Inference done 6285/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0036 s/iter. Total: 0.1064 s/iter. ETA=0:06:43
[07/29 17:24:17] d2.evaluation.evaluator INFO: Inference done 6331/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0037 s/iter. Total: 0.1064 s/iter. ETA=0:06:38
[07/29 17:24:22] d2.evaluation.evaluator INFO: Inference done 6378/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0037 s/iter. Total: 0.1064 s/iter. ETA=0:06:33
[07/29 17:24:27] d2.evaluation.evaluator INFO: Inference done 6424/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0037 s/iter. Total: 0.1064 s/iter. ETA=0:06:29
[07/29 17:24:32] d2.evaluation.evaluator INFO: Inference done 6470/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0037 s/iter. Total: 0.1065 s/iter. ETA=0:06:24
[07/29 17:24:37] d2.evaluation.evaluator INFO: Inference done 6516/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0037 s/iter. Total: 0.1065 s/iter. ETA=0:06:19
[07/29 17:24:42] d2.evaluation.evaluator INFO: Inference done 6562/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0037 s/iter. Total: 0.1065 s/iter. ETA=0:06:14
[07/29 17:24:47] d2.evaluation.evaluator INFO: Inference done 6608/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0038 s/iter. Total: 0.1065 s/iter. ETA=0:06:09
[07/29 17:24:52] d2.evaluation.evaluator INFO: Inference done 6655/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0038 s/iter. Total: 0.1065 s/iter. ETA=0:06:04
[07/29 17:24:57] d2.evaluation.evaluator INFO: Inference done 6702/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0038 s/iter. Total: 0.1066 s/iter. ETA=0:05:59
[07/29 17:25:02] d2.evaluation.evaluator INFO: Inference done 6749/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0038 s/iter. Total: 0.1066 s/iter. ETA=0:05:54
[07/29 17:25:07] d2.evaluation.evaluator INFO: Inference done 6796/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0038 s/iter. Total: 0.1066 s/iter. ETA=0:05:50
[07/29 17:25:12] d2.evaluation.evaluator INFO: Inference done 6842/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0038 s/iter. Total: 0.1066 s/iter. ETA=0:05:45
[07/29 17:25:17] d2.evaluation.evaluator INFO: Inference done 6888/10080. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0038 s/iter. Total: 0.1066 s/iter. ETA=0:05:40
[07/29 17:25:22] d2.evaluation.evaluator INFO: Inference done 6934/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0038 s/iter. Total: 0.1066 s/iter. ETA=0:05:35
[07/29 17:25:27] d2.evaluation.evaluator INFO: Inference done 6980/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0039 s/iter. Total: 0.1067 s/iter. ETA=0:05:30
[07/29 17:25:32] d2.evaluation.evaluator INFO: Inference done 7026/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0039 s/iter. Total: 0.1067 s/iter. ETA=0:05:25
[07/29 17:25:37] d2.evaluation.evaluator INFO: Inference done 7072/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0039 s/iter. Total: 0.1067 s/iter. ETA=0:05:20
[07/29 17:25:42] d2.evaluation.evaluator INFO: Inference done 7119/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0039 s/iter. Total: 0.1067 s/iter. ETA=0:05:15
[07/29 17:25:47] d2.evaluation.evaluator INFO: Inference done 7166/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0039 s/iter. Total: 0.1067 s/iter. ETA=0:05:10
[07/29 17:25:52] d2.evaluation.evaluator INFO: Inference done 7213/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0039 s/iter. Total: 0.1067 s/iter. ETA=0:05:05
[07/29 17:25:58] d2.evaluation.evaluator INFO: Inference done 7261/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0039 s/iter. Total: 0.1067 s/iter. ETA=0:05:00
[07/29 17:26:03] d2.evaluation.evaluator INFO: Inference done 7308/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0039 s/iter. Total: 0.1067 s/iter. ETA=0:04:55
[07/29 17:26:08] d2.evaluation.evaluator INFO: Inference done 7355/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0039 s/iter. Total: 0.1067 s/iter. ETA=0:04:50
[07/29 17:26:13] d2.evaluation.evaluator INFO: Inference done 7403/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0039 s/iter. Total: 0.1067 s/iter. ETA=0:04:45
[07/29 17:26:18] d2.evaluation.evaluator INFO: Inference done 7451/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0039 s/iter. Total: 0.1067 s/iter. ETA=0:04:40
[07/29 17:26:23] d2.evaluation.evaluator INFO: Inference done 7498/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0039 s/iter. Total: 0.1067 s/iter. ETA=0:04:35
[07/29 17:26:28] d2.evaluation.evaluator INFO: Inference done 7545/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0039 s/iter. Total: 0.1067 s/iter. ETA=0:04:30
[07/29 17:26:33] d2.evaluation.evaluator INFO: Inference done 7592/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0039 s/iter. Total: 0.1067 s/iter. ETA=0:04:25
[07/29 17:26:38] d2.evaluation.evaluator INFO: Inference done 7639/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0039 s/iter. Total: 0.1067 s/iter. ETA=0:04:20
[07/29 17:26:43] d2.evaluation.evaluator INFO: Inference done 7686/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0039 s/iter. Total: 0.1067 s/iter. ETA=0:04:15
[07/29 17:26:48] d2.evaluation.evaluator INFO: Inference done 7733/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0039 s/iter. Total: 0.1067 s/iter. ETA=0:04:10
[07/29 17:26:53] d2.evaluation.evaluator INFO: Inference done 7780/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0039 s/iter. Total: 0.1067 s/iter. ETA=0:04:05
[07/29 17:26:58] d2.evaluation.evaluator INFO: Inference done 7827/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0039 s/iter. Total: 0.1067 s/iter. ETA=0:04:00
[07/29 17:27:03] d2.evaluation.evaluator INFO: Inference done 7874/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0039 s/iter. Total: 0.1067 s/iter. ETA=0:03:55
[07/29 17:27:08] d2.evaluation.evaluator INFO: Inference done 7921/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0039 s/iter. Total: 0.1067 s/iter. ETA=0:03:50
[07/29 17:27:13] d2.evaluation.evaluator INFO: Inference done 7967/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0039 s/iter. Total: 0.1068 s/iter. ETA=0:03:45
[07/29 17:27:18] d2.evaluation.evaluator INFO: Inference done 8013/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0039 s/iter. Total: 0.1068 s/iter. ETA=0:03:40
[07/29 17:27:23] d2.evaluation.evaluator INFO: Inference done 8059/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0040 s/iter. Total: 0.1068 s/iter. ETA=0:03:35
[07/29 17:27:28] d2.evaluation.evaluator INFO: Inference done 8105/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0040 s/iter. Total: 0.1068 s/iter. ETA=0:03:30
[07/29 17:27:33] d2.evaluation.evaluator INFO: Inference done 8151/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0040 s/iter. Total: 0.1068 s/iter. ETA=0:03:26
[07/29 17:27:38] d2.evaluation.evaluator INFO: Inference done 8197/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0040 s/iter. Total: 0.1068 s/iter. ETA=0:03:21
[07/29 17:27:43] d2.evaluation.evaluator INFO: Inference done 8243/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0040 s/iter. Total: 0.1068 s/iter. ETA=0:03:16
[07/29 17:27:49] d2.evaluation.evaluator INFO: Inference done 8289/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0040 s/iter. Total: 0.1068 s/iter. ETA=0:03:11
[07/29 17:27:54] d2.evaluation.evaluator INFO: Inference done 8335/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0040 s/iter. Total: 0.1069 s/iter. ETA=0:03:06
[07/29 17:27:59] d2.evaluation.evaluator INFO: Inference done 8381/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0040 s/iter. Total: 0.1069 s/iter. ETA=0:03:01
[07/29 17:28:04] d2.evaluation.evaluator INFO: Inference done 8427/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0040 s/iter. Total: 0.1069 s/iter. ETA=0:02:56
[07/29 17:28:09] d2.evaluation.evaluator INFO: Inference done 8473/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0040 s/iter. Total: 0.1069 s/iter. ETA=0:02:51
[07/29 17:28:14] d2.evaluation.evaluator INFO: Inference done 8519/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1069 s/iter. ETA=0:02:46
[07/29 17:28:19] d2.evaluation.evaluator INFO: Inference done 8566/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1069 s/iter. ETA=0:02:41
[07/29 17:28:24] d2.evaluation.evaluator INFO: Inference done 8611/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1070 s/iter. ETA=0:02:37
[07/29 17:28:29] d2.evaluation.evaluator INFO: Inference done 8657/10080. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0041 s/iter. Total: 0.1070 s/iter. ETA=0:02:32
[07/29 17:28:34] d2.evaluation.evaluator INFO: Inference done 8703/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0041 s/iter. Total: 0.1070 s/iter. ETA=0:02:27
[07/29 17:28:39] d2.evaluation.evaluator INFO: Inference done 8749/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0041 s/iter. Total: 0.1070 s/iter. ETA=0:02:22
[07/29 17:28:44] d2.evaluation.evaluator INFO: Inference done 8795/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0041 s/iter. Total: 0.1071 s/iter. ETA=0:02:17
[07/29 17:28:49] d2.evaluation.evaluator INFO: Inference done 8840/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0042 s/iter. Total: 0.1071 s/iter. ETA=0:02:12
[07/29 17:28:54] d2.evaluation.evaluator INFO: Inference done 8885/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0042 s/iter. Total: 0.1071 s/iter. ETA=0:02:07
[07/29 17:29:00] d2.evaluation.evaluator INFO: Inference done 8931/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0042 s/iter. Total: 0.1071 s/iter. ETA=0:02:03
[07/29 17:29:05] d2.evaluation.evaluator INFO: Inference done 8977/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0042 s/iter. Total: 0.1071 s/iter. ETA=0:01:58
[07/29 17:29:10] d2.evaluation.evaluator INFO: Inference done 9023/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0042 s/iter. Total: 0.1071 s/iter. ETA=0:01:53
[07/29 17:29:15] d2.evaluation.evaluator INFO: Inference done 9069/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0042 s/iter. Total: 0.1072 s/iter. ETA=0:01:48
[07/29 17:29:20] d2.evaluation.evaluator INFO: Inference done 9115/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0042 s/iter. Total: 0.1072 s/iter. ETA=0:01:43
[07/29 17:29:25] d2.evaluation.evaluator INFO: Inference done 9162/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0042 s/iter. Total: 0.1072 s/iter. ETA=0:01:38
[07/29 17:29:30] d2.evaluation.evaluator INFO: Inference done 9208/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0042 s/iter. Total: 0.1072 s/iter. ETA=0:01:33
[07/29 17:29:35] d2.evaluation.evaluator INFO: Inference done 9255/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0042 s/iter. Total: 0.1072 s/iter. ETA=0:01:28
[07/29 17:29:40] d2.evaluation.evaluator INFO: Inference done 9301/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0042 s/iter. Total: 0.1072 s/iter. ETA=0:01:23
[07/29 17:29:45] d2.evaluation.evaluator INFO: Inference done 9347/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0042 s/iter. Total: 0.1072 s/iter. ETA=0:01:18
[07/29 17:29:50] d2.evaluation.evaluator INFO: Inference done 9393/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0043 s/iter. Total: 0.1072 s/iter. ETA=0:01:13
[07/29 17:29:55] d2.evaluation.evaluator INFO: Inference done 9439/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0043 s/iter. Total: 0.1072 s/iter. ETA=0:01:08
[07/29 17:30:00] d2.evaluation.evaluator INFO: Inference done 9484/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0043 s/iter. Total: 0.1073 s/iter. ETA=0:01:03
[07/29 17:30:05] d2.evaluation.evaluator INFO: Inference done 9530/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0043 s/iter. Total: 0.1073 s/iter. ETA=0:00:58
[07/29 17:30:10] d2.evaluation.evaluator INFO: Inference done 9576/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0043 s/iter. Total: 0.1073 s/iter. ETA=0:00:54
[07/29 17:30:15] d2.evaluation.evaluator INFO: Inference done 9620/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0043 s/iter. Total: 0.1073 s/iter. ETA=0:00:49
[07/29 17:30:20] d2.evaluation.evaluator INFO: Inference done 9664/10080. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0044 s/iter. Total: 0.1073 s/iter. ETA=0:00:44
[07/29 17:30:25] d2.evaluation.evaluator INFO: Inference done 9708/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1074 s/iter. ETA=0:00:39
[07/29 17:30:30] d2.evaluation.evaluator INFO: Inference done 9752/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1074 s/iter. ETA=0:00:35
[07/29 17:30:35] d2.evaluation.evaluator INFO: Inference done 9796/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0044 s/iter. Total: 0.1075 s/iter. ETA=0:00:30
[07/29 17:30:41] d2.evaluation.evaluator INFO: Inference done 9840/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0045 s/iter. Total: 0.1075 s/iter. ETA=0:00:25
[07/29 17:30:46] d2.evaluation.evaluator INFO: Inference done 9885/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0045 s/iter. Total: 0.1075 s/iter. ETA=0:00:20
[07/29 17:30:51] d2.evaluation.evaluator INFO: Inference done 9930/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0045 s/iter. Total: 0.1075 s/iter. ETA=0:00:16
[07/29 17:30:56] d2.evaluation.evaluator INFO: Inference done 9975/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0045 s/iter. Total: 0.1076 s/iter. ETA=0:00:11
[07/29 17:31:01] d2.evaluation.evaluator INFO: Inference done 10020/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0045 s/iter. Total: 0.1076 s/iter. ETA=0:00:06
[07/29 17:31:06] d2.evaluation.evaluator INFO: Inference done 10065/10080. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0046 s/iter. Total: 0.1076 s/iter. ETA=0:00:01
[07/29 17:31:08] d2.evaluation.evaluator INFO: Total inference time: 0:18:04.352484 (0.107628 s / iter per device, on 1 devices)
[07/29 17:31:08] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:17:07 (0.101997 s / iter per device, on 1 devices)
[07/29 17:31:08] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[07/29 17:31:08] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[07/29 17:31:08] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[07/29 17:31:13] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 4.62 seconds.
[07/29 17:31:13] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[07/29 17:31:13] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.40 seconds.
[07/29 17:31:13] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |
|:------:|:------:|:------:|:-----:|:------:|:------:|
| 55.769 | 67.706 | 62.548 | 5.962 | 54.238 | 50.994 |
[07/29 17:31:13] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP     | category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|:-----------|:-------|
| 1          | 57.137 | 2          | 52.165 | 3          | 62.279 |
| 4          | 34.121 | 5          | 27.181 | 6          | 27.425 |
| 7          | 2.355  | 8          | 17.104 | 9          | 76.407 |
| 10         | 73.050 | 11         | 69.884 | 12         | 72.472 |
| 13         | 48.306 | 14         | 59.219 | 15         | 66.647 |
| 16         | 77.254 | 17         | 62.529 | 18         | 87.701 |
| 19         | 58.827 | 20         | 47.408 | 21         | 53.017 |
| 22         | 50.451 | 23         | 70.942 | 24         | 81.571 |
| 25         | 67.533 | 26         | 46.729 | 27         | 58.132 |
| 28         | 78.216 | 29         | 58.966 | 30         | 28.042 |
[07/29 17:31:15] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *segm*
[07/29 17:31:24] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 8.17 seconds.
[07/29 17:31:24] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[07/29 17:31:24] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.42 seconds.
[07/29 17:31:24] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |
|:------:|:------:|:------:|:-----:|:------:|:------:|
| 47.720 | 63.981 | 54.358 | 0.521 | 40.332 | 47.635 |
[07/29 17:31:24] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|:-----------|:-------|
| 1          | 52.421 | 2          | 47.517 | 3          | 56.880 |
| 4          | 28.312 | 5          | 20.831 | 6          | 21.285 |
| 7          | 1.630  | 8          | 14.265 | 9          | 66.856 |
| 10         | 67.477 | 11         | 62.527 | 12         | 61.932 |
| 13         | 43.281 | 14         | 51.843 | 15         | 59.854 |
| 16         | 72.602 | 17         | 52.892 | 18         | 81.984 |
| 19         | 50.512 | 20         | 34.452 | 21         | 43.363 |
| 22         | 41.716 | 23         | 56.487 | 24         | 73.252 |
| 25         | 62.557 | 26         | 35.457 | 27         | 37.652 |
| 28         | 61.792 | 29         | 48.116 | 30         | 21.849 |
[07/29 17:31:25] d2.evaluation.testing INFO: copypaste: Task: bbox
[07/29 17:31:25] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/29 17:31:25] d2.evaluation.testing INFO: copypaste: 55.7690,67.7065,62.5481,5.9623,54.2383,50.9938
[07/29 17:31:25] d2.evaluation.testing INFO: copypaste: Task: segm
[07/29 17:31:25] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/29 17:31:25] d2.evaluation.testing INFO: copypaste: 47.7199,63.9810,54.3575,0.5206,40.3324,47.6346
[07/29 17:31:25] d2.utils.events INFO:  eta: 7:24:13  iter: 24999  total_loss: 2.896  loss_cls_stage0: 0.249  loss_box_reg_stage0: 0.2479  loss_cls_stage1: 0.2795  loss_box_reg_stage1: 0.6656  loss_cls_stage2: 0.2865  loss_box_reg_stage2: 0.8778  loss_mask: 0.07749  loss_rpn_cls: 0.01303  loss_rpn_loc: 0.05297  time: 0.6237  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:31:37] d2.utils.events INFO:  eta: 7:23:21  iter: 25019  total_loss: 2.627  loss_cls_stage0: 0.2345  loss_box_reg_stage0: 0.2423  loss_cls_stage1: 0.2761  loss_box_reg_stage1: 0.6311  loss_cls_stage2: 0.2515  loss_box_reg_stage2: 0.8536  loss_mask: 0.0707  loss_rpn_cls: 0.01932  loss_rpn_loc: 0.04451  time: 0.6237  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:31:50] d2.utils.events INFO:  eta: 7:23:12  iter: 25039  total_loss: 2.43  loss_cls_stage0: 0.2326  loss_box_reg_stage0: 0.221  loss_cls_stage1: 0.2385  loss_box_reg_stage1: 0.5731  loss_cls_stage2: 0.2159  loss_box_reg_stage2: 0.8218  loss_mask: 0.06724  loss_rpn_cls: 0.01561  loss_rpn_loc: 0.04261  time: 0.6237  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:32:02] d2.utils.events INFO:  eta: 7:22:50  iter: 25059  total_loss: 2.586  loss_cls_stage0: 0.2509  loss_box_reg_stage0: 0.2428  loss_cls_stage1: 0.2561  loss_box_reg_stage1: 0.6049  loss_cls_stage2: 0.2407  loss_box_reg_stage2: 0.8368  loss_mask: 0.06978  loss_rpn_cls: 0.01583  loss_rpn_loc: 0.04175  time: 0.6237  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:32:14] d2.utils.events INFO:  eta: 7:22:56  iter: 25079  total_loss: 2.905  loss_cls_stage0: 0.2772  loss_box_reg_stage0: 0.2623  loss_cls_stage1: 0.2909  loss_box_reg_stage1: 0.6792  loss_cls_stage2: 0.2824  loss_box_reg_stage2: 0.9079  loss_mask: 0.07681  loss_rpn_cls: 0.01595  loss_rpn_loc: 0.05203  time: 0.6237  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:32:27] d2.utils.events INFO:  eta: 7:22:31  iter: 25099  total_loss: 2.594  loss_cls_stage0: 0.23  loss_box_reg_stage0: 0.2437  loss_cls_stage1: 0.2299  loss_box_reg_stage1: 0.6725  loss_cls_stage2: 0.223  loss_box_reg_stage2: 0.8388  loss_mask: 0.06975  loss_rpn_cls: 0.01559  loss_rpn_loc: 0.04262  time: 0.6237  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:32:39] d2.utils.events INFO:  eta: 7:22:15  iter: 25119  total_loss: 2.765  loss_cls_stage0: 0.2588  loss_box_reg_stage0: 0.2641  loss_cls_stage1: 0.2765  loss_box_reg_stage1: 0.6996  loss_cls_stage2: 0.2721  loss_box_reg_stage2: 0.9209  loss_mask: 0.07282  loss_rpn_cls: 0.01686  loss_rpn_loc: 0.04616  time: 0.6237  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:32:52] d2.utils.events INFO:  eta: 7:21:59  iter: 25139  total_loss: 2.779  loss_cls_stage0: 0.2721  loss_box_reg_stage0: 0.2443  loss_cls_stage1: 0.2886  loss_box_reg_stage1: 0.6349  loss_cls_stage2: 0.2561  loss_box_reg_stage2: 0.9382  loss_mask: 0.07169  loss_rpn_cls: 0.01244  loss_rpn_loc: 0.04821  time: 0.6237  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:33:04] d2.utils.events INFO:  eta: 7:21:38  iter: 25159  total_loss: 2.6  loss_cls_stage0: 0.2509  loss_box_reg_stage0: 0.2397  loss_cls_stage1: 0.2587  loss_box_reg_stage1: 0.6308  loss_cls_stage2: 0.2205  loss_box_reg_stage2: 0.8347  loss_mask: 0.06899  loss_rpn_cls: 0.01444  loss_rpn_loc: 0.04613  time: 0.6237  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 17:33:16] d2.utils.events INFO:  eta: 7:21:26  iter: 25179  total_loss: 2.692  loss_cls_stage0: 0.231  loss_box_reg_stage0: 0.2246  loss_cls_stage1: 0.2401  loss_box_reg_stage1: 0.662  loss_cls_stage2: 0.2452  loss_box_reg_stage2: 0.8985  loss_mask: 0.0648  loss_rpn_cls: 0.01095  loss_rpn_loc: 0.05175  time: 0.6237  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:33:29] d2.utils.events INFO:  eta: 7:21:02  iter: 25199  total_loss: 2.798  loss_cls_stage0: 0.2684  loss_box_reg_stage0: 0.268  loss_cls_stage1: 0.2748  loss_box_reg_stage1: 0.6544  loss_cls_stage2: 0.262  loss_box_reg_stage2: 0.8699  loss_mask: 0.07193  loss_rpn_cls: 0.02015  loss_rpn_loc: 0.05818  time: 0.6236  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:33:41] d2.utils.events INFO:  eta: 7:20:34  iter: 25219  total_loss: 2.473  loss_cls_stage0: 0.2267  loss_box_reg_stage0: 0.2262  loss_cls_stage1: 0.2444  loss_box_reg_stage1: 0.5684  loss_cls_stage2: 0.2425  loss_box_reg_stage2: 0.8525  loss_mask: 0.0758  loss_rpn_cls: 0.01258  loss_rpn_loc: 0.04588  time: 0.6236  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:33:53] d2.utils.events INFO:  eta: 7:20:37  iter: 25239  total_loss: 2.741  loss_cls_stage0: 0.2225  loss_box_reg_stage0: 0.218  loss_cls_stage1: 0.2486  loss_box_reg_stage1: 0.6703  loss_cls_stage2: 0.2667  loss_box_reg_stage2: 0.9231  loss_mask: 0.06798  loss_rpn_cls: 0.009234  loss_rpn_loc: 0.03705  time: 0.6236  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:34:05] d2.utils.events INFO:  eta: 7:20:20  iter: 25259  total_loss: 2.716  loss_cls_stage0: 0.2613  loss_box_reg_stage0: 0.2671  loss_cls_stage1: 0.2576  loss_box_reg_stage1: 0.6549  loss_cls_stage2: 0.2523  loss_box_reg_stage2: 0.8743  loss_mask: 0.08198  loss_rpn_cls: 0.02398  loss_rpn_loc: 0.05168  time: 0.6236  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:34:18] d2.utils.events INFO:  eta: 7:20:23  iter: 25279  total_loss: 2.434  loss_cls_stage0: 0.2197  loss_box_reg_stage0: 0.2421  loss_cls_stage1: 0.2315  loss_box_reg_stage1: 0.5968  loss_cls_stage2: 0.2424  loss_box_reg_stage2: 0.761  loss_mask: 0.06993  loss_rpn_cls: 0.01501  loss_rpn_loc: 0.04002  time: 0.6236  data_time: 0.0054  lr: 0.00016  max_mem: 19679M
[07/29 17:34:30] d2.utils.events INFO:  eta: 7:20:03  iter: 25299  total_loss: 2.797  loss_cls_stage0: 0.2814  loss_box_reg_stage0: 0.2651  loss_cls_stage1: 0.2868  loss_box_reg_stage1: 0.6457  loss_cls_stage2: 0.2544  loss_box_reg_stage2: 0.8733  loss_mask: 0.07689  loss_rpn_cls: 0.01656  loss_rpn_loc: 0.04406  time: 0.6236  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:34:42] d2.utils.events INFO:  eta: 7:19:42  iter: 25319  total_loss: 2.766  loss_cls_stage0: 0.2638  loss_box_reg_stage0: 0.2831  loss_cls_stage1: 0.2753  loss_box_reg_stage1: 0.697  loss_cls_stage2: 0.2689  loss_box_reg_stage2: 0.8534  loss_mask: 0.07574  loss_rpn_cls: 0.01095  loss_rpn_loc: 0.04141  time: 0.6236  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:34:55] d2.utils.events INFO:  eta: 7:19:45  iter: 25339  total_loss: 2.571  loss_cls_stage0: 0.2373  loss_box_reg_stage0: 0.2442  loss_cls_stage1: 0.2605  loss_box_reg_stage1: 0.6276  loss_cls_stage2: 0.2551  loss_box_reg_stage2: 0.8434  loss_mask: 0.06685  loss_rpn_cls: 0.009439  loss_rpn_loc: 0.04841  time: 0.6236  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:35:07] d2.utils.events INFO:  eta: 7:19:17  iter: 25359  total_loss: 2.817  loss_cls_stage0: 0.2653  loss_box_reg_stage0: 0.2838  loss_cls_stage1: 0.2632  loss_box_reg_stage1: 0.719  loss_cls_stage2: 0.2605  loss_box_reg_stage2: 0.8858  loss_mask: 0.07234  loss_rpn_cls: 0.01831  loss_rpn_loc: 0.051  time: 0.6236  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:35:19] d2.utils.events INFO:  eta: 7:18:52  iter: 25379  total_loss: 2.968  loss_cls_stage0: 0.2829  loss_box_reg_stage0: 0.2836  loss_cls_stage1: 0.2882  loss_box_reg_stage1: 0.6989  loss_cls_stage2: 0.2817  loss_box_reg_stage2: 0.8829  loss_mask: 0.08287  loss_rpn_cls: 0.01922  loss_rpn_loc: 0.0576  time: 0.6235  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:35:31] d2.utils.events INFO:  eta: 7:18:54  iter: 25399  total_loss: 2.848  loss_cls_stage0: 0.2457  loss_box_reg_stage0: 0.245  loss_cls_stage1: 0.2692  loss_box_reg_stage1: 0.682  loss_cls_stage2: 0.2403  loss_box_reg_stage2: 0.881  loss_mask: 0.07307  loss_rpn_cls: 0.01843  loss_rpn_loc: 0.04722  time: 0.6235  data_time: 0.0057  lr: 0.00016  max_mem: 19679M
[07/29 17:35:44] d2.utils.events INFO:  eta: 7:18:43  iter: 25419  total_loss: 2.953  loss_cls_stage0: 0.2689  loss_box_reg_stage0: 0.2619  loss_cls_stage1: 0.294  loss_box_reg_stage1: 0.686  loss_cls_stage2: 0.2526  loss_box_reg_stage2: 0.8789  loss_mask: 0.07952  loss_rpn_cls: 0.01272  loss_rpn_loc: 0.04963  time: 0.6235  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:35:56] d2.utils.events INFO:  eta: 7:18:46  iter: 25439  total_loss: 2.835  loss_cls_stage0: 0.2577  loss_box_reg_stage0: 0.2604  loss_cls_stage1: 0.2779  loss_box_reg_stage1: 0.7023  loss_cls_stage2: 0.2653  loss_box_reg_stage2: 0.9025  loss_mask: 0.07415  loss_rpn_cls: 0.01622  loss_rpn_loc: 0.05822  time: 0.6235  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:36:09] d2.utils.events INFO:  eta: 7:18:39  iter: 25459  total_loss: 2.64  loss_cls_stage0: 0.2416  loss_box_reg_stage0: 0.2417  loss_cls_stage1: 0.2754  loss_box_reg_stage1: 0.6688  loss_cls_stage2: 0.2605  loss_box_reg_stage2: 0.8846  loss_mask: 0.0689  loss_rpn_cls: 0.01594  loss_rpn_loc: 0.0449  time: 0.6236  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:36:21] d2.utils.events INFO:  eta: 7:18:21  iter: 25479  total_loss: 2.871  loss_cls_stage0: 0.2677  loss_box_reg_stage0: 0.2706  loss_cls_stage1: 0.2795  loss_box_reg_stage1: 0.7044  loss_cls_stage2: 0.2839  loss_box_reg_stage2: 0.9201  loss_mask: 0.07162  loss_rpn_cls: 0.02268  loss_rpn_loc: 0.04813  time: 0.6235  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:36:34] d2.utils.events INFO:  eta: 7:18:09  iter: 25499  total_loss: 2.77  loss_cls_stage0: 0.2217  loss_box_reg_stage0: 0.246  loss_cls_stage1: 0.2272  loss_box_reg_stage1: 0.6952  loss_cls_stage2: 0.2347  loss_box_reg_stage2: 0.9472  loss_mask: 0.07361  loss_rpn_cls: 0.01183  loss_rpn_loc: 0.0489  time: 0.6235  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 17:36:46] d2.utils.events INFO:  eta: 7:17:57  iter: 25519  total_loss: 3.044  loss_cls_stage0: 0.2498  loss_box_reg_stage0: 0.2584  loss_cls_stage1: 0.2753  loss_box_reg_stage1: 0.725  loss_cls_stage2: 0.2922  loss_box_reg_stage2: 1.024  loss_mask: 0.07441  loss_rpn_cls: 0.01351  loss_rpn_loc: 0.04832  time: 0.6235  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 17:36:58] d2.utils.events INFO:  eta: 7:18:04  iter: 25539  total_loss: 2.966  loss_cls_stage0: 0.294  loss_box_reg_stage0: 0.2627  loss_cls_stage1: 0.3451  loss_box_reg_stage1: 0.7348  loss_cls_stage2: 0.3305  loss_box_reg_stage2: 1.007  loss_mask: 0.08378  loss_rpn_cls: 0.01273  loss_rpn_loc: 0.07366  time: 0.6235  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:37:11] d2.utils.events INFO:  eta: 7:18:04  iter: 25559  total_loss: 2.873  loss_cls_stage0: 0.2898  loss_box_reg_stage0: 0.287  loss_cls_stage1: 0.2887  loss_box_reg_stage1: 0.7081  loss_cls_stage2: 0.3074  loss_box_reg_stage2: 0.9103  loss_mask: 0.08163  loss_rpn_cls: 0.02252  loss_rpn_loc: 0.06522  time: 0.6235  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:37:23] d2.utils.events INFO:  eta: 7:17:53  iter: 25579  total_loss: 2.647  loss_cls_stage0: 0.2295  loss_box_reg_stage0: 0.2558  loss_cls_stage1: 0.2198  loss_box_reg_stage1: 0.6571  loss_cls_stage2: 0.206  loss_box_reg_stage2: 0.9276  loss_mask: 0.06969  loss_rpn_cls: 0.01225  loss_rpn_loc: 0.04235  time: 0.6235  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:37:35] d2.utils.events INFO:  eta: 7:17:07  iter: 25599  total_loss: 2.88  loss_cls_stage0: 0.2764  loss_box_reg_stage0: 0.2718  loss_cls_stage1: 0.2885  loss_box_reg_stage1: 0.676  loss_cls_stage2: 0.2888  loss_box_reg_stage2: 0.8485  loss_mask: 0.07187  loss_rpn_cls: 0.02723  loss_rpn_loc: 0.04525  time: 0.6235  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:37:48] d2.utils.events INFO:  eta: 7:16:43  iter: 25619  total_loss: 2.722  loss_cls_stage0: 0.2582  loss_box_reg_stage0: 0.2724  loss_cls_stage1: 0.2553  loss_box_reg_stage1: 0.6843  loss_cls_stage2: 0.2398  loss_box_reg_stage2: 0.8642  loss_mask: 0.08118  loss_rpn_cls: 0.01652  loss_rpn_loc: 0.03707  time: 0.6235  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:38:00] d2.utils.events INFO:  eta: 7:16:39  iter: 25639  total_loss: 2.966  loss_cls_stage0: 0.2557  loss_box_reg_stage0: 0.2498  loss_cls_stage1: 0.2751  loss_box_reg_stage1: 0.6674  loss_cls_stage2: 0.2488  loss_box_reg_stage2: 0.9356  loss_mask: 0.07439  loss_rpn_cls: 0.01372  loss_rpn_loc: 0.03694  time: 0.6235  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 17:38:13] d2.utils.events INFO:  eta: 7:16:12  iter: 25659  total_loss: 2.839  loss_cls_stage0: 0.2569  loss_box_reg_stage0: 0.2554  loss_cls_stage1: 0.2821  loss_box_reg_stage1: 0.6306  loss_cls_stage2: 0.2573  loss_box_reg_stage2: 0.8506  loss_mask: 0.06926  loss_rpn_cls: 0.01997  loss_rpn_loc: 0.04954  time: 0.6235  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:38:25] d2.utils.events INFO:  eta: 7:15:59  iter: 25679  total_loss: 2.99  loss_cls_stage0: 0.2742  loss_box_reg_stage0: 0.2763  loss_cls_stage1: 0.2941  loss_box_reg_stage1: 0.6926  loss_cls_stage2: 0.2592  loss_box_reg_stage2: 0.9444  loss_mask: 0.08088  loss_rpn_cls: 0.01646  loss_rpn_loc: 0.04173  time: 0.6235  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:38:37] d2.utils.events INFO:  eta: 7:15:45  iter: 25699  total_loss: 2.389  loss_cls_stage0: 0.1972  loss_box_reg_stage0: 0.2291  loss_cls_stage1: 0.2009  loss_box_reg_stage1: 0.5952  loss_cls_stage2: 0.1946  loss_box_reg_stage2: 0.8015  loss_mask: 0.07055  loss_rpn_cls: 0.01673  loss_rpn_loc: 0.04397  time: 0.6235  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:38:49] d2.utils.events INFO:  eta: 7:15:24  iter: 25719  total_loss: 2.464  loss_cls_stage0: 0.2164  loss_box_reg_stage0: 0.2431  loss_cls_stage1: 0.2227  loss_box_reg_stage1: 0.6084  loss_cls_stage2: 0.2254  loss_box_reg_stage2: 0.821  loss_mask: 0.07522  loss_rpn_cls: 0.01446  loss_rpn_loc: 0.04771  time: 0.6235  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 17:39:01] d2.utils.events INFO:  eta: 7:15:09  iter: 25739  total_loss: 2.737  loss_cls_stage0: 0.2234  loss_box_reg_stage0: 0.2594  loss_cls_stage1: 0.239  loss_box_reg_stage1: 0.6665  loss_cls_stage2: 0.2462  loss_box_reg_stage2: 0.8676  loss_mask: 0.07184  loss_rpn_cls: 0.01224  loss_rpn_loc: 0.04055  time: 0.6234  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:39:14] d2.utils.events INFO:  eta: 7:15:16  iter: 25759  total_loss: 2.664  loss_cls_stage0: 0.2608  loss_box_reg_stage0: 0.2657  loss_cls_stage1: 0.2759  loss_box_reg_stage1: 0.6293  loss_cls_stage2: 0.2685  loss_box_reg_stage2: 0.8905  loss_mask: 0.07679  loss_rpn_cls: 0.01298  loss_rpn_loc: 0.04246  time: 0.6234  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:39:26] d2.utils.events INFO:  eta: 7:15:14  iter: 25779  total_loss: 2.488  loss_cls_stage0: 0.2329  loss_box_reg_stage0: 0.2374  loss_cls_stage1: 0.2276  loss_box_reg_stage1: 0.6024  loss_cls_stage2: 0.222  loss_box_reg_stage2: 0.8055  loss_mask: 0.07392  loss_rpn_cls: 0.01088  loss_rpn_loc: 0.04957  time: 0.6234  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:39:39] d2.utils.events INFO:  eta: 7:15:20  iter: 25799  total_loss: 2.711  loss_cls_stage0: 0.2368  loss_box_reg_stage0: 0.2453  loss_cls_stage1: 0.2568  loss_box_reg_stage1: 0.6563  loss_cls_stage2: 0.2539  loss_box_reg_stage2: 0.8501  loss_mask: 0.07593  loss_rpn_cls: 0.01471  loss_rpn_loc: 0.04407  time: 0.6234  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:39:51] d2.utils.events INFO:  eta: 7:15:23  iter: 25819  total_loss: 2.465  loss_cls_stage0: 0.2143  loss_box_reg_stage0: 0.2413  loss_cls_stage1: 0.225  loss_box_reg_stage1: 0.6129  loss_cls_stage2: 0.2125  loss_box_reg_stage2: 0.8061  loss_mask: 0.07442  loss_rpn_cls: 0.01198  loss_rpn_loc: 0.05073  time: 0.6234  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:40:03] d2.utils.events INFO:  eta: 7:14:58  iter: 25839  total_loss: 2.391  loss_cls_stage0: 0.2314  loss_box_reg_stage0: 0.2439  loss_cls_stage1: 0.2295  loss_box_reg_stage1: 0.569  loss_cls_stage2: 0.213  loss_box_reg_stage2: 0.8013  loss_mask: 0.08196  loss_rpn_cls: 0.01179  loss_rpn_loc: 0.04539  time: 0.6234  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:40:16] d2.utils.events INFO:  eta: 7:14:25  iter: 25859  total_loss: 2.579  loss_cls_stage0: 0.2388  loss_box_reg_stage0: 0.2492  loss_cls_stage1: 0.2616  loss_box_reg_stage1: 0.6035  loss_cls_stage2: 0.2419  loss_box_reg_stage2: 0.863  loss_mask: 0.07406  loss_rpn_cls: 0.02178  loss_rpn_loc: 0.06032  time: 0.6234  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:40:28] d2.utils.events INFO:  eta: 7:14:12  iter: 25879  total_loss: 2.942  loss_cls_stage0: 0.2936  loss_box_reg_stage0: 0.2977  loss_cls_stage1: 0.3077  loss_box_reg_stage1: 0.7078  loss_cls_stage2: 0.2796  loss_box_reg_stage2: 0.8839  loss_mask: 0.08641  loss_rpn_cls: 0.01245  loss_rpn_loc: 0.04704  time: 0.6234  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:40:41] d2.utils.events INFO:  eta: 7:13:59  iter: 25899  total_loss: 2.824  loss_cls_stage0: 0.2576  loss_box_reg_stage0: 0.2571  loss_cls_stage1: 0.2524  loss_box_reg_stage1: 0.6665  loss_cls_stage2: 0.2479  loss_box_reg_stage2: 0.9457  loss_mask: 0.07161  loss_rpn_cls: 0.01195  loss_rpn_loc: 0.04233  time: 0.6234  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:40:53] d2.utils.events INFO:  eta: 7:13:43  iter: 25919  total_loss: 2.563  loss_cls_stage0: 0.2285  loss_box_reg_stage0: 0.2436  loss_cls_stage1: 0.2277  loss_box_reg_stage1: 0.6122  loss_cls_stage2: 0.2172  loss_box_reg_stage2: 0.7893  loss_mask: 0.06381  loss_rpn_cls: 0.01268  loss_rpn_loc: 0.04463  time: 0.6234  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:41:06] d2.utils.events INFO:  eta: 7:13:34  iter: 25939  total_loss: 2.657  loss_cls_stage0: 0.2651  loss_box_reg_stage0: 0.25  loss_cls_stage1: 0.2764  loss_box_reg_stage1: 0.6135  loss_cls_stage2: 0.2654  loss_box_reg_stage2: 0.8985  loss_mask: 0.08812  loss_rpn_cls: 0.01416  loss_rpn_loc: 0.04425  time: 0.6234  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:41:18] d2.utils.events INFO:  eta: 7:13:22  iter: 25959  total_loss: 2.564  loss_cls_stage0: 0.2359  loss_box_reg_stage0: 0.2515  loss_cls_stage1: 0.2416  loss_box_reg_stage1: 0.6485  loss_cls_stage2: 0.2368  loss_box_reg_stage2: 0.8421  loss_mask: 0.06895  loss_rpn_cls: 0.01745  loss_rpn_loc: 0.04123  time: 0.6234  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:41:30] d2.utils.events INFO:  eta: 7:13:09  iter: 25979  total_loss: 2.958  loss_cls_stage0: 0.2655  loss_box_reg_stage0: 0.2743  loss_cls_stage1: 0.2694  loss_box_reg_stage1: 0.7163  loss_cls_stage2: 0.2739  loss_box_reg_stage2: 0.9196  loss_mask: 0.08093  loss_rpn_cls: 0.01889  loss_rpn_loc: 0.04867  time: 0.6234  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:41:43] d2.utils.events INFO:  eta: 7:12:57  iter: 25999  total_loss: 2.538  loss_cls_stage0: 0.2397  loss_box_reg_stage0: 0.2425  loss_cls_stage1: 0.2824  loss_box_reg_stage1: 0.5926  loss_cls_stage2: 0.2543  loss_box_reg_stage2: 0.8001  loss_mask: 0.07373  loss_rpn_cls: 0.0143  loss_rpn_loc: 0.04712  time: 0.6234  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:41:55] d2.utils.events INFO:  eta: 7:12:43  iter: 26019  total_loss: 2.516  loss_cls_stage0: 0.2516  loss_box_reg_stage0: 0.2635  loss_cls_stage1: 0.2509  loss_box_reg_stage1: 0.586  loss_cls_stage2: 0.2085  loss_box_reg_stage2: 0.7359  loss_mask: 0.07985  loss_rpn_cls: 0.01856  loss_rpn_loc: 0.04623  time: 0.6234  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:42:07] d2.utils.events INFO:  eta: 7:12:30  iter: 26039  total_loss: 2.418  loss_cls_stage0: 0.2163  loss_box_reg_stage0: 0.232  loss_cls_stage1: 0.2312  loss_box_reg_stage1: 0.582  loss_cls_stage2: 0.213  loss_box_reg_stage2: 0.8239  loss_mask: 0.0745  loss_rpn_cls: 0.01459  loss_rpn_loc: 0.04227  time: 0.6234  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:42:20] d2.utils.events INFO:  eta: 7:12:23  iter: 26059  total_loss: 2.609  loss_cls_stage0: 0.2536  loss_box_reg_stage0: 0.2541  loss_cls_stage1: 0.2725  loss_box_reg_stage1: 0.6233  loss_cls_stage2: 0.2429  loss_box_reg_stage2: 0.8724  loss_mask: 0.07736  loss_rpn_cls: 0.01147  loss_rpn_loc: 0.0433  time: 0.6234  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 17:42:32] d2.utils.events INFO:  eta: 7:12:05  iter: 26079  total_loss: 3.034  loss_cls_stage0: 0.2655  loss_box_reg_stage0: 0.2631  loss_cls_stage1: 0.3252  loss_box_reg_stage1: 0.7257  loss_cls_stage2: 0.2911  loss_box_reg_stage2: 1.017  loss_mask: 0.07162  loss_rpn_cls: 0.01202  loss_rpn_loc: 0.04632  time: 0.6234  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:42:45] d2.utils.events INFO:  eta: 7:11:54  iter: 26099  total_loss: 2.71  loss_cls_stage0: 0.2427  loss_box_reg_stage0: 0.2476  loss_cls_stage1: 0.2571  loss_box_reg_stage1: 0.6457  loss_cls_stage2: 0.2608  loss_box_reg_stage2: 0.8448  loss_mask: 0.07366  loss_rpn_cls: 0.01879  loss_rpn_loc: 0.05407  time: 0.6234  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:42:57] d2.utils.events INFO:  eta: 7:11:40  iter: 26119  total_loss: 3.085  loss_cls_stage0: 0.2795  loss_box_reg_stage0: 0.2651  loss_cls_stage1: 0.2726  loss_box_reg_stage1: 0.6741  loss_cls_stage2: 0.268  loss_box_reg_stage2: 0.9666  loss_mask: 0.07528  loss_rpn_cls: 0.01445  loss_rpn_loc: 0.04501  time: 0.6234  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:43:10] d2.utils.events INFO:  eta: 7:11:30  iter: 26139  total_loss: 2.697  loss_cls_stage0: 0.2451  loss_box_reg_stage0: 0.2417  loss_cls_stage1: 0.2725  loss_box_reg_stage1: 0.6195  loss_cls_stage2: 0.2372  loss_box_reg_stage2: 0.8136  loss_mask: 0.07995  loss_rpn_cls: 0.01171  loss_rpn_loc: 0.04631  time: 0.6234  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 17:43:22] d2.utils.events INFO:  eta: 7:11:27  iter: 26159  total_loss: 2.492  loss_cls_stage0: 0.2411  loss_box_reg_stage0: 0.248  loss_cls_stage1: 0.2321  loss_box_reg_stage1: 0.5834  loss_cls_stage2: 0.2464  loss_box_reg_stage2: 0.8171  loss_mask: 0.0739  loss_rpn_cls: 0.01454  loss_rpn_loc: 0.0481  time: 0.6234  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:43:35] d2.utils.events INFO:  eta: 7:11:04  iter: 26179  total_loss: 2.551  loss_cls_stage0: 0.2347  loss_box_reg_stage0: 0.2451  loss_cls_stage1: 0.229  loss_box_reg_stage1: 0.6242  loss_cls_stage2: 0.2244  loss_box_reg_stage2: 0.8154  loss_mask: 0.0727  loss_rpn_cls: 0.0137  loss_rpn_loc: 0.05095  time: 0.6234  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:43:47] d2.utils.events INFO:  eta: 7:10:52  iter: 26199  total_loss: 2.576  loss_cls_stage0: 0.2117  loss_box_reg_stage0: 0.2437  loss_cls_stage1: 0.2366  loss_box_reg_stage1: 0.6582  loss_cls_stage2: 0.2251  loss_box_reg_stage2: 0.8988  loss_mask: 0.06928  loss_rpn_cls: 0.0132  loss_rpn_loc: 0.0409  time: 0.6234  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:44:00] d2.utils.events INFO:  eta: 7:11:18  iter: 26219  total_loss: 2.954  loss_cls_stage0: 0.2779  loss_box_reg_stage0: 0.2699  loss_cls_stage1: 0.3028  loss_box_reg_stage1: 0.6889  loss_cls_stage2: 0.2727  loss_box_reg_stage2: 0.931  loss_mask: 0.07694  loss_rpn_cls: 0.01042  loss_rpn_loc: 0.04387  time: 0.6234  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 17:44:12] d2.utils.events INFO:  eta: 7:10:51  iter: 26239  total_loss: 3.241  loss_cls_stage0: 0.3074  loss_box_reg_stage0: 0.2934  loss_cls_stage1: 0.3509  loss_box_reg_stage1: 0.7387  loss_cls_stage2: 0.3454  loss_box_reg_stage2: 1.024  loss_mask: 0.08459  loss_rpn_cls: 0.01591  loss_rpn_loc: 0.04678  time: 0.6233  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:44:24] d2.utils.events INFO:  eta: 7:10:41  iter: 26259  total_loss: 2.61  loss_cls_stage0: 0.2291  loss_box_reg_stage0: 0.2518  loss_cls_stage1: 0.2657  loss_box_reg_stage1: 0.604  loss_cls_stage2: 0.2585  loss_box_reg_stage2: 0.7791  loss_mask: 0.0729  loss_rpn_cls: 0.023  loss_rpn_loc: 0.04198  time: 0.6233  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:44:37] d2.utils.events INFO:  eta: 7:10:40  iter: 26279  total_loss: 2.598  loss_cls_stage0: 0.2277  loss_box_reg_stage0: 0.256  loss_cls_stage1: 0.2287  loss_box_reg_stage1: 0.573  loss_cls_stage2: 0.2259  loss_box_reg_stage2: 0.7828  loss_mask: 0.06729  loss_rpn_cls: 0.01169  loss_rpn_loc: 0.04031  time: 0.6234  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:44:50] d2.utils.events INFO:  eta: 7:10:22  iter: 26299  total_loss: 2.76  loss_cls_stage0: 0.252  loss_box_reg_stage0: 0.261  loss_cls_stage1: 0.2793  loss_box_reg_stage1: 0.6438  loss_cls_stage2: 0.2635  loss_box_reg_stage2: 0.8335  loss_mask: 0.07116  loss_rpn_cls: 0.01187  loss_rpn_loc: 0.04731  time: 0.6234  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:45:02] d2.utils.events INFO:  eta: 7:10:20  iter: 26319  total_loss: 2.473  loss_cls_stage0: 0.2232  loss_box_reg_stage0: 0.2318  loss_cls_stage1: 0.243  loss_box_reg_stage1: 0.5612  loss_cls_stage2: 0.2478  loss_box_reg_stage2: 0.7677  loss_mask: 0.06723  loss_rpn_cls: 0.01905  loss_rpn_loc: 0.05615  time: 0.6233  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 17:45:14] d2.utils.events INFO:  eta: 7:10:08  iter: 26339  total_loss: 2.84  loss_cls_stage0: 0.2483  loss_box_reg_stage0: 0.2905  loss_cls_stage1: 0.2444  loss_box_reg_stage1: 0.6789  loss_cls_stage2: 0.2312  loss_box_reg_stage2: 0.8807  loss_mask: 0.07539  loss_rpn_cls: 0.01938  loss_rpn_loc: 0.04248  time: 0.6233  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:45:26] d2.utils.events INFO:  eta: 7:10:03  iter: 26359  total_loss: 2.512  loss_cls_stage0: 0.2412  loss_box_reg_stage0: 0.2495  loss_cls_stage1: 0.237  loss_box_reg_stage1: 0.5744  loss_cls_stage2: 0.2297  loss_box_reg_stage2: 0.7743  loss_mask: 0.07434  loss_rpn_cls: 0.01254  loss_rpn_loc: 0.04162  time: 0.6233  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 17:45:39] d2.utils.events INFO:  eta: 7:09:50  iter: 26379  total_loss: 2.769  loss_cls_stage0: 0.2771  loss_box_reg_stage0: 0.2838  loss_cls_stage1: 0.2831  loss_box_reg_stage1: 0.6476  loss_cls_stage2: 0.2621  loss_box_reg_stage2: 0.8935  loss_mask: 0.0775  loss_rpn_cls: 0.01623  loss_rpn_loc: 0.03816  time: 0.6233  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:45:51] d2.utils.events INFO:  eta: 7:09:19  iter: 26399  total_loss: 2.342  loss_cls_stage0: 0.2242  loss_box_reg_stage0: 0.2234  loss_cls_stage1: 0.2462  loss_box_reg_stage1: 0.5689  loss_cls_stage2: 0.2296  loss_box_reg_stage2: 0.809  loss_mask: 0.07121  loss_rpn_cls: 0.01577  loss_rpn_loc: 0.04582  time: 0.6233  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:46:03] d2.utils.events INFO:  eta: 7:09:03  iter: 26419  total_loss: 2.984  loss_cls_stage0: 0.2596  loss_box_reg_stage0: 0.273  loss_cls_stage1: 0.2968  loss_box_reg_stage1: 0.6803  loss_cls_stage2: 0.2971  loss_box_reg_stage2: 0.9066  loss_mask: 0.07822  loss_rpn_cls: 0.01296  loss_rpn_loc: 0.04896  time: 0.6233  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:46:15] d2.utils.events INFO:  eta: 7:08:34  iter: 26439  total_loss: 2.659  loss_cls_stage0: 0.2413  loss_box_reg_stage0: 0.2476  loss_cls_stage1: 0.2631  loss_box_reg_stage1: 0.6523  loss_cls_stage2: 0.2722  loss_box_reg_stage2: 0.8754  loss_mask: 0.0748  loss_rpn_cls: 0.01529  loss_rpn_loc: 0.0545  time: 0.6233  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:46:27] d2.utils.events INFO:  eta: 7:08:09  iter: 26459  total_loss: 2.878  loss_cls_stage0: 0.2539  loss_box_reg_stage0: 0.2767  loss_cls_stage1: 0.2683  loss_box_reg_stage1: 0.6978  loss_cls_stage2: 0.2541  loss_box_reg_stage2: 0.9369  loss_mask: 0.07687  loss_rpn_cls: 0.01988  loss_rpn_loc: 0.04285  time: 0.6233  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:46:40] d2.utils.events INFO:  eta: 7:08:09  iter: 26479  total_loss: 2.984  loss_cls_stage0: 0.2663  loss_box_reg_stage0: 0.2763  loss_cls_stage1: 0.3084  loss_box_reg_stage1: 0.6865  loss_cls_stage2: 0.2773  loss_box_reg_stage2: 0.9808  loss_mask: 0.07968  loss_rpn_cls: 0.01998  loss_rpn_loc: 0.04289  time: 0.6233  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:46:52] d2.utils.events INFO:  eta: 7:07:44  iter: 26499  total_loss: 2.726  loss_cls_stage0: 0.2524  loss_box_reg_stage0: 0.2601  loss_cls_stage1: 0.2631  loss_box_reg_stage1: 0.6343  loss_cls_stage2: 0.2564  loss_box_reg_stage2: 0.8594  loss_mask: 0.07505  loss_rpn_cls: 0.01524  loss_rpn_loc: 0.03864  time: 0.6232  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:47:04] d2.utils.events INFO:  eta: 7:07:32  iter: 26519  total_loss: 2.85  loss_cls_stage0: 0.267  loss_box_reg_stage0: 0.2653  loss_cls_stage1: 0.2835  loss_box_reg_stage1: 0.664  loss_cls_stage2: 0.2606  loss_box_reg_stage2: 0.863  loss_mask: 0.0771  loss_rpn_cls: 0.01569  loss_rpn_loc: 0.05333  time: 0.6232  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:47:16] d2.utils.events INFO:  eta: 7:07:19  iter: 26539  total_loss: 2.602  loss_cls_stage0: 0.2591  loss_box_reg_stage0: 0.2542  loss_cls_stage1: 0.2748  loss_box_reg_stage1: 0.5988  loss_cls_stage2: 0.2509  loss_box_reg_stage2: 0.7959  loss_mask: 0.079  loss_rpn_cls: 0.01971  loss_rpn_loc: 0.04282  time: 0.6232  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:47:29] d2.utils.events INFO:  eta: 7:06:26  iter: 26559  total_loss: 2.318  loss_cls_stage0: 0.2084  loss_box_reg_stage0: 0.2299  loss_cls_stage1: 0.2107  loss_box_reg_stage1: 0.5443  loss_cls_stage2: 0.2301  loss_box_reg_stage2: 0.7758  loss_mask: 0.06835  loss_rpn_cls: 0.01269  loss_rpn_loc: 0.03853  time: 0.6232  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:47:40] d2.utils.events INFO:  eta: 7:05:47  iter: 26579  total_loss: 2.491  loss_cls_stage0: 0.2168  loss_box_reg_stage0: 0.2435  loss_cls_stage1: 0.2402  loss_box_reg_stage1: 0.6271  loss_cls_stage2: 0.223  loss_box_reg_stage2: 0.8488  loss_mask: 0.0675  loss_rpn_cls: 0.01301  loss_rpn_loc: 0.04185  time: 0.6232  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:47:53] d2.utils.events INFO:  eta: 7:05:50  iter: 26599  total_loss: 2.776  loss_cls_stage0: 0.2386  loss_box_reg_stage0: 0.261  loss_cls_stage1: 0.2513  loss_box_reg_stage1: 0.701  loss_cls_stage2: 0.2572  loss_box_reg_stage2: 0.9541  loss_mask: 0.07319  loss_rpn_cls: 0.007945  loss_rpn_loc: 0.06097  time: 0.6232  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:48:06] d2.utils.events INFO:  eta: 7:05:48  iter: 26619  total_loss: 2.886  loss_cls_stage0: 0.262  loss_box_reg_stage0: 0.2368  loss_cls_stage1: 0.299  loss_box_reg_stage1: 0.6586  loss_cls_stage2: 0.2878  loss_box_reg_stage2: 0.8361  loss_mask: 0.06694  loss_rpn_cls: 0.01423  loss_rpn_loc: 0.03963  time: 0.6232  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:48:18] d2.utils.events INFO:  eta: 7:05:06  iter: 26639  total_loss: 2.637  loss_cls_stage0: 0.258  loss_box_reg_stage0: 0.2559  loss_cls_stage1: 0.256  loss_box_reg_stage1: 0.6175  loss_cls_stage2: 0.2541  loss_box_reg_stage2: 0.8338  loss_mask: 0.07856  loss_rpn_cls: 0.02166  loss_rpn_loc: 0.04932  time: 0.6232  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:48:30] d2.utils.events INFO:  eta: 7:05:10  iter: 26659  total_loss: 2.765  loss_cls_stage0: 0.2643  loss_box_reg_stage0: 0.2526  loss_cls_stage1: 0.2842  loss_box_reg_stage1: 0.647  loss_cls_stage2: 0.2624  loss_box_reg_stage2: 0.838  loss_mask: 0.07222  loss_rpn_cls: 0.01756  loss_rpn_loc: 0.0606  time: 0.6232  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:48:43] d2.utils.events INFO:  eta: 7:05:00  iter: 26679  total_loss: 2.683  loss_cls_stage0: 0.2241  loss_box_reg_stage0: 0.2515  loss_cls_stage1: 0.2432  loss_box_reg_stage1: 0.6351  loss_cls_stage2: 0.2452  loss_box_reg_stage2: 0.9113  loss_mask: 0.07275  loss_rpn_cls: 0.01498  loss_rpn_loc: 0.0435  time: 0.6232  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 17:48:55] d2.utils.events INFO:  eta: 7:04:48  iter: 26699  total_loss: 2.569  loss_cls_stage0: 0.2743  loss_box_reg_stage0: 0.257  loss_cls_stage1: 0.2774  loss_box_reg_stage1: 0.6003  loss_cls_stage2: 0.2588  loss_box_reg_stage2: 0.7529  loss_mask: 0.07462  loss_rpn_cls: 0.01978  loss_rpn_loc: 0.05073  time: 0.6232  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:49:07] d2.utils.events INFO:  eta: 7:04:24  iter: 26719  total_loss: 2.696  loss_cls_stage0: 0.2202  loss_box_reg_stage0: 0.2582  loss_cls_stage1: 0.2636  loss_box_reg_stage1: 0.6654  loss_cls_stage2: 0.2482  loss_box_reg_stage2: 0.8693  loss_mask: 0.07036  loss_rpn_cls: 0.01064  loss_rpn_loc: 0.04372  time: 0.6232  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:49:20] d2.utils.events INFO:  eta: 7:04:33  iter: 26739  total_loss: 2.755  loss_cls_stage0: 0.2319  loss_box_reg_stage0: 0.2583  loss_cls_stage1: 0.2537  loss_box_reg_stage1: 0.6907  loss_cls_stage2: 0.2422  loss_box_reg_stage2: 0.9037  loss_mask: 0.07192  loss_rpn_cls: 0.0159  loss_rpn_loc: 0.03989  time: 0.6232  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 17:49:33] d2.utils.events INFO:  eta: 7:04:21  iter: 26759  total_loss: 2.602  loss_cls_stage0: 0.2315  loss_box_reg_stage0: 0.241  loss_cls_stage1: 0.2574  loss_box_reg_stage1: 0.6527  loss_cls_stage2: 0.2508  loss_box_reg_stage2: 0.9012  loss_mask: 0.06781  loss_rpn_cls: 0.01156  loss_rpn_loc: 0.0467  time: 0.6232  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:49:45] d2.utils.events INFO:  eta: 7:04:08  iter: 26779  total_loss: 2.655  loss_cls_stage0: 0.2324  loss_box_reg_stage0: 0.247  loss_cls_stage1: 0.2374  loss_box_reg_stage1: 0.6234  loss_cls_stage2: 0.2472  loss_box_reg_stage2: 0.8107  loss_mask: 0.07427  loss_rpn_cls: 0.0128  loss_rpn_loc: 0.04934  time: 0.6232  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 17:49:58] d2.utils.events INFO:  eta: 7:04:10  iter: 26799  total_loss: 2.697  loss_cls_stage0: 0.2599  loss_box_reg_stage0: 0.2742  loss_cls_stage1: 0.2708  loss_box_reg_stage1: 0.6211  loss_cls_stage2: 0.2396  loss_box_reg_stage2: 0.832  loss_mask: 0.07778  loss_rpn_cls: 0.01499  loss_rpn_loc: 0.04341  time: 0.6232  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:50:11] d2.utils.events INFO:  eta: 7:04:22  iter: 26819  total_loss: 2.669  loss_cls_stage0: 0.2655  loss_box_reg_stage0: 0.2476  loss_cls_stage1: 0.2752  loss_box_reg_stage1: 0.6771  loss_cls_stage2: 0.2501  loss_box_reg_stage2: 0.9285  loss_mask: 0.07472  loss_rpn_cls: 0.01127  loss_rpn_loc: 0.04538  time: 0.6232  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:50:24] d2.utils.events INFO:  eta: 7:04:09  iter: 26839  total_loss: 2.683  loss_cls_stage0: 0.2419  loss_box_reg_stage0: 0.2611  loss_cls_stage1: 0.2633  loss_box_reg_stage1: 0.6588  loss_cls_stage2: 0.2634  loss_box_reg_stage2: 0.8916  loss_mask: 0.07639  loss_rpn_cls: 0.01562  loss_rpn_loc: 0.04004  time: 0.6232  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 17:50:37] d2.utils.events INFO:  eta: 7:04:13  iter: 26859  total_loss: 2.873  loss_cls_stage0: 0.2934  loss_box_reg_stage0: 0.2817  loss_cls_stage1: 0.2847  loss_box_reg_stage1: 0.7062  loss_cls_stage2: 0.2668  loss_box_reg_stage2: 0.8896  loss_mask: 0.08492  loss_rpn_cls: 0.01558  loss_rpn_loc: 0.04431  time: 0.6232  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 17:50:49] d2.utils.events INFO:  eta: 7:04:27  iter: 26879  total_loss: 2.538  loss_cls_stage0: 0.2151  loss_box_reg_stage0: 0.2425  loss_cls_stage1: 0.2251  loss_box_reg_stage1: 0.631  loss_cls_stage2: 0.2098  loss_box_reg_stage2: 0.8436  loss_mask: 0.07111  loss_rpn_cls: 0.01642  loss_rpn_loc: 0.04281  time: 0.6233  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 17:51:02] d2.utils.events INFO:  eta: 7:03:55  iter: 26899  total_loss: 2.804  loss_cls_stage0: 0.2437  loss_box_reg_stage0: 0.2454  loss_cls_stage1: 0.2663  loss_box_reg_stage1: 0.6439  loss_cls_stage2: 0.2627  loss_box_reg_stage2: 0.8569  loss_mask: 0.06637  loss_rpn_cls: 0.01556  loss_rpn_loc: 0.04728  time: 0.6233  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 17:51:14] d2.utils.events INFO:  eta: 7:03:36  iter: 26919  total_loss: 2.675  loss_cls_stage0: 0.2575  loss_box_reg_stage0: 0.2543  loss_cls_stage1: 0.2675  loss_box_reg_stage1: 0.6048  loss_cls_stage2: 0.2583  loss_box_reg_stage2: 0.8557  loss_mask: 0.0764  loss_rpn_cls: 0.0196  loss_rpn_loc: 0.06899  time: 0.6233  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 17:51:26] d2.utils.events INFO:  eta: 7:03:30  iter: 26939  total_loss: 2.592  loss_cls_stage0: 0.2428  loss_box_reg_stage0: 0.2536  loss_cls_stage1: 0.248  loss_box_reg_stage1: 0.6346  loss_cls_stage2: 0.2485  loss_box_reg_stage2: 0.8455  loss_mask: 0.07966  loss_rpn_cls: 0.01628  loss_rpn_loc: 0.0534  time: 0.6232  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:51:39] d2.utils.events INFO:  eta: 7:03:06  iter: 26959  total_loss: 2.708  loss_cls_stage0: 0.2721  loss_box_reg_stage0: 0.2676  loss_cls_stage1: 0.2887  loss_box_reg_stage1: 0.6271  loss_cls_stage2: 0.2804  loss_box_reg_stage2: 0.8792  loss_mask: 0.08314  loss_rpn_cls: 0.01254  loss_rpn_loc: 0.04617  time: 0.6232  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:51:52] d2.utils.events INFO:  eta: 7:03:22  iter: 26979  total_loss: 2.613  loss_cls_stage0: 0.2162  loss_box_reg_stage0: 0.2569  loss_cls_stage1: 0.2311  loss_box_reg_stage1: 0.6765  loss_cls_stage2: 0.2183  loss_box_reg_stage2: 0.9794  loss_mask: 0.0691  loss_rpn_cls: 0.008893  loss_rpn_loc: 0.04198  time: 0.6232  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 17:52:04] d2.utils.events INFO:  eta: 7:02:46  iter: 26999  total_loss: 2.619  loss_cls_stage0: 0.2099  loss_box_reg_stage0: 0.2361  loss_cls_stage1: 0.2611  loss_box_reg_stage1: 0.6337  loss_cls_stage2: 0.2714  loss_box_reg_stage2: 0.8836  loss_mask: 0.06836  loss_rpn_cls: 0.01195  loss_rpn_loc: 0.05273  time: 0.6232  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:52:17] d2.utils.events INFO:  eta: 7:03:26  iter: 27019  total_loss: 2.928  loss_cls_stage0: 0.2736  loss_box_reg_stage0: 0.2609  loss_cls_stage1: 0.3107  loss_box_reg_stage1: 0.651  loss_cls_stage2: 0.2875  loss_box_reg_stage2: 0.9431  loss_mask: 0.07991  loss_rpn_cls: 0.01669  loss_rpn_loc: 0.04142  time: 0.6233  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:52:30] d2.utils.events INFO:  eta: 7:03:27  iter: 27039  total_loss: 2.942  loss_cls_stage0: 0.2333  loss_box_reg_stage0: 0.2581  loss_cls_stage1: 0.2667  loss_box_reg_stage1: 0.7069  loss_cls_stage2: 0.2573  loss_box_reg_stage2: 0.9352  loss_mask: 0.07268  loss_rpn_cls: 0.02149  loss_rpn_loc: 0.0604  time: 0.6233  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:52:42] d2.utils.events INFO:  eta: 7:02:35  iter: 27059  total_loss: 2.987  loss_cls_stage0: 0.2947  loss_box_reg_stage0: 0.2805  loss_cls_stage1: 0.3034  loss_box_reg_stage1: 0.6941  loss_cls_stage2: 0.2917  loss_box_reg_stage2: 0.9272  loss_mask: 0.07925  loss_rpn_cls: 0.02277  loss_rpn_loc: 0.04203  time: 0.6233  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:52:55] d2.utils.events INFO:  eta: 7:02:23  iter: 27079  total_loss: 2.542  loss_cls_stage0: 0.2043  loss_box_reg_stage0: 0.2508  loss_cls_stage1: 0.2007  loss_box_reg_stage1: 0.5865  loss_cls_stage2: 0.2127  loss_box_reg_stage2: 0.8422  loss_mask: 0.07217  loss_rpn_cls: 0.01487  loss_rpn_loc: 0.04727  time: 0.6233  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 17:53:07] d2.utils.events INFO:  eta: 7:02:21  iter: 27099  total_loss: 2.483  loss_cls_stage0: 0.2283  loss_box_reg_stage0: 0.2436  loss_cls_stage1: 0.2327  loss_box_reg_stage1: 0.5821  loss_cls_stage2: 0.221  loss_box_reg_stage2: 0.7631  loss_mask: 0.07137  loss_rpn_cls: 0.01472  loss_rpn_loc: 0.04089  time: 0.6233  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:53:19] d2.utils.events INFO:  eta: 7:02:23  iter: 27119  total_loss: 2.784  loss_cls_stage0: 0.2378  loss_box_reg_stage0: 0.2552  loss_cls_stage1: 0.2441  loss_box_reg_stage1: 0.6636  loss_cls_stage2: 0.2277  loss_box_reg_stage2: 0.9165  loss_mask: 0.0772  loss_rpn_cls: 0.01189  loss_rpn_loc: 0.05259  time: 0.6233  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:53:31] d2.utils.events INFO:  eta: 7:01:40  iter: 27139  total_loss: 2.645  loss_cls_stage0: 0.2493  loss_box_reg_stage0: 0.2378  loss_cls_stage1: 0.2388  loss_box_reg_stage1: 0.6148  loss_cls_stage2: 0.2065  loss_box_reg_stage2: 0.8807  loss_mask: 0.07578  loss_rpn_cls: 0.02251  loss_rpn_loc: 0.0499  time: 0.6232  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:53:44] d2.utils.events INFO:  eta: 7:01:33  iter: 27159  total_loss: 2.541  loss_cls_stage0: 0.2182  loss_box_reg_stage0: 0.2429  loss_cls_stage1: 0.2199  loss_box_reg_stage1: 0.6064  loss_cls_stage2: 0.2086  loss_box_reg_stage2: 0.8489  loss_mask: 0.06951  loss_rpn_cls: 0.01498  loss_rpn_loc: 0.04739  time: 0.6232  data_time: 0.0045  lr: 0.00016  max_mem: 19679M
[07/29 17:53:56] d2.utils.events INFO:  eta: 7:01:33  iter: 27179  total_loss: 2.584  loss_cls_stage0: 0.2307  loss_box_reg_stage0: 0.2409  loss_cls_stage1: 0.276  loss_box_reg_stage1: 0.6227  loss_cls_stage2: 0.2589  loss_box_reg_stage2: 0.8218  loss_mask: 0.07295  loss_rpn_cls: 0.008688  loss_rpn_loc: 0.04104  time: 0.6232  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 17:54:09] d2.utils.events INFO:  eta: 7:01:51  iter: 27199  total_loss: 2.863  loss_cls_stage0: 0.2463  loss_box_reg_stage0: 0.2334  loss_cls_stage1: 0.2902  loss_box_reg_stage1: 0.6516  loss_cls_stage2: 0.2825  loss_box_reg_stage2: 0.9113  loss_mask: 0.07026  loss_rpn_cls: 0.01466  loss_rpn_loc: 0.05356  time: 0.6232  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 17:54:22] d2.utils.events INFO:  eta: 7:01:12  iter: 27219  total_loss: 2.92  loss_cls_stage0: 0.248  loss_box_reg_stage0: 0.2639  loss_cls_stage1: 0.2796  loss_box_reg_stage1: 0.6849  loss_cls_stage2: 0.2598  loss_box_reg_stage2: 0.9651  loss_mask: 0.07562  loss_rpn_cls: 0.01426  loss_rpn_loc: 0.03959  time: 0.6233  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 17:54:34] d2.utils.events INFO:  eta: 7:01:24  iter: 27239  total_loss: 2.526  loss_cls_stage0: 0.229  loss_box_reg_stage0: 0.2381  loss_cls_stage1: 0.2676  loss_box_reg_stage1: 0.618  loss_cls_stage2: 0.2598  loss_box_reg_stage2: 0.8804  loss_mask: 0.06689  loss_rpn_cls: 0.01347  loss_rpn_loc: 0.06428  time: 0.6233  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 17:54:46] d2.utils.events INFO:  eta: 7:00:36  iter: 27259  total_loss: 2.874  loss_cls_stage0: 0.2684  loss_box_reg_stage0: 0.2824  loss_cls_stage1: 0.2649  loss_box_reg_stage1: 0.6833  loss_cls_stage2: 0.2484  loss_box_reg_stage2: 0.9812  loss_mask: 0.0846  loss_rpn_cls: 0.02089  loss_rpn_loc: 0.05621  time: 0.6232  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 17:54:58] d2.utils.events INFO:  eta: 6:59:50  iter: 27279  total_loss: 2.88  loss_cls_stage0: 0.2442  loss_box_reg_stage0: 0.2545  loss_cls_stage1: 0.281  loss_box_reg_stage1: 0.6818  loss_cls_stage2: 0.2976  loss_box_reg_stage2: 0.9576  loss_mask: 0.07622  loss_rpn_cls: 0.0183  loss_rpn_loc: 0.05434  time: 0.6232  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:55:11] d2.utils.events INFO:  eta: 7:00:11  iter: 27299  total_loss: 2.764  loss_cls_stage0: 0.2375  loss_box_reg_stage0: 0.2443  loss_cls_stage1: 0.2591  loss_box_reg_stage1: 0.677  loss_cls_stage2: 0.2567  loss_box_reg_stage2: 0.8892  loss_mask: 0.07248  loss_rpn_cls: 0.01478  loss_rpn_loc: 0.05694  time: 0.6232  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:55:24] d2.utils.events INFO:  eta: 7:00:05  iter: 27319  total_loss: 2.782  loss_cls_stage0: 0.2582  loss_box_reg_stage0: 0.2662  loss_cls_stage1: 0.2724  loss_box_reg_stage1: 0.6437  loss_cls_stage2: 0.2516  loss_box_reg_stage2: 0.9281  loss_mask: 0.07434  loss_rpn_cls: 0.01407  loss_rpn_loc: 0.03942  time: 0.6232  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:55:36] d2.utils.events INFO:  eta: 6:59:51  iter: 27339  total_loss: 2.846  loss_cls_stage0: 0.2524  loss_box_reg_stage0: 0.2663  loss_cls_stage1: 0.3083  loss_box_reg_stage1: 0.6819  loss_cls_stage2: 0.2739  loss_box_reg_stage2: 0.8414  loss_mask: 0.06671  loss_rpn_cls: 0.01259  loss_rpn_loc: 0.06254  time: 0.6232  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:55:48] d2.utils.events INFO:  eta: 6:59:33  iter: 27359  total_loss: 2.866  loss_cls_stage0: 0.2579  loss_box_reg_stage0: 0.2521  loss_cls_stage1: 0.2879  loss_box_reg_stage1: 0.6913  loss_cls_stage2: 0.2882  loss_box_reg_stage2: 0.9298  loss_mask: 0.06921  loss_rpn_cls: 0.01218  loss_rpn_loc: 0.04896  time: 0.6232  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:56:00] d2.utils.events INFO:  eta: 6:58:54  iter: 27379  total_loss: 2.953  loss_cls_stage0: 0.2768  loss_box_reg_stage0: 0.282  loss_cls_stage1: 0.2933  loss_box_reg_stage1: 0.7021  loss_cls_stage2: 0.2901  loss_box_reg_stage2: 0.9404  loss_mask: 0.07478  loss_rpn_cls: 0.01473  loss_rpn_loc: 0.04658  time: 0.6232  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:56:13] d2.utils.events INFO:  eta: 6:59:08  iter: 27399  total_loss: 2.91  loss_cls_stage0: 0.2521  loss_box_reg_stage0: 0.2528  loss_cls_stage1: 0.2954  loss_box_reg_stage1: 0.7148  loss_cls_stage2: 0.3054  loss_box_reg_stage2: 0.9807  loss_mask: 0.074  loss_rpn_cls: 0.01596  loss_rpn_loc: 0.04511  time: 0.6232  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:56:25] d2.utils.events INFO:  eta: 6:59:02  iter: 27419  total_loss: 2.686  loss_cls_stage0: 0.2315  loss_box_reg_stage0: 0.2475  loss_cls_stage1: 0.2649  loss_box_reg_stage1: 0.6356  loss_cls_stage2: 0.2562  loss_box_reg_stage2: 0.9047  loss_mask: 0.06962  loss_rpn_cls: 0.01734  loss_rpn_loc: 0.04091  time: 0.6232  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 17:56:38] d2.utils.events INFO:  eta: 6:59:52  iter: 27439  total_loss: 2.795  loss_cls_stage0: 0.2525  loss_box_reg_stage0: 0.2549  loss_cls_stage1: 0.2937  loss_box_reg_stage1: 0.6565  loss_cls_stage2: 0.2771  loss_box_reg_stage2: 0.8638  loss_mask: 0.0791  loss_rpn_cls: 0.01874  loss_rpn_loc: 0.05897  time: 0.6232  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:56:51] d2.utils.events INFO:  eta: 6:59:53  iter: 27459  total_loss: 2.857  loss_cls_stage0: 0.2673  loss_box_reg_stage0: 0.2546  loss_cls_stage1: 0.2908  loss_box_reg_stage1: 0.67  loss_cls_stage2: 0.2822  loss_box_reg_stage2: 0.8805  loss_mask: 0.07602  loss_rpn_cls: 0.01265  loss_rpn_loc: 0.04319  time: 0.6233  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:57:04] d2.utils.events INFO:  eta: 6:59:41  iter: 27479  total_loss: 2.602  loss_cls_stage0: 0.2269  loss_box_reg_stage0: 0.2594  loss_cls_stage1: 0.2289  loss_box_reg_stage1: 0.6186  loss_cls_stage2: 0.2221  loss_box_reg_stage2: 0.8117  loss_mask: 0.07781  loss_rpn_cls: 0.02408  loss_rpn_loc: 0.05467  time: 0.6233  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 17:57:16] d2.utils.events INFO:  eta: 6:59:14  iter: 27499  total_loss: 2.505  loss_cls_stage0: 0.2316  loss_box_reg_stage0: 0.2476  loss_cls_stage1: 0.2496  loss_box_reg_stage1: 0.595  loss_cls_stage2: 0.2102  loss_box_reg_stage2: 0.8193  loss_mask: 0.07181  loss_rpn_cls: 0.0205  loss_rpn_loc: 0.04167  time: 0.6232  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:57:28] d2.utils.events INFO:  eta: 6:58:44  iter: 27519  total_loss: 2.596  loss_cls_stage0: 0.2142  loss_box_reg_stage0: 0.2569  loss_cls_stage1: 0.2248  loss_box_reg_stage1: 0.6252  loss_cls_stage2: 0.2268  loss_box_reg_stage2: 0.8618  loss_mask: 0.06454  loss_rpn_cls: 0.02039  loss_rpn_loc: 0.03968  time: 0.6232  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:57:40] d2.utils.events INFO:  eta: 6:58:17  iter: 27539  total_loss: 2.772  loss_cls_stage0: 0.2613  loss_box_reg_stage0: 0.26  loss_cls_stage1: 0.2691  loss_box_reg_stage1: 0.6486  loss_cls_stage2: 0.2397  loss_box_reg_stage2: 0.8618  loss_mask: 0.07172  loss_rpn_cls: 0.01689  loss_rpn_loc: 0.04463  time: 0.6232  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:57:53] d2.utils.events INFO:  eta: 6:58:42  iter: 27559  total_loss: 2.67  loss_cls_stage0: 0.2307  loss_box_reg_stage0: 0.2428  loss_cls_stage1: 0.2492  loss_box_reg_stage1: 0.6331  loss_cls_stage2: 0.2324  loss_box_reg_stage2: 0.898  loss_mask: 0.0681  loss_rpn_cls: 0.0158  loss_rpn_loc: 0.04276  time: 0.6232  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 17:58:06] d2.utils.events INFO:  eta: 6:58:41  iter: 27579  total_loss: 2.768  loss_cls_stage0: 0.2386  loss_box_reg_stage0: 0.2566  loss_cls_stage1: 0.2448  loss_box_reg_stage1: 0.6463  loss_cls_stage2: 0.2238  loss_box_reg_stage2: 0.8917  loss_mask: 0.07522  loss_rpn_cls: 0.01916  loss_rpn_loc: 0.04217  time: 0.6232  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 17:58:18] d2.utils.events INFO:  eta: 6:58:37  iter: 27599  total_loss: 3.02  loss_cls_stage0: 0.2695  loss_box_reg_stage0: 0.2648  loss_cls_stage1: 0.2843  loss_box_reg_stage1: 0.6771  loss_cls_stage2: 0.2966  loss_box_reg_stage2: 0.9317  loss_mask: 0.07974  loss_rpn_cls: 0.01518  loss_rpn_loc: 0.044  time: 0.6232  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:58:30] d2.utils.events INFO:  eta: 6:58:16  iter: 27619  total_loss: 2.542  loss_cls_stage0: 0.2336  loss_box_reg_stage0: 0.2323  loss_cls_stage1: 0.2426  loss_box_reg_stage1: 0.6033  loss_cls_stage2: 0.2225  loss_box_reg_stage2: 0.848  loss_mask: 0.07215  loss_rpn_cls: 0.01083  loss_rpn_loc: 0.04241  time: 0.6232  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:58:42] d2.utils.events INFO:  eta: 6:58:06  iter: 27639  total_loss: 2.651  loss_cls_stage0: 0.2341  loss_box_reg_stage0: 0.236  loss_cls_stage1: 0.2471  loss_box_reg_stage1: 0.6266  loss_cls_stage2: 0.2509  loss_box_reg_stage2: 0.8666  loss_mask: 0.07583  loss_rpn_cls: 0.01549  loss_rpn_loc: 0.03841  time: 0.6232  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:58:55] d2.utils.events INFO:  eta: 6:58:10  iter: 27659  total_loss: 2.435  loss_cls_stage0: 0.2326  loss_box_reg_stage0: 0.2256  loss_cls_stage1: 0.2265  loss_box_reg_stage1: 0.5706  loss_cls_stage2: 0.2302  loss_box_reg_stage2: 0.8088  loss_mask: 0.06831  loss_rpn_cls: 0.01721  loss_rpn_loc: 0.05395  time: 0.6232  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:59:07] d2.utils.events INFO:  eta: 6:57:51  iter: 27679  total_loss: 2.877  loss_cls_stage0: 0.2592  loss_box_reg_stage0: 0.2583  loss_cls_stage1: 0.2989  loss_box_reg_stage1: 0.6602  loss_cls_stage2: 0.3116  loss_box_reg_stage2: 0.8787  loss_mask: 0.07443  loss_rpn_cls: 0.01654  loss_rpn_loc: 0.05979  time: 0.6232  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:59:20] d2.utils.events INFO:  eta: 6:57:39  iter: 27699  total_loss: 2.56  loss_cls_stage0: 0.2434  loss_box_reg_stage0: 0.2373  loss_cls_stage1: 0.2705  loss_box_reg_stage1: 0.6221  loss_cls_stage2: 0.2588  loss_box_reg_stage2: 0.8288  loss_mask: 0.06912  loss_rpn_cls: 0.01435  loss_rpn_loc: 0.04112  time: 0.6232  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 17:59:32] d2.utils.events INFO:  eta: 6:57:48  iter: 27719  total_loss: 2.725  loss_cls_stage0: 0.2426  loss_box_reg_stage0: 0.2259  loss_cls_stage1: 0.2603  loss_box_reg_stage1: 0.6791  loss_cls_stage2: 0.2622  loss_box_reg_stage2: 0.8109  loss_mask: 0.07351  loss_rpn_cls: 0.01035  loss_rpn_loc: 0.03764  time: 0.6232  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 17:59:45] d2.utils.events INFO:  eta: 6:57:35  iter: 27739  total_loss: 2.533  loss_cls_stage0: 0.2322  loss_box_reg_stage0: 0.2365  loss_cls_stage1: 0.2539  loss_box_reg_stage1: 0.5934  loss_cls_stage2: 0.2439  loss_box_reg_stage2: 0.8117  loss_mask: 0.06909  loss_rpn_cls: 0.01459  loss_rpn_loc: 0.04307  time: 0.6232  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 17:59:58] d2.utils.events INFO:  eta: 6:57:07  iter: 27759  total_loss: 2.937  loss_cls_stage0: 0.2647  loss_box_reg_stage0: 0.2423  loss_cls_stage1: 0.3224  loss_box_reg_stage1: 0.6599  loss_cls_stage2: 0.3396  loss_box_reg_stage2: 0.9  loss_mask: 0.07396  loss_rpn_cls: 0.01398  loss_rpn_loc: 0.0458  time: 0.6232  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 18:00:11] d2.utils.events INFO:  eta: 6:57:41  iter: 27779  total_loss: 2.951  loss_cls_stage0: 0.2544  loss_box_reg_stage0: 0.2403  loss_cls_stage1: 0.2988  loss_box_reg_stage1: 0.6817  loss_cls_stage2: 0.3026  loss_box_reg_stage2: 0.9866  loss_mask: 0.07419  loss_rpn_cls: 0.0103  loss_rpn_loc: 0.03788  time: 0.6232  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 18:00:23] d2.utils.events INFO:  eta: 6:57:00  iter: 27799  total_loss: 3.023  loss_cls_stage0: 0.2905  loss_box_reg_stage0: 0.2747  loss_cls_stage1: 0.3178  loss_box_reg_stage1: 0.7078  loss_cls_stage2: 0.2968  loss_box_reg_stage2: 0.9197  loss_mask: 0.07575  loss_rpn_cls: 0.01762  loss_rpn_loc: 0.0548  time: 0.6232  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 18:00:35] d2.utils.events INFO:  eta: 6:56:07  iter: 27819  total_loss: 2.542  loss_cls_stage0: 0.2423  loss_box_reg_stage0: 0.254  loss_cls_stage1: 0.2553  loss_box_reg_stage1: 0.616  loss_cls_stage2: 0.2524  loss_box_reg_stage2: 0.8148  loss_mask: 0.07348  loss_rpn_cls: 0.01142  loss_rpn_loc: 0.03829  time: 0.6232  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 18:00:47] d2.utils.events INFO:  eta: 6:55:46  iter: 27839  total_loss: 2.377  loss_cls_stage0: 0.2115  loss_box_reg_stage0: 0.2344  loss_cls_stage1: 0.2044  loss_box_reg_stage1: 0.5749  loss_cls_stage2: 0.2216  loss_box_reg_stage2: 0.7844  loss_mask: 0.07873  loss_rpn_cls: 0.01333  loss_rpn_loc: 0.04043  time: 0.6232  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 18:01:00] d2.utils.events INFO:  eta: 6:55:30  iter: 27859  total_loss: 2.448  loss_cls_stage0: 0.2246  loss_box_reg_stage0: 0.2456  loss_cls_stage1: 0.2477  loss_box_reg_stage1: 0.5864  loss_cls_stage2: 0.2525  loss_box_reg_stage2: 0.7493  loss_mask: 0.06907  loss_rpn_cls: 0.01229  loss_rpn_loc: 0.04326  time: 0.6232  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 18:01:12] d2.utils.events INFO:  eta: 6:54:14  iter: 27879  total_loss: 2.363  loss_cls_stage0: 0.2398  loss_box_reg_stage0: 0.2426  loss_cls_stage1: 0.2265  loss_box_reg_stage1: 0.5596  loss_cls_stage2: 0.2227  loss_box_reg_stage2: 0.7718  loss_mask: 0.07414  loss_rpn_cls: 0.02015  loss_rpn_loc: 0.05308  time: 0.6232  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 18:01:24] d2.utils.events INFO:  eta: 6:54:06  iter: 27899  total_loss: 2.438  loss_cls_stage0: 0.2376  loss_box_reg_stage0: 0.2344  loss_cls_stage1: 0.2429  loss_box_reg_stage1: 0.5511  loss_cls_stage2: 0.2301  loss_box_reg_stage2: 0.762  loss_mask: 0.07362  loss_rpn_cls: 0.009686  loss_rpn_loc: 0.04246  time: 0.6232  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 18:01:36] d2.utils.events INFO:  eta: 6:54:02  iter: 27919  total_loss: 2.511  loss_cls_stage0: 0.2176  loss_box_reg_stage0: 0.2369  loss_cls_stage1: 0.2423  loss_box_reg_stage1: 0.5747  loss_cls_stage2: 0.2267  loss_box_reg_stage2: 0.8084  loss_mask: 0.06763  loss_rpn_cls: 0.01235  loss_rpn_loc: 0.03866  time: 0.6232  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 18:01:48] d2.utils.events INFO:  eta: 6:53:35  iter: 27939  total_loss: 2.356  loss_cls_stage0: 0.2145  loss_box_reg_stage0: 0.2364  loss_cls_stage1: 0.2141  loss_box_reg_stage1: 0.5639  loss_cls_stage2: 0.1951  loss_box_reg_stage2: 0.7958  loss_mask: 0.0702  loss_rpn_cls: 0.0166  loss_rpn_loc: 0.04444  time: 0.6231  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 18:02:01] d2.utils.events INFO:  eta: 6:53:33  iter: 27959  total_loss: 2.651  loss_cls_stage0: 0.2468  loss_box_reg_stage0: 0.2565  loss_cls_stage1: 0.2421  loss_box_reg_stage1: 0.6341  loss_cls_stage2: 0.2331  loss_box_reg_stage2: 0.8926  loss_mask: 0.06926  loss_rpn_cls: 0.01413  loss_rpn_loc: 0.06073  time: 0.6231  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 18:02:14] d2.utils.events INFO:  eta: 6:53:10  iter: 27979  total_loss: 2.731  loss_cls_stage0: 0.2485  loss_box_reg_stage0: 0.247  loss_cls_stage1: 0.2761  loss_box_reg_stage1: 0.6264  loss_cls_stage2: 0.2545  loss_box_reg_stage2: 0.8841  loss_mask: 0.07091  loss_rpn_cls: 0.01616  loss_rpn_loc: 0.04374  time: 0.6232  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 18:02:26] d2.utils.events INFO:  eta: 6:52:57  iter: 27999  total_loss: 2.358  loss_cls_stage0: 0.2286  loss_box_reg_stage0: 0.2247  loss_cls_stage1: 0.2242  loss_box_reg_stage1: 0.5604  loss_cls_stage2: 0.2203  loss_box_reg_stage2: 0.819  loss_mask: 0.06823  loss_rpn_cls: 0.01605  loss_rpn_loc: 0.04381  time: 0.6231  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 18:02:39] d2.utils.events INFO:  eta: 6:52:43  iter: 28019  total_loss: 2.615  loss_cls_stage0: 0.2477  loss_box_reg_stage0: 0.2592  loss_cls_stage1: 0.2538  loss_box_reg_stage1: 0.6037  loss_cls_stage2: 0.257  loss_box_reg_stage2: 0.8387  loss_mask: 0.0802  loss_rpn_cls: 0.01146  loss_rpn_loc: 0.03955  time: 0.6231  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 18:02:51] d2.utils.events INFO:  eta: 6:52:30  iter: 28039  total_loss: 2.517  loss_cls_stage0: 0.2313  loss_box_reg_stage0: 0.2272  loss_cls_stage1: 0.2536  loss_box_reg_stage1: 0.6219  loss_cls_stage2: 0.2178  loss_box_reg_stage2: 0.8053  loss_mask: 0.06735  loss_rpn_cls: 0.01665  loss_rpn_loc: 0.05371  time: 0.6231  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 18:03:04] d2.utils.events INFO:  eta: 6:52:52  iter: 28059  total_loss: 2.429  loss_cls_stage0: 0.2505  loss_box_reg_stage0: 0.2476  loss_cls_stage1: 0.272  loss_box_reg_stage1: 0.5941  loss_cls_stage2: 0.2313  loss_box_reg_stage2: 0.7205  loss_mask: 0.08029  loss_rpn_cls: 0.01922  loss_rpn_loc: 0.0516  time: 0.6232  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 18:03:16] d2.utils.events INFO:  eta: 6:52:43  iter: 28079  total_loss: 2.573  loss_cls_stage0: 0.2324  loss_box_reg_stage0: 0.2441  loss_cls_stage1: 0.2536  loss_box_reg_stage1: 0.6283  loss_cls_stage2: 0.2537  loss_box_reg_stage2: 0.8484  loss_mask: 0.07261  loss_rpn_cls: 0.01571  loss_rpn_loc: 0.04009  time: 0.6231  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 18:03:28] d2.utils.events INFO:  eta: 6:52:33  iter: 28099  total_loss: 2.8  loss_cls_stage0: 0.2718  loss_box_reg_stage0: 0.2652  loss_cls_stage1: 0.268  loss_box_reg_stage1: 0.6502  loss_cls_stage2: 0.2448  loss_box_reg_stage2: 0.8508  loss_mask: 0.0761  loss_rpn_cls: 0.01112  loss_rpn_loc: 0.04105  time: 0.6231  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 18:03:41] d2.utils.events INFO:  eta: 6:52:20  iter: 28119  total_loss: 2.529  loss_cls_stage0: 0.2155  loss_box_reg_stage0: 0.2417  loss_cls_stage1: 0.2123  loss_box_reg_stage1: 0.6181  loss_cls_stage2: 0.2161  loss_box_reg_stage2: 0.8778  loss_mask: 0.06217  loss_rpn_cls: 0.01111  loss_rpn_loc: 0.04626  time: 0.6231  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 18:03:53] d2.utils.events INFO:  eta: 6:52:15  iter: 28139  total_loss: 2.74  loss_cls_stage0: 0.2351  loss_box_reg_stage0: 0.249  loss_cls_stage1: 0.247  loss_box_reg_stage1: 0.6799  loss_cls_stage2: 0.2386  loss_box_reg_stage2: 0.9492  loss_mask: 0.07018  loss_rpn_cls: 0.01611  loss_rpn_loc: 0.04766  time: 0.6231  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 18:04:06] d2.utils.events INFO:  eta: 6:51:56  iter: 28159  total_loss: 2.62  loss_cls_stage0: 0.2504  loss_box_reg_stage0: 0.2656  loss_cls_stage1: 0.2788  loss_box_reg_stage1: 0.6215  loss_cls_stage2: 0.2702  loss_box_reg_stage2: 0.8443  loss_mask: 0.0691  loss_rpn_cls: 0.01327  loss_rpn_loc: 0.03862  time: 0.6231  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 18:04:18] d2.utils.events INFO:  eta: 6:51:44  iter: 28179  total_loss: 3.062  loss_cls_stage0: 0.2507  loss_box_reg_stage0: 0.2695  loss_cls_stage1: 0.264  loss_box_reg_stage1: 0.7476  loss_cls_stage2: 0.2614  loss_box_reg_stage2: 0.9518  loss_mask: 0.0776  loss_rpn_cls: 0.01186  loss_rpn_loc: 0.04827  time: 0.6231  data_time: 0.0044  lr: 0.00016  max_mem: 19679M
[07/29 18:04:30] d2.utils.events INFO:  eta: 6:51:18  iter: 28199  total_loss: 2.805  loss_cls_stage0: 0.2397  loss_box_reg_stage0: 0.2552  loss_cls_stage1: 0.2689  loss_box_reg_stage1: 0.6365  loss_cls_stage2: 0.286  loss_box_reg_stage2: 0.892  loss_mask: 0.07393  loss_rpn_cls: 0.01041  loss_rpn_loc: 0.04256  time: 0.6231  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 18:04:42] d2.utils.events INFO:  eta: 6:50:50  iter: 28219  total_loss: 2.704  loss_cls_stage0: 0.2382  loss_box_reg_stage0: 0.2504  loss_cls_stage1: 0.2524  loss_box_reg_stage1: 0.6547  loss_cls_stage2: 0.2264  loss_box_reg_stage2: 0.8852  loss_mask: 0.07217  loss_rpn_cls: 0.01306  loss_rpn_loc: 0.04241  time: 0.6231  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 18:04:55] d2.utils.events INFO:  eta: 6:50:31  iter: 28239  total_loss: 2.66  loss_cls_stage0: 0.2425  loss_box_reg_stage0: 0.2784  loss_cls_stage1: 0.2196  loss_box_reg_stage1: 0.6258  loss_cls_stage2: 0.2081  loss_box_reg_stage2: 0.8395  loss_mask: 0.06734  loss_rpn_cls: 0.02011  loss_rpn_loc: 0.03985  time: 0.6231  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 18:05:07] d2.utils.events INFO:  eta: 6:50:50  iter: 28259  total_loss: 2.869  loss_cls_stage0: 0.2764  loss_box_reg_stage0: 0.2765  loss_cls_stage1: 0.2855  loss_box_reg_stage1: 0.6806  loss_cls_stage2: 0.2682  loss_box_reg_stage2: 0.8614  loss_mask: 0.07794  loss_rpn_cls: 0.01532  loss_rpn_loc: 0.04593  time: 0.6231  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 18:05:19] d2.utils.events INFO:  eta: 6:50:28  iter: 28279  total_loss: 2.775  loss_cls_stage0: 0.2417  loss_box_reg_stage0: 0.2558  loss_cls_stage1: 0.2948  loss_box_reg_stage1: 0.6705  loss_cls_stage2: 0.2873  loss_box_reg_stage2: 0.8889  loss_mask: 0.07756  loss_rpn_cls: 0.01028  loss_rpn_loc: 0.04593  time: 0.6231  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 18:05:32] d2.utils.events INFO:  eta: 6:50:03  iter: 28299  total_loss: 2.623  loss_cls_stage0: 0.2482  loss_box_reg_stage0: 0.2481  loss_cls_stage1: 0.2722  loss_box_reg_stage1: 0.5706  loss_cls_stage2: 0.249  loss_box_reg_stage2: 0.7575  loss_mask: 0.07399  loss_rpn_cls: 0.01328  loss_rpn_loc: 0.03964  time: 0.6231  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 18:05:44] d2.utils.events INFO:  eta: 6:49:37  iter: 28319  total_loss: 2.467  loss_cls_stage0: 0.2503  loss_box_reg_stage0: 0.2376  loss_cls_stage1: 0.2733  loss_box_reg_stage1: 0.5853  loss_cls_stage2: 0.2652  loss_box_reg_stage2: 0.8042  loss_mask: 0.06311  loss_rpn_cls: 0.01639  loss_rpn_loc: 0.05418  time: 0.6231  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 18:05:56] d2.utils.events INFO:  eta: 6:49:25  iter: 28339  total_loss: 2.861  loss_cls_stage0: 0.2567  loss_box_reg_stage0: 0.252  loss_cls_stage1: 0.2528  loss_box_reg_stage1: 0.6594  loss_cls_stage2: 0.2414  loss_box_reg_stage2: 0.8878  loss_mask: 0.07101  loss_rpn_cls: 0.01648  loss_rpn_loc: 0.04224  time: 0.6231  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 18:06:09] d2.utils.events INFO:  eta: 6:49:16  iter: 28359  total_loss: 2.591  loss_cls_stage0: 0.2471  loss_box_reg_stage0: 0.2432  loss_cls_stage1: 0.2656  loss_box_reg_stage1: 0.6171  loss_cls_stage2: 0.2533  loss_box_reg_stage2: 0.8551  loss_mask: 0.07446  loss_rpn_cls: 0.0152  loss_rpn_loc: 0.05683  time: 0.6231  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 18:06:21] d2.utils.events INFO:  eta: 6:49:34  iter: 28379  total_loss: 2.587  loss_cls_stage0: 0.246  loss_box_reg_stage0: 0.2318  loss_cls_stage1: 0.2469  loss_box_reg_stage1: 0.5797  loss_cls_stage2: 0.2351  loss_box_reg_stage2: 0.8342  loss_mask: 0.07496  loss_rpn_cls: 0.0177  loss_rpn_loc: 0.04113  time: 0.6231  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 18:06:35] d2.utils.events INFO:  eta: 6:49:28  iter: 28399  total_loss: 2.576  loss_cls_stage0: 0.2368  loss_box_reg_stage0: 0.2499  loss_cls_stage1: 0.2293  loss_box_reg_stage1: 0.6  loss_cls_stage2: 0.2398  loss_box_reg_stage2: 0.7928  loss_mask: 0.08288  loss_rpn_cls: 0.01215  loss_rpn_loc: 0.05109  time: 0.6231  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 18:06:47] d2.utils.events INFO:  eta: 6:49:09  iter: 28419  total_loss: 2.634  loss_cls_stage0: 0.2571  loss_box_reg_stage0: 0.2789  loss_cls_stage1: 0.2593  loss_box_reg_stage1: 0.6269  loss_cls_stage2: 0.2593  loss_box_reg_stage2: 0.8138  loss_mask: 0.08472  loss_rpn_cls: 0.01555  loss_rpn_loc: 0.04426  time: 0.6231  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 18:06:59] d2.utils.events INFO:  eta: 6:48:47  iter: 28439  total_loss: 2.515  loss_cls_stage0: 0.2134  loss_box_reg_stage0: 0.2511  loss_cls_stage1: 0.2163  loss_box_reg_stage1: 0.5806  loss_cls_stage2: 0.2067  loss_box_reg_stage2: 0.8365  loss_mask: 0.07126  loss_rpn_cls: 0.01621  loss_rpn_loc: 0.05194  time: 0.6231  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 18:07:12] d2.utils.events INFO:  eta: 6:48:17  iter: 28459  total_loss: 2.503  loss_cls_stage0: 0.2358  loss_box_reg_stage0: 0.2445  loss_cls_stage1: 0.2563  loss_box_reg_stage1: 0.6001  loss_cls_stage2: 0.2515  loss_box_reg_stage2: 0.8263  loss_mask: 0.06916  loss_rpn_cls: 0.01517  loss_rpn_loc: 0.05043  time: 0.6231  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 18:07:24] d2.utils.events INFO:  eta: 6:48:02  iter: 28479  total_loss: 2.684  loss_cls_stage0: 0.2431  loss_box_reg_stage0: 0.2543  loss_cls_stage1: 0.2499  loss_box_reg_stage1: 0.63  loss_cls_stage2: 0.2503  loss_box_reg_stage2: 0.8238  loss_mask: 0.07238  loss_rpn_cls: 0.0213  loss_rpn_loc: 0.04317  time: 0.6231  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 18:07:36] d2.utils.events INFO:  eta: 6:48:16  iter: 28499  total_loss: 2.731  loss_cls_stage0: 0.2641  loss_box_reg_stage0: 0.2618  loss_cls_stage1: 0.2758  loss_box_reg_stage1: 0.6506  loss_cls_stage2: 0.2549  loss_box_reg_stage2: 0.8628  loss_mask: 0.07305  loss_rpn_cls: 0.01613  loss_rpn_loc: 0.03994  time: 0.6231  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 18:07:48] d2.utils.events INFO:  eta: 6:48:03  iter: 28519  total_loss: 2.91  loss_cls_stage0: 0.2596  loss_box_reg_stage0: 0.2674  loss_cls_stage1: 0.2814  loss_box_reg_stage1: 0.7089  loss_cls_stage2: 0.2603  loss_box_reg_stage2: 0.9887  loss_mask: 0.07746  loss_rpn_cls: 0.01392  loss_rpn_loc: 0.04359  time: 0.6231  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 18:08:01] d2.utils.events INFO:  eta: 6:48:06  iter: 28539  total_loss: 2.863  loss_cls_stage0: 0.2739  loss_box_reg_stage0: 0.2841  loss_cls_stage1: 0.2953  loss_box_reg_stage1: 0.6755  loss_cls_stage2: 0.2597  loss_box_reg_stage2: 0.9415  loss_mask: 0.07138  loss_rpn_cls: 0.01121  loss_rpn_loc: 0.04076  time: 0.6231  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 18:08:14] d2.utils.events INFO:  eta: 6:47:21  iter: 28559  total_loss: 2.817  loss_cls_stage0: 0.2443  loss_box_reg_stage0: 0.2566  loss_cls_stage1: 0.2653  loss_box_reg_stage1: 0.6581  loss_cls_stage2: 0.2325  loss_box_reg_stage2: 0.8792  loss_mask: 0.08197  loss_rpn_cls: 0.01754  loss_rpn_loc: 0.06308  time: 0.6231  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 18:08:26] d2.utils.events INFO:  eta: 6:46:57  iter: 28579  total_loss: 2.485  loss_cls_stage0: 0.2384  loss_box_reg_stage0: 0.2653  loss_cls_stage1: 0.2433  loss_box_reg_stage1: 0.5885  loss_cls_stage2: 0.2262  loss_box_reg_stage2: 0.7576  loss_mask: 0.07976  loss_rpn_cls: 0.02216  loss_rpn_loc: 0.06689  time: 0.6230  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 18:08:38] d2.utils.events INFO:  eta: 6:46:44  iter: 28599  total_loss: 2.695  loss_cls_stage0: 0.2497  loss_box_reg_stage0: 0.2598  loss_cls_stage1: 0.2638  loss_box_reg_stage1: 0.6262  loss_cls_stage2: 0.2471  loss_box_reg_stage2: 0.8339  loss_mask: 0.07786  loss_rpn_cls: 0.0164  loss_rpn_loc: 0.03647  time: 0.6231  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 18:08:51] d2.utils.events INFO:  eta: 6:46:35  iter: 28619  total_loss: 2.866  loss_cls_stage0: 0.2523  loss_box_reg_stage0: 0.2585  loss_cls_stage1: 0.2717  loss_box_reg_stage1: 0.6569  loss_cls_stage2: 0.2752  loss_box_reg_stage2: 0.9436  loss_mask: 0.08006  loss_rpn_cls: 0.01881  loss_rpn_loc: 0.04598  time: 0.6230  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 18:09:03] d2.utils.events INFO:  eta: 6:46:30  iter: 28639  total_loss: 2.715  loss_cls_stage0: 0.2416  loss_box_reg_stage0: 0.2461  loss_cls_stage1: 0.2381  loss_box_reg_stage1: 0.6298  loss_cls_stage2: 0.2644  loss_box_reg_stage2: 0.8497  loss_mask: 0.07024  loss_rpn_cls: 0.01609  loss_rpn_loc: 0.04896  time: 0.6230  data_time: 0.0045  lr: 0.00016  max_mem: 19679M
[07/29 18:09:15] d2.utils.events INFO:  eta: 6:46:09  iter: 28659  total_loss: 2.735  loss_cls_stage0: 0.2445  loss_box_reg_stage0: 0.2577  loss_cls_stage1: 0.2459  loss_box_reg_stage1: 0.697  loss_cls_stage2: 0.239  loss_box_reg_stage2: 0.9486  loss_mask: 0.06911  loss_rpn_cls: 0.01287  loss_rpn_loc: 0.04484  time: 0.6230  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 18:09:28] d2.utils.events INFO:  eta: 6:46:11  iter: 28679  total_loss: 2.623  loss_cls_stage0: 0.2209  loss_box_reg_stage0: 0.2323  loss_cls_stage1: 0.2091  loss_box_reg_stage1: 0.6256  loss_cls_stage2: 0.2101  loss_box_reg_stage2: 0.8497  loss_mask: 0.0739  loss_rpn_cls: 0.01321  loss_rpn_loc: 0.04175  time: 0.6230  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 18:09:40] d2.utils.events INFO:  eta: 6:45:44  iter: 28699  total_loss: 3.039  loss_cls_stage0: 0.3236  loss_box_reg_stage0: 0.2962  loss_cls_stage1: 0.344  loss_box_reg_stage1: 0.7194  loss_cls_stage2: 0.3124  loss_box_reg_stage2: 0.9489  loss_mask: 0.07268  loss_rpn_cls: 0.01602  loss_rpn_loc: 0.04399  time: 0.6230  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 18:09:53] d2.utils.events INFO:  eta: 6:45:46  iter: 28719  total_loss: 2.775  loss_cls_stage0: 0.2301  loss_box_reg_stage0: 0.2284  loss_cls_stage1: 0.2402  loss_box_reg_stage1: 0.6691  loss_cls_stage2: 0.2203  loss_box_reg_stage2: 0.8875  loss_mask: 0.0633  loss_rpn_cls: 0.01759  loss_rpn_loc: 0.04163  time: 0.6230  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 18:10:05] d2.utils.events INFO:  eta: 6:45:33  iter: 28739  total_loss: 2.713  loss_cls_stage0: 0.2448  loss_box_reg_stage0: 0.2577  loss_cls_stage1: 0.2736  loss_box_reg_stage1: 0.6586  loss_cls_stage2: 0.258  loss_box_reg_stage2: 0.8692  loss_mask: 0.07153  loss_rpn_cls: 0.01229  loss_rpn_loc: 0.03926  time: 0.6230  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 18:10:18] d2.utils.events INFO:  eta: 6:45:15  iter: 28759  total_loss: 2.687  loss_cls_stage0: 0.2409  loss_box_reg_stage0: 0.2638  loss_cls_stage1: 0.2767  loss_box_reg_stage1: 0.6575  loss_cls_stage2: 0.254  loss_box_reg_stage2: 0.8987  loss_mask: 0.07127  loss_rpn_cls: 0.01247  loss_rpn_loc: 0.04069  time: 0.6230  data_time: 0.0053  lr: 0.00016  max_mem: 19679M
[07/29 18:10:30] d2.utils.events INFO:  eta: 6:44:37  iter: 28779  total_loss: 2.653  loss_cls_stage0: 0.2306  loss_box_reg_stage0: 0.2326  loss_cls_stage1: 0.2408  loss_box_reg_stage1: 0.6416  loss_cls_stage2: 0.2232  loss_box_reg_stage2: 0.9175  loss_mask: 0.06542  loss_rpn_cls: 0.009563  loss_rpn_loc: 0.03848  time: 0.6230  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 18:10:43] d2.utils.events INFO:  eta: 6:44:42  iter: 28799  total_loss: 2.508  loss_cls_stage0: 0.2273  loss_box_reg_stage0: 0.2379  loss_cls_stage1: 0.2358  loss_box_reg_stage1: 0.6065  loss_cls_stage2: 0.2357  loss_box_reg_stage2: 0.8297  loss_mask: 0.07014  loss_rpn_cls: 0.01501  loss_rpn_loc: 0.06595  time: 0.6230  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 18:10:55] d2.utils.events INFO:  eta: 6:44:31  iter: 28819  total_loss: 2.47  loss_cls_stage0: 0.2273  loss_box_reg_stage0: 0.2245  loss_cls_stage1: 0.2604  loss_box_reg_stage1: 0.5596  loss_cls_stage2: 0.2504  loss_box_reg_stage2: 0.7902  loss_mask: 0.06586  loss_rpn_cls: 0.013  loss_rpn_loc: 0.04485  time: 0.6230  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 18:11:08] d2.utils.events INFO:  eta: 6:44:47  iter: 28839  total_loss: 2.489  loss_cls_stage0: 0.2184  loss_box_reg_stage0: 0.2252  loss_cls_stage1: 0.2193  loss_box_reg_stage1: 0.5733  loss_cls_stage2: 0.227  loss_box_reg_stage2: 0.8611  loss_mask: 0.06808  loss_rpn_cls: 0.01348  loss_rpn_loc: 0.04214  time: 0.6231  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 18:11:21] d2.utils.events INFO:  eta: 6:44:34  iter: 28859  total_loss: 2.794  loss_cls_stage0: 0.2472  loss_box_reg_stage0: 0.2493  loss_cls_stage1: 0.2616  loss_box_reg_stage1: 0.678  loss_cls_stage2: 0.256  loss_box_reg_stage2: 0.8945  loss_mask: 0.08064  loss_rpn_cls: 0.01286  loss_rpn_loc: 0.0482  time: 0.6231  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 18:11:33] d2.utils.events INFO:  eta: 6:44:33  iter: 28879  total_loss: 2.841  loss_cls_stage0: 0.2372  loss_box_reg_stage0: 0.2536  loss_cls_stage1: 0.2646  loss_box_reg_stage1: 0.6525  loss_cls_stage2: 0.2598  loss_box_reg_stage2: 0.8538  loss_mask: 0.07488  loss_rpn_cls: 0.01283  loss_rpn_loc: 0.04865  time: 0.6231  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 18:11:45] d2.utils.events INFO:  eta: 6:44:15  iter: 28899  total_loss: 2.568  loss_cls_stage0: 0.2099  loss_box_reg_stage0: 0.2325  loss_cls_stage1: 0.2385  loss_box_reg_stage1: 0.624  loss_cls_stage2: 0.2394  loss_box_reg_stage2: 0.882  loss_mask: 0.07022  loss_rpn_cls: 0.015  loss_rpn_loc: 0.05091  time: 0.6230  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 18:11:58] d2.utils.events INFO:  eta: 6:43:53  iter: 28919  total_loss: 2.67  loss_cls_stage0: 0.2851  loss_box_reg_stage0: 0.265  loss_cls_stage1: 0.3252  loss_box_reg_stage1: 0.6072  loss_cls_stage2: 0.281  loss_box_reg_stage2: 0.8868  loss_mask: 0.07031  loss_rpn_cls: 0.01799  loss_rpn_loc: 0.059  time: 0.6230  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 18:12:11] d2.utils.events INFO:  eta: 6:43:59  iter: 28939  total_loss: 2.717  loss_cls_stage0: 0.2632  loss_box_reg_stage0: 0.2532  loss_cls_stage1: 0.2775  loss_box_reg_stage1: 0.6761  loss_cls_stage2: 0.2657  loss_box_reg_stage2: 0.8838  loss_mask: 0.07352  loss_rpn_cls: 0.01522  loss_rpn_loc: 0.0431  time: 0.6231  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 18:12:23] d2.utils.events INFO:  eta: 6:43:38  iter: 28959  total_loss: 2.815  loss_cls_stage0: 0.2302  loss_box_reg_stage0: 0.2529  loss_cls_stage1: 0.2673  loss_box_reg_stage1: 0.6875  loss_cls_stage2: 0.2701  loss_box_reg_stage2: 0.9335  loss_mask: 0.06576  loss_rpn_cls: 0.0143  loss_rpn_loc: 0.0506  time: 0.6230  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 18:12:35] d2.utils.events INFO:  eta: 6:43:07  iter: 28979  total_loss: 2.729  loss_cls_stage0: 0.2439  loss_box_reg_stage0: 0.2367  loss_cls_stage1: 0.2465  loss_box_reg_stage1: 0.619  loss_cls_stage2: 0.2396  loss_box_reg_stage2: 0.836  loss_mask: 0.07077  loss_rpn_cls: 0.01976  loss_rpn_loc: 0.05453  time: 0.6230  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 18:12:47] d2.utils.events INFO:  eta: 6:43:02  iter: 28999  total_loss: 2.564  loss_cls_stage0: 0.2309  loss_box_reg_stage0: 0.2442  loss_cls_stage1: 0.2472  loss_box_reg_stage1: 0.6163  loss_cls_stage2: 0.2303  loss_box_reg_stage2: 0.8366  loss_mask: 0.06656  loss_rpn_cls: 0.01971  loss_rpn_loc: 0.04715  time: 0.6230  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 18:13:00] d2.utils.events INFO:  eta: 6:42:32  iter: 29019  total_loss: 2.611  loss_cls_stage0: 0.2369  loss_box_reg_stage0: 0.2461  loss_cls_stage1: 0.2558  loss_box_reg_stage1: 0.631  loss_cls_stage2: 0.2272  loss_box_reg_stage2: 0.8102  loss_mask: 0.07579  loss_rpn_cls: 0.01709  loss_rpn_loc: 0.04557  time: 0.6230  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 18:13:12] d2.utils.events INFO:  eta: 6:41:48  iter: 29039  total_loss: 2.676  loss_cls_stage0: 0.2192  loss_box_reg_stage0: 0.2376  loss_cls_stage1: 0.2516  loss_box_reg_stage1: 0.6388  loss_cls_stage2: 0.2383  loss_box_reg_stage2: 0.869  loss_mask: 0.07283  loss_rpn_cls: 0.01828  loss_rpn_loc: 0.03979  time: 0.6230  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 18:13:24] d2.utils.events INFO:  eta: 6:41:12  iter: 29059  total_loss: 2.58  loss_cls_stage0: 0.2265  loss_box_reg_stage0: 0.24  loss_cls_stage1: 0.23  loss_box_reg_stage1: 0.6121  loss_cls_stage2: 0.232  loss_box_reg_stage2: 0.8756  loss_mask: 0.0736  loss_rpn_cls: 0.01966  loss_rpn_loc: 0.05338  time: 0.6230  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 18:13:37] d2.utils.events INFO:  eta: 6:41:09  iter: 29079  total_loss: 2.508  loss_cls_stage0: 0.2338  loss_box_reg_stage0: 0.2309  loss_cls_stage1: 0.2335  loss_box_reg_stage1: 0.6022  loss_cls_stage2: 0.2127  loss_box_reg_stage2: 0.8068  loss_mask: 0.06998  loss_rpn_cls: 0.01588  loss_rpn_loc: 0.04451  time: 0.6230  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 18:13:49] d2.utils.events INFO:  eta: 6:41:10  iter: 29099  total_loss: 2.737  loss_cls_stage0: 0.2503  loss_box_reg_stage0: 0.2634  loss_cls_stage1: 0.2506  loss_box_reg_stage1: 0.6177  loss_cls_stage2: 0.2409  loss_box_reg_stage2: 0.8828  loss_mask: 0.07073  loss_rpn_cls: 0.01542  loss_rpn_loc: 0.04122  time: 0.6230  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 18:14:02] d2.utils.events INFO:  eta: 6:41:11  iter: 29119  total_loss: 2.553  loss_cls_stage0: 0.218  loss_box_reg_stage0: 0.2355  loss_cls_stage1: 0.2285  loss_box_reg_stage1: 0.5788  loss_cls_stage2: 0.2031  loss_box_reg_stage2: 0.8492  loss_mask: 0.06389  loss_rpn_cls: 0.01134  loss_rpn_loc: 0.04197  time: 0.6230  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 18:14:14] d2.utils.events INFO:  eta: 6:40:58  iter: 29139  total_loss: 2.976  loss_cls_stage0: 0.2727  loss_box_reg_stage0: 0.2674  loss_cls_stage1: 0.3145  loss_box_reg_stage1: 0.6984  loss_cls_stage2: 0.2732  loss_box_reg_stage2: 0.9424  loss_mask: 0.07862  loss_rpn_cls: 0.01076  loss_rpn_loc: 0.04332  time: 0.6230  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 18:14:27] d2.utils.events INFO:  eta: 6:41:14  iter: 29159  total_loss: 3.081  loss_cls_stage0: 0.2659  loss_box_reg_stage0: 0.2603  loss_cls_stage1: 0.3029  loss_box_reg_stage1: 0.7162  loss_cls_stage2: 0.2984  loss_box_reg_stage2: 0.9542  loss_mask: 0.0734  loss_rpn_cls: 0.01367  loss_rpn_loc: 0.0411  time: 0.6230  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 18:14:40] d2.utils.events INFO:  eta: 6:40:56  iter: 29179  total_loss: 3.159  loss_cls_stage0: 0.2921  loss_box_reg_stage0: 0.2716  loss_cls_stage1: 0.3343  loss_box_reg_stage1: 0.7817  loss_cls_stage2: 0.3157  loss_box_reg_stage2: 0.9937  loss_mask: 0.06944  loss_rpn_cls: 0.01693  loss_rpn_loc: 0.04814  time: 0.6230  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 18:14:52] d2.utils.events INFO:  eta: 6:41:24  iter: 29199  total_loss: 2.932  loss_cls_stage0: 0.2569  loss_box_reg_stage0: 0.2647  loss_cls_stage1: 0.283  loss_box_reg_stage1: 0.6684  loss_cls_stage2: 0.2946  loss_box_reg_stage2: 0.9137  loss_mask: 0.07523  loss_rpn_cls: 0.01573  loss_rpn_loc: 0.04918  time: 0.6230  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 18:15:05] d2.utils.events INFO:  eta: 6:41:44  iter: 29219  total_loss: 2.771  loss_cls_stage0: 0.2476  loss_box_reg_stage0: 0.2717  loss_cls_stage1: 0.2635  loss_box_reg_stage1: 0.674  loss_cls_stage2: 0.2746  loss_box_reg_stage2: 0.8914  loss_mask: 0.07074  loss_rpn_cls: 0.01534  loss_rpn_loc: 0.05029  time: 0.6230  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 18:15:18] d2.utils.events INFO:  eta: 6:41:41  iter: 29239  total_loss: 2.979  loss_cls_stage0: 0.2455  loss_box_reg_stage0: 0.2672  loss_cls_stage1: 0.2617  loss_box_reg_stage1: 0.7229  loss_cls_stage2: 0.262  loss_box_reg_stage2: 0.9646  loss_mask: 0.07194  loss_rpn_cls: 0.01422  loss_rpn_loc: 0.05782  time: 0.6230  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 18:15:30] d2.utils.events INFO:  eta: 6:41:38  iter: 29259  total_loss: 2.794  loss_cls_stage0: 0.2448  loss_box_reg_stage0: 0.2422  loss_cls_stage1: 0.2568  loss_box_reg_stage1: 0.6941  loss_cls_stage2: 0.2547  loss_box_reg_stage2: 0.9953  loss_mask: 0.08006  loss_rpn_cls: 0.01094  loss_rpn_loc: 0.05247  time: 0.6231  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 18:15:43] d2.utils.events INFO:  eta: 6:41:48  iter: 29279  total_loss: 2.813  loss_cls_stage0: 0.2371  loss_box_reg_stage0: 0.2522  loss_cls_stage1: 0.2522  loss_box_reg_stage1: 0.6822  loss_cls_stage2: 0.2574  loss_box_reg_stage2: 0.972  loss_mask: 0.06957  loss_rpn_cls: 0.007925  loss_rpn_loc: 0.03978  time: 0.6231  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 18:15:56] d2.utils.events INFO:  eta: 6:41:41  iter: 29299  total_loss: 2.904  loss_cls_stage0: 0.2589  loss_box_reg_stage0: 0.2462  loss_cls_stage1: 0.2857  loss_box_reg_stage1: 0.6954  loss_cls_stage2: 0.3068  loss_box_reg_stage2: 0.9716  loss_mask: 0.06714  loss_rpn_cls: 0.0114  loss_rpn_loc: 0.04282  time: 0.6231  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 18:16:08] d2.utils.events INFO:  eta: 6:41:52  iter: 29319  total_loss: 2.589  loss_cls_stage0: 0.2683  loss_box_reg_stage0: 0.2316  loss_cls_stage1: 0.2983  loss_box_reg_stage1: 0.5985  loss_cls_stage2: 0.2976  loss_box_reg_stage2: 0.8218  loss_mask: 0.06897  loss_rpn_cls: 0.009849  loss_rpn_loc: 0.0306  time: 0.6231  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 18:16:21] d2.utils.events INFO:  eta: 6:42:12  iter: 29339  total_loss: 2.923  loss_cls_stage0: 0.2515  loss_box_reg_stage0: 0.2466  loss_cls_stage1: 0.2693  loss_box_reg_stage1: 0.761  loss_cls_stage2: 0.2419  loss_box_reg_stage2: 1.021  loss_mask: 0.07009  loss_rpn_cls: 0.00784  loss_rpn_loc: 0.0371  time: 0.6231  data_time: 0.0052  lr: 0.00016  max_mem: 19679M
[07/29 18:16:34] d2.utils.events INFO:  eta: 6:42:22  iter: 29359  total_loss: 3.022  loss_cls_stage0: 0.2496  loss_box_reg_stage0: 0.2469  loss_cls_stage1: 0.3072  loss_box_reg_stage1: 0.7096  loss_cls_stage2: 0.29  loss_box_reg_stage2: 1.004  loss_mask: 0.06957  loss_rpn_cls: 0.01084  loss_rpn_loc: 0.04252  time: 0.6231  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 18:16:47] d2.utils.events INFO:  eta: 6:42:11  iter: 29379  total_loss: 3.33  loss_cls_stage0: 0.2897  loss_box_reg_stage0: 0.2548  loss_cls_stage1: 0.348  loss_box_reg_stage1: 0.7485  loss_cls_stage2: 0.3309  loss_box_reg_stage2: 1.084  loss_mask: 0.08241  loss_rpn_cls: 0.01057  loss_rpn_loc: 0.05053  time: 0.6231  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 18:16:59] d2.utils.events INFO:  eta: 6:41:54  iter: 29399  total_loss: 3.224  loss_cls_stage0: 0.2554  loss_box_reg_stage0: 0.2442  loss_cls_stage1: 0.2949  loss_box_reg_stage1: 0.7625  loss_cls_stage2: 0.3018  loss_box_reg_stage2: 1.145  loss_mask: 0.07072  loss_rpn_cls: 0.00903  loss_rpn_loc: 0.04966  time: 0.6231  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 18:17:11] d2.utils.events INFO:  eta: 6:41:50  iter: 29419  total_loss: 3.068  loss_cls_stage0: 0.2604  loss_box_reg_stage0: 0.2678  loss_cls_stage1: 0.331  loss_box_reg_stage1: 0.7148  loss_cls_stage2: 0.322  loss_box_reg_stage2: 1.032  loss_mask: 0.0798  loss_rpn_cls: 0.01091  loss_rpn_loc: 0.04289  time: 0.6231  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 18:17:24] d2.utils.events INFO:  eta: 6:41:30  iter: 29439  total_loss: 3.135  loss_cls_stage0: 0.2835  loss_box_reg_stage0: 0.2503  loss_cls_stage1: 0.3627  loss_box_reg_stage1: 0.7011  loss_cls_stage2: 0.3279  loss_box_reg_stage2: 1.003  loss_mask: 0.07035  loss_rpn_cls: 0.01195  loss_rpn_loc: 0.0471  time: 0.6231  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 18:17:36] d2.utils.events INFO:  eta: 6:41:02  iter: 29459  total_loss: 3.027  loss_cls_stage0: 0.2615  loss_box_reg_stage0: 0.2594  loss_cls_stage1: 0.3173  loss_box_reg_stage1: 0.684  loss_cls_stage2: 0.3023  loss_box_reg_stage2: 0.9037  loss_mask: 0.07673  loss_rpn_cls: 0.01579  loss_rpn_loc: 0.04836  time: 0.6231  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 18:17:48] d2.utils.events INFO:  eta: 6:40:49  iter: 29479  total_loss: 2.822  loss_cls_stage0: 0.2643  loss_box_reg_stage0: 0.2576  loss_cls_stage1: 0.243  loss_box_reg_stage1: 0.6939  loss_cls_stage2: 0.2574  loss_box_reg_stage2: 0.8951  loss_mask: 0.07901  loss_rpn_cls: 0.0169  loss_rpn_loc: 0.04153  time: 0.6231  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 18:18:01] d2.utils.events INFO:  eta: 6:40:49  iter: 29499  total_loss: 2.786  loss_cls_stage0: 0.2635  loss_box_reg_stage0: 0.2507  loss_cls_stage1: 0.2829  loss_box_reg_stage1: 0.6308  loss_cls_stage2: 0.2925  loss_box_reg_stage2: 0.8377  loss_mask: 0.07781  loss_rpn_cls: 0.01753  loss_rpn_loc: 0.05325  time: 0.6231  data_time: 0.0044  lr: 0.00016  max_mem: 19679M
[07/29 18:18:13] d2.utils.events INFO:  eta: 6:40:36  iter: 29519  total_loss: 2.641  loss_cls_stage0: 0.2332  loss_box_reg_stage0: 0.2287  loss_cls_stage1: 0.2589  loss_box_reg_stage1: 0.6403  loss_cls_stage2: 0.2529  loss_box_reg_stage2: 0.8626  loss_mask: 0.06311  loss_rpn_cls: 0.01426  loss_rpn_loc: 0.05248  time: 0.6231  data_time: 0.0043  lr: 0.00016  max_mem: 19679M
[07/29 18:18:25] d2.utils.events INFO:  eta: 6:39:51  iter: 29539  total_loss: 2.629  loss_cls_stage0: 0.213  loss_box_reg_stage0: 0.218  loss_cls_stage1: 0.2589  loss_box_reg_stage1: 0.6349  loss_cls_stage2: 0.2526  loss_box_reg_stage2: 0.911  loss_mask: 0.06542  loss_rpn_cls: 0.01222  loss_rpn_loc: 0.05318  time: 0.6231  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 18:18:38] d2.utils.events INFO:  eta: 6:40:07  iter: 29559  total_loss: 2.719  loss_cls_stage0: 0.2477  loss_box_reg_stage0: 0.2548  loss_cls_stage1: 0.2564  loss_box_reg_stage1: 0.6322  loss_cls_stage2: 0.2567  loss_box_reg_stage2: 0.8282  loss_mask: 0.06799  loss_rpn_cls: 0.01966  loss_rpn_loc: 0.04489  time: 0.6231  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 18:18:50] d2.utils.events INFO:  eta: 6:39:48  iter: 29579  total_loss: 2.354  loss_cls_stage0: 0.201  loss_box_reg_stage0: 0.2269  loss_cls_stage1: 0.2277  loss_box_reg_stage1: 0.5442  loss_cls_stage2: 0.2308  loss_box_reg_stage2: 0.7792  loss_mask: 0.07078  loss_rpn_cls: 0.01798  loss_rpn_loc: 0.04678  time: 0.6231  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 18:19:02] d2.utils.events INFO:  eta: 6:39:03  iter: 29599  total_loss: 2.816  loss_cls_stage0: 0.2374  loss_box_reg_stage0: 0.2397  loss_cls_stage1: 0.2693  loss_box_reg_stage1: 0.7001  loss_cls_stage2: 0.271  loss_box_reg_stage2: 0.9399  loss_mask: 0.07231  loss_rpn_cls: 0.01179  loss_rpn_loc: 0.04122  time: 0.6230  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 18:19:14] d2.utils.events INFO:  eta: 6:38:43  iter: 29619  total_loss: 2.924  loss_cls_stage0: 0.2513  loss_box_reg_stage0: 0.2396  loss_cls_stage1: 0.2633  loss_box_reg_stage1: 0.7054  loss_cls_stage2: 0.2471  loss_box_reg_stage2: 0.9384  loss_mask: 0.06758  loss_rpn_cls: 0.01592  loss_rpn_loc: 0.05392  time: 0.6230  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 18:19:26] d2.utils.events INFO:  eta: 6:38:29  iter: 29639  total_loss: 2.434  loss_cls_stage0: 0.2286  loss_box_reg_stage0: 0.221  loss_cls_stage1: 0.2316  loss_box_reg_stage1: 0.5725  loss_cls_stage2: 0.2219  loss_box_reg_stage2: 0.7965  loss_mask: 0.06916  loss_rpn_cls: 0.01685  loss_rpn_loc: 0.04588  time: 0.6230  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 18:19:38] d2.utils.events INFO:  eta: 6:38:16  iter: 29659  total_loss: 2.71  loss_cls_stage0: 0.2745  loss_box_reg_stage0: 0.2741  loss_cls_stage1: 0.2884  loss_box_reg_stage1: 0.6628  loss_cls_stage2: 0.2654  loss_box_reg_stage2: 0.8237  loss_mask: 0.07495  loss_rpn_cls: 0.01635  loss_rpn_loc: 0.04345  time: 0.6230  data_time: 0.0047  lr: 0.00016  max_mem: 19679M
[07/29 18:19:51] d2.utils.events INFO:  eta: 6:38:02  iter: 29679  total_loss: 2.881  loss_cls_stage0: 0.3181  loss_box_reg_stage0: 0.2986  loss_cls_stage1: 0.3273  loss_box_reg_stage1: 0.6878  loss_cls_stage2: 0.306  loss_box_reg_stage2: 0.8867  loss_mask: 0.08973  loss_rpn_cls: 0.01991  loss_rpn_loc: 0.05257  time: 0.6230  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 18:20:03] d2.utils.events INFO:  eta: 6:37:47  iter: 29699  total_loss: 2.718  loss_cls_stage0: 0.2376  loss_box_reg_stage0: 0.2421  loss_cls_stage1: 0.2805  loss_box_reg_stage1: 0.6406  loss_cls_stage2: 0.2568  loss_box_reg_stage2: 0.8715  loss_mask: 0.06831  loss_rpn_cls: 0.0153  loss_rpn_loc: 0.04338  time: 0.6230  data_time: 0.0046  lr: 0.00016  max_mem: 19679M
[07/29 18:20:15] d2.utils.events INFO:  eta: 6:36:55  iter: 29719  total_loss: 2.725  loss_cls_stage0: 0.2528  loss_box_reg_stage0: 0.2523  loss_cls_stage1: 0.2781  loss_box_reg_stage1: 0.6364  loss_cls_stage2: 0.2618  loss_box_reg_stage2: 0.8992  loss_mask: 0.0684  loss_rpn_cls: 0.00881  loss_rpn_loc: 0.04234  time: 0.6230  data_time: 0.0049  lr: 0.00016  max_mem: 19679M
[07/29 18:20:28] d2.utils.events INFO:  eta: 6:36:29  iter: 29739  total_loss: 2.699  loss_cls_stage0: 0.2228  loss_box_reg_stage0: 0.2355  loss_cls_stage1: 0.2443  loss_box_reg_stage1: 0.6478  loss_cls_stage2: 0.2415  loss_box_reg_stage2: 0.8633  loss_mask: 0.07398  loss_rpn_cls: 0.01149  loss_rpn_loc: 0.04617  time: 0.6230  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 18:20:40] d2.utils.events INFO:  eta: 6:36:09  iter: 29759  total_loss: 2.902  loss_cls_stage0: 0.2382  loss_box_reg_stage0: 0.257  loss_cls_stage1: 0.2769  loss_box_reg_stage1: 0.7261  loss_cls_stage2: 0.287  loss_box_reg_stage2: 0.9921  loss_mask: 0.07154  loss_rpn_cls: 0.0134  loss_rpn_loc: 0.05504  time: 0.6230  data_time: 0.0050  lr: 0.00016  max_mem: 19679M
[07/29 18:20:53] d2.utils.events INFO:  eta: 6:36:05  iter: 29779  total_loss: 2.595  loss_cls_stage0: 0.2458  loss_box_reg_stage0: 0.2589  loss_cls_stage1: 0.2595  loss_box_reg_stage1: 0.6339  loss_cls_stage2: 0.256  loss_box_reg_stage2: 0.8451  loss_mask: 0.07666  loss_rpn_cls: 0.0219  loss_rpn_loc: 0.07088  time: 0.6230  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 18:21:06] d2.utils.events INFO:  eta: 6:35:50  iter: 29799  total_loss: 2.331  loss_cls_stage0: 0.2084  loss_box_reg_stage0: 0.2099  loss_cls_stage1: 0.2355  loss_box_reg_stage1: 0.5566  loss_cls_stage2: 0.2265  loss_box_reg_stage2: 0.8159  loss_mask: 0.06612  loss_rpn_cls: 0.01109  loss_rpn_loc: 0.04147  time: 0.6230  data_time: 0.0054  lr: 0.00016  max_mem: 19679M
[07/29 18:21:18] d2.utils.events INFO:  eta: 6:35:42  iter: 29819  total_loss: 2.851  loss_cls_stage0: 0.2512  loss_box_reg_stage0: 0.2453  loss_cls_stage1: 0.2737  loss_box_reg_stage1: 0.6676  loss_cls_stage2: 0.3039  loss_box_reg_stage2: 0.8692  loss_mask: 0.07688  loss_rpn_cls: 0.01653  loss_rpn_loc: 0.05282  time: 0.6230  data_time: 0.0048  lr: 0.00016  max_mem: 19679M
[07/29 18:21:31] d2.utils.events INFO:  eta: 6:35:11  iter: 29839  total_loss: 2.706  loss_cls_stage0: 0.2453  loss_box_reg_stage0: 0.247  loss_cls_stage1: 0.2874  loss_box_reg_stage1: 0.6301  loss_cls_stage2: 0.2679  loss_box_reg_stage2: 0.8734  loss_mask: 0.0659  loss_rpn_cls: 0.01305  loss_rpn_loc: 0.0443  time: 0.6230  data_time: 0.0051  lr: 0.00016  max_mem: 19679M
[07/29 18:21:44] d2.utils.events INFO:  eta: 6:35:09  iter: 29859  total_loss: 2.835  loss_cls_stage0: 0.2737  loss_box_reg_stage0: 0.2553  loss_cls_stage1: 0.2963  loss_box_reg_stage1: 0.6487  loss_cls_stage2: 0.2694  loss_box_reg_stage2: 0.9158  loss_mask: 0.07131  loss_rpn_cls: 0.009215  loss_rpn_loc: 0.03757  time: 0.6230  data_time: 0.0054  lr: 0.00016  max_mem: 19679M
[07/29 18:21:56] d2.utils.events INFO:  eta: 6:35:05  iter: 29879  total_loss: 2.765  loss_cls_stage0: 0.2309  loss_box_reg_stage0: 0.2461  loss_cls_stage1: 0.2712  loss_box_reg_stage1: 0.6746  loss_cls_stage2: 0.2844  loss_box_reg_stage2: 0.9239  loss_mask: 0.06492  loss_rpn_cls: 0.0105  loss_rpn_loc: 0.03917  time: 0.6230  data_time: 0.0050  lr: 0.00016  max_mem: 20016M
[07/29 18:22:09] d2.utils.events INFO:  eta: 6:34:54  iter: 29899  total_loss: 2.937  loss_cls_stage0: 0.28  loss_box_reg_stage0: 0.2778  loss_cls_stage1: 0.3209  loss_box_reg_stage1: 0.715  loss_cls_stage2: 0.3126  loss_box_reg_stage2: 0.9853  loss_mask: 0.07695  loss_rpn_cls: 0.01524  loss_rpn_loc: 0.05839  time: 0.6230  data_time: 0.0049  lr: 0.00016  max_mem: 20016M
[07/29 18:22:21] d2.utils.events INFO:  eta: 6:35:13  iter: 29919  total_loss: 2.896  loss_cls_stage0: 0.2383  loss_box_reg_stage0: 0.2602  loss_cls_stage1: 0.2832  loss_box_reg_stage1: 0.7152  loss_cls_stage2: 0.304  loss_box_reg_stage2: 0.9628  loss_mask: 0.07528  loss_rpn_cls: 0.01482  loss_rpn_loc: 0.05785  time: 0.6230  data_time: 0.0048  lr: 0.00016  max_mem: 20016M
[07/29 18:22:34] d2.utils.events INFO:  eta: 6:34:46  iter: 29939  total_loss: 3.062  loss_cls_stage0: 0.2645  loss_box_reg_stage0: 0.2684  loss_cls_stage1: 0.2982  loss_box_reg_stage1: 0.7485  loss_cls_stage2: 0.3007  loss_box_reg_stage2: 1.069  loss_mask: 0.07057  loss_rpn_cls: 0.012  loss_rpn_loc: 0.06324  time: 0.6230  data_time: 0.0048  lr: 0.00016  max_mem: 20016M
[07/29 18:22:46] d2.utils.events INFO:  eta: 6:34:47  iter: 29959  total_loss: 2.505  loss_cls_stage0: 0.2247  loss_box_reg_stage0: 0.2334  loss_cls_stage1: 0.251  loss_box_reg_stage1: 0.6005  loss_cls_stage2: 0.2241  loss_box_reg_stage2: 0.7717  loss_mask: 0.06667  loss_rpn_cls: 0.01624  loss_rpn_loc: 0.05407  time: 0.6230  data_time: 0.0050  lr: 0.00016  max_mem: 20016M
[07/29 18:22:58] d2.utils.events INFO:  eta: 6:34:21  iter: 29979  total_loss: 2.896  loss_cls_stage0: 0.2504  loss_box_reg_stage0: 0.2665  loss_cls_stage1: 0.2871  loss_box_reg_stage1: 0.7091  loss_cls_stage2: 0.2772  loss_box_reg_stage2: 0.9169  loss_mask: 0.07436  loss_rpn_cls: 0.01525  loss_rpn_loc: 0.04228  time: 0.6230  data_time: 0.0049  lr: 0.00016  max_mem: 20016M
[07/29 18:23:11] fvcore.common.checkpoint INFO: Saving checkpoint to ./output/model_0029999.pth
[07/29 18:23:13] d2.data.datasets.coco INFO: Loading datasets/tless/tless_annotations_test.json takes 1.41 seconds.
[07/29 18:23:13] d2.data.datasets.coco INFO: Loaded 10080 images in COCO format from datasets/tless/tless_annotations_test.json
[07/29 18:23:14] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[07/29 18:23:14] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[07/29 18:23:14] d2.data.common INFO: Serializing 10080 elements to byte tensors and concatenating them all ...
[07/29 18:23:14] d2.data.common INFO: Serialized dataset takes 50.77 MiB
[07/29 18:23:15] d2.evaluation.evaluator INFO: Start inference on 10080 batches
[07/29 18:23:17] d2.evaluation.evaluator INFO: Inference done 11/10080. Dataloading: 0.0007 s/iter. Inference: 0.1014 s/iter. Eval: 0.0015 s/iter. Total: 0.1036 s/iter. ETA=0:17:23
[07/29 18:23:22] d2.evaluation.evaluator INFO: Inference done 59/10080. Dataloading: 0.0009 s/iter. Inference: 0.1019 s/iter. Eval: 0.0020 s/iter. Total: 0.1048 s/iter. ETA=0:17:30
[07/29 18:23:27] d2.evaluation.evaluator INFO: Inference done 108/10080. Dataloading: 0.0009 s/iter. Inference: 0.1016 s/iter. Eval: 0.0018 s/iter. Total: 0.1043 s/iter. ETA=0:17:20
[07/29 18:23:32] d2.evaluation.evaluator INFO: Inference done 157/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0017 s/iter. Total: 0.1041 s/iter. ETA=0:17:13
[07/29 18:23:37] d2.evaluation.evaluator INFO: Inference done 206/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0015 s/iter. Total: 0.1039 s/iter. ETA=0:17:05
[07/29 18:23:42] d2.evaluation.evaluator INFO: Inference done 255/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0015 s/iter. Total: 0.1038 s/iter. ETA=0:16:59
[07/29 18:23:47] d2.evaluation.evaluator INFO: Inference done 304/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0014 s/iter. Total: 0.1037 s/iter. ETA=0:16:54
[07/29 18:23:52] d2.evaluation.evaluator INFO: Inference done 353/10080. Dataloading: 0.0010 s/iter. Inference: 0.1012 s/iter. Eval: 0.0013 s/iter. Total: 0.1035 s/iter. ETA=0:16:46
[07/29 18:23:57] d2.evaluation.evaluator INFO: Inference done 402/10080. Dataloading: 0.0010 s/iter. Inference: 0.1011 s/iter. Eval: 0.0013 s/iter. Total: 0.1034 s/iter. ETA=0:16:40
[07/29 18:24:02] d2.evaluation.evaluator INFO: Inference done 451/10080. Dataloading: 0.0010 s/iter. Inference: 0.1011 s/iter. Eval: 0.0013 s/iter. Total: 0.1034 s/iter. ETA=0:16:35
[07/29 18:24:07] d2.evaluation.evaluator INFO: Inference done 500/10080. Dataloading: 0.0010 s/iter. Inference: 0.1010 s/iter. Eval: 0.0012 s/iter. Total: 0.1033 s/iter. ETA=0:16:29
[07/29 18:24:12] d2.evaluation.evaluator INFO: Inference done 550/10080. Dataloading: 0.0010 s/iter. Inference: 0.1009 s/iter. Eval: 0.0011 s/iter. Total: 0.1031 s/iter. ETA=0:16:22
[07/29 18:24:17] d2.evaluation.evaluator INFO: Inference done 600/10080. Dataloading: 0.0010 s/iter. Inference: 0.1009 s/iter. Eval: 0.0011 s/iter. Total: 0.1029 s/iter. ETA=0:16:15
[07/29 18:24:22] d2.evaluation.evaluator INFO: Inference done 650/10080. Dataloading: 0.0010 s/iter. Inference: 0.1007 s/iter. Eval: 0.0010 s/iter. Total: 0.1028 s/iter. ETA=0:16:08
[07/29 18:24:27] d2.evaluation.evaluator INFO: Inference done 700/10080. Dataloading: 0.0010 s/iter. Inference: 0.1007 s/iter. Eval: 0.0009 s/iter. Total: 0.1026 s/iter. ETA=0:16:02
[07/29 18:24:33] d2.evaluation.evaluator INFO: Inference done 750/10080. Dataloading: 0.0010 s/iter. Inference: 0.1006 s/iter. Eval: 0.0009 s/iter. Total: 0.1025 s/iter. ETA=0:15:56
[07/29 18:24:38] d2.evaluation.evaluator INFO: Inference done 800/10080. Dataloading: 0.0010 s/iter. Inference: 0.1006 s/iter. Eval: 0.0008 s/iter. Total: 0.1025 s/iter. ETA=0:15:50
[07/29 18:24:43] d2.evaluation.evaluator INFO: Inference done 850/10080. Dataloading: 0.0010 s/iter. Inference: 0.1006 s/iter. Eval: 0.0008 s/iter. Total: 0.1024 s/iter. ETA=0:15:44
[07/29 18:24:48] d2.evaluation.evaluator INFO: Inference done 900/10080. Dataloading: 0.0010 s/iter. Inference: 0.1005 s/iter. Eval: 0.0008 s/iter. Total: 0.1023 s/iter. ETA=0:15:39
[07/29 18:24:53] d2.evaluation.evaluator INFO: Inference done 950/10080. Dataloading: 0.0010 s/iter. Inference: 0.1005 s/iter. Eval: 0.0008 s/iter. Total: 0.1023 s/iter. ETA=0:15:33
[07/29 18:24:58] d2.evaluation.evaluator INFO: Inference done 999/10080. Dataloading: 0.0010 s/iter. Inference: 0.1005 s/iter. Eval: 0.0008 s/iter. Total: 0.1023 s/iter. ETA=0:15:28
[07/29 18:25:03] d2.evaluation.evaluator INFO: Inference done 1048/10080. Dataloading: 0.0010 s/iter. Inference: 0.1005 s/iter. Eval: 0.0007 s/iter. Total: 0.1023 s/iter. ETA=0:15:24
[07/29 18:25:08] d2.evaluation.evaluator INFO: Inference done 1097/10080. Dataloading: 0.0010 s/iter. Inference: 0.1006 s/iter. Eval: 0.0007 s/iter. Total: 0.1023 s/iter. ETA=0:15:19
[07/29 18:25:13] d2.evaluation.evaluator INFO: Inference done 1147/10080. Dataloading: 0.0010 s/iter. Inference: 0.1005 s/iter. Eval: 0.0007 s/iter. Total: 0.1023 s/iter. ETA=0:15:14
[07/29 18:25:18] d2.evaluation.evaluator INFO: Inference done 1197/10080. Dataloading: 0.0010 s/iter. Inference: 0.1005 s/iter. Eval: 0.0008 s/iter. Total: 0.1023 s/iter. ETA=0:15:08
[07/29 18:25:23] d2.evaluation.evaluator INFO: Inference done 1246/10080. Dataloading: 0.0010 s/iter. Inference: 0.1006 s/iter. Eval: 0.0008 s/iter. Total: 0.1023 s/iter. ETA=0:15:03
[07/29 18:25:28] d2.evaluation.evaluator INFO: Inference done 1295/10080. Dataloading: 0.0010 s/iter. Inference: 0.1006 s/iter. Eval: 0.0008 s/iter. Total: 0.1024 s/iter. ETA=0:14:59
[07/29 18:25:33] d2.evaluation.evaluator INFO: Inference done 1344/10080. Dataloading: 0.0010 s/iter. Inference: 0.1006 s/iter. Eval: 0.0008 s/iter. Total: 0.1024 s/iter. ETA=0:14:54
[07/29 18:25:38] d2.evaluation.evaluator INFO: Inference done 1393/10080. Dataloading: 0.0010 s/iter. Inference: 0.1006 s/iter. Eval: 0.0008 s/iter. Total: 0.1024 s/iter. ETA=0:14:49
[07/29 18:25:43] d2.evaluation.evaluator INFO: Inference done 1442/10080. Dataloading: 0.0010 s/iter. Inference: 0.1006 s/iter. Eval: 0.0008 s/iter. Total: 0.1024 s/iter. ETA=0:14:44
[07/29 18:25:48] d2.evaluation.evaluator INFO: Inference done 1491/10080. Dataloading: 0.0010 s/iter. Inference: 0.1006 s/iter. Eval: 0.0008 s/iter. Total: 0.1025 s/iter. ETA=0:14:40
[07/29 18:25:53] d2.evaluation.evaluator INFO: Inference done 1540/10080. Dataloading: 0.0010 s/iter. Inference: 0.1007 s/iter. Eval: 0.0008 s/iter. Total: 0.1025 s/iter. ETA=0:14:35
[07/29 18:25:59] d2.evaluation.evaluator INFO: Inference done 1589/10080. Dataloading: 0.0010 s/iter. Inference: 0.1007 s/iter. Eval: 0.0008 s/iter. Total: 0.1025 s/iter. ETA=0:14:30
[07/29 18:26:04] d2.evaluation.evaluator INFO: Inference done 1638/10080. Dataloading: 0.0010 s/iter. Inference: 0.1007 s/iter. Eval: 0.0008 s/iter. Total: 0.1026 s/iter. ETA=0:14:25
[07/29 18:26:09] d2.evaluation.evaluator INFO: Inference done 1687/10080. Dataloading: 0.0010 s/iter. Inference: 0.1007 s/iter. Eval: 0.0009 s/iter. Total: 0.1026 s/iter. ETA=0:14:20
[07/29 18:26:14] d2.evaluation.evaluator INFO: Inference done 1736/10080. Dataloading: 0.0010 s/iter. Inference: 0.1007 s/iter. Eval: 0.0009 s/iter. Total: 0.1026 s/iter. ETA=0:14:16
[07/29 18:26:19] d2.evaluation.evaluator INFO: Inference done 1785/10080. Dataloading: 0.0010 s/iter. Inference: 0.1007 s/iter. Eval: 0.0009 s/iter. Total: 0.1026 s/iter. ETA=0:14:11
[07/29 18:26:24] d2.evaluation.evaluator INFO: Inference done 1834/10080. Dataloading: 0.0010 s/iter. Inference: 0.1007 s/iter. Eval: 0.0009 s/iter. Total: 0.1026 s/iter. ETA=0:14:06
[07/29 18:26:29] d2.evaluation.evaluator INFO: Inference done 1883/10080. Dataloading: 0.0010 s/iter. Inference: 0.1008 s/iter. Eval: 0.0009 s/iter. Total: 0.1026 s/iter. ETA=0:14:01
[07/29 18:26:34] d2.evaluation.evaluator INFO: Inference done 1932/10080. Dataloading: 0.0010 s/iter. Inference: 0.1008 s/iter. Eval: 0.0009 s/iter. Total: 0.1027 s/iter. ETA=0:13:56
[07/29 18:26:39] d2.evaluation.evaluator INFO: Inference done 1982/10080. Dataloading: 0.0010 s/iter. Inference: 0.1007 s/iter. Eval: 0.0009 s/iter. Total: 0.1026 s/iter. ETA=0:13:51
[07/29 18:26:44] d2.evaluation.evaluator INFO: Inference done 2031/10080. Dataloading: 0.0010 s/iter. Inference: 0.1008 s/iter. Eval: 0.0009 s/iter. Total: 0.1027 s/iter. ETA=0:13:46
[07/29 18:26:49] d2.evaluation.evaluator INFO: Inference done 2079/10080. Dataloading: 0.0010 s/iter. Inference: 0.1008 s/iter. Eval: 0.0009 s/iter. Total: 0.1027 s/iter. ETA=0:13:41
[07/29 18:26:54] d2.evaluation.evaluator INFO: Inference done 2127/10080. Dataloading: 0.0010 s/iter. Inference: 0.1008 s/iter. Eval: 0.0009 s/iter. Total: 0.1028 s/iter. ETA=0:13:37
[07/29 18:26:59] d2.evaluation.evaluator INFO: Inference done 2175/10080. Dataloading: 0.0010 s/iter. Inference: 0.1008 s/iter. Eval: 0.0010 s/iter. Total: 0.1028 s/iter. ETA=0:13:32
[07/29 18:27:04] d2.evaluation.evaluator INFO: Inference done 2223/10080. Dataloading: 0.0010 s/iter. Inference: 0.1009 s/iter. Eval: 0.0010 s/iter. Total: 0.1028 s/iter. ETA=0:13:28
[07/29 18:27:09] d2.evaluation.evaluator INFO: Inference done 2272/10080. Dataloading: 0.0010 s/iter. Inference: 0.1009 s/iter. Eval: 0.0010 s/iter. Total: 0.1029 s/iter. ETA=0:13:23
[07/29 18:27:14] d2.evaluation.evaluator INFO: Inference done 2321/10080. Dataloading: 0.0010 s/iter. Inference: 0.1009 s/iter. Eval: 0.0010 s/iter. Total: 0.1029 s/iter. ETA=0:13:18
[07/29 18:27:20] d2.evaluation.evaluator INFO: Inference done 2370/10080. Dataloading: 0.0010 s/iter. Inference: 0.1009 s/iter. Eval: 0.0010 s/iter. Total: 0.1029 s/iter. ETA=0:13:13
[07/29 18:27:25] d2.evaluation.evaluator INFO: Inference done 2418/10080. Dataloading: 0.0010 s/iter. Inference: 0.1009 s/iter. Eval: 0.0010 s/iter. Total: 0.1029 s/iter. ETA=0:13:08
[07/29 18:27:30] d2.evaluation.evaluator INFO: Inference done 2467/10080. Dataloading: 0.0010 s/iter. Inference: 0.1009 s/iter. Eval: 0.0010 s/iter. Total: 0.1029 s/iter. ETA=0:13:03
[07/29 18:27:35] d2.evaluation.evaluator INFO: Inference done 2516/10080. Dataloading: 0.0010 s/iter. Inference: 0.1009 s/iter. Eval: 0.0010 s/iter. Total: 0.1029 s/iter. ETA=0:12:58
[07/29 18:27:40] d2.evaluation.evaluator INFO: Inference done 2565/10080. Dataloading: 0.0010 s/iter. Inference: 0.1009 s/iter. Eval: 0.0010 s/iter. Total: 0.1029 s/iter. ETA=0:12:53
[07/29 18:27:45] d2.evaluation.evaluator INFO: Inference done 2614/10080. Dataloading: 0.0010 s/iter. Inference: 0.1009 s/iter. Eval: 0.0010 s/iter. Total: 0.1029 s/iter. ETA=0:12:48
[07/29 18:27:50] d2.evaluation.evaluator INFO: Inference done 2663/10080. Dataloading: 0.0010 s/iter. Inference: 0.1009 s/iter. Eval: 0.0010 s/iter. Total: 0.1029 s/iter. ETA=0:12:43
[07/29 18:27:55] d2.evaluation.evaluator INFO: Inference done 2712/10080. Dataloading: 0.0010 s/iter. Inference: 0.1008 s/iter. Eval: 0.0011 s/iter. Total: 0.1029 s/iter. ETA=0:12:38
[07/29 18:28:00] d2.evaluation.evaluator INFO: Inference done 2761/10080. Dataloading: 0.0010 s/iter. Inference: 0.1008 s/iter. Eval: 0.0011 s/iter. Total: 0.1029 s/iter. ETA=0:12:33
[07/29 18:28:05] d2.evaluation.evaluator INFO: Inference done 2810/10080. Dataloading: 0.0010 s/iter. Inference: 0.1008 s/iter. Eval: 0.0011 s/iter. Total: 0.1029 s/iter. ETA=0:12:28
[07/29 18:28:10] d2.evaluation.evaluator INFO: Inference done 2859/10080. Dataloading: 0.0010 s/iter. Inference: 0.1008 s/iter. Eval: 0.0011 s/iter. Total: 0.1029 s/iter. ETA=0:12:23
[07/29 18:28:15] d2.evaluation.evaluator INFO: Inference done 2908/10080. Dataloading: 0.0010 s/iter. Inference: 0.1008 s/iter. Eval: 0.0011 s/iter. Total: 0.1029 s/iter. ETA=0:12:18
[07/29 18:28:20] d2.evaluation.evaluator INFO: Inference done 2957/10080. Dataloading: 0.0010 s/iter. Inference: 0.1008 s/iter. Eval: 0.0011 s/iter. Total: 0.1029 s/iter. ETA=0:12:13
[07/29 18:28:25] d2.evaluation.evaluator INFO: Inference done 3006/10080. Dataloading: 0.0010 s/iter. Inference: 0.1008 s/iter. Eval: 0.0011 s/iter. Total: 0.1030 s/iter. ETA=0:12:08
[07/29 18:28:30] d2.evaluation.evaluator INFO: Inference done 3054/10080. Dataloading: 0.0010 s/iter. Inference: 0.1009 s/iter. Eval: 0.0011 s/iter. Total: 0.1030 s/iter. ETA=0:12:03
[07/29 18:28:35] d2.evaluation.evaluator INFO: Inference done 3101/10080. Dataloading: 0.0010 s/iter. Inference: 0.1009 s/iter. Eval: 0.0011 s/iter. Total: 0.1031 s/iter. ETA=0:11:59
[07/29 18:28:40] d2.evaluation.evaluator INFO: Inference done 3148/10080. Dataloading: 0.0010 s/iter. Inference: 0.1009 s/iter. Eval: 0.0012 s/iter. Total: 0.1031 s/iter. ETA=0:11:54
[07/29 18:28:45] d2.evaluation.evaluator INFO: Inference done 3196/10080. Dataloading: 0.0010 s/iter. Inference: 0.1009 s/iter. Eval: 0.0012 s/iter. Total: 0.1032 s/iter. ETA=0:11:50
[07/29 18:28:50] d2.evaluation.evaluator INFO: Inference done 3243/10080. Dataloading: 0.0010 s/iter. Inference: 0.1009 s/iter. Eval: 0.0013 s/iter. Total: 0.1032 s/iter. ETA=0:11:45
[07/29 18:28:55] d2.evaluation.evaluator INFO: Inference done 3291/10080. Dataloading: 0.0010 s/iter. Inference: 0.1009 s/iter. Eval: 0.0013 s/iter. Total: 0.1032 s/iter. ETA=0:11:40
[07/29 18:29:00] d2.evaluation.evaluator INFO: Inference done 3339/10080. Dataloading: 0.0010 s/iter. Inference: 0.1010 s/iter. Eval: 0.0013 s/iter. Total: 0.1033 s/iter. ETA=0:11:36
[07/29 18:29:06] d2.evaluation.evaluator INFO: Inference done 3387/10080. Dataloading: 0.0010 s/iter. Inference: 0.1010 s/iter. Eval: 0.0013 s/iter. Total: 0.1033 s/iter. ETA=0:11:31
[07/29 18:29:11] d2.evaluation.evaluator INFO: Inference done 3435/10080. Dataloading: 0.0010 s/iter. Inference: 0.1010 s/iter. Eval: 0.0013 s/iter. Total: 0.1033 s/iter. ETA=0:11:26
[07/29 18:29:16] d2.evaluation.evaluator INFO: Inference done 3483/10080. Dataloading: 0.0010 s/iter. Inference: 0.1010 s/iter. Eval: 0.0013 s/iter. Total: 0.1033 s/iter. ETA=0:11:21
[07/29 18:29:21] d2.evaluation.evaluator INFO: Inference done 3532/10080. Dataloading: 0.0010 s/iter. Inference: 0.1010 s/iter. Eval: 0.0013 s/iter. Total: 0.1034 s/iter. ETA=0:11:16
[07/29 18:29:26] d2.evaluation.evaluator INFO: Inference done 3580/10080. Dataloading: 0.0010 s/iter. Inference: 0.1010 s/iter. Eval: 0.0013 s/iter. Total: 0.1034 s/iter. ETA=0:11:11
[07/29 18:29:31] d2.evaluation.evaluator INFO: Inference done 3628/10080. Dataloading: 0.0010 s/iter. Inference: 0.1010 s/iter. Eval: 0.0014 s/iter. Total: 0.1034 s/iter. ETA=0:11:07
[07/29 18:29:36] d2.evaluation.evaluator INFO: Inference done 3676/10080. Dataloading: 0.0010 s/iter. Inference: 0.1010 s/iter. Eval: 0.0014 s/iter. Total: 0.1034 s/iter. ETA=0:11:02
[07/29 18:29:41] d2.evaluation.evaluator INFO: Inference done 3724/10080. Dataloading: 0.0010 s/iter. Inference: 0.1010 s/iter. Eval: 0.0014 s/iter. Total: 0.1035 s/iter. ETA=0:10:57
[07/29 18:29:46] d2.evaluation.evaluator INFO: Inference done 3772/10080. Dataloading: 0.0010 s/iter. Inference: 0.1010 s/iter. Eval: 0.0014 s/iter. Total: 0.1035 s/iter. ETA=0:10:52
[07/29 18:29:51] d2.evaluation.evaluator INFO: Inference done 3820/10080. Dataloading: 0.0010 s/iter. Inference: 0.1010 s/iter. Eval: 0.0015 s/iter. Total: 0.1035 s/iter. ETA=0:10:47
[07/29 18:29:56] d2.evaluation.evaluator INFO: Inference done 3868/10080. Dataloading: 0.0010 s/iter. Inference: 0.1010 s/iter. Eval: 0.0015 s/iter. Total: 0.1035 s/iter. ETA=0:10:43
[07/29 18:30:01] d2.evaluation.evaluator INFO: Inference done 3915/10080. Dataloading: 0.0010 s/iter. Inference: 0.1010 s/iter. Eval: 0.0015 s/iter. Total: 0.1036 s/iter. ETA=0:10:38
[07/29 18:30:06] d2.evaluation.evaluator INFO: Inference done 3962/10080. Dataloading: 0.0010 s/iter. Inference: 0.1010 s/iter. Eval: 0.0015 s/iter. Total: 0.1036 s/iter. ETA=0:10:33
[07/29 18:30:11] d2.evaluation.evaluator INFO: Inference done 4011/10080. Dataloading: 0.0010 s/iter. Inference: 0.1010 s/iter. Eval: 0.0016 s/iter. Total: 0.1036 s/iter. ETA=0:10:28
[07/29 18:30:16] d2.evaluation.evaluator INFO: Inference done 4059/10080. Dataloading: 0.0010 s/iter. Inference: 0.1010 s/iter. Eval: 0.0016 s/iter. Total: 0.1036 s/iter. ETA=0:10:23
[07/29 18:30:21] d2.evaluation.evaluator INFO: Inference done 4107/10080. Dataloading: 0.0010 s/iter. Inference: 0.1010 s/iter. Eval: 0.0016 s/iter. Total: 0.1037 s/iter. ETA=0:10:19
[07/29 18:30:26] d2.evaluation.evaluator INFO: Inference done 4155/10080. Dataloading: 0.0010 s/iter. Inference: 0.1010 s/iter. Eval: 0.0017 s/iter. Total: 0.1037 s/iter. ETA=0:10:14
[07/29 18:30:31] d2.evaluation.evaluator INFO: Inference done 4202/10080. Dataloading: 0.0010 s/iter. Inference: 0.1010 s/iter. Eval: 0.0017 s/iter. Total: 0.1037 s/iter. ETA=0:10:09
[07/29 18:30:37] d2.evaluation.evaluator INFO: Inference done 4249/10080. Dataloading: 0.0010 s/iter. Inference: 0.1010 s/iter. Eval: 0.0017 s/iter. Total: 0.1038 s/iter. ETA=0:10:05
[07/29 18:30:42] d2.evaluation.evaluator INFO: Inference done 4296/10080. Dataloading: 0.0010 s/iter. Inference: 0.1010 s/iter. Eval: 0.0017 s/iter. Total: 0.1038 s/iter. ETA=0:10:00
[07/29 18:30:47] d2.evaluation.evaluator INFO: Inference done 4344/10080. Dataloading: 0.0010 s/iter. Inference: 0.1010 s/iter. Eval: 0.0018 s/iter. Total: 0.1038 s/iter. ETA=0:09:55
[07/29 18:30:52] d2.evaluation.evaluator INFO: Inference done 4392/10080. Dataloading: 0.0010 s/iter. Inference: 0.1011 s/iter. Eval: 0.0018 s/iter. Total: 0.1038 s/iter. ETA=0:09:50
[07/29 18:30:57] d2.evaluation.evaluator INFO: Inference done 4441/10080. Dataloading: 0.0010 s/iter. Inference: 0.1011 s/iter. Eval: 0.0017 s/iter. Total: 0.1038 s/iter. ETA=0:09:45
[07/29 18:31:02] d2.evaluation.evaluator INFO: Inference done 4490/10080. Dataloading: 0.0010 s/iter. Inference: 0.1011 s/iter. Eval: 0.0017 s/iter. Total: 0.1038 s/iter. ETA=0:09:40
[07/29 18:31:07] d2.evaluation.evaluator INFO: Inference done 4538/10080. Dataloading: 0.0010 s/iter. Inference: 0.1011 s/iter. Eval: 0.0017 s/iter. Total: 0.1038 s/iter. ETA=0:09:35
[07/29 18:31:12] d2.evaluation.evaluator INFO: Inference done 4585/10080. Dataloading: 0.0010 s/iter. Inference: 0.1011 s/iter. Eval: 0.0018 s/iter. Total: 0.1039 s/iter. ETA=0:09:30
[07/29 18:31:17] d2.evaluation.evaluator INFO: Inference done 4633/10080. Dataloading: 0.0010 s/iter. Inference: 0.1011 s/iter. Eval: 0.0018 s/iter. Total: 0.1039 s/iter. ETA=0:09:25
[07/29 18:31:22] d2.evaluation.evaluator INFO: Inference done 4680/10080. Dataloading: 0.0010 s/iter. Inference: 0.1011 s/iter. Eval: 0.0018 s/iter. Total: 0.1039 s/iter. ETA=0:09:21
[07/29 18:31:27] d2.evaluation.evaluator INFO: Inference done 4728/10080. Dataloading: 0.0010 s/iter. Inference: 0.1011 s/iter. Eval: 0.0018 s/iter. Total: 0.1039 s/iter. ETA=0:09:16
[07/29 18:31:32] d2.evaluation.evaluator INFO: Inference done 4772/10080. Dataloading: 0.0010 s/iter. Inference: 0.1012 s/iter. Eval: 0.0019 s/iter. Total: 0.1040 s/iter. ETA=0:09:12
[07/29 18:31:37] d2.evaluation.evaluator INFO: Inference done 4819/10080. Dataloading: 0.0010 s/iter. Inference: 0.1012 s/iter. Eval: 0.0019 s/iter. Total: 0.1041 s/iter. ETA=0:09:07
[07/29 18:31:42] d2.evaluation.evaluator INFO: Inference done 4866/10080. Dataloading: 0.0010 s/iter. Inference: 0.1012 s/iter. Eval: 0.0019 s/iter. Total: 0.1041 s/iter. ETA=0:09:02
[07/29 18:31:47] d2.evaluation.evaluator INFO: Inference done 4913/10080. Dataloading: 0.0010 s/iter. Inference: 0.1012 s/iter. Eval: 0.0019 s/iter. Total: 0.1041 s/iter. ETA=0:08:58
[07/29 18:31:52] d2.evaluation.evaluator INFO: Inference done 4960/10080. Dataloading: 0.0010 s/iter. Inference: 0.1012 s/iter. Eval: 0.0019 s/iter. Total: 0.1042 s/iter. ETA=0:08:53
[07/29 18:31:57] d2.evaluation.evaluator INFO: Inference done 5008/10080. Dataloading: 0.0010 s/iter. Inference: 0.1012 s/iter. Eval: 0.0019 s/iter. Total: 0.1042 s/iter. ETA=0:08:48
[07/29 18:32:02] d2.evaluation.evaluator INFO: Inference done 5056/10080. Dataloading: 0.0010 s/iter. Inference: 0.1012 s/iter. Eval: 0.0019 s/iter. Total: 0.1042 s/iter. ETA=0:08:43
[07/29 18:32:07] d2.evaluation.evaluator INFO: Inference done 5104/10080. Dataloading: 0.0010 s/iter. Inference: 0.1012 s/iter. Eval: 0.0019 s/iter. Total: 0.1042 s/iter. ETA=0:08:38
[07/29 18:32:12] d2.evaluation.evaluator INFO: Inference done 5152/10080. Dataloading: 0.0010 s/iter. Inference: 0.1012 s/iter. Eval: 0.0019 s/iter. Total: 0.1042 s/iter. ETA=0:08:33
[07/29 18:32:17] d2.evaluation.evaluator INFO: Inference done 5200/10080. Dataloading: 0.0010 s/iter. Inference: 0.1012 s/iter. Eval: 0.0019 s/iter. Total: 0.1042 s/iter. ETA=0:08:28
[07/29 18:32:22] d2.evaluation.evaluator INFO: Inference done 5248/10080. Dataloading: 0.0010 s/iter. Inference: 0.1012 s/iter. Eval: 0.0019 s/iter. Total: 0.1042 s/iter. ETA=0:08:23
[07/29 18:32:28] d2.evaluation.evaluator INFO: Inference done 5296/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0019 s/iter. Total: 0.1042 s/iter. ETA=0:08:18
[07/29 18:32:33] d2.evaluation.evaluator INFO: Inference done 5344/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0019 s/iter. Total: 0.1042 s/iter. ETA=0:08:13
[07/29 18:32:38] d2.evaluation.evaluator INFO: Inference done 5392/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0019 s/iter. Total: 0.1042 s/iter. ETA=0:08:08
[07/29 18:32:43] d2.evaluation.evaluator INFO: Inference done 5440/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0019 s/iter. Total: 0.1042 s/iter. ETA=0:08:03
[07/29 18:32:48] d2.evaluation.evaluator INFO: Inference done 5488/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0019 s/iter. Total: 0.1042 s/iter. ETA=0:07:58
[07/29 18:32:53] d2.evaluation.evaluator INFO: Inference done 5537/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0019 s/iter. Total: 0.1042 s/iter. ETA=0:07:53
[07/29 18:32:58] d2.evaluation.evaluator INFO: Inference done 5585/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0019 s/iter. Total: 0.1042 s/iter. ETA=0:07:48
[07/29 18:33:03] d2.evaluation.evaluator INFO: Inference done 5633/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0019 s/iter. Total: 0.1042 s/iter. ETA=0:07:43
[07/29 18:33:08] d2.evaluation.evaluator INFO: Inference done 5681/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0019 s/iter. Total: 0.1043 s/iter. ETA=0:07:38
[07/29 18:33:13] d2.evaluation.evaluator INFO: Inference done 5728/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0020 s/iter. Total: 0.1043 s/iter. ETA=0:07:33
[07/29 18:33:18] d2.evaluation.evaluator INFO: Inference done 5775/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0020 s/iter. Total: 0.1043 s/iter. ETA=0:07:28
[07/29 18:33:23] d2.evaluation.evaluator INFO: Inference done 5822/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0020 s/iter. Total: 0.1043 s/iter. ETA=0:07:24
[07/29 18:33:28] d2.evaluation.evaluator INFO: Inference done 5870/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0020 s/iter. Total: 0.1043 s/iter. ETA=0:07:19
[07/29 18:33:33] d2.evaluation.evaluator INFO: Inference done 5919/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0020 s/iter. Total: 0.1043 s/iter. ETA=0:07:14
[07/29 18:33:38] d2.evaluation.evaluator INFO: Inference done 5968/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0020 s/iter. Total: 0.1043 s/iter. ETA=0:07:08
[07/29 18:33:43] d2.evaluation.evaluator INFO: Inference done 6017/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0020 s/iter. Total: 0.1043 s/iter. ETA=0:07:03
[07/29 18:33:48] d2.evaluation.evaluator INFO: Inference done 6065/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0020 s/iter. Total: 0.1043 s/iter. ETA=0:06:58
[07/29 18:33:53] d2.evaluation.evaluator INFO: Inference done 6113/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0020 s/iter. Total: 0.1043 s/iter. ETA=0:06:53
[07/29 18:33:58] d2.evaluation.evaluator INFO: Inference done 6161/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0020 s/iter. Total: 0.1043 s/iter. ETA=0:06:48
[07/29 18:34:03] d2.evaluation.evaluator INFO: Inference done 6209/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0020 s/iter. Total: 0.1043 s/iter. ETA=0:06:43
[07/29 18:34:08] d2.evaluation.evaluator INFO: Inference done 6257/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0020 s/iter. Total: 0.1043 s/iter. ETA=0:06:38
[07/29 18:34:14] d2.evaluation.evaluator INFO: Inference done 6305/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0020 s/iter. Total: 0.1043 s/iter. ETA=0:06:33
[07/29 18:34:19] d2.evaluation.evaluator INFO: Inference done 6353/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0020 s/iter. Total: 0.1043 s/iter. ETA=0:06:28
[07/29 18:34:24] d2.evaluation.evaluator INFO: Inference done 6401/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0020 s/iter. Total: 0.1044 s/iter. ETA=0:06:23
[07/29 18:34:29] d2.evaluation.evaluator INFO: Inference done 6449/10080. Dataloading: 0.0010 s/iter. Inference: 0.1013 s/iter. Eval: 0.0020 s/iter. Total: 0.1044 s/iter. ETA=0:06:18
[07/29 18:34:34] d2.evaluation.evaluator INFO: Inference done 6497/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0020 s/iter. Total: 0.1044 s/iter. ETA=0:06:13
[07/29 18:34:39] d2.evaluation.evaluator INFO: Inference done 6545/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0020 s/iter. Total: 0.1044 s/iter. ETA=0:06:08
[07/29 18:34:44] d2.evaluation.evaluator INFO: Inference done 6592/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0020 s/iter. Total: 0.1044 s/iter. ETA=0:06:04
[07/29 18:34:49] d2.evaluation.evaluator INFO: Inference done 6639/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0020 s/iter. Total: 0.1044 s/iter. ETA=0:05:59
[07/29 18:34:54] d2.evaluation.evaluator INFO: Inference done 6686/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0021 s/iter. Total: 0.1044 s/iter. ETA=0:05:54
[07/29 18:34:59] d2.evaluation.evaluator INFO: Inference done 6733/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0021 s/iter. Total: 0.1045 s/iter. ETA=0:05:49
[07/29 18:35:04] d2.evaluation.evaluator INFO: Inference done 6780/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0021 s/iter. Total: 0.1045 s/iter. ETA=0:05:44
[07/29 18:35:09] d2.evaluation.evaluator INFO: Inference done 6826/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0021 s/iter. Total: 0.1045 s/iter. ETA=0:05:40
[07/29 18:35:14] d2.evaluation.evaluator INFO: Inference done 6873/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0021 s/iter. Total: 0.1045 s/iter. ETA=0:05:35
[07/29 18:35:19] d2.evaluation.evaluator INFO: Inference done 6920/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0021 s/iter. Total: 0.1046 s/iter. ETA=0:05:30
[07/29 18:35:24] d2.evaluation.evaluator INFO: Inference done 6967/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:05:25
[07/29 18:35:29] d2.evaluation.evaluator INFO: Inference done 7015/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:05:20
[07/29 18:35:34] d2.evaluation.evaluator INFO: Inference done 7063/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:05:15
[07/29 18:35:39] d2.evaluation.evaluator INFO: Inference done 7111/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:05:10
[07/29 18:35:45] d2.evaluation.evaluator INFO: Inference done 7159/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:05:05
[07/29 18:35:50] d2.evaluation.evaluator INFO: Inference done 7207/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:05:00
[07/29 18:35:55] d2.evaluation.evaluator INFO: Inference done 7255/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:04:55
[07/29 18:36:00] d2.evaluation.evaluator INFO: Inference done 7303/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:04:50
[07/29 18:36:05] d2.evaluation.evaluator INFO: Inference done 7351/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:04:45
[07/29 18:36:10] d2.evaluation.evaluator INFO: Inference done 7400/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:04:40
[07/29 18:36:15] d2.evaluation.evaluator INFO: Inference done 7449/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:04:35
[07/29 18:36:20] d2.evaluation.evaluator INFO: Inference done 7498/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:04:30
[07/29 18:36:25] d2.evaluation.evaluator INFO: Inference done 7547/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:04:24
[07/29 18:36:30] d2.evaluation.evaluator INFO: Inference done 7595/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:04:19
[07/29 18:36:35] d2.evaluation.evaluator INFO: Inference done 7643/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:04:14
[07/29 18:36:40] d2.evaluation.evaluator INFO: Inference done 7690/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:04:10
[07/29 18:36:45] d2.evaluation.evaluator INFO: Inference done 7738/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:04:05
[07/29 18:36:50] d2.evaluation.evaluator INFO: Inference done 7786/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:04:00
[07/29 18:36:55] d2.evaluation.evaluator INFO: Inference done 7833/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:03:55
[07/29 18:37:00] d2.evaluation.evaluator INFO: Inference done 7881/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1046 s/iter. ETA=0:03:50
[07/29 18:37:05] d2.evaluation.evaluator INFO: Inference done 7929/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1047 s/iter. ETA=0:03:45
[07/29 18:37:11] d2.evaluation.evaluator INFO: Inference done 7977/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1047 s/iter. ETA=0:03:40
[07/29 18:37:16] d2.evaluation.evaluator INFO: Inference done 8025/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1047 s/iter. ETA=0:03:35
[07/29 18:37:21] d2.evaluation.evaluator INFO: Inference done 8073/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1047 s/iter. ETA=0:03:30
[07/29 18:37:26] d2.evaluation.evaluator INFO: Inference done 8120/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1047 s/iter. ETA=0:03:25
[07/29 18:37:31] d2.evaluation.evaluator INFO: Inference done 8167/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0022 s/iter. Total: 0.1047 s/iter. ETA=0:03:20
[07/29 18:37:36] d2.evaluation.evaluator INFO: Inference done 8214/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0023 s/iter. Total: 0.1047 s/iter. ETA=0:03:15
[07/29 18:37:41] d2.evaluation.evaluator INFO: Inference done 8261/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0023 s/iter. Total: 0.1047 s/iter. ETA=0:03:10
[07/29 18:37:46] d2.evaluation.evaluator INFO: Inference done 8308/10080. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0023 s/iter. Total: 0.1048 s/iter. ETA=0:03:05
[07/29 18:37:51] d2.evaluation.evaluator INFO: Inference done 8355/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0023 s/iter. Total: 0.1048 s/iter. ETA=0:03:00
[07/29 18:37:56] d2.evaluation.evaluator INFO: Inference done 8402/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0023 s/iter. Total: 0.1048 s/iter. ETA=0:02:55
[07/29 18:38:01] d2.evaluation.evaluator INFO: Inference done 8449/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0023 s/iter. Total: 0.1048 s/iter. ETA=0:02:50
[07/29 18:38:06] d2.evaluation.evaluator INFO: Inference done 8496/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0023 s/iter. Total: 0.1048 s/iter. ETA=0:02:46
[07/29 18:38:11] d2.evaluation.evaluator INFO: Inference done 8544/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0023 s/iter. Total: 0.1048 s/iter. ETA=0:02:40
[07/29 18:38:16] d2.evaluation.evaluator INFO: Inference done 8591/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0023 s/iter. Total: 0.1048 s/iter. ETA=0:02:36
[07/29 18:38:21] d2.evaluation.evaluator INFO: Inference done 8637/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0023 s/iter. Total: 0.1048 s/iter. ETA=0:02:31
[07/29 18:38:26] d2.evaluation.evaluator INFO: Inference done 8683/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0024 s/iter. Total: 0.1049 s/iter. ETA=0:02:26
[07/29 18:38:31] d2.evaluation.evaluator INFO: Inference done 8729/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0024 s/iter. Total: 0.1049 s/iter. ETA=0:02:21
[07/29 18:38:36] d2.evaluation.evaluator INFO: Inference done 8776/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0024 s/iter. Total: 0.1049 s/iter. ETA=0:02:16
[07/29 18:38:41] d2.evaluation.evaluator INFO: Inference done 8823/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0024 s/iter. Total: 0.1049 s/iter. ETA=0:02:11
[07/29 18:38:46] d2.evaluation.evaluator INFO: Inference done 8869/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0024 s/iter. Total: 0.1050 s/iter. ETA=0:02:07
[07/29 18:38:52] d2.evaluation.evaluator INFO: Inference done 8916/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0024 s/iter. Total: 0.1050 s/iter. ETA=0:02:02
[07/29 18:38:57] d2.evaluation.evaluator INFO: Inference done 8963/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0024 s/iter. Total: 0.1050 s/iter. ETA=0:01:57
[07/29 18:39:02] d2.evaluation.evaluator INFO: Inference done 9009/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0025 s/iter. Total: 0.1050 s/iter. ETA=0:01:52
[07/29 18:39:07] d2.evaluation.evaluator INFO: Inference done 9057/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0024 s/iter. Total: 0.1050 s/iter. ETA=0:01:47
[07/29 18:39:12] d2.evaluation.evaluator INFO: Inference done 9104/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0025 s/iter. Total: 0.1050 s/iter. ETA=0:01:42
[07/29 18:39:17] d2.evaluation.evaluator INFO: Inference done 9151/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0025 s/iter. Total: 0.1050 s/iter. ETA=0:01:37
[07/29 18:39:22] d2.evaluation.evaluator INFO: Inference done 9199/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0025 s/iter. Total: 0.1050 s/iter. ETA=0:01:32
[07/29 18:39:27] d2.evaluation.evaluator INFO: Inference done 9247/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0025 s/iter. Total: 0.1050 s/iter. ETA=0:01:27
[07/29 18:39:32] d2.evaluation.evaluator INFO: Inference done 9295/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0025 s/iter. Total: 0.1050 s/iter. ETA=0:01:22
[07/29 18:39:37] d2.evaluation.evaluator INFO: Inference done 9342/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0025 s/iter. Total: 0.1050 s/iter. ETA=0:01:17
[07/29 18:39:42] d2.evaluation.evaluator INFO: Inference done 9390/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0025 s/iter. Total: 0.1050 s/iter. ETA=0:01:12
[07/29 18:39:47] d2.evaluation.evaluator INFO: Inference done 9437/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0025 s/iter. Total: 0.1050 s/iter. ETA=0:01:07
[07/29 18:39:52] d2.evaluation.evaluator INFO: Inference done 9485/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0025 s/iter. Total: 0.1050 s/iter. ETA=0:01:02
[07/29 18:39:57] d2.evaluation.evaluator INFO: Inference done 9534/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0025 s/iter. Total: 0.1050 s/iter. ETA=0:00:57
[07/29 18:40:02] d2.evaluation.evaluator INFO: Inference done 9583/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0025 s/iter. Total: 0.1050 s/iter. ETA=0:00:52
[07/29 18:40:07] d2.evaluation.evaluator INFO: Inference done 9628/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0025 s/iter. Total: 0.1051 s/iter. ETA=0:00:47
[07/29 18:40:12] d2.evaluation.evaluator INFO: Inference done 9673/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0025 s/iter. Total: 0.1051 s/iter. ETA=0:00:42
[07/29 18:40:17] d2.evaluation.evaluator INFO: Inference done 9718/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0026 s/iter. Total: 0.1051 s/iter. ETA=0:00:38
[07/29 18:40:22] d2.evaluation.evaluator INFO: Inference done 9763/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0026 s/iter. Total: 0.1052 s/iter. ETA=0:00:33
[07/29 18:40:27] d2.evaluation.evaluator INFO: Inference done 9808/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0026 s/iter. Total: 0.1052 s/iter. ETA=0:00:28
[07/29 18:40:32] d2.evaluation.evaluator INFO: Inference done 9853/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0026 s/iter. Total: 0.1052 s/iter. ETA=0:00:23
[07/29 18:40:37] d2.evaluation.evaluator INFO: Inference done 9899/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0027 s/iter. Total: 0.1052 s/iter. ETA=0:00:19
[07/29 18:40:43] d2.evaluation.evaluator INFO: Inference done 9945/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0027 s/iter. Total: 0.1053 s/iter. ETA=0:00:14
[07/29 18:40:48] d2.evaluation.evaluator INFO: Inference done 9992/10080. Dataloading: 0.0010 s/iter. Inference: 0.1016 s/iter. Eval: 0.0027 s/iter. Total: 0.1053 s/iter. ETA=0:00:09
[07/29 18:40:53] d2.evaluation.evaluator INFO: Inference done 10040/10080. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0027 s/iter. Total: 0.1053 s/iter. ETA=0:00:04
[07/29 18:40:57] d2.evaluation.evaluator INFO: Total inference time: 0:17:40.786810 (0.105289 s / iter per device, on 1 devices)
[07/29 18:40:57] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:17:03 (0.101546 s / iter per device, on 1 devices)
[07/29 18:40:57] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[07/29 18:40:57] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[07/29 18:40:57] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[07/29 18:41:01] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 4.46 seconds.
[07/29 18:41:02] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[07/29 18:41:02] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.28 seconds.
[07/29 18:41:02] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |
|:------:|:------:|:------:|:-----:|:------:|:------:|
| 23.965 | 32.917 | 26.395 | 2.460 | 23.661 | 21.029 |
[07/29 18:41:02] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP     | category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|:-----------|:-------|
| 1          | 31.083 | 2          | 13.758 | 3          | 20.371 |
| 4          | 6.276  | 5          | 0.481  | 6          | 0.918  |
| 7          | 0.027  | 8          | 0.778  | 9          | 52.372 |
| 10         | 63.302 | 11         | 42.554 | 12         | 39.982 |
| 13         | 18.137 | 14         | 12.779 | 15         | 27.136 |
| 16         | 34.593 | 17         | 23.936 | 18         | 40.344 |
| 19         | 24.406 | 20         | 11.035 | 21         | 20.608 |
| 22         | 25.813 | 23         | 28.846 | 24         | 59.826 |
| 25         | 28.875 | 26         | 21.095 | 27         | 1.416  |
| 28         | 53.742 | 29         | 9.604  | 30         | 4.854  |
[07/29 18:41:03] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *segm*
[07/29 18:41:10] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 7.06 seconds.
[07/29 18:41:10] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[07/29 18:41:10] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.30 seconds.
[07/29 18:41:10] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:------:|:------:|:------:|:-----:|:-----:|:-----:|
| 10.857 | 17.895 | 11.699 | 0.127 | 9.999 | 7.934 |
[07/29 18:41:11] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|:-----------|:-------|
| 1          | 20.324 | 2          | 10.083 | 3          | 14.112 |
| 4          | 1.518  | 5          | 0.042  | 6          | 0.008  |
| 7          | 0.000  | 8          | 0.000  | 9          | 13.108 |
| 10         | 37.413 | 11         | 31.560 | 12         | 25.733 |
| 13         | 12.065 | 14         | 7.143  | 15         | 23.025 |
| 16         | 23.087 | 17         | 2.439  | 18         | 11.260 |
| 19         | 2.364  | 20         | 0.586  | 21         | 4.729  |
| 22         | 17.636 | 23         | 5.119  | 24         | 39.014 |
| 25         | 10.766 | 26         | 5.744  | 27         | 0.003  |
| 28         | 5.809  | 29         | 0.558  | 30         | 0.471  |
[07/29 18:41:11] d2.evaluation.testing INFO: copypaste: Task: bbox
[07/29 18:41:11] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/29 18:41:11] d2.evaluation.testing INFO: copypaste: 23.9650,32.9171,26.3948,2.4597,23.6612,21.0291
[07/29 18:41:11] d2.evaluation.testing INFO: copypaste: Task: segm
[07/29 18:41:11] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/29 18:41:11] d2.evaluation.testing INFO: copypaste: 10.8573,17.8955,11.6986,0.1272,9.9991,7.9344
[07/29 18:41:11] d2.utils.events INFO:  eta: 6:34:14  iter: 29999  total_loss: 2.547  loss_cls_stage0: 0.2641  loss_box_reg_stage0: 0.2653  loss_cls_stage1: 0.2613  loss_box_reg_stage1: 0.616  loss_cls_stage2: 0.2506  loss_box_reg_stage2: 0.7582  loss_mask: 0.07454  loss_rpn_cls: 0.015  loss_rpn_loc: 0.04499  time: 0.6230  data_time: 0.0047  lr: 0.00016  max_mem: 20016M
[07/29 18:41:23] d2.utils.events INFO:  eta: 6:33:28  iter: 30019  total_loss: 2.592  loss_cls_stage0: 0.2687  loss_box_reg_stage0: 0.2316  loss_cls_stage1: 0.2773  loss_box_reg_stage1: 0.6085  loss_cls_stage2: 0.2515  loss_box_reg_stage2: 0.8013  loss_mask: 0.07071  loss_rpn_cls: 0.014  loss_rpn_loc: 0.04928  time: 0.6230  data_time: 0.0048  lr: 0.00016  max_mem: 20016M
[07/29 18:41:35] d2.utils.events INFO:  eta: 6:33:21  iter: 30039  total_loss: 2.684  loss_cls_stage0: 0.2277  loss_box_reg_stage0: 0.2543  loss_cls_stage1: 0.2456  loss_box_reg_stage1: 0.6422  loss_cls_stage2: 0.2534  loss_box_reg_stage2: 0.9294  loss_mask: 0.0708  loss_rpn_cls: 0.0184  loss_rpn_loc: 0.05264  time: 0.6230  data_time: 0.0048  lr: 0.00016  max_mem: 20016M
[07/29 18:41:48] d2.utils.events INFO:  eta: 6:33:20  iter: 30059  total_loss: 2.812  loss_cls_stage0: 0.2332  loss_box_reg_stage0: 0.2376  loss_cls_stage1: 0.233  loss_box_reg_stage1: 0.6205  loss_cls_stage2: 0.212  loss_box_reg_stage2: 0.8361  loss_mask: 0.06879  loss_rpn_cls: 0.0181  loss_rpn_loc: 0.04203  time: 0.6230  data_time: 0.0049  lr: 0.00016  max_mem: 20016M
[07/29 18:42:00] d2.utils.events INFO:  eta: 6:32:56  iter: 30079  total_loss: 2.676  loss_cls_stage0: 0.233  loss_box_reg_stage0: 0.2613  loss_cls_stage1: 0.2513  loss_box_reg_stage1: 0.6345  loss_cls_stage2: 0.2536  loss_box_reg_stage2: 0.8469  loss_mask: 0.07399  loss_rpn_cls: 0.01553  loss_rpn_loc: 0.03837  time: 0.6230  data_time: 0.0048  lr: 0.00016  max_mem: 20016M
[07/29 18:42:12] d2.utils.events INFO:  eta: 6:32:42  iter: 30099  total_loss: 2.71  loss_cls_stage0: 0.2415  loss_box_reg_stage0: 0.2407  loss_cls_stage1: 0.2423  loss_box_reg_stage1: 0.628  loss_cls_stage2: 0.2318  loss_box_reg_stage2: 0.8397  loss_mask: 0.06918  loss_rpn_cls: 0.01306  loss_rpn_loc: 0.0514  time: 0.6229  data_time: 0.0051  lr: 0.00016  max_mem: 20016M
[07/29 18:42:24] d2.utils.events INFO:  eta: 6:32:08  iter: 30119  total_loss: 2.562  loss_cls_stage0: 0.2476  loss_box_reg_stage0: 0.2424  loss_cls_stage1: 0.241  loss_box_reg_stage1: 0.5825  loss_cls_stage2: 0.228  loss_box_reg_stage2: 0.8037  loss_mask: 0.06769  loss_rpn_cls: 0.01653  loss_rpn_loc: 0.04439  time: 0.6229  data_time: 0.0052  lr: 0.00016  max_mem: 20016M
[07/29 18:42:37] d2.utils.events INFO:  eta: 6:31:55  iter: 30139  total_loss: 2.566  loss_cls_stage0: 0.2336  loss_box_reg_stage0: 0.2486  loss_cls_stage1: 0.247  loss_box_reg_stage1: 0.6184  loss_cls_stage2: 0.2349  loss_box_reg_stage2: 0.8169  loss_mask: 0.06895  loss_rpn_cls: 0.01228  loss_rpn_loc: 0.04447  time: 0.6229  data_time: 0.0051  lr: 0.00016  max_mem: 20016M
[07/29 18:42:49] d2.utils.events INFO:  eta: 6:31:23  iter: 30159  total_loss: 2.959  loss_cls_stage0: 0.272  loss_box_reg_stage0: 0.2868  loss_cls_stage1: 0.2647  loss_box_reg_stage1: 0.7092  loss_cls_stage2: 0.2515  loss_box_reg_stage2: 0.911  loss_mask: 0.07728  loss_rpn_cls: 0.01423  loss_rpn_loc: 0.04661  time: 0.6229  data_time: 0.0049  lr: 0.00016  max_mem: 20016M
[07/29 18:43:01] d2.utils.events INFO:  eta: 6:31:08  iter: 30179  total_loss: 2.55  loss_cls_stage0: 0.2539  loss_box_reg_stage0: 0.2535  loss_cls_stage1: 0.2554  loss_box_reg_stage1: 0.5879  loss_cls_stage2: 0.2368  loss_box_reg_stage2: 0.7917  loss_mask: 0.07358  loss_rpn_cls: 0.01244  loss_rpn_loc: 0.05995  time: 0.6229  data_time: 0.0047  lr: 0.00016  max_mem: 20016M
[07/29 18:43:13] d2.utils.events INFO:  eta: 6:30:26  iter: 30199  total_loss: 2.452  loss_cls_stage0: 0.2072  loss_box_reg_stage0: 0.2419  loss_cls_stage1: 0.2311  loss_box_reg_stage1: 0.633  loss_cls_stage2: 0.1992  loss_box_reg_stage2: 0.8206  loss_mask: 0.06327  loss_rpn_cls: 0.01471  loss_rpn_loc: 0.04607  time: 0.6229  data_time: 0.0051  lr: 0.00016  max_mem: 20016M
[07/29 18:43:26] d2.utils.events INFO:  eta: 6:30:18  iter: 30219  total_loss: 2.632  loss_cls_stage0: 0.2335  loss_box_reg_stage0: 0.2321  loss_cls_stage1: 0.2605  loss_box_reg_stage1: 0.629  loss_cls_stage2: 0.2479  loss_box_reg_stage2: 0.8626  loss_mask: 0.0692  loss_rpn_cls: 0.01577  loss_rpn_loc: 0.04715  time: 0.6229  data_time: 0.0047  lr: 0.00016  max_mem: 20016M
[07/29 18:43:38] d2.utils.events INFO:  eta: 6:30:01  iter: 30239  total_loss: 2.751  loss_cls_stage0: 0.2385  loss_box_reg_stage0: 0.2185  loss_cls_stage1: 0.2698  loss_box_reg_stage1: 0.6656  loss_cls_stage2: 0.2581  loss_box_reg_stage2: 0.9435  loss_mask: 0.06377  loss_rpn_cls: 0.01696  loss_rpn_loc: 0.04156  time: 0.6229  data_time: 0.0052  lr: 0.00016  max_mem: 20016M
[07/29 18:43:51] d2.utils.events INFO:  eta: 6:29:45  iter: 30259  total_loss: 2.604  loss_cls_stage0: 0.2373  loss_box_reg_stage0: 0.2228  loss_cls_stage1: 0.2368  loss_box_reg_stage1: 0.6239  loss_cls_stage2: 0.2399  loss_box_reg_stage2: 0.8566  loss_mask: 0.06784  loss_rpn_cls: 0.01019  loss_rpn_loc: 0.0406  time: 0.6229  data_time: 0.0051  lr: 0.00016  max_mem: 20016M
[07/29 18:44:04] d2.utils.events INFO:  eta: 6:29:40  iter: 30279  total_loss: 2.889  loss_cls_stage0: 0.2483  loss_box_reg_stage0: 0.2345  loss_cls_stage1: 0.2924  loss_box_reg_stage1: 0.6419  loss_cls_stage2: 0.2811  loss_box_reg_stage2: 0.943  loss_mask: 0.06778  loss_rpn_cls: 0.007378  loss_rpn_loc: 0.03762  time: 0.6229  data_time: 0.0052  lr: 0.00016  max_mem: 20016M
[07/29 18:44:16] d2.utils.events INFO:  eta: 6:29:17  iter: 30299  total_loss: 2.818  loss_cls_stage0: 0.2507  loss_box_reg_stage0: 0.2366  loss_cls_stage1: 0.2569  loss_box_reg_stage1: 0.6697  loss_cls_stage2: 0.2563  loss_box_reg_stage2: 0.9412  loss_mask: 0.07125  loss_rpn_cls: 0.01409  loss_rpn_loc: 0.04937  time: 0.6229  data_time: 0.0047  lr: 0.00016  max_mem: 20016M
[07/29 18:44:29] d2.utils.events INFO:  eta: 6:28:57  iter: 30319  total_loss: 3.065  loss_cls_stage0: 0.2691  loss_box_reg_stage0: 0.2419  loss_cls_stage1: 0.3025  loss_box_reg_stage1: 0.6797  loss_cls_stage2: 0.3023  loss_box_reg_stage2: 0.998  loss_mask: 0.07617  loss_rpn_cls: 0.01281  loss_rpn_loc: 0.04201  time: 0.6229  data_time: 0.0046  lr: 0.00016  max_mem: 20016M
[07/29 18:44:41] d2.utils.events INFO:  eta: 6:28:06  iter: 30339  total_loss: 2.642  loss_cls_stage0: 0.2452  loss_box_reg_stage0: 0.2431  loss_cls_stage1: 0.2572  loss_box_reg_stage1: 0.6435  loss_cls_stage2: 0.2355  loss_box_reg_stage2: 0.8898  loss_mask: 0.07677  loss_rpn_cls: 0.01487  loss_rpn_loc: 0.03802  time: 0.6229  data_time: 0.0047  lr: 0.00016  max_mem: 20016M
[07/29 18:44:53] d2.utils.events INFO:  eta: 6:27:59  iter: 30359  total_loss: 2.581  loss_cls_stage0: 0.2371  loss_box_reg_stage0: 0.2315  loss_cls_stage1: 0.2563  loss_box_reg_stage1: 0.6138  loss_cls_stage2: 0.2552  loss_box_reg_stage2: 0.8968  loss_mask: 0.06728  loss_rpn_cls: 0.008809  loss_rpn_loc: 0.04051  time: 0.6229  data_time: 0.0049  lr: 0.00016  max_mem: 20016M
[07/29 18:45:05] d2.utils.events INFO:  eta: 6:27:37  iter: 30379  total_loss: 2.968  loss_cls_stage0: 0.272  loss_box_reg_stage0: 0.2646  loss_cls_stage1: 0.3153  loss_box_reg_stage1: 0.7019  loss_cls_stage2: 0.3285  loss_box_reg_stage2: 1.021  loss_mask: 0.06835  loss_rpn_cls: 0.01573  loss_rpn_loc: 0.04299  time: 0.6229  data_time: 0.0050  lr: 0.00016  max_mem: 20016M
[07/29 18:45:18] d2.utils.events INFO:  eta: 6:27:34  iter: 30399  total_loss: 2.869  loss_cls_stage0: 0.2947  loss_box_reg_stage0: 0.2662  loss_cls_stage1: 0.3178  loss_box_reg_stage1: 0.7011  loss_cls_stage2: 0.3103  loss_box_reg_stage2: 0.9147  loss_mask: 0.08133  loss_rpn_cls: 0.01506  loss_rpn_loc: 0.05056  time: 0.6229  data_time: 0.0048  lr: 0.00016  max_mem: 20016M
[07/29 18:45:30] d2.utils.events INFO:  eta: 6:27:16  iter: 30419  total_loss: 2.571  loss_cls_stage0: 0.2251  loss_box_reg_stage0: 0.231  loss_cls_stage1: 0.2624  loss_box_reg_stage1: 0.5894  loss_cls_stage2: 0.246  loss_box_reg_stage2: 0.8353  loss_mask: 0.06707  loss_rpn_cls: 0.01642  loss_rpn_loc: 0.04535  time: 0.6229  data_time: 0.0050  lr: 0.00016  max_mem: 20016M
[07/29 18:45:43] d2.utils.events INFO:  eta: 6:27:13  iter: 30439  total_loss: 2.793  loss_cls_stage0: 0.2513  loss_box_reg_stage0: 0.2604  loss_cls_stage1: 0.2694  loss_box_reg_stage1: 0.6564  loss_cls_stage2: 0.2442  loss_box_reg_stage2: 0.9303  loss_mask: 0.07847  loss_rpn_cls: 0.02253  loss_rpn_loc: 0.05169  time: 0.6229  data_time: 0.0053  lr: 0.00016  max_mem: 20016M
[07/29 18:45:55] d2.utils.events INFO:  eta: 6:27:00  iter: 30459  total_loss: 2.645  loss_cls_stage0: 0.2407  loss_box_reg_stage0: 0.2676  loss_cls_stage1: 0.2458  loss_box_reg_stage1: 0.628  loss_cls_stage2: 0.2228  loss_box_reg_stage2: 0.846  loss_mask: 0.07785  loss_rpn_cls: 0.0184  loss_rpn_loc: 0.06042  time: 0.6229  data_time: 0.0051  lr: 0.00016  max_mem: 20016M
[07/29 18:46:07] d2.utils.events INFO:  eta: 6:26:30  iter: 30479  total_loss: 2.631  loss_cls_stage0: 0.2226  loss_box_reg_stage0: 0.2421  loss_cls_stage1: 0.2139  loss_box_reg_stage1: 0.5965  loss_cls_stage2: 0.2246  loss_box_reg_stage2: 0.8862  loss_mask: 0.06817  loss_rpn_cls: 0.01267  loss_rpn_loc: 0.05036  time: 0.6229  data_time: 0.0050  lr: 0.00016  max_mem: 20016M
[07/29 18:46:20] d2.utils.events INFO:  eta: 6:26:26  iter: 30499  total_loss: 2.685  loss_cls_stage0: 0.228  loss_box_reg_stage0: 0.2381  loss_cls_stage1: 0.2308  loss_box_reg_stage1: 0.6783  loss_cls_stage2: 0.2399  loss_box_reg_stage2: 0.9302  loss_mask: 0.07074  loss_rpn_cls: 0.008885  loss_rpn_loc: 0.0419  time: 0.6229  data_time: 0.0049  lr: 0.00016  max_mem: 20016M
[07/29 18:46:32] d2.utils.events INFO:  eta: 6:26:13  iter: 30519  total_loss: 2.978  loss_cls_stage0: 0.2712  loss_box_reg_stage0: 0.2749  loss_cls_stage1: 0.2823  loss_box_reg_stage1: 0.7005  loss_cls_stage2: 0.2699  loss_box_reg_stage2: 0.9558  loss_mask: 0.0831  loss_rpn_cls: 0.01295  loss_rpn_loc: 0.04734  time: 0.6229  data_time: 0.0051  lr: 0.00016  max_mem: 20016M
[07/29 18:46:45] d2.utils.events INFO:  eta: 6:26:01  iter: 30539  total_loss: 2.612  loss_cls_stage0: 0.2416  loss_box_reg_stage0: 0.2229  loss_cls_stage1: 0.2546  loss_box_reg_stage1: 0.6074  loss_cls_stage2: 0.2465  loss_box_reg_stage2: 0.8717  loss_mask: 0.06702  loss_rpn_cls: 0.0172  loss_rpn_loc: 0.05233  time: 0.6229  data_time: 0.0050  lr: 0.00016  max_mem: 20016M
[07/29 18:46:57] d2.utils.events INFO:  eta: 6:25:22  iter: 30559  total_loss: 2.998  loss_cls_stage0: 0.2765  loss_box_reg_stage0: 0.2565  loss_cls_stage1: 0.3081  loss_box_reg_stage1: 0.7156  loss_cls_stage2: 0.329  loss_box_reg_stage2: 0.9118  loss_mask: 0.07057  loss_rpn_cls: 0.01606  loss_rpn_loc: 0.0458  time: 0.6229  data_time: 0.0049  lr: 0.00016  max_mem: 20016M
[07/29 18:47:09] d2.utils.events INFO:  eta: 6:25:36  iter: 30579  total_loss: 2.862  loss_cls_stage0: 0.2417  loss_box_reg_stage0: 0.2688  loss_cls_stage1: 0.249  loss_box_reg_stage1: 0.6736  loss_cls_stage2: 0.2576  loss_box_reg_stage2: 0.9249  loss_mask: 0.07377  loss_rpn_cls: 0.01243  loss_rpn_loc: 0.05044  time: 0.6229  data_time: 0.0047  lr: 0.00016  max_mem: 20016M
[07/29 18:47:22] d2.utils.events INFO:  eta: 6:25:38  iter: 30599  total_loss: 2.739  loss_cls_stage0: 0.2532  loss_box_reg_stage0: 0.2544  loss_cls_stage1: 0.2499  loss_box_reg_stage1: 0.6719  loss_cls_stage2: 0.2615  loss_box_reg_stage2: 0.9341  loss_mask: 0.07929  loss_rpn_cls: 0.01831  loss_rpn_loc: 0.05755  time: 0.6229  data_time: 0.0051  lr: 0.00016  max_mem: 20016M
[07/29 18:47:34] d2.utils.events INFO:  eta: 6:25:20  iter: 30619  total_loss: 2.596  loss_cls_stage0: 0.2307  loss_box_reg_stage0: 0.2361  loss_cls_stage1: 0.2359  loss_box_reg_stage1: 0.6029  loss_cls_stage2: 0.2407  loss_box_reg_stage2: 0.8403  loss_mask: 0.07023  loss_rpn_cls: 0.01852  loss_rpn_loc: 0.05362  time: 0.6228  data_time: 0.0052  lr: 0.00016  max_mem: 20016M
[07/29 18:47:46] d2.utils.events INFO:  eta: 6:25:14  iter: 30639  total_loss: 2.263  loss_cls_stage0: 0.1811  loss_box_reg_stage0: 0.2022  loss_cls_stage1: 0.1823  loss_box_reg_stage1: 0.5385  loss_cls_stage2: 0.1933  loss_box_reg_stage2: 0.7574  loss_mask: 0.06367  loss_rpn_cls: 0.01519  loss_rpn_loc: 0.04265  time: 0.6228  data_time: 0.0052  lr: 0.00016  max_mem: 20016M
[07/29 18:47:59] d2.utils.events INFO:  eta: 6:25:12  iter: 30659  total_loss: 2.954  loss_cls_stage0: 0.2544  loss_box_reg_stage0: 0.2536  loss_cls_stage1: 0.2662  loss_box_reg_stage1: 0.6958  loss_cls_stage2: 0.2813  loss_box_reg_stage2: 1.002  loss_mask: 0.06619  loss_rpn_cls: 0.009956  loss_rpn_loc: 0.04617  time: 0.6228  data_time: 0.0050  lr: 0.00016  max_mem: 20016M
[07/29 18:48:12] d2.utils.events INFO:  eta: 6:25:13  iter: 30679  total_loss: 2.916  loss_cls_stage0: 0.2586  loss_box_reg_stage0: 0.2402  loss_cls_stage1: 0.2952  loss_box_reg_stage1: 0.679  loss_cls_stage2: 0.2895  loss_box_reg_stage2: 0.9727  loss_mask: 0.07104  loss_rpn_cls: 0.01545  loss_rpn_loc: 0.04479  time: 0.6229  data_time: 0.0051  lr: 0.00016  max_mem: 20016M
[07/29 18:48:24] d2.utils.events INFO:  eta: 6:25:33  iter: 30699  total_loss: 2.389  loss_cls_stage0: 0.1946  loss_box_reg_stage0: 0.2114  loss_cls_stage1: 0.2178  loss_box_reg_stage1: 0.5855  loss_cls_stage2: 0.2285  loss_box_reg_stage2: 0.8167  loss_mask: 0.07172  loss_rpn_cls: 0.0158  loss_rpn_loc: 0.04189  time: 0.6229  data_time: 0.0049  lr: 0.00016  max_mem: 20016M
[07/29 18:48:37] d2.utils.events INFO:  eta: 6:25:30  iter: 30719  total_loss: 2.867  loss_cls_stage0: 0.277  loss_box_reg_stage0: 0.2828  loss_cls_stage1: 0.2758  loss_box_reg_stage1: 0.6576  loss_cls_stage2: 0.2469  loss_box_reg_stage2: 0.9002  loss_mask: 0.07803  loss_rpn_cls: 0.01437  loss_rpn_loc: 0.05882  time: 0.6229  data_time: 0.0047  lr: 0.00016  max_mem: 20016M
[07/29 18:48:50] d2.utils.events INFO:  eta: 6:25:30  iter: 30739  total_loss: 2.422  loss_cls_stage0: 0.2207  loss_box_reg_stage0: 0.2195  loss_cls_stage1: 0.2424  loss_box_reg_stage1: 0.5799  loss_cls_stage2: 0.237  loss_box_reg_stage2: 0.8144  loss_mask: 0.06625  loss_rpn_cls: 0.01965  loss_rpn_loc: 0.03939  time: 0.6229  data_time: 0.0050  lr: 0.00016  max_mem: 20016M
[07/29 18:49:02] d2.utils.events INFO:  eta: 6:25:17  iter: 30759  total_loss: 2.539  loss_cls_stage0: 0.2157  loss_box_reg_stage0: 0.2263  loss_cls_stage1: 0.2547  loss_box_reg_stage1: 0.6003  loss_cls_stage2: 0.2618  loss_box_reg_stage2: 0.8378  loss_mask: 0.06323  loss_rpn_cls: 0.01619  loss_rpn_loc: 0.04371  time: 0.6229  data_time: 0.0050  lr: 0.00016  max_mem: 20016M
[07/29 18:49:15] d2.utils.events INFO:  eta: 6:25:14  iter: 30779  total_loss: 2.86  loss_cls_stage0: 0.2327  loss_box_reg_stage0: 0.2354  loss_cls_stage1: 0.2713  loss_box_reg_stage1: 0.6981  loss_cls_stage2: 0.2741  loss_box_reg_stage2: 0.949  loss_mask: 0.07194  loss_rpn_cls: 0.01432  loss_rpn_loc: 0.04502  time: 0.6229  data_time: 0.0051  lr: 0.00016  max_mem: 20016M
[07/29 18:49:28] d2.utils.events INFO:  eta: 6:25:18  iter: 30799  total_loss: 2.672  loss_cls_stage0: 0.2219  loss_box_reg_stage0: 0.2271  loss_cls_stage1: 0.244  loss_box_reg_stage1: 0.6554  loss_cls_stage2: 0.2628  loss_box_reg_stage2: 0.9049  loss_mask: 0.06417  loss_rpn_cls: 0.007898  loss_rpn_loc: 0.03668  time: 0.6229  data_time: 0.0051  lr: 0.00016  max_mem: 20016M
[07/29 18:49:41] d2.utils.events INFO:  eta: 6:25:29  iter: 30819  total_loss: 2.917  loss_cls_stage0: 0.2448  loss_box_reg_stage0: 0.2621  loss_cls_stage1: 0.273  loss_box_reg_stage1: 0.7095  loss_cls_stage2: 0.2923  loss_box_reg_stage2: 0.9613  loss_mask: 0.07639  loss_rpn_cls: 0.01348  loss_rpn_loc: 0.05307  time: 0.6229  data_time: 0.0050  lr: 0.00016  max_mem: 20016M
[07/29 18:49:53] d2.utils.events INFO:  eta: 6:25:04  iter: 30839  total_loss: 2.254  loss_cls_stage0: 0.1833  loss_box_reg_stage0: 0.2036  loss_cls_stage1: 0.2012  loss_box_reg_stage1: 0.5262  loss_cls_stage2: 0.1938  loss_box_reg_stage2: 0.7957  loss_mask: 0.07527  loss_rpn_cls: 0.01708  loss_rpn_loc: 0.04108  time: 0.6229  data_time: 0.0050  lr: 0.00016  max_mem: 20016M
[07/29 18:50:05] d2.utils.events INFO:  eta: 6:24:40  iter: 30859  total_loss: 2.673  loss_cls_stage0: 0.2464  loss_box_reg_stage0: 0.2417  loss_cls_stage1: 0.2753  loss_box_reg_stage1: 0.6121  loss_cls_stage2: 0.2608  loss_box_reg_stage2: 0.8545  loss_mask: 0.0679  loss_rpn_cls: 0.01072  loss_rpn_loc: 0.05198  time: 0.6229  data_time: 0.0051  lr: 0.00016  max_mem: 20016M
[07/29 18:50:18] d2.utils.events INFO:  eta: 6:24:11  iter: 30879  total_loss: 2.699  loss_cls_stage0: 0.2445  loss_box_reg_stage0: 0.2456  loss_cls_stage1: 0.2618  loss_box_reg_stage1: 0.6232  loss_cls_stage2: 0.2495  loss_box_reg_stage2: 0.835  loss_mask: 0.07216  loss_rpn_cls: 0.02024  loss_rpn_loc: 0.05359  time: 0.6229  data_time: 0.0050  lr: 0.00016  max_mem: 20016M
[07/29 18:50:30] d2.utils.events INFO:  eta: 6:24:04  iter: 30899  total_loss: 2.448  loss_cls_stage0: 0.2265  loss_box_reg_stage0: 0.2372  loss_cls_stage1: 0.2297  loss_box_reg_stage1: 0.5565  loss_cls_stage2: 0.2217  loss_box_reg_stage2: 0.7768  loss_mask: 0.06776  loss_rpn_cls: 0.01931  loss_rpn_loc: 0.04055  time: 0.6229  data_time: 0.0054  lr: 0.00016  max_mem: 20016M
[07/29 18:50:43] d2.utils.events INFO:  eta: 6:23:46  iter: 30919  total_loss: 2.88  loss_cls_stage0: 0.2494  loss_box_reg_stage0: 0.2642  loss_cls_stage1: 0.2567  loss_box_reg_stage1: 0.6758  loss_cls_stage2: 0.2445  loss_box_reg_stage2: 0.8102  loss_mask: 0.07293  loss_rpn_cls: 0.01898  loss_rpn_loc: 0.04984  time: 0.6229  data_time: 0.0047  lr: 0.00016  max_mem: 20016M
[07/29 18:50:55] d2.utils.events INFO:  eta: 6:23:20  iter: 30939  total_loss: 2.684  loss_cls_stage0: 0.226  loss_box_reg_stage0: 0.2573  loss_cls_stage1: 0.2507  loss_box_reg_stage1: 0.6518  loss_cls_stage2: 0.23  loss_box_reg_stage2: 0.8624  loss_mask: 0.06925  loss_rpn_cls: 0.01626  loss_rpn_loc: 0.04009  time: 0.6229  data_time: 0.0047  lr: 0.00016  max_mem: 20016M
[07/29 18:51:01] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 422, in run_step
    loss_dict = self.model(data)
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 167, in forward
    _, detector_losses = self.roi_heads(images, features, proposals, gt_instances)
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/roi_heads/cascade_rcnn.py", line 144, in forward
    losses = self._forward_box(features, proposals, targets)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/roi_heads/cascade_rcnn.py", line 174, in _forward_box
    predictions = self._run_stage(features, proposals, k)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/roi_heads/cascade_rcnn.py", line 268, in _run_stage
    box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/modeling/poolers.py", line 261, in forward
    output.index_put_((inds,), pooler(x[level], pooler_fmt_boxes_level))
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hoenig/.local/lib/python3.8/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/hoenig/.local/lib/python3.8/site-packages/torchvision/ops/roi_align.py", line 61, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/hoenig/.local/lib/python3.8/site-packages/torch/_ops.py", line 143, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: CUDA out of memory. Tried to allocate 352.00 MiB (GPU 0; 23.67 GiB total capacity; 19.27 GiB already allocated; 289.06 MiB free; 20.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[07/29 18:51:01] d2.engine.hooks INFO: Overall training speed: 25947 iterations in 4:29:22 (0.6229 s / it)
[07/29 18:51:01] d2.engine.hooks INFO: Total training time: 6:01:45 (1:32:22 on hooks)
[07/29 18:51:01] d2.utils.events INFO:  eta: 6:23:08  iter: 30949  total_loss: 2.731  loss_cls_stage0: 0.2332  loss_box_reg_stage0: 0.2627  loss_cls_stage1: 0.2355  loss_box_reg_stage1: 0.6525  loss_cls_stage2: 0.2322  loss_box_reg_stage2: 0.8093  loss_mask: 0.06723  loss_rpn_cls: 0.02777  loss_rpn_loc: 0.04696  time: 0.6229  data_time: 0.0050  lr: 0.00016  max_mem: 20016M
[07/29 19:45:24] detectron2 INFO: Rank of current process: 0. World size: 1
[07/29 19:45:24] detectron2 INFO: Environment info:
----------------------  -------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.10 (default, May 26 2023, 14:05:08) [GCC 9.4.0]
numpy                   1.23.5
detectron2              0.6 @/home/hoenig/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 9.4
CUDA compiler           CUDA 11.6
detectron2 arch flags   8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          525.125.06
CUDA_HOME               /usr/local/cuda
Pillow                  9.3.0
torchvision             0.13.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221122
iopath                  0.1.9
cv2                     4.7.0
----------------------  -------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[07/29 19:45:24] detectron2 INFO: Command line arguments: Namespace(config_file='projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_s_3x.py', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=True)
[07/29 19:45:24] detectron2 INFO: Contents of args.config_file=projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_s_3x.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcascade_mask_rcnn_mvitv2_t_3x[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15moptimizer[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mlr_multiplier[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtrain[39m


[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mdepth[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mlast_block_indexes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m([39m[38;5;141m0[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m13[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m15[39m[38;5;15m)[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15minit_checkpoint[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://ImageNetPretrained/mvitv2/MViTv2_S_in1k.pyth[39m[38;5;186m"[39m

[07/29 19:45:24] d2.config.lazy WARNING: The config contains objects that cannot serialize to a valid yaml. ./output/config.yaml is human-readable but cannot be loaded.
[07/29 19:45:24] d2.config.lazy WARNING: Config is saved using cloudpickle at ./output/config.yaml.pkl.
[07/29 19:45:24] detectron2 INFO: Full config saved to ./output/config.yaml
[07/29 19:45:24] d2.utils.env INFO: Using a generated random seed 27740261
[07/29 19:45:25] detectron2 INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): MViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      )
      (blocks): ModuleList(
        (0): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=96, out_features=192, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (2): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (3): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=192, out_features=384, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (4): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (6): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.107)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.120)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.133)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.147)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (12): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.160)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (13): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.173)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (14): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.187)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=384, out_features=768, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (15): MultiScaleBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (scale2_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (scale3_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (scale4_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (scale5_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): CascadeROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): ModuleList(
      (0): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (1): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (2): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
    )
    (box_predictor): ModuleList(
      (0): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=31, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (1): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=31, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (2): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=31, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 30, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[07/29 19:45:37] d2.data.datasets.coco INFO: Loading datasets/tless/tless_annotations_train.json takes 10.94 seconds.
[07/29 19:45:37] d2.data.datasets.coco INFO: Loaded 50000 images in COCO format from datasets/tless/tless_annotations_train.json
[07/29 19:45:40] d2.data.build INFO: Removed 0 images with no usable annotations. 50000 images left.
[07/29 19:45:41] d2.data.build INFO: Distribution of instances among all 30 categories:
[36m|  category  | #instances   | category   | #instances   | category   | #instances   |
|:----------:|:-------------|:-----------|:-------------|:-----------|:-------------|
|     1      | 19873        | 2          | 21790        | 3          | 21461        |
|     4      | 21610        | 5          | 23092        | 6          | 23830        |
|     7      | 26345        | 8          | 26271        | 9          | 22606        |
|     10     | 22289        | 11         | 22313        | 12         | 23290        |
|     13     | 20476        | 14         | 22314        | 15         | 22261        |
|     16     | 21595        | 17         | 24514        | 18         | 25429        |
|     19     | 23097        | 20         | 22733        | 21         | 23375        |
|     22     | 23035        | 23         | 24216        | 24         | 22070        |
|     25     | 23853        | 26         | 24387        | 27         | 25290        |
|     28     | 23366        | 29         | 24099        | 30         | 22921        |
|            |              |            |              |            |              |
|   total    | 693801       |            |              |            |              |[0m
[07/29 19:45:41] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [<detectron2.data.transforms.augmentation_impl.RandomApply object at 0x7f78deb0b9a0>, ResizeShortestEdge(short_edge_length=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], max_size=1333, sample_style='choice'), RandomFlip()]
[07/29 19:45:41] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[07/29 19:45:41] d2.data.common INFO: Serializing 50000 elements to byte tensors and concatenating them all ...
[07/29 19:45:43] d2.data.common INFO: Serialized dataset takes 389.07 MiB
[07/29 19:45:44] fvcore.common.checkpoint INFO: [Checkpointer] Loading from ./output/model_0029999.pth ...
[07/29 19:45:45] d2.checkpoint.c2_model_loading INFO: Following weights matched with model:
| Names in Model                                  | Names in Checkpoint                                                                                                                                                                                                       | Shapes                                                                                                                   |
|:------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|
| backbone.bottom_up.blocks.0.attn.*              | backbone.bottom_up.blocks.0.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (96,) (96,96) (288,) (288,96) (111,96) (111,96)     |
| backbone.bottom_up.blocks.0.mlp.fc1.*           | backbone.bottom_up.blocks.0.mlp.fc1.{bias,weight}                                                                                                                                                                         | (384,) (384,96)                                                                                                          |
| backbone.bottom_up.blocks.0.mlp.fc2.*           | backbone.bottom_up.blocks.0.mlp.fc2.{bias,weight}                                                                                                                                                                         | (96,) (96,384)                                                                                                           |
| backbone.bottom_up.blocks.0.norm1.*             | backbone.bottom_up.blocks.0.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| backbone.bottom_up.blocks.0.norm2.*             | backbone.bottom_up.blocks.0.norm2.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| backbone.bottom_up.blocks.1.attn.*              | backbone.bottom_up.blocks.1.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,96) (55,96) (55,96)    |
| backbone.bottom_up.blocks.1.mlp.fc1.*           | backbone.bottom_up.blocks.1.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| backbone.bottom_up.blocks.1.mlp.fc2.*           | backbone.bottom_up.blocks.1.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| backbone.bottom_up.blocks.1.norm1.*             | backbone.bottom_up.blocks.1.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| backbone.bottom_up.blocks.1.norm2.*             | backbone.bottom_up.blocks.1.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| backbone.bottom_up.blocks.1.proj.*              | backbone.bottom_up.blocks.1.proj.{bias,weight}                                                                                                                                                                            | (192,) (192,96)                                                                                                          |
| backbone.bottom_up.blocks.10.attn.*             | backbone.bottom_up.blocks.10.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.10.mlp.fc1.*          | backbone.bottom_up.blocks.10.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.10.mlp.fc2.*          | backbone.bottom_up.blocks.10.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.10.norm1.*            | backbone.bottom_up.blocks.10.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.10.norm2.*            | backbone.bottom_up.blocks.10.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.11.attn.*             | backbone.bottom_up.blocks.11.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.11.mlp.fc1.*          | backbone.bottom_up.blocks.11.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.11.mlp.fc2.*          | backbone.bottom_up.blocks.11.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.11.norm1.*            | backbone.bottom_up.blocks.11.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.11.norm2.*            | backbone.bottom_up.blocks.11.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.12.attn.*             | backbone.bottom_up.blocks.12.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.12.mlp.fc1.*          | backbone.bottom_up.blocks.12.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.12.mlp.fc2.*          | backbone.bottom_up.blocks.12.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.12.norm1.*            | backbone.bottom_up.blocks.12.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.12.norm2.*            | backbone.bottom_up.blocks.12.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.13.attn.*             | backbone.bottom_up.blocks.13.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.13.mlp.fc1.*          | backbone.bottom_up.blocks.13.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.13.mlp.fc2.*          | backbone.bottom_up.blocks.13.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.13.norm1.*            | backbone.bottom_up.blocks.13.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.13.norm2.*            | backbone.bottom_up.blocks.13.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.14.attn.*             | backbone.bottom_up.blocks.14.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.14.mlp.fc1.*          | backbone.bottom_up.blocks.14.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| backbone.bottom_up.blocks.14.mlp.fc2.*          | backbone.bottom_up.blocks.14.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| backbone.bottom_up.blocks.14.norm1.*            | backbone.bottom_up.blocks.14.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.14.norm2.*            | backbone.bottom_up.blocks.14.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| backbone.bottom_up.blocks.14.proj.*             | backbone.bottom_up.blocks.14.proj.{bias,weight}                                                                                                                                                                           | (768,) (768,384)                                                                                                         |
| backbone.bottom_up.blocks.15.attn.*             | backbone.bottom_up.blocks.15.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,768) (13,96) (13,96) |
| backbone.bottom_up.blocks.15.mlp.fc1.*          | backbone.bottom_up.blocks.15.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| backbone.bottom_up.blocks.15.mlp.fc2.*          | backbone.bottom_up.blocks.15.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| backbone.bottom_up.blocks.15.norm1.*            | backbone.bottom_up.blocks.15.norm1.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| backbone.bottom_up.blocks.15.norm2.*            | backbone.bottom_up.blocks.15.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| backbone.bottom_up.blocks.2.attn.*              | backbone.bottom_up.blocks.2.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,192) (55,96) (55,96)   |
| backbone.bottom_up.blocks.2.mlp.fc1.*           | backbone.bottom_up.blocks.2.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| backbone.bottom_up.blocks.2.mlp.fc2.*           | backbone.bottom_up.blocks.2.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| backbone.bottom_up.blocks.2.norm1.*             | backbone.bottom_up.blocks.2.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| backbone.bottom_up.blocks.2.norm2.*             | backbone.bottom_up.blocks.2.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| backbone.bottom_up.blocks.3.attn.*              | backbone.bottom_up.blocks.3.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,192) (55,96) (55,96) |
| backbone.bottom_up.blocks.3.mlp.fc1.*           | backbone.bottom_up.blocks.3.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.3.mlp.fc2.*           | backbone.bottom_up.blocks.3.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.3.norm1.*             | backbone.bottom_up.blocks.3.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| backbone.bottom_up.blocks.3.norm2.*             | backbone.bottom_up.blocks.3.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.3.proj.*              | backbone.bottom_up.blocks.3.proj.{bias,weight}                                                                                                                                                                            | (384,) (384,192)                                                                                                         |
| backbone.bottom_up.blocks.4.attn.*              | backbone.bottom_up.blocks.4.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.4.mlp.fc1.*           | backbone.bottom_up.blocks.4.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.4.mlp.fc2.*           | backbone.bottom_up.blocks.4.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.4.norm1.*             | backbone.bottom_up.blocks.4.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.4.norm2.*             | backbone.bottom_up.blocks.4.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.5.attn.*              | backbone.bottom_up.blocks.5.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.5.mlp.fc1.*           | backbone.bottom_up.blocks.5.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.5.mlp.fc2.*           | backbone.bottom_up.blocks.5.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.5.norm1.*             | backbone.bottom_up.blocks.5.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.5.norm2.*             | backbone.bottom_up.blocks.5.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.6.attn.*              | backbone.bottom_up.blocks.6.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.6.mlp.fc1.*           | backbone.bottom_up.blocks.6.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.6.mlp.fc2.*           | backbone.bottom_up.blocks.6.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.6.norm1.*             | backbone.bottom_up.blocks.6.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.6.norm2.*             | backbone.bottom_up.blocks.6.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.7.attn.*              | backbone.bottom_up.blocks.7.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.7.mlp.fc1.*           | backbone.bottom_up.blocks.7.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.7.mlp.fc2.*           | backbone.bottom_up.blocks.7.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.7.norm1.*             | backbone.bottom_up.blocks.7.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.7.norm2.*             | backbone.bottom_up.blocks.7.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.8.attn.*              | backbone.bottom_up.blocks.8.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.8.mlp.fc1.*           | backbone.bottom_up.blocks.8.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.8.mlp.fc2.*           | backbone.bottom_up.blocks.8.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.8.norm1.*             | backbone.bottom_up.blocks.8.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.8.norm2.*             | backbone.bottom_up.blocks.8.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.9.attn.*              | backbone.bottom_up.blocks.9.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| backbone.bottom_up.blocks.9.mlp.fc1.*           | backbone.bottom_up.blocks.9.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| backbone.bottom_up.blocks.9.mlp.fc2.*           | backbone.bottom_up.blocks.9.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| backbone.bottom_up.blocks.9.norm1.*             | backbone.bottom_up.blocks.9.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.blocks.9.norm2.*             | backbone.bottom_up.blocks.9.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| backbone.bottom_up.patch_embed.proj.*           | backbone.bottom_up.patch_embed.proj.{bias,weight}                                                                                                                                                                         | (96,) (96,3,7,7)                                                                                                         |
| backbone.bottom_up.scale2_norm.*                | backbone.bottom_up.scale2_norm.{bias,weight}                                                                                                                                                                              | (96,) (96,)                                                                                                              |
| backbone.bottom_up.scale3_norm.*                | backbone.bottom_up.scale3_norm.{bias,weight}                                                                                                                                                                              | (192,) (192,)                                                                                                            |
| backbone.bottom_up.scale4_norm.*                | backbone.bottom_up.scale4_norm.{bias,weight}                                                                                                                                                                              | (384,) (384,)                                                                                                            |
| backbone.bottom_up.scale5_norm.*                | backbone.bottom_up.scale5_norm.{bias,weight}                                                                                                                                                                              | (768,) (768,)                                                                                                            |
| backbone.fpn_lateral2.*                         | backbone.fpn_lateral2.{bias,weight}                                                                                                                                                                                       | (256,) (256,96,1,1)                                                                                                      |
| backbone.fpn_lateral3.*                         | backbone.fpn_lateral3.{bias,weight}                                                                                                                                                                                       | (256,) (256,192,1,1)                                                                                                     |
| backbone.fpn_lateral4.*                         | backbone.fpn_lateral4.{bias,weight}                                                                                                                                                                                       | (256,) (256,384,1,1)                                                                                                     |
| backbone.fpn_lateral5.*                         | backbone.fpn_lateral5.{bias,weight}                                                                                                                                                                                       | (256,) (256,768,1,1)                                                                                                     |
| backbone.fpn_output2.*                          | backbone.fpn_output2.{bias,weight}                                                                                                                                                                                        | (256,) (256,256,3,3)                                                                                                     |
| backbone.fpn_output3.*                          | backbone.fpn_output3.{bias,weight}                                                                                                                                                                                        | (256,) (256,256,3,3)                                                                                                     |
| backbone.fpn_output4.*                          | backbone.fpn_output4.{bias,weight}                                                                                                                                                                                        | (256,) (256,256,3,3)                                                                                                     |
| backbone.fpn_output5.*                          | backbone.fpn_output5.{bias,weight}                                                                                                                                                                                        | (256,) (256,256,3,3)                                                                                                     |
| proposal_generator.rpn_head.anchor_deltas.*     | proposal_generator.rpn_head.anchor_deltas.{bias,weight}                                                                                                                                                                   | (12,) (12,256,1,1)                                                                                                       |
| proposal_generator.rpn_head.conv.conv0.*        | proposal_generator.rpn_head.conv.conv0.{bias,weight}                                                                                                                                                                      | (256,) (256,256,3,3)                                                                                                     |
| proposal_generator.rpn_head.conv.conv1.*        | proposal_generator.rpn_head.conv.conv1.{bias,weight}                                                                                                                                                                      | (256,) (256,256,3,3)                                                                                                     |
| proposal_generator.rpn_head.objectness_logits.* | proposal_generator.rpn_head.objectness_logits.{bias,weight}                                                                                                                                                               | (3,) (3,256,1,1)                                                                                                         |
| roi_heads.box_head.0.conv1.*                    | roi_heads.box_head.0.conv1.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.0.conv2.*                    | roi_heads.box_head.0.conv2.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.0.conv3.*                    | roi_heads.box_head.0.conv3.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.0.conv4.*                    | roi_heads.box_head.0.conv4.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.0.fc1.*                      | roi_heads.box_head.0.fc1.{bias,weight}                                                                                                                                                                                    | (1024,) (1024,12544)                                                                                                     |
| roi_heads.box_head.1.conv1.*                    | roi_heads.box_head.1.conv1.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.1.conv2.*                    | roi_heads.box_head.1.conv2.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.1.conv3.*                    | roi_heads.box_head.1.conv3.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.1.conv4.*                    | roi_heads.box_head.1.conv4.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.1.fc1.*                      | roi_heads.box_head.1.fc1.{bias,weight}                                                                                                                                                                                    | (1024,) (1024,12544)                                                                                                     |
| roi_heads.box_head.2.conv1.*                    | roi_heads.box_head.2.conv1.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.2.conv2.*                    | roi_heads.box_head.2.conv2.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.2.conv3.*                    | roi_heads.box_head.2.conv3.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.2.conv4.*                    | roi_heads.box_head.2.conv4.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                     | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.box_head.2.fc1.*                      | roi_heads.box_head.2.fc1.{bias,weight}                                                                                                                                                                                    | (1024,) (1024,12544)                                                                                                     |
| roi_heads.box_predictor.0.bbox_pred.*           | roi_heads.box_predictor.0.bbox_pred.{bias,weight}                                                                                                                                                                         | (4,) (4,1024)                                                                                                            |
| roi_heads.box_predictor.0.cls_score.*           | roi_heads.box_predictor.0.cls_score.{bias,weight}                                                                                                                                                                         | (31,) (31,1024)                                                                                                          |
| roi_heads.box_predictor.1.bbox_pred.*           | roi_heads.box_predictor.1.bbox_pred.{bias,weight}                                                                                                                                                                         | (4,) (4,1024)                                                                                                            |
| roi_heads.box_predictor.1.cls_score.*           | roi_heads.box_predictor.1.cls_score.{bias,weight}                                                                                                                                                                         | (31,) (31,1024)                                                                                                          |
| roi_heads.box_predictor.2.bbox_pred.*           | roi_heads.box_predictor.2.bbox_pred.{bias,weight}                                                                                                                                                                         | (4,) (4,1024)                                                                                                            |
| roi_heads.box_predictor.2.cls_score.*           | roi_heads.box_predictor.2.cls_score.{bias,weight}                                                                                                                                                                         | (31,) (31,1024)                                                                                                          |
| roi_heads.mask_head.deconv.*                    | roi_heads.mask_head.deconv.{bias,weight}                                                                                                                                                                                  | (256,) (256,256,2,2)                                                                                                     |
| roi_heads.mask_head.mask_fcn1.*                 | roi_heads.mask_head.mask_fcn1.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                  | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.mask_head.mask_fcn2.*                 | roi_heads.mask_head.mask_fcn2.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                  | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.mask_head.mask_fcn3.*                 | roi_heads.mask_head.mask_fcn3.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                  | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.mask_head.mask_fcn4.*                 | roi_heads.mask_head.mask_fcn4.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}                                                                                                  | (256,) () (256,) (256,) (256,) (256,256,3,3)                                                                             |
| roi_heads.mask_head.predictor.*                 | roi_heads.mask_head.predictor.{bias,weight}                                                                                                                                                                               | (30,) (30,256,1,1)                                                                                                       |
[07/29 19:45:45] fvcore.common.checkpoint INFO: Loading trainer from ./output/model_0029999.pth ...
[07/29 19:45:45] d2.engine.train_loop INFO: Starting training from iteration 30000
[07/29 19:45:59] d2.utils.events INFO:  eta: 6:20:59  iter: 30019  total_loss: 2.851  loss_cls_stage0: 0.2438  loss_box_reg_stage0: 0.2486  loss_cls_stage1: 0.2813  loss_box_reg_stage1: 0.6658  loss_cls_stage2: 0.2838  loss_box_reg_stage2: 0.8909  loss_mask: 0.0777  loss_rpn_cls: 0.009596  loss_rpn_loc: 0.03948  time: 0.6143  data_time: 0.0136  lr: 0.00016  max_mem: 15072M
[07/29 19:46:12] d2.utils.events INFO:  eta: 6:34:25  iter: 30039  total_loss: 2.741  loss_cls_stage0: 0.2436  loss_box_reg_stage0: 0.2419  loss_cls_stage1: 0.273  loss_box_reg_stage1: 0.664  loss_cls_stage2: 0.2635  loss_box_reg_stage2: 0.9475  loss_mask: 0.07708  loss_rpn_cls: 0.01289  loss_rpn_loc: 0.04224  time: 0.6339  data_time: 0.0047  lr: 0.00016  max_mem: 17860M
[07/29 19:46:25] d2.utils.events INFO:  eta: 6:33:34  iter: 30059  total_loss: 2.946  loss_cls_stage0: 0.242  loss_box_reg_stage0: 0.2321  loss_cls_stage1: 0.2779  loss_box_reg_stage1: 0.6895  loss_cls_stage2: 0.2915  loss_box_reg_stage2: 0.9976  loss_mask: 0.06936  loss_rpn_cls: 0.009551  loss_rpn_loc: 0.04829  time: 0.6275  data_time: 0.0045  lr: 0.00016  max_mem: 17860M
[07/29 19:46:37] d2.utils.events INFO:  eta: 6:31:45  iter: 30079  total_loss: 2.745  loss_cls_stage0: 0.239  loss_box_reg_stage0: 0.2439  loss_cls_stage1: 0.287  loss_box_reg_stage1: 0.6684  loss_cls_stage2: 0.3048  loss_box_reg_stage2: 0.9083  loss_mask: 0.06509  loss_rpn_cls: 0.01446  loss_rpn_loc: 0.042  time: 0.6228  data_time: 0.0047  lr: 0.00016  max_mem: 17860M
[07/29 19:46:49] d2.utils.events INFO:  eta: 6:28:33  iter: 30099  total_loss: 2.872  loss_cls_stage0: 0.2478  loss_box_reg_stage0: 0.2421  loss_cls_stage1: 0.2982  loss_box_reg_stage1: 0.6856  loss_cls_stage2: 0.2737  loss_box_reg_stage2: 0.9402  loss_mask: 0.07125  loss_rpn_cls: 0.01768  loss_rpn_loc: 0.04647  time: 0.6190  data_time: 0.0048  lr: 0.00016  max_mem: 17860M
[07/29 19:47:02] d2.utils.events INFO:  eta: 6:29:02  iter: 30119  total_loss: 2.72  loss_cls_stage0: 0.2217  loss_box_reg_stage0: 0.248  loss_cls_stage1: 0.2677  loss_box_reg_stage1: 0.6566  loss_cls_stage2: 0.2516  loss_box_reg_stage2: 0.9228  loss_mask: 0.07791  loss_rpn_cls: 0.0105  loss_rpn_loc: 0.03797  time: 0.6193  data_time: 0.0049  lr: 0.00016  max_mem: 17860M
[07/29 19:47:14] d2.utils.events INFO:  eta: 6:31:07  iter: 30139  total_loss: 2.866  loss_cls_stage0: 0.2531  loss_box_reg_stage0: 0.2686  loss_cls_stage1: 0.2818  loss_box_reg_stage1: 0.6776  loss_cls_stage2: 0.2966  loss_box_reg_stage2: 0.9556  loss_mask: 0.06354  loss_rpn_cls: 0.01937  loss_rpn_loc: 0.04944  time: 0.6208  data_time: 0.0046  lr: 0.00016  max_mem: 17860M
[07/29 19:47:27] d2.utils.events INFO:  eta: 6:31:01  iter: 30159  total_loss: 2.29  loss_cls_stage0: 0.2003  loss_box_reg_stage0: 0.2132  loss_cls_stage1: 0.2157  loss_box_reg_stage1: 0.5255  loss_cls_stage2: 0.1968  loss_box_reg_stage2: 0.7739  loss_mask: 0.06471  loss_rpn_cls: 0.01439  loss_rpn_loc: 0.05871  time: 0.6213  data_time: 0.0046  lr: 0.00016  max_mem: 17860M
[07/29 19:47:39] d2.utils.events INFO:  eta: 6:29:22  iter: 30179  total_loss: 2.498  loss_cls_stage0: 0.2302  loss_box_reg_stage0: 0.2451  loss_cls_stage1: 0.2278  loss_box_reg_stage1: 0.5546  loss_cls_stage2: 0.2149  loss_box_reg_stage2: 0.8109  loss_mask: 0.07082  loss_rpn_cls: 0.01344  loss_rpn_loc: 0.04562  time: 0.6206  data_time: 0.0047  lr: 0.00016  max_mem: 17860M
[07/29 19:47:51] d2.utils.events INFO:  eta: 6:27:55  iter: 30199  total_loss: 3.019  loss_cls_stage0: 0.2772  loss_box_reg_stage0: 0.2789  loss_cls_stage1: 0.2867  loss_box_reg_stage1: 0.6847  loss_cls_stage2: 0.2642  loss_box_reg_stage2: 0.9729  loss_mask: 0.08043  loss_rpn_cls: 0.01739  loss_rpn_loc: 0.04866  time: 0.6197  data_time: 0.0047  lr: 0.00016  max_mem: 17860M
[07/29 19:48:04] d2.utils.events INFO:  eta: 6:27:19  iter: 30219  total_loss: 2.409  loss_cls_stage0: 0.2277  loss_box_reg_stage0: 0.2345  loss_cls_stage1: 0.222  loss_box_reg_stage1: 0.5725  loss_cls_stage2: 0.2179  loss_box_reg_stage2: 0.7723  loss_mask: 0.07605  loss_rpn_cls: 0.01359  loss_rpn_loc: 0.04571  time: 0.6208  data_time: 0.0050  lr: 0.00016  max_mem: 17860M
[07/29 19:48:17] d2.utils.events INFO:  eta: 6:28:00  iter: 30239  total_loss: 2.592  loss_cls_stage0: 0.2551  loss_box_reg_stage0: 0.243  loss_cls_stage1: 0.2733  loss_box_reg_stage1: 0.635  loss_cls_stage2: 0.2632  loss_box_reg_stage2: 0.9146  loss_mask: 0.07351  loss_rpn_cls: 0.01038  loss_rpn_loc: 0.04156  time: 0.6223  data_time: 0.0052  lr: 0.00016  max_mem: 17860M
[07/29 19:48:29] d2.utils.events INFO:  eta: 6:28:04  iter: 30259  total_loss: 2.789  loss_cls_stage0: 0.224  loss_box_reg_stage0: 0.2318  loss_cls_stage1: 0.2547  loss_box_reg_stage1: 0.6652  loss_cls_stage2: 0.2521  loss_box_reg_stage2: 0.8855  loss_mask: 0.06689  loss_rpn_cls: 0.01681  loss_rpn_loc: 0.06021  time: 0.6225  data_time: 0.0046  lr: 0.00016  max_mem: 17860M
[07/29 19:48:42] d2.utils.events INFO:  eta: 6:28:13  iter: 30279  total_loss: 2.746  loss_cls_stage0: 0.2441  loss_box_reg_stage0: 0.2439  loss_cls_stage1: 0.257  loss_box_reg_stage1: 0.6383  loss_cls_stage2: 0.259  loss_box_reg_stage2: 0.9184  loss_mask: 0.06778  loss_rpn_cls: 0.01406  loss_rpn_loc: 0.0461  time: 0.6227  data_time: 0.0047  lr: 0.00016  max_mem: 17860M
[07/29 19:48:54] d2.utils.events INFO:  eta: 6:28:01  iter: 30299  total_loss: 2.64  loss_cls_stage0: 0.2335  loss_box_reg_stage0: 0.2274  loss_cls_stage1: 0.2597  loss_box_reg_stage1: 0.6336  loss_cls_stage2: 0.2601  loss_box_reg_stage2: 0.8244  loss_mask: 0.07163  loss_rpn_cls: 0.01579  loss_rpn_loc: 0.04715  time: 0.6232  data_time: 0.0051  lr: 0.00016  max_mem: 17860M
[07/29 19:49:06] d2.utils.events INFO:  eta: 6:27:10  iter: 30319  total_loss: 2.484  loss_cls_stage0: 0.2157  loss_box_reg_stage0: 0.2329  loss_cls_stage1: 0.247  loss_box_reg_stage1: 0.5645  loss_cls_stage2: 0.2528  loss_box_reg_stage2: 0.8031  loss_mask: 0.07444  loss_rpn_cls: 0.0266  loss_rpn_loc: 0.04603  time: 0.6221  data_time: 0.0049  lr: 0.00016  max_mem: 17860M
[07/29 19:49:19] d2.utils.events INFO:  eta: 6:27:36  iter: 30339  total_loss: 2.605  loss_cls_stage0: 0.2398  loss_box_reg_stage0: 0.25  loss_cls_stage1: 0.2457  loss_box_reg_stage1: 0.5568  loss_cls_stage2: 0.2383  loss_box_reg_stage2: 0.8023  loss_mask: 0.07248  loss_rpn_cls: 0.02249  loss_rpn_loc: 0.04367  time: 0.6229  data_time: 0.0048  lr: 0.00016  max_mem: 17860M
[07/29 19:49:23] d2.engine.hooks INFO: Overall training speed: 344 iterations in 0:03:34 (0.6232 s / it)
[07/29 19:49:23] d2.engine.hooks INFO: Total training time: 0:03:34 (0:00:00 on hooks)
[07/29 19:49:23] d2.utils.events INFO:  eta: 6:27:54  iter: 30346  total_loss: 2.605  loss_cls_stage0: 0.2429  loss_box_reg_stage0: 0.2449  loss_cls_stage1: 0.2415  loss_box_reg_stage1: 0.5926  loss_cls_stage2: 0.2292  loss_box_reg_stage2: 0.8427  loss_mask: 0.07248  loss_rpn_cls: 0.01685  loss_rpn_loc: 0.04521  time: 0.6230  data_time: 0.0048  lr: 0.00016  max_mem: 17860M
[08/02 11:08:47] detectron2 INFO: Rank of current process: 0. World size: 1
[08/02 11:08:48] detectron2 INFO: Environment info:
----------------------  -------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.10 (default, May 26 2023, 14:05:08) [GCC 9.4.0]
numpy                   1.23.5
detectron2              0.6 @/home/hoenig/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 9.4
CUDA compiler           CUDA 11.6
detectron2 arch flags   8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          525.125.06
CUDA_HOME               /usr/local/cuda
Pillow                  9.3.0
torchvision             0.13.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221122
iopath                  0.1.9
cv2                     4.7.0
----------------------  -------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[08/02 11:08:48] detectron2 INFO: Command line arguments: Namespace(config_file='projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_s_3x.py', dist_url='tcp://127.0.0.1:50152', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[08/02 11:08:49] detectron2 INFO: Contents of args.config_file=projects/MViTv2/configs/cascade_mask_rcnn_mvitv2_s_3x.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcascade_mask_rcnn_mvitv2_t_3x[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15moptimizer[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mlr_multiplier[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtrain[39m


[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mdepth[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mlast_block_indexes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m([39m[38;5;141m0[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m13[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m15[39m[38;5;15m)[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15minit_checkpoint[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://ImageNetPretrained/mvitv2/MViTv2_S_in1k.pyth[39m[38;5;186m"[39m

[08/02 11:08:49] d2.config.lazy WARNING: The config contains objects that cannot serialize to a valid yaml. ./output/config.yaml is human-readable but cannot be loaded.
[08/02 11:08:49] d2.config.lazy WARNING: Config is saved using cloudpickle at ./output/config.yaml.pkl.
[08/02 11:08:49] detectron2 INFO: Full config saved to ./output/config.yaml
[08/02 11:08:49] d2.utils.env INFO: Using a generated random seed 52270407
[08/02 11:08:55] fvcore.common.checkpoint INFO: [Checkpointer] Loading from detectron2://ImageNetPretrained/mvitv2/MViTv2_S_in1k.pyth ...
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv1.norm.bias in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv1.norm.weight in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv2.norm.bias in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv2.norm.weight in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv3.norm.bias in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv3.norm.weight in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv4.norm.bias in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv4.norm.weight in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv1.norm.bias in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv1.norm.weight in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv2.norm.bias in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv2.norm.weight in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv3.norm.bias in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv3.norm.weight in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv4.norm.bias in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv4.norm.weight in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv1.norm.bias in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv1.norm.weight in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv2.norm.bias in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv2.norm.weight in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv3.norm.bias in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv3.norm.weight in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv4.norm.bias in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv4.norm.weight in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn1.norm.bias in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn1.norm.weight in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn2.norm.bias in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn2.norm.weight in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn3.norm.bias in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn3.norm.weight in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn4.norm.bias in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn4.norm.weight in model is torch.Size([256]).
[08/02 11:08:56] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[08/02 11:08:56] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone.bottom_up:
| Names in Model      | Names in Checkpoint                                                                                                                                                                                    | Shapes                                                                                                                   |
|:--------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|
| blocks.0.attn.*     | blocks.0.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (96,) (96,96) (288,) (288,96) (111,96) (111,96)     |
| blocks.0.mlp.fc1.*  | blocks.0.mlp.fc1.{bias,weight}                                                                                                                                                                         | (384,) (384,96)                                                                                                          |
| blocks.0.mlp.fc2.*  | blocks.0.mlp.fc2.{bias,weight}                                                                                                                                                                         | (96,) (96,384)                                                                                                           |
| blocks.0.norm1.*    | blocks.0.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.0.norm2.*    | blocks.0.norm2.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.attn.*     | blocks.1.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,96) (55,96) (55,96)    |
| blocks.1.mlp.fc1.*  | blocks.1.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.1.mlp.fc2.*  | blocks.1.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.1.norm1.*    | blocks.1.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.norm2.*    | blocks.1.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.1.proj.*     | blocks.1.proj.{bias,weight}                                                                                                                                                                            | (192,) (192,96)                                                                                                          |
| blocks.10.attn.*    | blocks.10.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.10.mlp.fc1.* | blocks.10.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.10.mlp.fc2.* | blocks.10.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.10.norm1.*   | blocks.10.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.10.norm2.*   | blocks.10.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.attn.*    | blocks.11.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.11.mlp.fc1.* | blocks.11.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.11.mlp.fc2.* | blocks.11.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.11.norm1.*   | blocks.11.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.norm2.*   | blocks.11.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.attn.*    | blocks.12.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.12.mlp.fc1.* | blocks.12.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.12.mlp.fc2.* | blocks.12.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.12.norm1.*   | blocks.12.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.norm2.*   | blocks.12.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.attn.*    | blocks.13.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.13.mlp.fc1.* | blocks.13.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.13.mlp.fc2.* | blocks.13.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.13.norm1.*   | blocks.13.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.norm2.*   | blocks.13.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.attn.*    | blocks.14.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,384) (27,96) (27,96) |
| blocks.14.mlp.fc1.* | blocks.14.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.14.mlp.fc2.* | blocks.14.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.14.norm1.*   | blocks.14.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.norm2.*   | blocks.14.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.14.proj.*    | blocks.14.proj.{bias,weight}                                                                                                                                                                           | (768,) (768,384)                                                                                                         |
| blocks.15.attn.*    | blocks.15.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,768) (13,96) (13,96) |
| blocks.15.mlp.fc1.* | blocks.15.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.15.mlp.fc2.* | blocks.15.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.15.norm1.*   | blocks.15.norm1.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.15.norm2.*   | blocks.15.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.2.attn.*     | blocks.2.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,192) (55,96) (55,96)   |
| blocks.2.mlp.fc1.*  | blocks.2.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.2.mlp.fc2.*  | blocks.2.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.2.norm1.*    | blocks.2.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.2.norm2.*    | blocks.2.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.3.attn.*     | blocks.3.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,192) (55,96) (55,96) |
| blocks.3.mlp.fc1.*  | blocks.3.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.3.mlp.fc2.*  | blocks.3.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.3.norm1.*    | blocks.3.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.3.norm2.*    | blocks.3.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.3.proj.*     | blocks.3.proj.{bias,weight}                                                                                                                                                                            | (384,) (384,192)                                                                                                         |
| blocks.4.attn.*     | blocks.4.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.4.mlp.fc1.*  | blocks.4.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.4.mlp.fc2.*  | blocks.4.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.4.norm1.*    | blocks.4.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.4.norm2.*    | blocks.4.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.5.attn.*     | blocks.5.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.5.mlp.fc1.*  | blocks.5.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.5.mlp.fc2.*  | blocks.5.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.5.norm1.*    | blocks.5.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.5.norm2.*    | blocks.5.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.6.attn.*     | blocks.6.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.6.mlp.fc1.*  | blocks.6.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.6.mlp.fc2.*  | blocks.6.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.6.norm1.*    | blocks.6.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.6.norm2.*    | blocks.6.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.attn.*     | blocks.7.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.7.mlp.fc1.*  | blocks.7.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.7.mlp.fc2.*  | blocks.7.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.7.norm1.*    | blocks.7.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.norm2.*    | blocks.7.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.attn.*     | blocks.8.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.8.mlp.fc1.*  | blocks.8.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.8.mlp.fc2.*  | blocks.8.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.8.norm1.*    | blocks.8.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.norm2.*    | blocks.8.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.attn.*     | blocks.9.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.9.mlp.fc1.*  | blocks.9.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.9.mlp.fc2.*  | blocks.9.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.9.norm1.*    | blocks.9.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.norm2.*    | blocks.9.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| patch_embed.proj.*  | patch_embed.proj.{bias,weight}                                                                                                                                                                         | (96,) (96,3,7,7)                                                                                                         |
[08/02 11:08:57] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.bottom_up.scale2_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale3_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale4_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale5_norm.{bias, weight}[0m
[34mbackbone.fpn_lateral2.{bias, weight}[0m
[34mbackbone.fpn_lateral3.{bias, weight}[0m
[34mbackbone.fpn_lateral4.{bias, weight}[0m
[34mbackbone.fpn_lateral5.{bias, weight}[0m
[34mbackbone.fpn_output2.{bias, weight}[0m
[34mbackbone.fpn_output3.{bias, weight}[0m
[34mbackbone.fpn_output4.{bias, weight}[0m
[34mbackbone.fpn_output5.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv0.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv1.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mroi_heads.box_head.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv1.weight[0m
[34mroi_heads.box_head.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv2.weight[0m
[34mroi_heads.box_head.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv3.weight[0m
[34mroi_heads.box_head.0.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv4.weight[0m
[34mroi_heads.box_head.0.fc1.{bias, weight}[0m
[34mroi_heads.box_head.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv1.weight[0m
[34mroi_heads.box_head.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv2.weight[0m
[34mroi_heads.box_head.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv3.weight[0m
[34mroi_heads.box_head.1.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv4.weight[0m
[34mroi_heads.box_head.1.fc1.{bias, weight}[0m
[34mroi_heads.box_head.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv1.weight[0m
[34mroi_heads.box_head.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv2.weight[0m
[34mroi_heads.box_head.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv3.weight[0m
[34mroi_heads.box_head.2.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv4.weight[0m
[34mroi_heads.box_head.2.fc1.{bias, weight}[0m
[34mroi_heads.box_predictor.0.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.0.cls_score.{bias, weight}[0m
[34mroi_heads.box_predictor.1.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.1.cls_score.{bias, weight}[0m
[34mroi_heads.box_predictor.2.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.2.cls_score.{bias, weight}[0m
[34mroi_heads.mask_head.deconv.{bias, weight}[0m
[34mroi_heads.mask_head.mask_fcn1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn1.weight[0m
[34mroi_heads.mask_head.mask_fcn2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn2.weight[0m
[34mroi_heads.mask_head.mask_fcn3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn3.weight[0m
[34mroi_heads.mask_head.mask_fcn4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn4.weight[0m
[34mroi_heads.mask_head.predictor.{bias, weight}[0m
[08/02 11:08:57] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mhead.projection.{bias, weight}[0m
  [35mnorm.{bias, weight}[0m
[08/02 11:09:01] d2.data.datasets.coco INFO: Loading datasets/tless/tless_annotations_test.json takes 4.35 seconds.
[08/02 11:09:01] d2.data.datasets.coco INFO: Loaded 10080 images in COCO format from datasets/tless/tless_annotations_test.json
[08/02 11:09:02] d2.data.build INFO: Distribution of instances among all 30 categories:
[36m|  category  | #instances   | category   | #instances   | category   | #instances   |
|:----------:|:-------------|:-----------|:-------------|:-----------|:-------------|
|     1      | 8190         | 2          | 4720         | 3          | 4000         |
|     4      | 6255         | 5          | 1960         | 6          | 1007         |
|     7      | 2511         | 8          | 1512         | 9          | 2511         |
|     10     | 1495         | 11         | 1907         | 12         | 1471         |
|     13     | 1486         | 14         | 1505         | 15         | 1509         |
|     16     | 1993         | 17         | 1507         | 18         | 1495         |
|     19     | 1985         | 20         | 2477         | 21         | 1880         |
|     22     | 1977         | 23         | 2514         | 24         | 1983         |
|     25     | 1001         | 26         | 1008         | 27         | 983          |
|     28     | 1973         | 29         | 1006         | 30         | 1487         |
|            |              |            |              |            |              |
|   total    | 67308        |            |              |            |              |[0m
[08/02 11:09:02] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[08/02 11:09:02] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[08/02 11:09:02] d2.data.common INFO: Serializing 10080 elements to byte tensors and concatenating them all ...
[08/02 11:09:03] d2.data.common INFO: Serialized dataset takes 50.77 MiB
[08/02 11:09:06] d2.evaluation.evaluator INFO: Start inference on 10080 batches
[08/02 11:09:13] d2.evaluation.evaluator INFO: Inference done 1/10080. Dataloading: 0.3702 s/iter. Inference: 5.6958 s/iter. Eval: 0.0842 s/iter. Total: 6.1531 s/iter. ETA=17:13:37
[08/02 11:09:18] d2.evaluation.evaluator INFO: Inference done 17/10080. Dataloading: 0.0023 s/iter. Inference: 0.2206 s/iter. Eval: 0.0901 s/iter. Total: 0.3131 s/iter. ETA=0:52:30
[08/02 11:09:23] d2.evaluation.evaluator INFO: Inference done 29/10080. Dataloading: 0.0030 s/iter. Inference: 0.2265 s/iter. Eval: 0.1384 s/iter. Total: 0.3680 s/iter. ETA=1:01:38
[08/02 11:09:28] d2.evaluation.evaluator INFO: Inference done 45/10080. Dataloading: 0.0030 s/iter. Inference: 0.2236 s/iter. Eval: 0.1225 s/iter. Total: 0.3492 s/iter. ETA=0:58:24
[08/02 11:09:33] d2.evaluation.evaluator INFO: Inference done 61/10080. Dataloading: 0.0030 s/iter. Inference: 0.2214 s/iter. Eval: 0.1149 s/iter. Total: 0.3394 s/iter. ETA=0:56:40
[08/02 11:09:38] d2.evaluation.evaluator INFO: Inference done 77/10080. Dataloading: 0.0028 s/iter. Inference: 0.2224 s/iter. Eval: 0.1128 s/iter. Total: 0.3382 s/iter. ETA=0:56:22
[08/02 11:09:44] d2.evaluation.evaluator INFO: Inference done 92/10080. Dataloading: 0.0028 s/iter. Inference: 0.2258 s/iter. Eval: 0.1113 s/iter. Total: 0.3400 s/iter. ETA=0:56:36
[08/02 11:09:49] d2.evaluation.evaluator INFO: Inference done 105/10080. Dataloading: 0.0028 s/iter. Inference: 0.2320 s/iter. Eval: 0.1124 s/iter. Total: 0.3473 s/iter. ETA=0:57:44
[08/02 11:09:54] d2.evaluation.evaluator INFO: Inference done 120/10080. Dataloading: 0.0029 s/iter. Inference: 0.2334 s/iter. Eval: 0.1113 s/iter. Total: 0.3478 s/iter. ETA=0:57:43
[08/02 11:09:59] d2.evaluation.evaluator INFO: Inference done 134/10080. Dataloading: 0.0029 s/iter. Inference: 0.2341 s/iter. Eval: 0.1123 s/iter. Total: 0.3495 s/iter. ETA=0:57:56
[08/02 11:10:04] d2.evaluation.evaluator INFO: Inference done 149/10080. Dataloading: 0.0029 s/iter. Inference: 0.2347 s/iter. Eval: 0.1123 s/iter. Total: 0.3501 s/iter. ETA=0:57:57
[08/02 11:10:10] d2.evaluation.evaluator INFO: Inference done 164/10080. Dataloading: 0.0030 s/iter. Inference: 0.2340 s/iter. Eval: 0.1130 s/iter. Total: 0.3503 s/iter. ETA=0:57:53
[08/02 11:10:15] d2.evaluation.evaluator INFO: Inference done 180/10080. Dataloading: 0.0030 s/iter. Inference: 0.2323 s/iter. Eval: 0.1130 s/iter. Total: 0.3485 s/iter. ETA=0:57:30
[08/02 11:10:20] d2.evaluation.evaluator INFO: Inference done 195/10080. Dataloading: 0.0030 s/iter. Inference: 0.2320 s/iter. Eval: 0.1129 s/iter. Total: 0.3480 s/iter. ETA=0:57:19
[08/02 11:10:25] d2.evaluation.evaluator INFO: Inference done 209/10080. Dataloading: 0.0029 s/iter. Inference: 0.2338 s/iter. Eval: 0.1133 s/iter. Total: 0.3502 s/iter. ETA=0:57:36
[08/02 11:10:31] d2.evaluation.evaluator INFO: Inference done 223/10080. Dataloading: 0.0029 s/iter. Inference: 0.2345 s/iter. Eval: 0.1136 s/iter. Total: 0.3512 s/iter. ETA=0:57:41
[08/02 11:10:36] d2.evaluation.evaluator INFO: Inference done 238/10080. Dataloading: 0.0029 s/iter. Inference: 0.2345 s/iter. Eval: 0.1134 s/iter. Total: 0.3510 s/iter. ETA=0:57:34
[08/02 11:10:41] d2.evaluation.evaluator INFO: Inference done 253/10080. Dataloading: 0.0029 s/iter. Inference: 0.2349 s/iter. Eval: 0.1134 s/iter. Total: 0.3514 s/iter. ETA=0:57:32
[08/02 11:10:46] d2.evaluation.evaluator INFO: Inference done 268/10080. Dataloading: 0.0029 s/iter. Inference: 0.2345 s/iter. Eval: 0.1134 s/iter. Total: 0.3510 s/iter. ETA=0:57:23
[08/02 11:10:51] d2.evaluation.evaluator INFO: Inference done 282/10080. Dataloading: 0.0029 s/iter. Inference: 0.2348 s/iter. Eval: 0.1137 s/iter. Total: 0.3515 s/iter. ETA=0:57:23
[08/02 11:10:57] d2.evaluation.evaluator INFO: Inference done 297/10080. Dataloading: 0.0029 s/iter. Inference: 0.2348 s/iter. Eval: 0.1139 s/iter. Total: 0.3518 s/iter. ETA=0:57:21
[08/02 11:11:02] d2.evaluation.evaluator INFO: Inference done 312/10080. Dataloading: 0.0030 s/iter. Inference: 0.2346 s/iter. Eval: 0.1139 s/iter. Total: 0.3516 s/iter. ETA=0:57:14
[08/02 11:11:07] d2.evaluation.evaluator INFO: Inference done 328/10080. Dataloading: 0.0029 s/iter. Inference: 0.2335 s/iter. Eval: 0.1138 s/iter. Total: 0.3504 s/iter. ETA=0:56:57
[08/02 11:11:12] d2.evaluation.evaluator INFO: Inference done 342/10080. Dataloading: 0.0030 s/iter. Inference: 0.2341 s/iter. Eval: 0.1140 s/iter. Total: 0.3513 s/iter. ETA=0:57:00
[08/02 11:11:18] d2.evaluation.evaluator INFO: Inference done 357/10080. Dataloading: 0.0030 s/iter. Inference: 0.2341 s/iter. Eval: 0.1140 s/iter. Total: 0.3513 s/iter. ETA=0:56:55
[08/02 11:11:23] d2.evaluation.evaluator INFO: Inference done 372/10080. Dataloading: 0.0030 s/iter. Inference: 0.2339 s/iter. Eval: 0.1141 s/iter. Total: 0.3512 s/iter. ETA=0:56:49
[08/02 11:11:28] d2.evaluation.evaluator INFO: Inference done 387/10080. Dataloading: 0.0030 s/iter. Inference: 0.2337 s/iter. Eval: 0.1142 s/iter. Total: 0.3511 s/iter. ETA=0:56:43
[08/02 11:11:33] d2.evaluation.evaluator INFO: Inference done 402/10080. Dataloading: 0.0030 s/iter. Inference: 0.2329 s/iter. Eval: 0.1144 s/iter. Total: 0.3504 s/iter. ETA=0:56:31
[08/02 11:11:38] d2.evaluation.evaluator INFO: Inference done 417/10080. Dataloading: 0.0029 s/iter. Inference: 0.2323 s/iter. Eval: 0.1145 s/iter. Total: 0.3499 s/iter. ETA=0:56:21
[08/02 11:11:43] d2.evaluation.evaluator INFO: Inference done 432/10080. Dataloading: 0.0030 s/iter. Inference: 0.2323 s/iter. Eval: 0.1145 s/iter. Total: 0.3499 s/iter. ETA=0:56:15
[08/02 11:11:49] d2.evaluation.evaluator INFO: Inference done 447/10080. Dataloading: 0.0029 s/iter. Inference: 0.2320 s/iter. Eval: 0.1145 s/iter. Total: 0.3496 s/iter. ETA=0:56:07
[08/02 11:11:54] d2.evaluation.evaluator INFO: Inference done 461/10080. Dataloading: 0.0029 s/iter. Inference: 0.2323 s/iter. Eval: 0.1146 s/iter. Total: 0.3499 s/iter. ETA=0:56:06
[08/02 11:11:59] d2.evaluation.evaluator INFO: Inference done 475/10080. Dataloading: 0.0029 s/iter. Inference: 0.2328 s/iter. Eval: 0.1146 s/iter. Total: 0.3506 s/iter. ETA=0:56:07
[08/02 11:12:04] d2.evaluation.evaluator INFO: Inference done 489/10080. Dataloading: 0.0029 s/iter. Inference: 0.2336 s/iter. Eval: 0.1149 s/iter. Total: 0.3516 s/iter. ETA=0:56:12
[08/02 11:12:09] d2.evaluation.evaluator INFO: Inference done 504/10080. Dataloading: 0.0029 s/iter. Inference: 0.2331 s/iter. Eval: 0.1152 s/iter. Total: 0.3515 s/iter. ETA=0:56:05
[08/02 11:12:15] d2.evaluation.evaluator INFO: Inference done 520/10080. Dataloading: 0.0029 s/iter. Inference: 0.2322 s/iter. Eval: 0.1152 s/iter. Total: 0.3506 s/iter. ETA=0:55:51
[08/02 11:12:20] d2.evaluation.evaluator INFO: Inference done 534/10080. Dataloading: 0.0030 s/iter. Inference: 0.2325 s/iter. Eval: 0.1153 s/iter. Total: 0.3509 s/iter. ETA=0:55:49
[08/02 11:12:25] d2.evaluation.evaluator INFO: Inference done 546/10080. Dataloading: 0.0030 s/iter. Inference: 0.2325 s/iter. Eval: 0.1170 s/iter. Total: 0.3526 s/iter. ETA=0:56:02
[08/02 11:12:30] d2.evaluation.evaluator INFO: Inference done 561/10080. Dataloading: 0.0029 s/iter. Inference: 0.2321 s/iter. Eval: 0.1170 s/iter. Total: 0.3522 s/iter. ETA=0:55:52
[08/02 11:12:35] d2.evaluation.evaluator INFO: Inference done 575/10080. Dataloading: 0.0030 s/iter. Inference: 0.2324 s/iter. Eval: 0.1172 s/iter. Total: 0.3527 s/iter. ETA=0:55:52
[08/02 11:12:40] d2.evaluation.evaluator INFO: Inference done 589/10080. Dataloading: 0.0030 s/iter. Inference: 0.2327 s/iter. Eval: 0.1174 s/iter. Total: 0.3532 s/iter. ETA=0:55:52
