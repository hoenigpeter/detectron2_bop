[09/18 11:27:09] detectron2 INFO: Rank of current process: 0. World size: 1
[09/18 11:27:09] detectron2 INFO: Environment info:
----------------------  -------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.10 (default, May 26 2023, 14:05:08) [GCC 9.4.0]
numpy                   1.23.5
detectron2              0.6 @/home/hoenig/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 9.4
CUDA compiler           CUDA 11.6
detectron2 arch flags   8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          525.125.06
CUDA_HOME               /usr/local/cuda
Pillow                  9.3.0
torchvision             0.13.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221122
iopath                  0.1.9
cv2                     4.7.0
----------------------  -------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/18 11:27:09] detectron2 INFO: Command line arguments: Namespace(config_file='', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[09/18 11:27:09] d2.config.lazy WARNING: The config contains objects that cannot serialize to a valid yaml. ./mvit2_lmo_output/config.yaml is human-readable but cannot be loaded.
[09/18 11:27:09] d2.config.lazy WARNING: Config is saved using cloudpickle at ./mvit2_lmo_output/config.yaml.pkl.
[09/18 11:27:09] detectron2 INFO: Full config saved to ./mvit2_lmo_output/config.yaml
[09/18 11:27:09] d2.utils.env INFO: Using a generated random seed 12198921
[09/18 11:27:10] detectron2 INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): MViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      )
      (blocks): ModuleList(
        (0): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=96, out_features=192, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (2): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (3): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=192, out_features=384, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (4): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (6): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.107)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.120)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.133)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.147)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (12): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.160)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (13): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.173)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (14): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.187)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=384, out_features=768, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (15): MultiScaleBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (scale2_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (scale3_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (scale4_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (scale5_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): CascadeROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): ModuleList(
      (0): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (1): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (2): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
    )
    (box_predictor): ModuleList(
      (0): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=9, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (1): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=9, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (2): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=9, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 8, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[09/18 11:27:15] d2.data.datasets.coco INFO: Loading datasets/BOP_DATASETS/lmo/lmo_annotations_train.json takes 3.95 seconds.
[09/18 11:27:15] d2.data.datasets.coco INFO: Loaded 50000 images in COCO format from datasets/BOP_DATASETS/lmo/lmo_annotations_train.json
[09/18 11:27:16] d2.data.build INFO: Removed 15 images with no usable annotations. 49985 images left.
[09/18 11:27:17] d2.data.build INFO: Distribution of instances among all 15 categories:
[36m|  category  | #instances   | category   | #instances   | category   | #instances   |
|:----------:|:-------------|:-----------|:-------------|:-----------|:-------------|
|     1      | 39818        | 2          | 43308        | 3          | 41854        |
|     4      | 44001        | 5          | 40269        | 6          | 41817        |
|     7      | 41826        | 8          | 41544        | 9          | 0            |
|     10     | 0            | 11         | 0            | 12         | 0            |
|     13     | 0            | 14         | 0            | 15         | 0            |
|            |              |            |              |            |              |
|   total    | 334437       |            |              |            |              |[0m
[09/18 11:27:17] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: []
[09/18 11:27:17] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[09/18 11:27:17] d2.data.common INFO: Serializing 49985 elements to byte tensors and concatenating them all ...
[09/18 11:27:17] d2.data.common INFO: Serialized dataset takes 161.23 MiB
[09/18 11:27:19] fvcore.common.checkpoint INFO: [Checkpointer] Loading from detectron2://ImageNetPretrained/mvitv2/MViTv2_S_in1k.pyth ...
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv1.norm.bias in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv1.norm.weight in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv2.norm.bias in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv2.norm.weight in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv3.norm.bias in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv3.norm.weight in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv4.norm.bias in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv4.norm.weight in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv1.norm.bias in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv1.norm.weight in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv2.norm.bias in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv2.norm.weight in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv3.norm.bias in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv3.norm.weight in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv4.norm.bias in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv4.norm.weight in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv1.norm.bias in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv1.norm.weight in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv2.norm.bias in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv2.norm.weight in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv3.norm.bias in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv3.norm.weight in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv4.norm.bias in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv4.norm.weight in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn1.norm.bias in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn1.norm.weight in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn2.norm.bias in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn2.norm.weight in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn3.norm.bias in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn3.norm.weight in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn4.norm.bias in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn4.norm.weight in model is torch.Size([256]).
[09/18 11:27:19] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:27:19] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone.bottom_up:
| Names in Model      | Names in Checkpoint                                                                                                                                                                                    | Shapes                                                                                                                   |
|:--------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|
| blocks.0.attn.*     | blocks.0.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (96,) (96,96) (288,) (288,96) (111,96) (111,96)     |
| blocks.0.mlp.fc1.*  | blocks.0.mlp.fc1.{bias,weight}                                                                                                                                                                         | (384,) (384,96)                                                                                                          |
| blocks.0.mlp.fc2.*  | blocks.0.mlp.fc2.{bias,weight}                                                                                                                                                                         | (96,) (96,384)                                                                                                           |
| blocks.0.norm1.*    | blocks.0.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.0.norm2.*    | blocks.0.norm2.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.attn.*     | blocks.1.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,96) (55,96) (55,96)    |
| blocks.1.mlp.fc1.*  | blocks.1.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.1.mlp.fc2.*  | blocks.1.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.1.norm1.*    | blocks.1.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.norm2.*    | blocks.1.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.1.proj.*     | blocks.1.proj.{bias,weight}                                                                                                                                                                            | (192,) (192,96)                                                                                                          |
| blocks.10.attn.*    | blocks.10.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.10.mlp.fc1.* | blocks.10.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.10.mlp.fc2.* | blocks.10.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.10.norm1.*   | blocks.10.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.10.norm2.*   | blocks.10.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.attn.*    | blocks.11.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.11.mlp.fc1.* | blocks.11.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.11.mlp.fc2.* | blocks.11.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.11.norm1.*   | blocks.11.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.norm2.*   | blocks.11.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.attn.*    | blocks.12.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.12.mlp.fc1.* | blocks.12.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.12.mlp.fc2.* | blocks.12.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.12.norm1.*   | blocks.12.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.norm2.*   | blocks.12.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.attn.*    | blocks.13.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.13.mlp.fc1.* | blocks.13.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.13.mlp.fc2.* | blocks.13.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.13.norm1.*   | blocks.13.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.norm2.*   | blocks.13.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.attn.*    | blocks.14.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,384) (27,96) (27,96) |
| blocks.14.mlp.fc1.* | blocks.14.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.14.mlp.fc2.* | blocks.14.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.14.norm1.*   | blocks.14.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.norm2.*   | blocks.14.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.14.proj.*    | blocks.14.proj.{bias,weight}                                                                                                                                                                           | (768,) (768,384)                                                                                                         |
| blocks.15.attn.*    | blocks.15.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,768) (13,96) (13,96) |
| blocks.15.mlp.fc1.* | blocks.15.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.15.mlp.fc2.* | blocks.15.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.15.norm1.*   | blocks.15.norm1.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.15.norm2.*   | blocks.15.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.2.attn.*     | blocks.2.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,192) (55,96) (55,96)   |
| blocks.2.mlp.fc1.*  | blocks.2.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.2.mlp.fc2.*  | blocks.2.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.2.norm1.*    | blocks.2.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.2.norm2.*    | blocks.2.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.3.attn.*     | blocks.3.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,192) (55,96) (55,96) |
| blocks.3.mlp.fc1.*  | blocks.3.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.3.mlp.fc2.*  | blocks.3.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.3.norm1.*    | blocks.3.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.3.norm2.*    | blocks.3.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.3.proj.*     | blocks.3.proj.{bias,weight}                                                                                                                                                                            | (384,) (384,192)                                                                                                         |
| blocks.4.attn.*     | blocks.4.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.4.mlp.fc1.*  | blocks.4.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.4.mlp.fc2.*  | blocks.4.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.4.norm1.*    | blocks.4.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.4.norm2.*    | blocks.4.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.5.attn.*     | blocks.5.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.5.mlp.fc1.*  | blocks.5.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.5.mlp.fc2.*  | blocks.5.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.5.norm1.*    | blocks.5.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.5.norm2.*    | blocks.5.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.6.attn.*     | blocks.6.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.6.mlp.fc1.*  | blocks.6.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.6.mlp.fc2.*  | blocks.6.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.6.norm1.*    | blocks.6.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.6.norm2.*    | blocks.6.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.attn.*     | blocks.7.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.7.mlp.fc1.*  | blocks.7.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.7.mlp.fc2.*  | blocks.7.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.7.norm1.*    | blocks.7.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.norm2.*    | blocks.7.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.attn.*     | blocks.8.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.8.mlp.fc1.*  | blocks.8.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.8.mlp.fc2.*  | blocks.8.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.8.norm1.*    | blocks.8.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.norm2.*    | blocks.8.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.attn.*     | blocks.9.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.9.mlp.fc1.*  | blocks.9.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.9.mlp.fc2.*  | blocks.9.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.9.norm1.*    | blocks.9.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.norm2.*    | blocks.9.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| patch_embed.proj.*  | patch_embed.proj.{bias,weight}                                                                                                                                                                         | (96,) (96,3,7,7)                                                                                                         |
[09/18 11:27:19] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.bottom_up.scale2_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale3_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale4_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale5_norm.{bias, weight}[0m
[34mbackbone.fpn_lateral2.{bias, weight}[0m
[34mbackbone.fpn_lateral3.{bias, weight}[0m
[34mbackbone.fpn_lateral4.{bias, weight}[0m
[34mbackbone.fpn_lateral5.{bias, weight}[0m
[34mbackbone.fpn_output2.{bias, weight}[0m
[34mbackbone.fpn_output3.{bias, weight}[0m
[34mbackbone.fpn_output4.{bias, weight}[0m
[34mbackbone.fpn_output5.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv0.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv1.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mroi_heads.box_head.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv1.weight[0m
[34mroi_heads.box_head.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv2.weight[0m
[34mroi_heads.box_head.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv3.weight[0m
[34mroi_heads.box_head.0.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv4.weight[0m
[34mroi_heads.box_head.0.fc1.{bias, weight}[0m
[34mroi_heads.box_head.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv1.weight[0m
[34mroi_heads.box_head.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv2.weight[0m
[34mroi_heads.box_head.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv3.weight[0m
[34mroi_heads.box_head.1.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv4.weight[0m
[34mroi_heads.box_head.1.fc1.{bias, weight}[0m
[34mroi_heads.box_head.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv1.weight[0m
[34mroi_heads.box_head.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv2.weight[0m
[34mroi_heads.box_head.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv3.weight[0m
[34mroi_heads.box_head.2.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv4.weight[0m
[34mroi_heads.box_head.2.fc1.{bias, weight}[0m
[34mroi_heads.box_predictor.0.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.0.cls_score.{bias, weight}[0m
[34mroi_heads.box_predictor.1.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.1.cls_score.{bias, weight}[0m
[34mroi_heads.box_predictor.2.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.2.cls_score.{bias, weight}[0m
[34mroi_heads.mask_head.deconv.{bias, weight}[0m
[34mroi_heads.mask_head.mask_fcn1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn1.weight[0m
[34mroi_heads.mask_head.mask_fcn2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn2.weight[0m
[34mroi_heads.mask_head.mask_fcn3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn3.weight[0m
[34mroi_heads.mask_head.mask_fcn4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn4.weight[0m
[34mroi_heads.mask_head.predictor.{bias, weight}[0m
[09/18 11:27:19] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mhead.projection.{bias, weight}[0m
  [35mnorm.{bias, weight}[0m
[09/18 11:27:19] d2.engine.train_loop INFO: Starting training from iteration 0
[09/18 11:27:28] d2.utils.events INFO:  eta: 12:37:31  iter: 19  total_loss: 6.97  loss_cls_stage0: 1.784  loss_box_reg_stage0: 0.06188  loss_cls_stage1: 1.904  loss_box_reg_stage1: 0.04011  loss_cls_stage2: 1.773  loss_box_reg_stage2: 0.0134  loss_mask: 0.6932  loss_rpn_cls: 0.6774  loss_rpn_loc: 0.05781  time: 0.3619  data_time: 0.0085  lr: 6.7198e-06  max_mem: 6703M
[09/18 11:27:35] d2.utils.events INFO:  eta: 12:55:46  iter: 39  total_loss: 2.504  loss_cls_stage0: 0.3319  loss_box_reg_stage0: 0.1106  loss_cls_stage1: 0.3349  loss_box_reg_stage1: 0.0606  loss_cls_stage2: 0.2335  loss_box_reg_stage2: 0.01832  loss_mask: 0.6928  loss_rpn_cls: 0.6053  loss_rpn_loc: 0.05815  time: 0.3716  data_time: 0.0040  lr: 1.3625e-05  max_mem: 6720M
[09/18 11:27:43] d2.utils.events INFO:  eta: 13:12:43  iter: 59  total_loss: 1.878  loss_cls_stage0: 0.254  loss_box_reg_stage0: 0.1115  loss_cls_stage1: 0.157  loss_box_reg_stage1: 0.05835  loss_cls_stage2: 0.1223  loss_box_reg_stage2: 0.02378  loss_mask: 0.6923  loss_rpn_cls: 0.3434  loss_rpn_loc: 0.06133  time: 0.3780  data_time: 0.0043  lr: 2.053e-05  max_mem: 6782M
[09/18 11:27:51] d2.utils.events INFO:  eta: 13:23:46  iter: 79  total_loss: 2.031  loss_cls_stage0: 0.3546  loss_box_reg_stage0: 0.2197  loss_cls_stage1: 0.2014  loss_box_reg_stage1: 0.1205  loss_cls_stage2: 0.1321  loss_box_reg_stage2: 0.04112  loss_mask: 0.6915  loss_rpn_cls: 0.2285  loss_rpn_loc: 0.05609  time: 0.3850  data_time: 0.0042  lr: 2.7435e-05  max_mem: 6972M
[09/18 11:27:56] d2.engine.hooks INFO: Overall training speed: 89 iterations in 0:00:35 (0.3933 s / it)
[09/18 11:27:56] d2.engine.hooks INFO: Total training time: 0:00:35 (0:00:00 on hooks)
[09/18 11:27:56] d2.utils.events INFO:  eta: 13:26:42  iter: 91  total_loss: 2.363  loss_cls_stage0: 0.4542  loss_box_reg_stage0: 0.2994  loss_cls_stage1: 0.2588  loss_box_reg_stage1: 0.1655  loss_cls_stage2: 0.1588  loss_box_reg_stage2: 0.06353  loss_mask: 0.6905  loss_rpn_cls: 0.1889  loss_rpn_loc: 0.05442  time: 0.3891  data_time: 0.0041  lr: 3.1233e-05  max_mem: 7049M
[09/18 11:51:57] detectron2 INFO: Rank of current process: 0. World size: 1
[09/18 11:51:58] detectron2 INFO: Environment info:
----------------------  -------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.10 (default, May 26 2023, 14:05:08) [GCC 9.4.0]
numpy                   1.23.5
detectron2              0.6 @/home/hoenig/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 9.4
CUDA compiler           CUDA 11.6
detectron2 arch flags   8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          525.125.06
CUDA_HOME               /usr/local/cuda
Pillow                  9.3.0
torchvision             0.13.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221122
iopath                  0.1.9
cv2                     4.7.0
----------------------  -------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/18 11:51:58] detectron2 INFO: Command line arguments: Namespace(config_file='', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[09/18 11:51:58] d2.config.lazy WARNING: The config contains objects that cannot serialize to a valid yaml. ./mvit2_lmo_output/config.yaml is human-readable but cannot be loaded.
[09/18 11:51:58] d2.config.lazy WARNING: Config is saved using cloudpickle at ./mvit2_lmo_output/config.yaml.pkl.
[09/18 11:51:58] detectron2 INFO: Full config saved to ./mvit2_lmo_output/config.yaml
[09/18 11:51:58] d2.utils.env INFO: Using a generated random seed 60644840
[09/18 11:51:58] detectron2 INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): MViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      )
      (blocks): ModuleList(
        (0): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=96, out_features=192, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (2): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (3): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=192, out_features=384, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (4): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (6): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.107)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.120)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.133)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.147)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (12): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.160)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (13): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.173)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (14): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.187)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=384, out_features=768, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (15): MultiScaleBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (scale2_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (scale3_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (scale4_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (scale5_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): CascadeROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): ModuleList(
      (0): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (1): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (2): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
    )
    (box_predictor): ModuleList(
      (0): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=9, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (1): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=9, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (2): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=9, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 8, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[09/18 11:52:03] d2.data.datasets.coco INFO: Loading datasets/BOP_DATASETS/lmo/lmo_annotations_train.json takes 3.92 seconds.
[09/18 11:52:03] d2.data.datasets.coco INFO: Loaded 50000 images in COCO format from datasets/BOP_DATASETS/lmo/lmo_annotations_train.json
[09/18 11:52:04] d2.data.build INFO: Removed 15 images with no usable annotations. 49985 images left.
[09/18 11:52:05] d2.data.build INFO: Distribution of instances among all 8 categories:
[36m|  category  | #instances   | category   | #instances   | category   | #instances   |
|:----------:|:-------------|:-----------|:-------------|:-----------|:-------------|
|     1      | 39818        | 2          | 43308        | 3          | 41854        |
|     4      | 44001        | 5          | 40269        | 6          | 41817        |
|     7      | 41826        | 8          | 41544        |            |              |
|   total    | 334437       |            |              |            |              |[0m
[09/18 11:52:05] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: []
[09/18 11:52:05] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[09/18 11:52:05] d2.data.common INFO: Serializing 49985 elements to byte tensors and concatenating them all ...
[09/18 11:52:06] d2.data.common INFO: Serialized dataset takes 161.23 MiB
[09/18 11:52:07] fvcore.common.checkpoint INFO: [Checkpointer] Loading from detectron2://ImageNetPretrained/mvitv2/MViTv2_S_in1k.pyth ...
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv1.norm.bias in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv1.norm.weight in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv2.norm.bias in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv2.norm.weight in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv3.norm.bias in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv3.norm.weight in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv4.norm.bias in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv4.norm.weight in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv1.norm.bias in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv1.norm.weight in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv2.norm.bias in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv2.norm.weight in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv3.norm.bias in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv3.norm.weight in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv4.norm.bias in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv4.norm.weight in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv1.norm.bias in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv1.norm.weight in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv2.norm.bias in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv2.norm.weight in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv3.norm.bias in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv3.norm.weight in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv4.norm.bias in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv4.norm.weight in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn1.norm.bias in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn1.norm.weight in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn2.norm.bias in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn2.norm.weight in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn3.norm.bias in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn3.norm.weight in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn4.norm.bias in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn4.norm.weight in model is torch.Size([256]).
[09/18 11:52:07] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:52:07] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone.bottom_up:
| Names in Model      | Names in Checkpoint                                                                                                                                                                                    | Shapes                                                                                                                   |
|:--------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|
| blocks.0.attn.*     | blocks.0.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (96,) (96,96) (288,) (288,96) (111,96) (111,96)     |
| blocks.0.mlp.fc1.*  | blocks.0.mlp.fc1.{bias,weight}                                                                                                                                                                         | (384,) (384,96)                                                                                                          |
| blocks.0.mlp.fc2.*  | blocks.0.mlp.fc2.{bias,weight}                                                                                                                                                                         | (96,) (96,384)                                                                                                           |
| blocks.0.norm1.*    | blocks.0.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.0.norm2.*    | blocks.0.norm2.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.attn.*     | blocks.1.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,96) (55,96) (55,96)    |
| blocks.1.mlp.fc1.*  | blocks.1.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.1.mlp.fc2.*  | blocks.1.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.1.norm1.*    | blocks.1.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.norm2.*    | blocks.1.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.1.proj.*     | blocks.1.proj.{bias,weight}                                                                                                                                                                            | (192,) (192,96)                                                                                                          |
| blocks.10.attn.*    | blocks.10.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.10.mlp.fc1.* | blocks.10.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.10.mlp.fc2.* | blocks.10.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.10.norm1.*   | blocks.10.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.10.norm2.*   | blocks.10.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.attn.*    | blocks.11.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.11.mlp.fc1.* | blocks.11.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.11.mlp.fc2.* | blocks.11.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.11.norm1.*   | blocks.11.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.norm2.*   | blocks.11.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.attn.*    | blocks.12.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.12.mlp.fc1.* | blocks.12.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.12.mlp.fc2.* | blocks.12.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.12.norm1.*   | blocks.12.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.norm2.*   | blocks.12.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.attn.*    | blocks.13.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.13.mlp.fc1.* | blocks.13.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.13.mlp.fc2.* | blocks.13.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.13.norm1.*   | blocks.13.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.norm2.*   | blocks.13.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.attn.*    | blocks.14.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,384) (27,96) (27,96) |
| blocks.14.mlp.fc1.* | blocks.14.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.14.mlp.fc2.* | blocks.14.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.14.norm1.*   | blocks.14.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.norm2.*   | blocks.14.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.14.proj.*    | blocks.14.proj.{bias,weight}                                                                                                                                                                           | (768,) (768,384)                                                                                                         |
| blocks.15.attn.*    | blocks.15.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,768) (13,96) (13,96) |
| blocks.15.mlp.fc1.* | blocks.15.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.15.mlp.fc2.* | blocks.15.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.15.norm1.*   | blocks.15.norm1.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.15.norm2.*   | blocks.15.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.2.attn.*     | blocks.2.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,192) (55,96) (55,96)   |
| blocks.2.mlp.fc1.*  | blocks.2.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.2.mlp.fc2.*  | blocks.2.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.2.norm1.*    | blocks.2.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.2.norm2.*    | blocks.2.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.3.attn.*     | blocks.3.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,192) (55,96) (55,96) |
| blocks.3.mlp.fc1.*  | blocks.3.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.3.mlp.fc2.*  | blocks.3.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.3.norm1.*    | blocks.3.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.3.norm2.*    | blocks.3.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.3.proj.*     | blocks.3.proj.{bias,weight}                                                                                                                                                                            | (384,) (384,192)                                                                                                         |
| blocks.4.attn.*     | blocks.4.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.4.mlp.fc1.*  | blocks.4.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.4.mlp.fc2.*  | blocks.4.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.4.norm1.*    | blocks.4.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.4.norm2.*    | blocks.4.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.5.attn.*     | blocks.5.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.5.mlp.fc1.*  | blocks.5.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.5.mlp.fc2.*  | blocks.5.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.5.norm1.*    | blocks.5.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.5.norm2.*    | blocks.5.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.6.attn.*     | blocks.6.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.6.mlp.fc1.*  | blocks.6.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.6.mlp.fc2.*  | blocks.6.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.6.norm1.*    | blocks.6.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.6.norm2.*    | blocks.6.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.attn.*     | blocks.7.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.7.mlp.fc1.*  | blocks.7.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.7.mlp.fc2.*  | blocks.7.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.7.norm1.*    | blocks.7.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.norm2.*    | blocks.7.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.attn.*     | blocks.8.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.8.mlp.fc1.*  | blocks.8.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.8.mlp.fc2.*  | blocks.8.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.8.norm1.*    | blocks.8.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.norm2.*    | blocks.8.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.attn.*     | blocks.9.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.9.mlp.fc1.*  | blocks.9.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.9.mlp.fc2.*  | blocks.9.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.9.norm1.*    | blocks.9.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.norm2.*    | blocks.9.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| patch_embed.proj.*  | patch_embed.proj.{bias,weight}                                                                                                                                                                         | (96,) (96,3,7,7)                                                                                                         |
[09/18 11:52:07] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.bottom_up.scale2_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale3_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale4_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale5_norm.{bias, weight}[0m
[34mbackbone.fpn_lateral2.{bias, weight}[0m
[34mbackbone.fpn_lateral3.{bias, weight}[0m
[34mbackbone.fpn_lateral4.{bias, weight}[0m
[34mbackbone.fpn_lateral5.{bias, weight}[0m
[34mbackbone.fpn_output2.{bias, weight}[0m
[34mbackbone.fpn_output3.{bias, weight}[0m
[34mbackbone.fpn_output4.{bias, weight}[0m
[34mbackbone.fpn_output5.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv0.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv1.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mroi_heads.box_head.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv1.weight[0m
[34mroi_heads.box_head.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv2.weight[0m
[34mroi_heads.box_head.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv3.weight[0m
[34mroi_heads.box_head.0.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv4.weight[0m
[34mroi_heads.box_head.0.fc1.{bias, weight}[0m
[34mroi_heads.box_head.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv1.weight[0m
[34mroi_heads.box_head.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv2.weight[0m
[34mroi_heads.box_head.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv3.weight[0m
[34mroi_heads.box_head.1.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv4.weight[0m
[34mroi_heads.box_head.1.fc1.{bias, weight}[0m
[34mroi_heads.box_head.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv1.weight[0m
[34mroi_heads.box_head.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv2.weight[0m
[34mroi_heads.box_head.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv3.weight[0m
[34mroi_heads.box_head.2.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv4.weight[0m
[34mroi_heads.box_head.2.fc1.{bias, weight}[0m
[34mroi_heads.box_predictor.0.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.0.cls_score.{bias, weight}[0m
[34mroi_heads.box_predictor.1.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.1.cls_score.{bias, weight}[0m
[34mroi_heads.box_predictor.2.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.2.cls_score.{bias, weight}[0m
[34mroi_heads.mask_head.deconv.{bias, weight}[0m
[34mroi_heads.mask_head.mask_fcn1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn1.weight[0m
[34mroi_heads.mask_head.mask_fcn2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn2.weight[0m
[34mroi_heads.mask_head.mask_fcn3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn3.weight[0m
[34mroi_heads.mask_head.mask_fcn4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn4.weight[0m
[34mroi_heads.mask_head.predictor.{bias, weight}[0m
[09/18 11:52:07] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mhead.projection.{bias, weight}[0m
  [35mnorm.{bias, weight}[0m
[09/18 11:52:07] d2.engine.train_loop INFO: Starting training from iteration 0
[09/18 11:52:16] d2.utils.events INFO:  eta: 11:44:17  iter: 19  total_loss: 7.131  loss_cls_stage0: 1.931  loss_box_reg_stage0: 0.04502  loss_cls_stage1: 1.762  loss_box_reg_stage1: 0.01234  loss_cls_stage2: 1.94  loss_box_reg_stage2: 0.004302  loss_mask: 0.6931  loss_rpn_cls: 0.6887  loss_rpn_loc: 0.05934  time: 0.3427  data_time: 0.0078  lr: 6.7198e-06  max_mem: 6699M
[09/18 11:52:23] d2.utils.events INFO:  eta: 12:52:27  iter: 39  total_loss: 2.721  loss_cls_stage0: 0.4161  loss_box_reg_stage0: 0.1252  loss_cls_stage1: 0.3174  loss_box_reg_stage1: 0.07378  loss_cls_stage2: 0.3121  loss_box_reg_stage2: 0.02888  loss_mask: 0.6929  loss_rpn_cls: 0.6294  loss_rpn_loc: 0.05733  time: 0.3643  data_time: 0.0039  lr: 1.3625e-05  max_mem: 6768M
[09/18 11:52:31] d2.utils.events INFO:  eta: 13:17:17  iter: 59  total_loss: 2.02  loss_cls_stage0: 0.3077  loss_box_reg_stage0: 0.1531  loss_cls_stage1: 0.1802  loss_box_reg_stage1: 0.08209  loss_cls_stage2: 0.1212  loss_box_reg_stage2: 0.02877  loss_mask: 0.6923  loss_rpn_cls: 0.3766  loss_rpn_loc: 0.05807  time: 0.3728  data_time: 0.0039  lr: 2.053e-05  max_mem: 6817M
[09/18 11:52:39] d2.utils.events INFO:  eta: 13:24:31  iter: 79  total_loss: 2.037  loss_cls_stage0: 0.356  loss_box_reg_stage0: 0.2299  loss_cls_stage1: 0.1992  loss_box_reg_stage1: 0.1285  loss_cls_stage2: 0.1227  loss_box_reg_stage2: 0.03987  loss_mask: 0.6914  loss_rpn_cls: 0.2189  loss_rpn_loc: 0.05307  time: 0.3789  data_time: 0.0042  lr: 2.7435e-05  max_mem: 6888M
[09/18 11:52:43] d2.engine.hooks INFO: Overall training speed: 87 iterations in 0:00:33 (0.3843 s / it)
[09/18 11:52:43] d2.engine.hooks INFO: Total training time: 0:00:33 (0:00:00 on hooks)
[09/18 11:52:43] d2.utils.events INFO:  eta: 13:26:24  iter: 89  total_loss: 2.077  loss_cls_stage0: 0.3678  loss_box_reg_stage0: 0.2573  loss_cls_stage1: 0.2126  loss_box_reg_stage1: 0.1292  loss_cls_stage2: 0.1315  loss_box_reg_stage2: 0.04352  loss_mask: 0.69  loss_rpn_cls: 0.1989  loss_rpn_loc: 0.05307  time: 0.3827  data_time: 0.0041  lr: 3.0542e-05  max_mem: 6923M
[09/18 11:52:51] detectron2 INFO: Rank of current process: 0. World size: 1
[09/18 11:52:52] detectron2 INFO: Environment info:
----------------------  -------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.10 (default, May 26 2023, 14:05:08) [GCC 9.4.0]
numpy                   1.23.5
detectron2              0.6 @/home/hoenig/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 9.4
CUDA compiler           CUDA 11.6
detectron2 arch flags   8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          525.125.06
CUDA_HOME               /usr/local/cuda
Pillow                  9.3.0
torchvision             0.13.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221122
iopath                  0.1.9
cv2                     4.7.0
----------------------  -------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/18 11:52:52] detectron2 INFO: Command line arguments: Namespace(config_file='', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[09/18 11:52:52] d2.config.lazy WARNING: The config contains objects that cannot serialize to a valid yaml. ./mvit2_lmo_output/config.yaml is human-readable but cannot be loaded.
[09/18 11:52:52] d2.config.lazy WARNING: Config is saved using cloudpickle at ./mvit2_lmo_output/config.yaml.pkl.
[09/18 11:52:52] detectron2 INFO: Full config saved to ./mvit2_lmo_output/config.yaml
[09/18 11:52:52] d2.utils.env INFO: Using a generated random seed 54917626
[09/18 11:52:53] detectron2 INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): MViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      )
      (blocks): ModuleList(
        (0): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=96, out_features=192, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (2): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (3): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=192, out_features=384, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (4): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (6): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.107)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.120)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.133)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.147)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (12): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.160)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (13): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.173)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (14): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.187)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=384, out_features=768, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (15): MultiScaleBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (scale2_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (scale3_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (scale4_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (scale5_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): CascadeROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): ModuleList(
      (0): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (1): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (2): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
    )
    (box_predictor): ModuleList(
      (0): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=9, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (1): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=9, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (2): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=9, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 8, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[09/18 11:52:57] d2.data.datasets.coco INFO: Loading datasets/BOP_DATASETS/lmo/lmo_annotations_train.json takes 4.11 seconds.
[09/18 11:52:58] d2.data.datasets.coco INFO: Loaded 50000 images in COCO format from datasets/BOP_DATASETS/lmo/lmo_annotations_train.json
[09/18 11:52:59] d2.data.build INFO: Removed 15 images with no usable annotations. 49985 images left.
[09/18 11:53:00] d2.data.build INFO: Distribution of instances among all 8 categories:
[36m|  category  | #instances   | category   | #instances   | category   | #instances   |
|:----------:|:-------------|:-----------|:-------------|:-----------|:-------------|
|     1      | 39818        | 2          | 43308        | 3          | 41854        |
|     4      | 44001        | 5          | 40269        | 6          | 41817        |
|     7      | 41826        | 8          | 41544        |            |              |
|   total    | 334437       |            |              |            |              |[0m
[09/18 11:53:00] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: []
[09/18 11:53:00] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[09/18 11:53:00] d2.data.common INFO: Serializing 49985 elements to byte tensors and concatenating them all ...
[09/18 11:53:00] d2.data.common INFO: Serialized dataset takes 161.23 MiB
[09/18 11:53:02] fvcore.common.checkpoint INFO: [Checkpointer] Loading from detectron2://ImageNetPretrained/mvitv2/MViTv2_S_in1k.pyth ...
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv1.norm.bias in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv1.norm.weight in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv2.norm.bias in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv2.norm.weight in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv3.norm.bias in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv3.norm.weight in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv4.norm.bias in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv4.norm.weight in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv1.norm.bias in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv1.norm.weight in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv2.norm.bias in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv2.norm.weight in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv3.norm.bias in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv3.norm.weight in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv4.norm.bias in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv4.norm.weight in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv1.norm.bias in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv1.norm.weight in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv2.norm.bias in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv2.norm.weight in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv3.norm.bias in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv3.norm.weight in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv4.norm.bias in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv4.norm.weight in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn1.norm.bias in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn1.norm.weight in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn2.norm.bias in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn2.norm.weight in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn3.norm.bias in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn3.norm.weight in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn4.norm.bias in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn4.norm.weight in model is torch.Size([256]).
[09/18 11:53:02] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 11:53:02] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone.bottom_up:
| Names in Model      | Names in Checkpoint                                                                                                                                                                                    | Shapes                                                                                                                   |
|:--------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|
| blocks.0.attn.*     | blocks.0.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (96,) (96,96) (288,) (288,96) (111,96) (111,96)     |
| blocks.0.mlp.fc1.*  | blocks.0.mlp.fc1.{bias,weight}                                                                                                                                                                         | (384,) (384,96)                                                                                                          |
| blocks.0.mlp.fc2.*  | blocks.0.mlp.fc2.{bias,weight}                                                                                                                                                                         | (96,) (96,384)                                                                                                           |
| blocks.0.norm1.*    | blocks.0.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.0.norm2.*    | blocks.0.norm2.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.attn.*     | blocks.1.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,96) (55,96) (55,96)    |
| blocks.1.mlp.fc1.*  | blocks.1.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.1.mlp.fc2.*  | blocks.1.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.1.norm1.*    | blocks.1.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.norm2.*    | blocks.1.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.1.proj.*     | blocks.1.proj.{bias,weight}                                                                                                                                                                            | (192,) (192,96)                                                                                                          |
| blocks.10.attn.*    | blocks.10.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.10.mlp.fc1.* | blocks.10.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.10.mlp.fc2.* | blocks.10.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.10.norm1.*   | blocks.10.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.10.norm2.*   | blocks.10.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.attn.*    | blocks.11.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.11.mlp.fc1.* | blocks.11.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.11.mlp.fc2.* | blocks.11.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.11.norm1.*   | blocks.11.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.norm2.*   | blocks.11.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.attn.*    | blocks.12.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.12.mlp.fc1.* | blocks.12.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.12.mlp.fc2.* | blocks.12.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.12.norm1.*   | blocks.12.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.norm2.*   | blocks.12.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.attn.*    | blocks.13.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.13.mlp.fc1.* | blocks.13.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.13.mlp.fc2.* | blocks.13.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.13.norm1.*   | blocks.13.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.norm2.*   | blocks.13.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.attn.*    | blocks.14.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,384) (27,96) (27,96) |
| blocks.14.mlp.fc1.* | blocks.14.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.14.mlp.fc2.* | blocks.14.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.14.norm1.*   | blocks.14.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.norm2.*   | blocks.14.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.14.proj.*    | blocks.14.proj.{bias,weight}                                                                                                                                                                           | (768,) (768,384)                                                                                                         |
| blocks.15.attn.*    | blocks.15.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,768) (13,96) (13,96) |
| blocks.15.mlp.fc1.* | blocks.15.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.15.mlp.fc2.* | blocks.15.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.15.norm1.*   | blocks.15.norm1.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.15.norm2.*   | blocks.15.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.2.attn.*     | blocks.2.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,192) (55,96) (55,96)   |
| blocks.2.mlp.fc1.*  | blocks.2.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.2.mlp.fc2.*  | blocks.2.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.2.norm1.*    | blocks.2.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.2.norm2.*    | blocks.2.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.3.attn.*     | blocks.3.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,192) (55,96) (55,96) |
| blocks.3.mlp.fc1.*  | blocks.3.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.3.mlp.fc2.*  | blocks.3.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.3.norm1.*    | blocks.3.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.3.norm2.*    | blocks.3.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.3.proj.*     | blocks.3.proj.{bias,weight}                                                                                                                                                                            | (384,) (384,192)                                                                                                         |
| blocks.4.attn.*     | blocks.4.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.4.mlp.fc1.*  | blocks.4.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.4.mlp.fc2.*  | blocks.4.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.4.norm1.*    | blocks.4.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.4.norm2.*    | blocks.4.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.5.attn.*     | blocks.5.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.5.mlp.fc1.*  | blocks.5.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.5.mlp.fc2.*  | blocks.5.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.5.norm1.*    | blocks.5.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.5.norm2.*    | blocks.5.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.6.attn.*     | blocks.6.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.6.mlp.fc1.*  | blocks.6.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.6.mlp.fc2.*  | blocks.6.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.6.norm1.*    | blocks.6.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.6.norm2.*    | blocks.6.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.attn.*     | blocks.7.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.7.mlp.fc1.*  | blocks.7.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.7.mlp.fc2.*  | blocks.7.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.7.norm1.*    | blocks.7.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.norm2.*    | blocks.7.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.attn.*     | blocks.8.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.8.mlp.fc1.*  | blocks.8.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.8.mlp.fc2.*  | blocks.8.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.8.norm1.*    | blocks.8.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.norm2.*    | blocks.8.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.attn.*     | blocks.9.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.9.mlp.fc1.*  | blocks.9.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.9.mlp.fc2.*  | blocks.9.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.9.norm1.*    | blocks.9.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.norm2.*    | blocks.9.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| patch_embed.proj.*  | patch_embed.proj.{bias,weight}                                                                                                                                                                         | (96,) (96,3,7,7)                                                                                                         |
[09/18 11:53:02] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.bottom_up.scale2_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale3_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale4_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale5_norm.{bias, weight}[0m
[34mbackbone.fpn_lateral2.{bias, weight}[0m
[34mbackbone.fpn_lateral3.{bias, weight}[0m
[34mbackbone.fpn_lateral4.{bias, weight}[0m
[34mbackbone.fpn_lateral5.{bias, weight}[0m
[34mbackbone.fpn_output2.{bias, weight}[0m
[34mbackbone.fpn_output3.{bias, weight}[0m
[34mbackbone.fpn_output4.{bias, weight}[0m
[34mbackbone.fpn_output5.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv0.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv1.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mroi_heads.box_head.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv1.weight[0m
[34mroi_heads.box_head.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv2.weight[0m
[34mroi_heads.box_head.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv3.weight[0m
[34mroi_heads.box_head.0.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv4.weight[0m
[34mroi_heads.box_head.0.fc1.{bias, weight}[0m
[34mroi_heads.box_head.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv1.weight[0m
[34mroi_heads.box_head.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv2.weight[0m
[34mroi_heads.box_head.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv3.weight[0m
[34mroi_heads.box_head.1.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv4.weight[0m
[34mroi_heads.box_head.1.fc1.{bias, weight}[0m
[34mroi_heads.box_head.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv1.weight[0m
[34mroi_heads.box_head.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv2.weight[0m
[34mroi_heads.box_head.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv3.weight[0m
[34mroi_heads.box_head.2.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv4.weight[0m
[34mroi_heads.box_head.2.fc1.{bias, weight}[0m
[34mroi_heads.box_predictor.0.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.0.cls_score.{bias, weight}[0m
[34mroi_heads.box_predictor.1.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.1.cls_score.{bias, weight}[0m
[34mroi_heads.box_predictor.2.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.2.cls_score.{bias, weight}[0m
[34mroi_heads.mask_head.deconv.{bias, weight}[0m
[34mroi_heads.mask_head.mask_fcn1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn1.weight[0m
[34mroi_heads.mask_head.mask_fcn2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn2.weight[0m
[34mroi_heads.mask_head.mask_fcn3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn3.weight[0m
[34mroi_heads.mask_head.mask_fcn4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn4.weight[0m
[34mroi_heads.mask_head.predictor.{bias, weight}[0m
[09/18 11:53:02] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mhead.projection.{bias, weight}[0m
  [35mnorm.{bias, weight}[0m
[09/18 11:53:02] d2.engine.train_loop INFO: Starting training from iteration 0
[09/18 11:53:11] d2.utils.events INFO:  eta: 12:15:46  iter: 19  total_loss: 7.133  loss_cls_stage0: 1.892  loss_box_reg_stage0: 0.03498  loss_cls_stage1: 1.877  loss_box_reg_stage1: 0.008836  loss_cls_stage2: 1.891  loss_box_reg_stage2: 0.002497  loss_mask: 0.6931  loss_rpn_cls: 0.6796  loss_rpn_loc: 0.06506  time: 0.3544  data_time: 0.0086  lr: 6.7198e-06  max_mem: 6695M
[09/18 11:53:18] d2.utils.events INFO:  eta: 13:00:01  iter: 39  total_loss: 2.295  loss_cls_stage0: 0.3088  loss_box_reg_stage0: 0.064  loss_cls_stage1: 0.2869  loss_box_reg_stage1: 0.03288  loss_cls_stage2: 0.238  loss_box_reg_stage2: 0.01143  loss_mask: 0.6929  loss_rpn_cls: 0.613  loss_rpn_loc: 0.05836  time: 0.3698  data_time: 0.0042  lr: 1.3625e-05  max_mem: 6696M
[09/18 11:53:26] d2.utils.events INFO:  eta: 13:19:19  iter: 59  total_loss: 1.843  loss_cls_stage0: 0.2426  loss_box_reg_stage0: 0.1027  loss_cls_stage1: 0.1587  loss_box_reg_stage1: 0.0705  loss_cls_stage2: 0.1161  loss_box_reg_stage2: 0.02057  loss_mask: 0.6922  loss_rpn_cls: 0.3345  loss_rpn_loc: 0.06148  time: 0.3777  data_time: 0.0039  lr: 2.053e-05  max_mem: 6753M
[09/18 11:53:35] d2.utils.events INFO:  eta: 13:27:22  iter: 79  total_loss: 2.128  loss_cls_stage0: 0.3651  loss_box_reg_stage0: 0.2145  loss_cls_stage1: 0.2376  loss_box_reg_stage1: 0.1351  loss_cls_stage2: 0.1395  loss_box_reg_stage2: 0.04486  loss_mask: 0.6914  loss_rpn_cls: 0.2399  loss_rpn_loc: 0.05035  time: 0.3876  data_time: 0.0043  lr: 2.7435e-05  max_mem: 6959M
[09/18 11:53:43] d2.utils.events INFO:  eta: 13:36:24  iter: 99  total_loss: 2.462  loss_cls_stage0: 0.4789  loss_box_reg_stage0: 0.3554  loss_cls_stage1: 0.268  loss_box_reg_stage1: 0.2181  loss_cls_stage2: 0.1529  loss_box_reg_stage2: 0.07146  loss_mask: 0.6878  loss_rpn_cls: 0.1686  loss_rpn_loc: 0.05444  time: 0.3969  data_time: 0.0042  lr: 3.434e-05  max_mem: 7131M
[09/18 11:53:51] d2.utils.events INFO:  eta: 13:42:28  iter: 119  total_loss: 3.121  loss_cls_stage0: 0.7111  loss_box_reg_stage0: 0.6129  loss_cls_stage1: 0.3478  loss_box_reg_stage1: 0.3237  loss_cls_stage2: 0.1738  loss_box_reg_stage2: 0.0955  loss_mask: 0.6735  loss_rpn_cls: 0.1303  loss_rpn_loc: 0.05216  time: 0.3985  data_time: 0.0040  lr: 4.1245e-05  max_mem: 7427M
[09/18 11:54:00] d2.utils.events INFO:  eta: 13:58:36  iter: 139  total_loss: 3.34  loss_cls_stage0: 0.7018  loss_box_reg_stage0: 0.7344  loss_cls_stage1: 0.3787  loss_box_reg_stage1: 0.4106  loss_cls_stage2: 0.181  loss_box_reg_stage2: 0.1225  loss_mask: 0.6432  loss_rpn_cls: 0.09172  loss_rpn_loc: 0.04941  time: 0.4016  data_time: 0.0041  lr: 4.815e-05  max_mem: 7555M
[09/18 11:54:08] d2.utils.events INFO:  eta: 14:05:15  iter: 159  total_loss: 3.297  loss_cls_stage0: 0.598  loss_box_reg_stage0: 0.7607  loss_cls_stage1: 0.3623  loss_box_reg_stage1: 0.4879  loss_cls_stage2: 0.19  loss_box_reg_stage2: 0.1544  loss_mask: 0.6009  loss_rpn_cls: 0.07531  loss_rpn_loc: 0.04715  time: 0.4030  data_time: 0.0042  lr: 5.5055e-05  max_mem: 7555M
[09/18 11:54:16] d2.utils.events INFO:  eta: 14:10:30  iter: 179  total_loss: 3.652  loss_cls_stage0: 0.5436  loss_box_reg_stage0: 0.7874  loss_cls_stage1: 0.4059  loss_box_reg_stage1: 0.7482  loss_cls_stage2: 0.2517  loss_box_reg_stage2: 0.2791  loss_mask: 0.5368  loss_rpn_cls: 0.06982  loss_rpn_loc: 0.04952  time: 0.4050  data_time: 0.0041  lr: 6.1961e-05  max_mem: 7555M
[09/18 11:54:25] d2.utils.events INFO:  eta: 14:15:55  iter: 199  total_loss: 4.117  loss_cls_stage0: 0.5353  loss_box_reg_stage0: 0.7544  loss_cls_stage1: 0.421  loss_box_reg_stage1: 0.9464  loss_cls_stage2: 0.3039  loss_box_reg_stage2: 0.5303  loss_mask: 0.5115  loss_rpn_cls: 0.05153  loss_rpn_loc: 0.0466  time: 0.4067  data_time: 0.0040  lr: 6.8866e-05  max_mem: 7560M
[09/18 11:54:34] d2.utils.events INFO:  eta: 14:21:58  iter: 219  total_loss: 4.272  loss_cls_stage0: 0.4329  loss_box_reg_stage0: 0.7093  loss_cls_stage1: 0.3923  loss_box_reg_stage1: 1.123  loss_cls_stage2: 0.307  loss_box_reg_stage2: 0.7432  loss_mask: 0.444  loss_rpn_cls: 0.04315  loss_rpn_loc: 0.04278  time: 0.4090  data_time: 0.0040  lr: 7.5771e-05  max_mem: 7560M
[09/18 11:54:42] d2.utils.events INFO:  eta: 14:25:58  iter: 239  total_loss: 4.486  loss_cls_stage0: 0.4492  loss_box_reg_stage0: 0.6554  loss_cls_stage1: 0.4005  loss_box_reg_stage1: 1.154  loss_cls_stage2: 0.3423  loss_box_reg_stage2: 0.9932  loss_mask: 0.4182  loss_rpn_cls: 0.04359  loss_rpn_loc: 0.04534  time: 0.4107  data_time: 0.0043  lr: 8.2676e-05  max_mem: 7560M
[09/18 11:54:51] d2.utils.events INFO:  eta: 14:30:59  iter: 259  total_loss: 4.665  loss_cls_stage0: 0.4149  loss_box_reg_stage0: 0.6137  loss_cls_stage1: 0.41  loss_box_reg_stage1: 1.139  loss_cls_stage2: 0.3445  loss_box_reg_stage2: 1.138  loss_mask: 0.3778  loss_rpn_cls: 0.0381  loss_rpn_loc: 0.04196  time: 0.4124  data_time: 0.0041  lr: 8.9581e-05  max_mem: 7560M
[09/18 11:54:59] d2.utils.events INFO:  eta: 14:32:48  iter: 279  total_loss: 4.764  loss_cls_stage0: 0.3844  loss_box_reg_stage0: 0.5986  loss_cls_stage1: 0.3701  loss_box_reg_stage1: 1.228  loss_cls_stage2: 0.3438  loss_box_reg_stage2: 1.351  loss_mask: 0.3358  loss_rpn_cls: 0.03742  loss_rpn_loc: 0.0436  time: 0.4135  data_time: 0.0040  lr: 9.6486e-05  max_mem: 7560M
[09/18 11:55:08] d2.utils.events INFO:  eta: 14:33:07  iter: 299  total_loss: 4.581  loss_cls_stage0: 0.3532  loss_box_reg_stage0: 0.5683  loss_cls_stage1: 0.3194  loss_box_reg_stage1: 1.189  loss_cls_stage2: 0.3181  loss_box_reg_stage2: 1.473  loss_mask: 0.3018  loss_rpn_cls: 0.03815  loss_rpn_loc: 0.03737  time: 0.4139  data_time: 0.0043  lr: 0.00010339  max_mem: 7560M
[09/18 11:55:16] d2.utils.events INFO:  eta: 14:32:59  iter: 319  total_loss: 4.782  loss_cls_stage0: 0.3439  loss_box_reg_stage0: 0.5599  loss_cls_stage1: 0.3355  loss_box_reg_stage1: 1.241  loss_cls_stage2: 0.3248  loss_box_reg_stage2: 1.49  loss_mask: 0.2789  loss_rpn_cls: 0.04199  loss_rpn_loc: 0.03943  time: 0.4144  data_time: 0.0040  lr: 0.0001103  max_mem: 7560M
[09/18 11:55:25] d2.utils.events INFO:  eta: 14:32:37  iter: 339  total_loss: 4.351  loss_cls_stage0: 0.3469  loss_box_reg_stage0: 0.5535  loss_cls_stage1: 0.3102  loss_box_reg_stage1: 1.119  loss_cls_stage2: 0.3006  loss_box_reg_stage2: 1.354  loss_mask: 0.2505  loss_rpn_cls: 0.0359  loss_rpn_loc: 0.03709  time: 0.4145  data_time: 0.0042  lr: 0.0001172  max_mem: 7560M
[09/18 11:55:33] d2.utils.events INFO:  eta: 14:33:12  iter: 359  total_loss: 4.455  loss_cls_stage0: 0.3463  loss_box_reg_stage0: 0.5709  loss_cls_stage1: 0.3128  loss_box_reg_stage1: 1.167  loss_cls_stage2: 0.3183  loss_box_reg_stage2: 1.317  loss_mask: 0.2693  loss_rpn_cls: 0.02926  loss_rpn_loc: 0.03862  time: 0.4152  data_time: 0.0042  lr: 0.00012411  max_mem: 7560M
[09/18 11:55:42] d2.utils.events INFO:  eta: 14:32:51  iter: 379  total_loss: 4.51  loss_cls_stage0: 0.3195  loss_box_reg_stage0: 0.5117  loss_cls_stage1: 0.2961  loss_box_reg_stage1: 1.2  loss_cls_stage2: 0.2917  loss_box_reg_stage2: 1.472  loss_mask: 0.2488  loss_rpn_cls: 0.03325  loss_rpn_loc: 0.03691  time: 0.4154  data_time: 0.0040  lr: 0.00013101  max_mem: 7560M
[09/18 11:55:50] d2.utils.events INFO:  eta: 14:32:55  iter: 399  total_loss: 4.324  loss_cls_stage0: 0.3322  loss_box_reg_stage0: 0.559  loss_cls_stage1: 0.3092  loss_box_reg_stage1: 1.169  loss_cls_stage2: 0.2847  loss_box_reg_stage2: 1.36  loss_mask: 0.2397  loss_rpn_cls: 0.02995  loss_rpn_loc: 0.03807  time: 0.4158  data_time: 0.0042  lr: 0.00013792  max_mem: 7560M
[09/18 11:55:58] d2.utils.events INFO:  eta: 14:32:19  iter: 419  total_loss: 4.16  loss_cls_stage0: 0.2892  loss_box_reg_stage0: 0.4936  loss_cls_stage1: 0.2878  loss_box_reg_stage1: 1.166  loss_cls_stage2: 0.2673  loss_box_reg_stage2: 1.444  loss_mask: 0.2131  loss_rpn_cls: 0.03421  loss_rpn_loc: 0.03726  time: 0.4160  data_time: 0.0044  lr: 0.00014482  max_mem: 7560M
[09/18 11:56:07] d2.utils.events INFO:  eta: 14:31:57  iter: 439  total_loss: 4.22  loss_cls_stage0: 0.3334  loss_box_reg_stage0: 0.5085  loss_cls_stage1: 0.3114  loss_box_reg_stage1: 1.12  loss_cls_stage2: 0.2829  loss_box_reg_stage2: 1.436  loss_mask: 0.214  loss_rpn_cls: 0.04239  loss_rpn_loc: 0.03415  time: 0.4161  data_time: 0.0043  lr: 0.00015173  max_mem: 7560M
[09/18 11:56:15] d2.utils.events INFO:  eta: 14:31:43  iter: 459  total_loss: 4.024  loss_cls_stage0: 0.284  loss_box_reg_stage0: 0.4631  loss_cls_stage1: 0.284  loss_box_reg_stage1: 1.076  loss_cls_stage2: 0.2681  loss_box_reg_stage2: 1.361  loss_mask: 0.1998  loss_rpn_cls: 0.02843  loss_rpn_loc: 0.03619  time: 0.4162  data_time: 0.0043  lr: 0.00015863  max_mem: 7560M
[09/18 11:56:24] d2.utils.events INFO:  eta: 14:31:35  iter: 479  total_loss: 4.215  loss_cls_stage0: 0.3178  loss_box_reg_stage0: 0.4807  loss_cls_stage1: 0.3146  loss_box_reg_stage1: 1.115  loss_cls_stage2: 0.2966  loss_box_reg_stage2: 1.383  loss_mask: 0.2108  loss_rpn_cls: 0.02793  loss_rpn_loc: 0.03533  time: 0.4166  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 11:56:32] d2.utils.events INFO:  eta: 14:30:45  iter: 499  total_loss: 4.269  loss_cls_stage0: 0.2954  loss_box_reg_stage0: 0.4614  loss_cls_stage1: 0.2661  loss_box_reg_stage1: 1.134  loss_cls_stage2: 0.2561  loss_box_reg_stage2: 1.499  loss_mask: 0.201  loss_rpn_cls: 0.02539  loss_rpn_loc: 0.035  time: 0.4165  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 11:56:41] d2.utils.events INFO:  eta: 14:31:21  iter: 519  total_loss: 4.088  loss_cls_stage0: 0.2662  loss_box_reg_stage0: 0.4697  loss_cls_stage1: 0.2582  loss_box_reg_stage1: 1.14  loss_cls_stage2: 0.2573  loss_box_reg_stage2: 1.426  loss_mask: 0.1736  loss_rpn_cls: 0.02329  loss_rpn_loc: 0.03165  time: 0.4168  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 11:56:49] d2.utils.events INFO:  eta: 14:31:21  iter: 539  total_loss: 3.808  loss_cls_stage0: 0.258  loss_box_reg_stage0: 0.4087  loss_cls_stage1: 0.2471  loss_box_reg_stage1: 1.059  loss_cls_stage2: 0.2377  loss_box_reg_stage2: 1.326  loss_mask: 0.172  loss_rpn_cls: 0.02484  loss_rpn_loc: 0.03696  time: 0.4170  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 11:56:58] d2.utils.events INFO:  eta: 14:31:12  iter: 559  total_loss: 4.111  loss_cls_stage0: 0.2955  loss_box_reg_stage0: 0.4875  loss_cls_stage1: 0.2633  loss_box_reg_stage1: 1.177  loss_cls_stage2: 0.2405  loss_box_reg_stage2: 1.373  loss_mask: 0.1912  loss_rpn_cls: 0.02592  loss_rpn_loc: 0.03672  time: 0.4172  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 11:57:06] d2.utils.events INFO:  eta: 14:30:36  iter: 579  total_loss: 3.663  loss_cls_stage0: 0.2401  loss_box_reg_stage0: 0.4248  loss_cls_stage1: 0.2359  loss_box_reg_stage1: 0.9994  loss_cls_stage2: 0.2121  loss_box_reg_stage2: 1.254  loss_mask: 0.1622  loss_rpn_cls: 0.02349  loss_rpn_loc: 0.03319  time: 0.4171  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 11:57:14] d2.utils.events INFO:  eta: 14:30:03  iter: 599  total_loss: 3.671  loss_cls_stage0: 0.2538  loss_box_reg_stage0: 0.3998  loss_cls_stage1: 0.2487  loss_box_reg_stage1: 1.03  loss_cls_stage2: 0.2091  loss_box_reg_stage2: 1.358  loss_mask: 0.1687  loss_rpn_cls: 0.01813  loss_rpn_loc: 0.03532  time: 0.4171  data_time: 0.0039  lr: 0.00016  max_mem: 7560M
[09/18 11:57:23] d2.utils.events INFO:  eta: 14:29:34  iter: 619  total_loss: 3.388  loss_cls_stage0: 0.2542  loss_box_reg_stage0: 0.4034  loss_cls_stage1: 0.2196  loss_box_reg_stage1: 0.942  loss_cls_stage2: 0.2065  loss_box_reg_stage2: 1.235  loss_mask: 0.1658  loss_rpn_cls: 0.02281  loss_rpn_loc: 0.03553  time: 0.4171  data_time: 0.0039  lr: 0.00016  max_mem: 7560M
[09/18 11:57:31] d2.utils.events INFO:  eta: 14:29:26  iter: 639  total_loss: 3.366  loss_cls_stage0: 0.2329  loss_box_reg_stage0: 0.3772  loss_cls_stage1: 0.2063  loss_box_reg_stage1: 0.9396  loss_cls_stage2: 0.1768  loss_box_reg_stage2: 1.292  loss_mask: 0.1536  loss_rpn_cls: 0.0225  loss_rpn_loc: 0.03226  time: 0.4175  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 11:57:40] d2.utils.events INFO:  eta: 14:29:14  iter: 659  total_loss: 3.306  loss_cls_stage0: 0.2229  loss_box_reg_stage0: 0.3777  loss_cls_stage1: 0.1796  loss_box_reg_stage1: 0.9093  loss_cls_stage2: 0.1771  loss_box_reg_stage2: 1.263  loss_mask: 0.1465  loss_rpn_cls: 0.02468  loss_rpn_loc: 0.03356  time: 0.4175  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 11:57:48] d2.utils.events INFO:  eta: 14:28:59  iter: 679  total_loss: 3.64  loss_cls_stage0: 0.231  loss_box_reg_stage0: 0.4015  loss_cls_stage1: 0.2109  loss_box_reg_stage1: 0.9679  loss_cls_stage2: 0.2124  loss_box_reg_stage2: 1.298  loss_mask: 0.1571  loss_rpn_cls: 0.02303  loss_rpn_loc: 0.03358  time: 0.4174  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 11:57:56] d2.utils.events INFO:  eta: 14:28:54  iter: 699  total_loss: 3.507  loss_cls_stage0: 0.258  loss_box_reg_stage0: 0.4148  loss_cls_stage1: 0.2264  loss_box_reg_stage1: 0.9717  loss_cls_stage2: 0.2104  loss_box_reg_stage2: 1.168  loss_mask: 0.1549  loss_rpn_cls: 0.02724  loss_rpn_loc: 0.03259  time: 0.4175  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 11:58:05] d2.utils.events INFO:  eta: 14:28:38  iter: 719  total_loss: 3.599  loss_cls_stage0: 0.2359  loss_box_reg_stage0: 0.4139  loss_cls_stage1: 0.2004  loss_box_reg_stage1: 0.9854  loss_cls_stage2: 0.1994  loss_box_reg_stage2: 1.304  loss_mask: 0.1441  loss_rpn_cls: 0.02257  loss_rpn_loc: 0.03004  time: 0.4174  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 11:58:13] d2.utils.events INFO:  eta: 14:28:24  iter: 739  total_loss: 3.386  loss_cls_stage0: 0.2255  loss_box_reg_stage0: 0.3728  loss_cls_stage1: 0.1971  loss_box_reg_stage1: 0.9688  loss_cls_stage2: 0.1903  loss_box_reg_stage2: 1.283  loss_mask: 0.1413  loss_rpn_cls: 0.01392  loss_rpn_loc: 0.03063  time: 0.4174  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 11:58:22] d2.utils.events INFO:  eta: 14:28:37  iter: 759  total_loss: 3.574  loss_cls_stage0: 0.2569  loss_box_reg_stage0: 0.4131  loss_cls_stage1: 0.2414  loss_box_reg_stage1: 0.9486  loss_cls_stage2: 0.2287  loss_box_reg_stage2: 1.227  loss_mask: 0.1377  loss_rpn_cls: 0.02021  loss_rpn_loc: 0.02806  time: 0.4178  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 11:58:30] d2.utils.events INFO:  eta: 14:28:13  iter: 779  total_loss: 3.171  loss_cls_stage0: 0.2317  loss_box_reg_stage0: 0.3553  loss_cls_stage1: 0.1918  loss_box_reg_stage1: 0.9118  loss_cls_stage2: 0.1819  loss_box_reg_stage2: 1.138  loss_mask: 0.1395  loss_rpn_cls: 0.02447  loss_rpn_loc: 0.03044  time: 0.4177  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 11:58:38] d2.utils.events INFO:  eta: 14:27:59  iter: 799  total_loss: 3.156  loss_cls_stage0: 0.2245  loss_box_reg_stage0: 0.385  loss_cls_stage1: 0.1882  loss_box_reg_stage1: 0.8789  loss_cls_stage2: 0.173  loss_box_reg_stage2: 1.068  loss_mask: 0.148  loss_rpn_cls: 0.02647  loss_rpn_loc: 0.03053  time: 0.4176  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 11:58:47] d2.utils.events INFO:  eta: 14:27:49  iter: 819  total_loss: 3.027  loss_cls_stage0: 0.2088  loss_box_reg_stage0: 0.3468  loss_cls_stage1: 0.18  loss_box_reg_stage1: 0.8515  loss_cls_stage2: 0.175  loss_box_reg_stage2: 1.159  loss_mask: 0.126  loss_rpn_cls: 0.01422  loss_rpn_loc: 0.02968  time: 0.4177  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 11:58:55] d2.utils.events INFO:  eta: 14:27:41  iter: 839  total_loss: 3.357  loss_cls_stage0: 0.2389  loss_box_reg_stage0: 0.3933  loss_cls_stage1: 0.2044  loss_box_reg_stage1: 0.9489  loss_cls_stage2: 0.1758  loss_box_reg_stage2: 1.212  loss_mask: 0.1562  loss_rpn_cls: 0.0188  loss_rpn_loc: 0.0296  time: 0.4177  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 11:59:04] d2.utils.events INFO:  eta: 14:27:36  iter: 859  total_loss: 3.326  loss_cls_stage0: 0.2127  loss_box_reg_stage0: 0.3786  loss_cls_stage1: 0.2051  loss_box_reg_stage1: 0.906  loss_cls_stage2: 0.1939  loss_box_reg_stage2: 1.168  loss_mask: 0.1311  loss_rpn_cls: 0.01968  loss_rpn_loc: 0.02844  time: 0.4178  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 11:59:12] d2.utils.events INFO:  eta: 14:27:28  iter: 879  total_loss: 2.986  loss_cls_stage0: 0.1964  loss_box_reg_stage0: 0.3314  loss_cls_stage1: 0.188  loss_box_reg_stage1: 0.8269  loss_cls_stage2: 0.1754  loss_box_reg_stage2: 1.077  loss_mask: 0.1288  loss_rpn_cls: 0.0171  loss_rpn_loc: 0.02836  time: 0.4179  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 11:59:20] d2.utils.events INFO:  eta: 14:27:27  iter: 899  total_loss: 3.02  loss_cls_stage0: 0.1979  loss_box_reg_stage0: 0.3178  loss_cls_stage1: 0.1885  loss_box_reg_stage1: 0.8081  loss_cls_stage2: 0.177  loss_box_reg_stage2: 1.134  loss_mask: 0.1264  loss_rpn_cls: 0.0161  loss_rpn_loc: 0.02666  time: 0.4180  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 11:59:29] d2.utils.events INFO:  eta: 14:27:30  iter: 919  total_loss: 3.31  loss_cls_stage0: 0.2049  loss_box_reg_stage0: 0.3376  loss_cls_stage1: 0.2  loss_box_reg_stage1: 0.896  loss_cls_stage2: 0.1997  loss_box_reg_stage2: 1.178  loss_mask: 0.1227  loss_rpn_cls: 0.01171  loss_rpn_loc: 0.02651  time: 0.4181  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 11:59:37] d2.utils.events INFO:  eta: 14:27:22  iter: 939  total_loss: 3.261  loss_cls_stage0: 0.2269  loss_box_reg_stage0: 0.3869  loss_cls_stage1: 0.2159  loss_box_reg_stage1: 0.9277  loss_cls_stage2: 0.1932  loss_box_reg_stage2: 1.18  loss_mask: 0.122  loss_rpn_cls: 0.01509  loss_rpn_loc: 0.02868  time: 0.4181  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 11:59:46] d2.utils.events INFO:  eta: 14:27:14  iter: 959  total_loss: 3.287  loss_cls_stage0: 0.2054  loss_box_reg_stage0: 0.3608  loss_cls_stage1: 0.1967  loss_box_reg_stage1: 0.9304  loss_cls_stage2: 0.1897  loss_box_reg_stage2: 1.189  loss_mask: 0.1214  loss_rpn_cls: 0.01416  loss_rpn_loc: 0.02733  time: 0.4181  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 11:59:54] d2.utils.events INFO:  eta: 14:27:00  iter: 979  total_loss: 2.981  loss_cls_stage0: 0.2059  loss_box_reg_stage0: 0.3601  loss_cls_stage1: 0.1945  loss_box_reg_stage1: 0.8488  loss_cls_stage2: 0.1809  loss_box_reg_stage2: 1.117  loss_mask: 0.1197  loss_rpn_cls: 0.02104  loss_rpn_loc: 0.03062  time: 0.4181  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:00:02] d2.utils.events INFO:  eta: 14:26:48  iter: 999  total_loss: 2.985  loss_cls_stage0: 0.1907  loss_box_reg_stage0: 0.3241  loss_cls_stage1: 0.1752  loss_box_reg_stage1: 0.7906  loss_cls_stage2: 0.1463  loss_box_reg_stage2: 1.094  loss_mask: 0.1177  loss_rpn_cls: 0.01696  loss_rpn_loc: 0.02998  time: 0.4181  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:00:11] d2.utils.events INFO:  eta: 14:26:50  iter: 1019  total_loss: 3.001  loss_cls_stage0: 0.1964  loss_box_reg_stage0: 0.3498  loss_cls_stage1: 0.1802  loss_box_reg_stage1: 0.8193  loss_cls_stage2: 0.1417  loss_box_reg_stage2: 1.073  loss_mask: 0.1309  loss_rpn_cls: 0.02084  loss_rpn_loc: 0.02915  time: 0.4180  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:00:19] d2.utils.events INFO:  eta: 14:27:11  iter: 1039  total_loss: 2.829  loss_cls_stage0: 0.1999  loss_box_reg_stage0: 0.3176  loss_cls_stage1: 0.1825  loss_box_reg_stage1: 0.7768  loss_cls_stage2: 0.1684  loss_box_reg_stage2: 1.062  loss_mask: 0.118  loss_rpn_cls: 0.01352  loss_rpn_loc: 0.02595  time: 0.4180  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:00:27] d2.utils.events INFO:  eta: 14:27:23  iter: 1059  total_loss: 3.202  loss_cls_stage0: 0.2193  loss_box_reg_stage0: 0.3549  loss_cls_stage1: 0.1888  loss_box_reg_stage1: 0.8588  loss_cls_stage2: 0.191  loss_box_reg_stage2: 1.187  loss_mask: 0.123  loss_rpn_cls: 0.01781  loss_rpn_loc: 0.02689  time: 0.4179  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:00:36] d2.utils.events INFO:  eta: 14:26:25  iter: 1079  total_loss: 2.951  loss_cls_stage0: 0.195  loss_box_reg_stage0: 0.333  loss_cls_stage1: 0.1617  loss_box_reg_stage1: 0.8269  loss_cls_stage2: 0.1476  loss_box_reg_stage2: 1.118  loss_mask: 0.1206  loss_rpn_cls: 0.01081  loss_rpn_loc: 0.02749  time: 0.4178  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:00:44] d2.utils.events INFO:  eta: 14:25:41  iter: 1099  total_loss: 3.332  loss_cls_stage0: 0.2108  loss_box_reg_stage0: 0.3611  loss_cls_stage1: 0.1859  loss_box_reg_stage1: 0.9269  loss_cls_stage2: 0.1748  loss_box_reg_stage2: 1.245  loss_mask: 0.1216  loss_rpn_cls: 0.01386  loss_rpn_loc: 0.02937  time: 0.4177  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:00:52] d2.utils.events INFO:  eta: 14:25:47  iter: 1119  total_loss: 3.196  loss_cls_stage0: 0.2248  loss_box_reg_stage0: 0.3748  loss_cls_stage1: 0.1969  loss_box_reg_stage1: 0.8849  loss_cls_stage2: 0.1994  loss_box_reg_stage2: 1.15  loss_mask: 0.1248  loss_rpn_cls: 0.01674  loss_rpn_loc: 0.03053  time: 0.4177  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:01:01] d2.utils.events INFO:  eta: 14:25:34  iter: 1139  total_loss: 2.8  loss_cls_stage0: 0.1822  loss_box_reg_stage0: 0.3163  loss_cls_stage1: 0.169  loss_box_reg_stage1: 0.7908  loss_cls_stage2: 0.1659  loss_box_reg_stage2: 1.062  loss_mask: 0.1111  loss_rpn_cls: 0.01178  loss_rpn_loc: 0.02668  time: 0.4178  data_time: 0.0039  lr: 0.00016  max_mem: 7560M
[09/18 12:01:09] d2.utils.events INFO:  eta: 14:25:51  iter: 1159  total_loss: 3.141  loss_cls_stage0: 0.2131  loss_box_reg_stage0: 0.3486  loss_cls_stage1: 0.166  loss_box_reg_stage1: 0.8486  loss_cls_stage2: 0.1687  loss_box_reg_stage2: 1.166  loss_mask: 0.1181  loss_rpn_cls: 0.01347  loss_rpn_loc: 0.02562  time: 0.4179  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:01:18] d2.utils.events INFO:  eta: 14:26:02  iter: 1179  total_loss: 3.034  loss_cls_stage0: 0.236  loss_box_reg_stage0: 0.3717  loss_cls_stage1: 0.1871  loss_box_reg_stage1: 0.8283  loss_cls_stage2: 0.1594  loss_box_reg_stage2: 1.037  loss_mask: 0.1213  loss_rpn_cls: 0.01787  loss_rpn_loc: 0.02702  time: 0.4180  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:01:26] d2.utils.events INFO:  eta: 14:25:38  iter: 1199  total_loss: 3.061  loss_cls_stage0: 0.2097  loss_box_reg_stage0: 0.336  loss_cls_stage1: 0.1802  loss_box_reg_stage1: 0.8357  loss_cls_stage2: 0.1542  loss_box_reg_stage2: 1.165  loss_mask: 0.1046  loss_rpn_cls: 0.01803  loss_rpn_loc: 0.02466  time: 0.4180  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:01:35] d2.utils.events INFO:  eta: 14:25:04  iter: 1219  total_loss: 2.983  loss_cls_stage0: 0.2079  loss_box_reg_stage0: 0.3303  loss_cls_stage1: 0.1679  loss_box_reg_stage1: 0.8268  loss_cls_stage2: 0.1593  loss_box_reg_stage2: 1.161  loss_mask: 0.1098  loss_rpn_cls: 0.01771  loss_rpn_loc: 0.02792  time: 0.4180  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:01:43] d2.utils.events INFO:  eta: 14:24:36  iter: 1239  total_loss: 3.18  loss_cls_stage0: 0.2071  loss_box_reg_stage0: 0.3383  loss_cls_stage1: 0.1819  loss_box_reg_stage1: 0.8701  loss_cls_stage2: 0.1599  loss_box_reg_stage2: 1.209  loss_mask: 0.1034  loss_rpn_cls: 0.01082  loss_rpn_loc: 0.02857  time: 0.4181  data_time: 0.0046  lr: 0.00016  max_mem: 7560M
[09/18 12:01:51] d2.utils.events INFO:  eta: 14:24:14  iter: 1259  total_loss: 2.922  loss_cls_stage0: 0.1743  loss_box_reg_stage0: 0.3033  loss_cls_stage1: 0.1588  loss_box_reg_stage1: 0.7779  loss_cls_stage2: 0.1676  loss_box_reg_stage2: 1.157  loss_mask: 0.1107  loss_rpn_cls: 0.01286  loss_rpn_loc: 0.027  time: 0.4181  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:02:00] d2.utils.events INFO:  eta: 14:23:06  iter: 1279  total_loss: 2.979  loss_cls_stage0: 0.2013  loss_box_reg_stage0: 0.3477  loss_cls_stage1: 0.1727  loss_box_reg_stage1: 0.815  loss_cls_stage2: 0.1569  loss_box_reg_stage2: 1.074  loss_mask: 0.122  loss_rpn_cls: 0.01142  loss_rpn_loc: 0.0273  time: 0.4181  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:02:08] d2.utils.events INFO:  eta: 14:22:53  iter: 1299  total_loss: 2.962  loss_cls_stage0: 0.1823  loss_box_reg_stage0: 0.3072  loss_cls_stage1: 0.1776  loss_box_reg_stage1: 0.8323  loss_cls_stage2: 0.1724  loss_box_reg_stage2: 1.131  loss_mask: 0.1092  loss_rpn_cls: 0.0111  loss_rpn_loc: 0.02301  time: 0.4181  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:02:17] d2.utils.events INFO:  eta: 14:22:33  iter: 1319  total_loss: 2.878  loss_cls_stage0: 0.1959  loss_box_reg_stage0: 0.3362  loss_cls_stage1: 0.1815  loss_box_reg_stage1: 0.7956  loss_cls_stage2: 0.1615  loss_box_reg_stage2: 1.027  loss_mask: 0.1101  loss_rpn_cls: 0.01259  loss_rpn_loc: 0.02761  time: 0.4181  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:02:25] d2.utils.events INFO:  eta: 14:22:45  iter: 1339  total_loss: 2.981  loss_cls_stage0: 0.2108  loss_box_reg_stage0: 0.3146  loss_cls_stage1: 0.1676  loss_box_reg_stage1: 0.8451  loss_cls_stage2: 0.1699  loss_box_reg_stage2: 1.138  loss_mask: 0.1095  loss_rpn_cls: 0.01226  loss_rpn_loc: 0.02631  time: 0.4183  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:02:34] d2.utils.events INFO:  eta: 14:22:18  iter: 1359  total_loss: 2.921  loss_cls_stage0: 0.1698  loss_box_reg_stage0: 0.3148  loss_cls_stage1: 0.1418  loss_box_reg_stage1: 0.8019  loss_cls_stage2: 0.1271  loss_box_reg_stage2: 1.076  loss_mask: 0.1076  loss_rpn_cls: 0.01123  loss_rpn_loc: 0.02663  time: 0.4184  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:02:42] d2.utils.events INFO:  eta: 14:23:00  iter: 1379  total_loss: 3.037  loss_cls_stage0: 0.1857  loss_box_reg_stage0: 0.3312  loss_cls_stage1: 0.1683  loss_box_reg_stage1: 0.8852  loss_cls_stage2: 0.1553  loss_box_reg_stage2: 1.057  loss_mask: 0.1124  loss_rpn_cls: 0.01127  loss_rpn_loc: 0.02619  time: 0.4185  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:02:51] d2.utils.events INFO:  eta: 14:22:44  iter: 1399  total_loss: 3.041  loss_cls_stage0: 0.2013  loss_box_reg_stage0: 0.3629  loss_cls_stage1: 0.1883  loss_box_reg_stage1: 0.8273  loss_cls_stage2: 0.1716  loss_box_reg_stage2: 1.033  loss_mask: 0.1131  loss_rpn_cls: 0.01411  loss_rpn_loc: 0.02677  time: 0.4185  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:02:59] d2.utils.events INFO:  eta: 14:22:15  iter: 1419  total_loss: 3.129  loss_cls_stage0: 0.1897  loss_box_reg_stage0: 0.3577  loss_cls_stage1: 0.1694  loss_box_reg_stage1: 0.8708  loss_cls_stage2: 0.1622  loss_box_reg_stage2: 1.16  loss_mask: 0.1145  loss_rpn_cls: 0.01379  loss_rpn_loc: 0.02887  time: 0.4185  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:03:04] detectron2 INFO: Rank of current process: 0. World size: 1
[09/18 12:03:04] detectron2 INFO: Environment info:
----------------------  -------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.10 (default, May 26 2023, 14:05:08) [GCC 9.4.0]
numpy                   1.23.5
detectron2              0.6 @/home/hoenig/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 9.4
CUDA compiler           CUDA 11.6
detectron2 arch flags   8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          525.125.06
CUDA_HOME               /usr/local/cuda
Pillow                  9.3.0
torchvision             0.13.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221122
iopath                  0.1.9
cv2                     4.7.0
----------------------  -------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/18 12:03:04] detectron2 INFO: Command line arguments: Namespace(config_file='', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[09/18 12:03:04] d2.config.lazy WARNING: The config contains objects that cannot serialize to a valid yaml. ./mvit2_lmo_output/config.yaml is human-readable but cannot be loaded.
[09/18 12:03:04] d2.config.lazy WARNING: Config is saved using cloudpickle at ./mvit2_lmo_output/config.yaml.pkl.
[09/18 12:03:04] detectron2 INFO: Full config saved to ./mvit2_lmo_output/config.yaml
[09/18 12:03:04] d2.utils.env INFO: Using a generated random seed 7190260
[09/18 12:03:05] detectron2 INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): MViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      )
      (blocks): ModuleList(
        (0): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=96, out_features=192, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (2): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (3): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=192, out_features=384, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (4): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (6): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.107)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.120)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.133)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.147)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (12): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.160)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (13): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.173)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (14): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.187)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=384, out_features=768, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (15): MultiScaleBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (scale2_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (scale3_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (scale4_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (scale5_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): CascadeROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): ModuleList(
      (0): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (1): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (2): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
    )
    (box_predictor): ModuleList(
      (0): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=9, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (1): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=9, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (2): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=9, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 8, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[09/18 12:03:08] d2.utils.events INFO:  eta: 14:23:05  iter: 1439  total_loss: 3.192  loss_cls_stage0: 0.2194  loss_box_reg_stage0: 0.3726  loss_cls_stage1: 0.1851  loss_box_reg_stage1: 0.9059  loss_cls_stage2: 0.1711  loss_box_reg_stage2: 1.194  loss_mask: 0.1196  loss_rpn_cls: 0.01537  loss_rpn_loc: 0.02995  time: 0.4188  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:03:10] d2.data.datasets.coco INFO: Loading datasets/BOP_DATASETS/lmo/lmo_annotations_train.json takes 3.98 seconds.
[09/18 12:03:10] d2.data.datasets.coco INFO: Loaded 50000 images in COCO format from datasets/BOP_DATASETS/lmo/lmo_annotations_train.json
[09/18 12:03:12] d2.data.build INFO: Removed 15 images with no usable annotations. 49985 images left.
[09/18 12:03:13] d2.data.build INFO: Distribution of instances among all 8 categories:
[36m|  category  | #instances   | category   | #instances   | category   | #instances   |
|:----------:|:-------------|:-----------|:-------------|:-----------|:-------------|
|     1      | 39818        | 2          | 43308        | 3          | 41854        |
|     4      | 44001        | 5          | 40269        | 6          | 41817        |
|     7      | 41826        | 8          | 41544        |            |              |
|   total    | 334437       |            |              |            |              |[0m
[09/18 12:03:13] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: []
[09/18 12:03:13] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[09/18 12:03:13] d2.data.common INFO: Serializing 49985 elements to byte tensors and concatenating them all ...
[09/18 12:03:14] d2.data.common INFO: Serialized dataset takes 161.23 MiB
[09/18 12:03:15] fvcore.common.checkpoint INFO: [Checkpointer] Loading from detectron2://ImageNetPretrained/mvitv2/MViTv2_S_in1k.pyth ...
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv1.norm.bias in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv1.norm.weight in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv2.norm.bias in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv2.norm.weight in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv3.norm.bias in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv3.norm.weight in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv4.norm.bias in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv4.norm.weight in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv1.norm.bias in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv1.norm.weight in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv2.norm.bias in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv2.norm.weight in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv3.norm.bias in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv3.norm.weight in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv4.norm.bias in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv4.norm.weight in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv1.norm.bias in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv1.norm.weight in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv2.norm.bias in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv2.norm.weight in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv3.norm.bias in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv3.norm.weight in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv4.norm.bias in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv4.norm.weight in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn1.norm.bias in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn1.norm.weight in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn2.norm.bias in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn2.norm.weight in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn3.norm.bias in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn3.norm.weight in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn4.norm.bias in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn4.norm.weight in model is torch.Size([256]).
[09/18 12:03:15] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:03:15] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone.bottom_up:
| Names in Model      | Names in Checkpoint                                                                                                                                                                                    | Shapes                                                                                                                   |
|:--------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|
| blocks.0.attn.*     | blocks.0.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (96,) (96,96) (288,) (288,96) (111,96) (111,96)     |
| blocks.0.mlp.fc1.*  | blocks.0.mlp.fc1.{bias,weight}                                                                                                                                                                         | (384,) (384,96)                                                                                                          |
| blocks.0.mlp.fc2.*  | blocks.0.mlp.fc2.{bias,weight}                                                                                                                                                                         | (96,) (96,384)                                                                                                           |
| blocks.0.norm1.*    | blocks.0.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.0.norm2.*    | blocks.0.norm2.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.attn.*     | blocks.1.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,96) (55,96) (55,96)    |
| blocks.1.mlp.fc1.*  | blocks.1.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.1.mlp.fc2.*  | blocks.1.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.1.norm1.*    | blocks.1.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.norm2.*    | blocks.1.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.1.proj.*     | blocks.1.proj.{bias,weight}                                                                                                                                                                            | (192,) (192,96)                                                                                                          |
| blocks.10.attn.*    | blocks.10.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.10.mlp.fc1.* | blocks.10.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.10.mlp.fc2.* | blocks.10.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.10.norm1.*   | blocks.10.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.10.norm2.*   | blocks.10.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.attn.*    | blocks.11.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.11.mlp.fc1.* | blocks.11.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.11.mlp.fc2.* | blocks.11.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.11.norm1.*   | blocks.11.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.norm2.*   | blocks.11.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.attn.*    | blocks.12.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.12.mlp.fc1.* | blocks.12.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.12.mlp.fc2.* | blocks.12.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.12.norm1.*   | blocks.12.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.norm2.*   | blocks.12.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.attn.*    | blocks.13.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.13.mlp.fc1.* | blocks.13.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.13.mlp.fc2.* | blocks.13.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.13.norm1.*   | blocks.13.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.norm2.*   | blocks.13.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.attn.*    | blocks.14.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,384) (27,96) (27,96) |
| blocks.14.mlp.fc1.* | blocks.14.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.14.mlp.fc2.* | blocks.14.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.14.norm1.*   | blocks.14.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.norm2.*   | blocks.14.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.14.proj.*    | blocks.14.proj.{bias,weight}                                                                                                                                                                           | (768,) (768,384)                                                                                                         |
| blocks.15.attn.*    | blocks.15.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,768) (13,96) (13,96) |
| blocks.15.mlp.fc1.* | blocks.15.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.15.mlp.fc2.* | blocks.15.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.15.norm1.*   | blocks.15.norm1.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.15.norm2.*   | blocks.15.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.2.attn.*     | blocks.2.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,192) (55,96) (55,96)   |
| blocks.2.mlp.fc1.*  | blocks.2.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.2.mlp.fc2.*  | blocks.2.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.2.norm1.*    | blocks.2.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.2.norm2.*    | blocks.2.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.3.attn.*     | blocks.3.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,192) (55,96) (55,96) |
| blocks.3.mlp.fc1.*  | blocks.3.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.3.mlp.fc2.*  | blocks.3.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.3.norm1.*    | blocks.3.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.3.norm2.*    | blocks.3.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.3.proj.*     | blocks.3.proj.{bias,weight}                                                                                                                                                                            | (384,) (384,192)                                                                                                         |
| blocks.4.attn.*     | blocks.4.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.4.mlp.fc1.*  | blocks.4.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.4.mlp.fc2.*  | blocks.4.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.4.norm1.*    | blocks.4.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.4.norm2.*    | blocks.4.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.5.attn.*     | blocks.5.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.5.mlp.fc1.*  | blocks.5.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.5.mlp.fc2.*  | blocks.5.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.5.norm1.*    | blocks.5.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.5.norm2.*    | blocks.5.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.6.attn.*     | blocks.6.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.6.mlp.fc1.*  | blocks.6.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.6.mlp.fc2.*  | blocks.6.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.6.norm1.*    | blocks.6.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.6.norm2.*    | blocks.6.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.attn.*     | blocks.7.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.7.mlp.fc1.*  | blocks.7.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.7.mlp.fc2.*  | blocks.7.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.7.norm1.*    | blocks.7.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.norm2.*    | blocks.7.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.attn.*     | blocks.8.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.8.mlp.fc1.*  | blocks.8.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.8.mlp.fc2.*  | blocks.8.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.8.norm1.*    | blocks.8.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.norm2.*    | blocks.8.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.attn.*     | blocks.9.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.9.mlp.fc1.*  | blocks.9.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.9.mlp.fc2.*  | blocks.9.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.9.norm1.*    | blocks.9.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.norm2.*    | blocks.9.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| patch_embed.proj.*  | patch_embed.proj.{bias,weight}                                                                                                                                                                         | (96,) (96,3,7,7)                                                                                                         |
[09/18 12:03:16] d2.utils.events INFO:  eta: 14:23:00  iter: 1459  total_loss: 2.599  loss_cls_stage0: 0.1804  loss_box_reg_stage0: 0.2944  loss_cls_stage1: 0.1484  loss_box_reg_stage1: 0.7263  loss_cls_stage2: 0.1453  loss_box_reg_stage2: 0.9744  loss_mask: 0.1038  loss_rpn_cls: 0.0143  loss_rpn_loc: 0.02533  time: 0.4188  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:03:25] d2.utils.events INFO:  eta: 14:22:43  iter: 1479  total_loss: 2.924  loss_cls_stage0: 0.1875  loss_box_reg_stage0: 0.3234  loss_cls_stage1: 0.1561  loss_box_reg_stage1: 0.818  loss_cls_stage2: 0.1425  loss_box_reg_stage2: 1.128  loss_mask: 0.1076  loss_rpn_cls: 0.01403  loss_rpn_loc: 0.02453  time: 0.4187  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:03:33] d2.utils.events INFO:  eta: 14:22:34  iter: 1499  total_loss: 3.036  loss_cls_stage0: 0.1963  loss_box_reg_stage0: 0.326  loss_cls_stage1: 0.1857  loss_box_reg_stage1: 0.8484  loss_cls_stage2: 0.167  loss_box_reg_stage2: 1.185  loss_mask: 0.1102  loss_rpn_cls: 0.01591  loss_rpn_loc: 0.02491  time: 0.4188  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:03:41] d2.utils.events INFO:  eta: 14:22:17  iter: 1519  total_loss: 2.51  loss_cls_stage0: 0.1675  loss_box_reg_stage0: 0.2885  loss_cls_stage1: 0.1464  loss_box_reg_stage1: 0.7459  loss_cls_stage2: 0.129  loss_box_reg_stage2: 1.022  loss_mask: 0.1035  loss_rpn_cls: 0.01599  loss_rpn_loc: 0.0252  time: 0.4188  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:03:50] d2.utils.events INFO:  eta: 14:21:17  iter: 1539  total_loss: 2.569  loss_cls_stage0: 0.1574  loss_box_reg_stage0: 0.2964  loss_cls_stage1: 0.125  loss_box_reg_stage1: 0.7411  loss_cls_stage2: 0.1139  loss_box_reg_stage2: 1.071  loss_mask: 0.1046  loss_rpn_cls: 0.01336  loss_rpn_loc: 0.02478  time: 0.4187  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:03:58] d2.utils.events INFO:  eta: 14:20:52  iter: 1559  total_loss: 2.848  loss_cls_stage0: 0.1872  loss_box_reg_stage0: 0.3196  loss_cls_stage1: 0.1537  loss_box_reg_stage1: 0.7753  loss_cls_stage2: 0.1277  loss_box_reg_stage2: 1.042  loss_mask: 0.1002  loss_rpn_cls: 0.01361  loss_rpn_loc: 0.02629  time: 0.4187  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:04:06] d2.utils.events INFO:  eta: 14:20:39  iter: 1579  total_loss: 2.692  loss_cls_stage0: 0.1718  loss_box_reg_stage0: 0.2928  loss_cls_stage1: 0.1415  loss_box_reg_stage1: 0.7282  loss_cls_stage2: 0.134  loss_box_reg_stage2: 1.026  loss_mask: 0.09874  loss_rpn_cls: 0.008917  loss_rpn_loc: 0.02364  time: 0.4186  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:04:15] d2.utils.events INFO:  eta: 14:20:19  iter: 1599  total_loss: 2.726  loss_cls_stage0: 0.1776  loss_box_reg_stage0: 0.3351  loss_cls_stage1: 0.1337  loss_box_reg_stage1: 0.7666  loss_cls_stage2: 0.1202  loss_box_reg_stage2: 1.03  loss_mask: 0.1034  loss_rpn_cls: 0.01214  loss_rpn_loc: 0.02324  time: 0.4185  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:04:23] d2.utils.events INFO:  eta: 14:20:11  iter: 1619  total_loss: 2.528  loss_cls_stage0: 0.1589  loss_box_reg_stage0: 0.3084  loss_cls_stage1: 0.122  loss_box_reg_stage1: 0.7184  loss_cls_stage2: 0.1307  loss_box_reg_stage2: 1.055  loss_mask: 0.09669  loss_rpn_cls: 0.01231  loss_rpn_loc: 0.02599  time: 0.4185  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:04:31] d2.utils.events INFO:  eta: 14:19:50  iter: 1639  total_loss: 2.664  loss_cls_stage0: 0.1732  loss_box_reg_stage0: 0.3102  loss_cls_stage1: 0.1302  loss_box_reg_stage1: 0.7425  loss_cls_stage2: 0.1293  loss_box_reg_stage2: 1.007  loss_mask: 0.09895  loss_rpn_cls: 0.01295  loss_rpn_loc: 0.02422  time: 0.4185  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:04:40] d2.utils.events INFO:  eta: 14:19:47  iter: 1659  total_loss: 2.616  loss_cls_stage0: 0.1724  loss_box_reg_stage0: 0.2893  loss_cls_stage1: 0.1385  loss_box_reg_stage1: 0.7541  loss_cls_stage2: 0.1205  loss_box_reg_stage2: 0.9975  loss_mask: 0.09948  loss_rpn_cls: 0.01317  loss_rpn_loc: 0.02364  time: 0.4185  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:04:48] d2.utils.events INFO:  eta: 14:19:58  iter: 1679  total_loss: 2.665  loss_cls_stage0: 0.1693  loss_box_reg_stage0: 0.2831  loss_cls_stage1: 0.1496  loss_box_reg_stage1: 0.7606  loss_cls_stage2: 0.1447  loss_box_reg_stage2: 1.094  loss_mask: 0.0999  loss_rpn_cls: 0.01037  loss_rpn_loc: 0.02264  time: 0.4185  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:04:57] d2.utils.events INFO:  eta: 14:19:50  iter: 1699  total_loss: 2.468  loss_cls_stage0: 0.1521  loss_box_reg_stage0: 0.2868  loss_cls_stage1: 0.1269  loss_box_reg_stage1: 0.6826  loss_cls_stage2: 0.1275  loss_box_reg_stage2: 0.9329  loss_mask: 0.09661  loss_rpn_cls: 0.007757  loss_rpn_loc: 0.02472  time: 0.4186  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:05:05] d2.utils.events INFO:  eta: 14:20:24  iter: 1719  total_loss: 2.484  loss_cls_stage0: 0.1624  loss_box_reg_stage0: 0.2713  loss_cls_stage1: 0.1439  loss_box_reg_stage1: 0.6725  loss_cls_stage2: 0.1236  loss_box_reg_stage2: 0.9749  loss_mask: 0.09048  loss_rpn_cls: 0.007173  loss_rpn_loc: 0.02377  time: 0.4187  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:05:14] d2.utils.events INFO:  eta: 14:19:55  iter: 1739  total_loss: 2.652  loss_cls_stage0: 0.1656  loss_box_reg_stage0: 0.2859  loss_cls_stage1: 0.1327  loss_box_reg_stage1: 0.7786  loss_cls_stage2: 0.1443  loss_box_reg_stage2: 0.9788  loss_mask: 0.1028  loss_rpn_cls: 0.009589  loss_rpn_loc: 0.02586  time: 0.4187  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:05:22] d2.utils.events INFO:  eta: 14:19:25  iter: 1759  total_loss: 2.697  loss_cls_stage0: 0.1817  loss_box_reg_stage0: 0.3159  loss_cls_stage1: 0.1554  loss_box_reg_stage1: 0.7837  loss_cls_stage2: 0.1417  loss_box_reg_stage2: 0.9999  loss_mask: 0.1042  loss_rpn_cls: 0.01016  loss_rpn_loc: 0.0237  time: 0.4187  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:05:30] d2.utils.events INFO:  eta: 14:19:20  iter: 1779  total_loss: 2.548  loss_cls_stage0: 0.1543  loss_box_reg_stage0: 0.3022  loss_cls_stage1: 0.1247  loss_box_reg_stage1: 0.721  loss_cls_stage2: 0.132  loss_box_reg_stage2: 0.8953  loss_mask: 0.09819  loss_rpn_cls: 0.01024  loss_rpn_loc: 0.02345  time: 0.4186  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:05:39] d2.utils.events INFO:  eta: 14:19:14  iter: 1799  total_loss: 2.963  loss_cls_stage0: 0.174  loss_box_reg_stage0: 0.3274  loss_cls_stage1: 0.1588  loss_box_reg_stage1: 0.8321  loss_cls_stage2: 0.1533  loss_box_reg_stage2: 1.033  loss_mask: 0.09757  loss_rpn_cls: 0.01274  loss_rpn_loc: 0.02416  time: 0.4186  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:05:47] d2.utils.events INFO:  eta: 14:19:15  iter: 1819  total_loss: 2.405  loss_cls_stage0: 0.1478  loss_box_reg_stage0: 0.2792  loss_cls_stage1: 0.1186  loss_box_reg_stage1: 0.6919  loss_cls_stage2: 0.1021  loss_box_reg_stage2: 0.9383  loss_mask: 0.09314  loss_rpn_cls: 0.01397  loss_rpn_loc: 0.02319  time: 0.4187  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:05:55] d2.utils.events INFO:  eta: 14:18:51  iter: 1839  total_loss: 2.417  loss_cls_stage0: 0.1711  loss_box_reg_stage0: 0.3008  loss_cls_stage1: 0.1548  loss_box_reg_stage1: 0.6933  loss_cls_stage2: 0.1168  loss_box_reg_stage2: 0.9161  loss_mask: 0.1027  loss_rpn_cls: 0.0129  loss_rpn_loc: 0.02442  time: 0.4186  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:06:04] d2.utils.events INFO:  eta: 14:18:45  iter: 1859  total_loss: 2.6  loss_cls_stage0: 0.16  loss_box_reg_stage0: 0.297  loss_cls_stage1: 0.1329  loss_box_reg_stage1: 0.7164  loss_cls_stage2: 0.128  loss_box_reg_stage2: 0.9061  loss_mask: 0.09434  loss_rpn_cls: 0.01171  loss_rpn_loc: 0.02466  time: 0.4187  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:06:12] d2.utils.events INFO:  eta: 14:18:50  iter: 1879  total_loss: 2.582  loss_cls_stage0: 0.1645  loss_box_reg_stage0: 0.2781  loss_cls_stage1: 0.1429  loss_box_reg_stage1: 0.7171  loss_cls_stage2: 0.1311  loss_box_reg_stage2: 1.033  loss_mask: 0.09095  loss_rpn_cls: 0.009903  loss_rpn_loc: 0.02442  time: 0.4187  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:06:21] d2.utils.events INFO:  eta: 14:18:26  iter: 1899  total_loss: 2.518  loss_cls_stage0: 0.1547  loss_box_reg_stage0: 0.2803  loss_cls_stage1: 0.1114  loss_box_reg_stage1: 0.7271  loss_cls_stage2: 0.1154  loss_box_reg_stage2: 0.9556  loss_mask: 0.1017  loss_rpn_cls: 0.01502  loss_rpn_loc: 0.02284  time: 0.4187  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:06:29] d2.utils.events INFO:  eta: 14:17:47  iter: 1919  total_loss: 2.504  loss_cls_stage0: 0.1673  loss_box_reg_stage0: 0.2979  loss_cls_stage1: 0.1335  loss_box_reg_stage1: 0.736  loss_cls_stage2: 0.1313  loss_box_reg_stage2: 0.9997  loss_mask: 0.09571  loss_rpn_cls: 0.009386  loss_rpn_loc: 0.02477  time: 0.4186  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:06:37] d2.utils.events INFO:  eta: 14:17:39  iter: 1939  total_loss: 2.749  loss_cls_stage0: 0.1917  loss_box_reg_stage0: 0.3271  loss_cls_stage1: 0.1577  loss_box_reg_stage1: 0.7826  loss_cls_stage2: 0.1574  loss_box_reg_stage2: 1.055  loss_mask: 0.09904  loss_rpn_cls: 0.01515  loss_rpn_loc: 0.02709  time: 0.4186  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:06:46] d2.utils.events INFO:  eta: 14:17:41  iter: 1959  total_loss: 2.543  loss_cls_stage0: 0.1623  loss_box_reg_stage0: 0.2892  loss_cls_stage1: 0.1221  loss_box_reg_stage1: 0.6902  loss_cls_stage2: 0.1345  loss_box_reg_stage2: 0.9155  loss_mask: 0.09765  loss_rpn_cls: 0.007188  loss_rpn_loc: 0.02189  time: 0.4187  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:06:53] detectron2 INFO: Rank of current process: 0. World size: 1
[09/18 12:06:54] detectron2 INFO: Environment info:
----------------------  -------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.10 (default, May 26 2023, 14:05:08) [GCC 9.4.0]
numpy                   1.23.5
detectron2              0.6 @/home/hoenig/.local/lib/python3.8/site-packages/detectron2
Compiler                GCC 9.4
CUDA compiler           CUDA 11.6
detectron2 arch flags   8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.12.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version          525.125.06
CUDA_HOME               /usr/local/cuda
Pillow                  9.3.0
torchvision             0.13.0+cu116 @/home/hoenig/.local/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221122
iopath                  0.1.9
cv2                     4.7.0
----------------------  -------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/18 12:06:54] detectron2 INFO: Command line arguments: Namespace(config_file='', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[09/18 12:06:54] d2.config.lazy WARNING: The config contains objects that cannot serialize to a valid yaml. ./mvit2_lmo_output/config.yaml is human-readable but cannot be loaded.
[09/18 12:06:54] d2.config.lazy WARNING: Config is saved using cloudpickle at ./mvit2_lmo_output/config.yaml.pkl.
[09/18 12:06:54] detectron2 INFO: Full config saved to ./mvit2_lmo_output/config.yaml
[09/18 12:06:54] d2.utils.env INFO: Using a generated random seed 56984105
[09/18 12:06:54] d2.utils.events INFO:  eta: 14:17:53  iter: 1979  total_loss: 2.525  loss_cls_stage0: 0.1773  loss_box_reg_stage0: 0.3135  loss_cls_stage1: 0.1407  loss_box_reg_stage1: 0.6931  loss_cls_stage2: 0.1246  loss_box_reg_stage2: 0.9386  loss_mask: 0.09496  loss_rpn_cls: 0.008726  loss_rpn_loc: 0.02342  time: 0.4188  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:06:55] detectron2 INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): MViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      )
      (blocks): ModuleList(
        (0): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MultiScaleBlock(
          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=96, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=96, out_features=192, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (2): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (3): MultiScaleBlock(
          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=192, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=192, out_features=384, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (4): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (6): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.107)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.120)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.133)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.147)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (12): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.160)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (13): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.173)
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (14): MultiScaleBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=384, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.187)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (proj): Linear(in_features=384, out_features=768, bias=True)
          (pool_skip): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        )
        (15): MultiScaleBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (pool_q): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_k): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
            (pool_v): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (scale2_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (scale3_norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (scale4_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (scale5_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): CascadeROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): ModuleList(
      (0): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (1): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (2): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
    )
    (box_predictor): ModuleList(
      (0): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=9, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (1): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=9, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (2): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=9, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 8, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[09/18 12:07:00] d2.data.datasets.coco INFO: Loading datasets/BOP_DATASETS/lmo/lmo_annotations_train.json takes 3.93 seconds.
[09/18 12:07:00] d2.data.datasets.coco INFO: Loaded 50000 images in COCO format from datasets/BOP_DATASETS/lmo/lmo_annotations_train.json
[09/18 12:07:02] d2.data.build INFO: Removed 15 images with no usable annotations. 49985 images left.
[09/18 12:07:03] d2.data.build INFO: Distribution of instances among all 8 categories:
[36m|  category  | #instances   | category   | #instances   | category   | #instances   |
|:----------:|:-------------|:-----------|:-------------|:-----------|:-------------|
|     1      | 39818        | 2          | 43308        | 3          | 41854        |
|     4      | 44001        | 5          | 40269        | 6          | 41817        |
|     7      | 41826        | 8          | 41544        |            |              |
|   total    | 334437       |            |              |            |              |[0m
[09/18 12:07:03] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: []
[09/18 12:07:03] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[09/18 12:07:03] d2.data.common INFO: Serializing 49985 elements to byte tensors and concatenating them all ...
[09/18 12:07:03] d2.utils.events INFO:  eta: 14:17:50  iter: 1999  total_loss: 2.473  loss_cls_stage0: 0.1427  loss_box_reg_stage0: 0.2694  loss_cls_stage1: 0.1061  loss_box_reg_stage1: 0.6762  loss_cls_stage2: 0.1055  loss_box_reg_stage2: 0.9477  loss_mask: 0.08228  loss_rpn_cls: 0.01266  loss_rpn_loc: 0.02419  time: 0.4188  data_time: 0.0045  lr: 0.00016  max_mem: 7560M
[09/18 12:07:03] d2.data.common INFO: Serialized dataset takes 161.23 MiB
[09/18 12:07:04] fvcore.common.checkpoint INFO: [Checkpointer] Loading from detectron2://ImageNetPretrained/mvitv2/MViTv2_S_in1k.pyth ...
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv1.norm.bias in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv1.norm.weight in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv2.norm.bias in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv2.norm.weight in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv3.norm.bias in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv3.norm.weight in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv4.norm.bias in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.0.conv4.norm.weight in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv1.norm.bias in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv1.norm.weight in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv2.norm.bias in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv2.norm.weight in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv3.norm.bias in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv3.norm.weight in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv4.norm.bias in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.1.conv4.norm.weight in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv1.norm.bias in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv1.norm.weight in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv2.norm.bias in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv2.norm.weight in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv3.norm.bias in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv3.norm.weight in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv4.norm.bias in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.box_head.2.conv4.norm.weight in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn1.norm.bias in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn1.norm.weight in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn2.norm.bias in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn2.norm.weight in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn3.norm.bias in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn3.norm.weight in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.bias in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn4.norm.bias in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.bias will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: Shape of norm.weight in checkpoint is torch.Size([768]), while shape of roi_heads.mask_head.mask_fcn4.norm.weight in model is torch.Size([256]).
[09/18 12:07:05] d2.checkpoint.c2_model_loading WARNING: norm.weight will not be loaded. Please double check and see if this is desired.
[09/18 12:07:05] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone.bottom_up:
| Names in Model      | Names in Checkpoint                                                                                                                                                                                    | Shapes                                                                                                                   |
|:--------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|
| blocks.0.attn.*     | blocks.0.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (96,) (96,96) (288,) (288,96) (111,96) (111,96)     |
| blocks.0.mlp.fc1.*  | blocks.0.mlp.fc1.{bias,weight}                                                                                                                                                                         | (384,) (384,96)                                                                                                          |
| blocks.0.mlp.fc2.*  | blocks.0.mlp.fc2.{bias,weight}                                                                                                                                                                         | (96,) (96,384)                                                                                                           |
| blocks.0.norm1.*    | blocks.0.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.0.norm2.*    | blocks.0.norm2.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.attn.*     | blocks.1.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,96) (55,96) (55,96)    |
| blocks.1.mlp.fc1.*  | blocks.1.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.1.mlp.fc2.*  | blocks.1.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.1.norm1.*    | blocks.1.norm1.{bias,weight}                                                                                                                                                                           | (96,) (96,)                                                                                                              |
| blocks.1.norm2.*    | blocks.1.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.1.proj.*     | blocks.1.proj.{bias,weight}                                                                                                                                                                            | (192,) (192,96)                                                                                                          |
| blocks.10.attn.*    | blocks.10.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.10.mlp.fc1.* | blocks.10.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.10.mlp.fc2.* | blocks.10.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.10.norm1.*   | blocks.10.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.10.norm2.*   | blocks.10.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.attn.*    | blocks.11.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.11.mlp.fc1.* | blocks.11.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.11.mlp.fc2.* | blocks.11.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.11.norm1.*   | blocks.11.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.11.norm2.*   | blocks.11.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.attn.*    | blocks.12.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.12.mlp.fc1.* | blocks.12.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.12.mlp.fc2.* | blocks.12.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.12.norm1.*   | blocks.12.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.12.norm2.*   | blocks.12.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.attn.*    | blocks.13.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.13.mlp.fc1.* | blocks.13.mlp.fc1.{bias,weight}                                                                                                                                                                        | (1536,) (1536,384)                                                                                                       |
| blocks.13.mlp.fc2.* | blocks.13.mlp.fc2.{bias,weight}                                                                                                                                                                        | (384,) (384,1536)                                                                                                        |
| blocks.13.norm1.*   | blocks.13.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.13.norm2.*   | blocks.13.norm2.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.attn.*    | blocks.14.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,384) (27,96) (27,96) |
| blocks.14.mlp.fc1.* | blocks.14.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.14.mlp.fc2.* | blocks.14.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.14.norm1.*   | blocks.14.norm1.{bias,weight}                                                                                                                                                                          | (384,) (384,)                                                                                                            |
| blocks.14.norm2.*   | blocks.14.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.14.proj.*    | blocks.14.proj.{bias,weight}                                                                                                                                                                           | (768,) (768,384)                                                                                                         |
| blocks.15.attn.*    | blocks.15.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w} | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (768,) (768,768) (2304,) (2304,768) (13,96) (13,96) |
| blocks.15.mlp.fc1.* | blocks.15.mlp.fc1.{bias,weight}                                                                                                                                                                        | (3072,) (3072,768)                                                                                                       |
| blocks.15.mlp.fc2.* | blocks.15.mlp.fc2.{bias,weight}                                                                                                                                                                        | (768,) (768,3072)                                                                                                        |
| blocks.15.norm1.*   | blocks.15.norm1.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.15.norm2.*   | blocks.15.norm2.{bias,weight}                                                                                                                                                                          | (768,) (768,)                                                                                                            |
| blocks.2.attn.*     | blocks.2.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (192,) (192,192) (576,) (576,192) (55,96) (55,96)   |
| blocks.2.mlp.fc1.*  | blocks.2.mlp.fc1.{bias,weight}                                                                                                                                                                         | (768,) (768,192)                                                                                                         |
| blocks.2.mlp.fc2.*  | blocks.2.mlp.fc2.{bias,weight}                                                                                                                                                                         | (192,) (192,768)                                                                                                         |
| blocks.2.norm1.*    | blocks.2.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.2.norm2.*    | blocks.2.norm2.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.3.attn.*     | blocks.3.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,192) (55,96) (55,96) |
| blocks.3.mlp.fc1.*  | blocks.3.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.3.mlp.fc2.*  | blocks.3.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.3.norm1.*    | blocks.3.norm1.{bias,weight}                                                                                                                                                                           | (192,) (192,)                                                                                                            |
| blocks.3.norm2.*    | blocks.3.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.3.proj.*     | blocks.3.proj.{bias,weight}                                                                                                                                                                            | (384,) (384,192)                                                                                                         |
| blocks.4.attn.*     | blocks.4.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.4.mlp.fc1.*  | blocks.4.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.4.mlp.fc2.*  | blocks.4.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.4.norm1.*    | blocks.4.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.4.norm2.*    | blocks.4.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.5.attn.*     | blocks.5.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.5.mlp.fc1.*  | blocks.5.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.5.mlp.fc2.*  | blocks.5.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.5.norm1.*    | blocks.5.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.5.norm2.*    | blocks.5.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.6.attn.*     | blocks.6.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.6.mlp.fc1.*  | blocks.6.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.6.mlp.fc2.*  | blocks.6.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.6.norm1.*    | blocks.6.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.6.norm2.*    | blocks.6.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.attn.*     | blocks.7.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.7.mlp.fc1.*  | blocks.7.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.7.mlp.fc2.*  | blocks.7.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.7.norm1.*    | blocks.7.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.7.norm2.*    | blocks.7.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.attn.*     | blocks.8.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.8.mlp.fc1.*  | blocks.8.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.8.mlp.fc2.*  | blocks.8.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.8.norm1.*    | blocks.8.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.8.norm2.*    | blocks.8.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.attn.*     | blocks.9.attn.{norm_k.bias,norm_k.weight,norm_q.bias,norm_q.weight,norm_v.bias,norm_v.weight,pool_k.weight,pool_q.weight,pool_v.weight,proj.bias,proj.weight,qkv.bias,qkv.weight,rel_pos_h,rel_pos_w}  | (96,) (96,) (96,) (96,) (96,) (96,) (96,1,3,3) (96,1,3,3) (96,1,3,3) (384,) (384,384) (1152,) (1152,384) (27,96) (27,96) |
| blocks.9.mlp.fc1.*  | blocks.9.mlp.fc1.{bias,weight}                                                                                                                                                                         | (1536,) (1536,384)                                                                                                       |
| blocks.9.mlp.fc2.*  | blocks.9.mlp.fc2.{bias,weight}                                                                                                                                                                         | (384,) (384,1536)                                                                                                        |
| blocks.9.norm1.*    | blocks.9.norm1.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| blocks.9.norm2.*    | blocks.9.norm2.{bias,weight}                                                                                                                                                                           | (384,) (384,)                                                                                                            |
| patch_embed.proj.*  | patch_embed.proj.{bias,weight}                                                                                                                                                                         | (96,) (96,3,7,7)                                                                                                         |
[09/18 12:07:05] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.bottom_up.scale2_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale3_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale4_norm.{bias, weight}[0m
[34mbackbone.bottom_up.scale5_norm.{bias, weight}[0m
[34mbackbone.fpn_lateral2.{bias, weight}[0m
[34mbackbone.fpn_lateral3.{bias, weight}[0m
[34mbackbone.fpn_lateral4.{bias, weight}[0m
[34mbackbone.fpn_lateral5.{bias, weight}[0m
[34mbackbone.fpn_output2.{bias, weight}[0m
[34mbackbone.fpn_output3.{bias, weight}[0m
[34mbackbone.fpn_output4.{bias, weight}[0m
[34mbackbone.fpn_output5.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv0.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv1.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mroi_heads.box_head.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv1.weight[0m
[34mroi_heads.box_head.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv2.weight[0m
[34mroi_heads.box_head.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv3.weight[0m
[34mroi_heads.box_head.0.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.0.conv4.weight[0m
[34mroi_heads.box_head.0.fc1.{bias, weight}[0m
[34mroi_heads.box_head.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv1.weight[0m
[34mroi_heads.box_head.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv2.weight[0m
[34mroi_heads.box_head.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv3.weight[0m
[34mroi_heads.box_head.1.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.1.conv4.weight[0m
[34mroi_heads.box_head.1.fc1.{bias, weight}[0m
[34mroi_heads.box_head.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv1.weight[0m
[34mroi_heads.box_head.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv2.weight[0m
[34mroi_heads.box_head.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv3.weight[0m
[34mroi_heads.box_head.2.conv4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.box_head.2.conv4.weight[0m
[34mroi_heads.box_head.2.fc1.{bias, weight}[0m
[34mroi_heads.box_predictor.0.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.0.cls_score.{bias, weight}[0m
[34mroi_heads.box_predictor.1.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.1.cls_score.{bias, weight}[0m
[34mroi_heads.box_predictor.2.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.2.cls_score.{bias, weight}[0m
[34mroi_heads.mask_head.deconv.{bias, weight}[0m
[34mroi_heads.mask_head.mask_fcn1.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn1.weight[0m
[34mroi_heads.mask_head.mask_fcn2.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn2.weight[0m
[34mroi_heads.mask_head.mask_fcn3.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn3.weight[0m
[34mroi_heads.mask_head.mask_fcn4.norm.{bias, running_mean, running_var, weight}[0m
[34mroi_heads.mask_head.mask_fcn4.weight[0m
[34mroi_heads.mask_head.predictor.{bias, weight}[0m
[09/18 12:07:05] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mhead.projection.{bias, weight}[0m
  [35mnorm.{bias, weight}[0m
[09/18 12:07:05] d2.engine.train_loop INFO: Starting training from iteration 0
[09/18 12:07:15] d2.utils.events INFO:  eta: 14:18:28  iter: 2019  total_loss: 2.562  loss_cls_stage0: 0.1592  loss_box_reg_stage0: 0.2798  loss_cls_stage1: 0.1319  loss_box_reg_stage1: 0.718  loss_cls_stage2: 0.115  loss_box_reg_stage2: 1.003  loss_mask: 0.09523  loss_rpn_cls: 0.01069  loss_rpn_loc: 0.02455  time: 0.4204  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:07:22] d2.utils.events INFO:  eta: 1 day, 0:59:39  iter: 19  total_loss: 7.127  loss_cls_stage0: 1.857  loss_box_reg_stage0: 0.03246  loss_cls_stage1: 1.727  loss_box_reg_stage1: 0.01085  loss_cls_stage2: 2.062  loss_box_reg_stage2: 0.005851  loss_mask: 0.693  loss_rpn_cls: 0.6834  loss_rpn_loc: 0.05988  time: 0.7262  data_time: 0.0080  lr: 6.7198e-06  max_mem: 6698M
[09/18 12:07:30] d2.utils.events INFO:  eta: 14:19:02  iter: 2039  total_loss: 2.548  loss_cls_stage0: 0.1742  loss_box_reg_stage0: 0.3062  loss_cls_stage1: 0.1397  loss_box_reg_stage1: 0.7138  loss_cls_stage2: 0.129  loss_box_reg_stage2: 0.9576  loss_mask: 0.09888  loss_rpn_cls: 0.01259  loss_rpn_loc: 0.02427  time: 0.4239  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:07:38] d2.utils.events INFO:  eta: 1 day, 2:57:25  iter: 39  total_loss: 2.636  loss_cls_stage0: 0.377  loss_box_reg_stage0: 0.1068  loss_cls_stage1: 0.2945  loss_box_reg_stage1: 0.05718  loss_cls_stage2: 0.3457  loss_box_reg_stage2: 0.01602  loss_mask: 0.6929  loss_rpn_cls: 0.6226  loss_rpn_loc: 0.0701  time: 0.7611  data_time: 0.0038  lr: 1.3625e-05  max_mem: 6736M
[09/18 12:07:46] d2.utils.events INFO:  eta: 14:19:17  iter: 2059  total_loss: 2.38  loss_cls_stage0: 0.1487  loss_box_reg_stage0: 0.2731  loss_cls_stage1: 0.1259  loss_box_reg_stage1: 0.6739  loss_cls_stage2: 0.115  loss_box_reg_stage2: 0.9776  loss_mask: 0.09848  loss_rpn_cls: 0.007612  loss_rpn_loc: 0.02463  time: 0.4274  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:07:54] d2.utils.events INFO:  eta: 1 day, 3:17:30  iter: 59  total_loss: 1.906  loss_cls_stage0: 0.275  loss_box_reg_stage0: 0.1236  loss_cls_stage1: 0.1684  loss_box_reg_stage1: 0.08144  loss_cls_stage2: 0.1173  loss_box_reg_stage2: 0.02839  loss_mask: 0.6922  loss_rpn_cls: 0.3541  loss_rpn_loc: 0.05572  time: 0.7724  data_time: 0.0039  lr: 2.053e-05  max_mem: 6785M
[09/18 12:07:56] d2.engine.hooks INFO: Overall training speed: 60 iterations in 0:00:46 (0.7771 s / it)
[09/18 12:07:56] d2.engine.hooks INFO: Total training time: 0:00:46 (0:00:00 on hooks)
[09/18 12:07:56] d2.utils.events INFO:  eta: 1 day, 3:18:49  iter: 62  total_loss: 1.887  loss_cls_stage0: 0.275  loss_box_reg_stage0: 0.1236  loss_cls_stage1: 0.1684  loss_box_reg_stage1: 0.08144  loss_cls_stage2: 0.1117  loss_box_reg_stage2: 0.02732  loss_mask: 0.6922  loss_rpn_cls: 0.3324  loss_rpn_loc: 0.05572  time: 0.7731  data_time: 0.0040  lr: 2.1221e-05  max_mem: 6785M
[09/18 12:07:59] d2.utils.events INFO:  eta: 14:20:11  iter: 2079  total_loss: 2.872  loss_cls_stage0: 0.1808  loss_box_reg_stage0: 0.3127  loss_cls_stage1: 0.151  loss_box_reg_stage1: 0.8059  loss_cls_stage2: 0.1423  loss_box_reg_stage2: 1.049  loss_mask: 0.1046  loss_rpn_cls: 0.01024  loss_rpn_loc: 0.02499  time: 0.4296  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:08:08] d2.utils.events INFO:  eta: 14:20:19  iter: 2099  total_loss: 2.991  loss_cls_stage0: 0.1759  loss_box_reg_stage0: 0.3  loss_cls_stage1: 0.1528  loss_box_reg_stage1: 0.8845  loss_cls_stage2: 0.1761  loss_box_reg_stage2: 1.143  loss_mask: 0.09813  loss_rpn_cls: 0.01193  loss_rpn_loc: 0.02416  time: 0.4296  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:08:16] d2.utils.events INFO:  eta: 14:20:13  iter: 2119  total_loss: 2.605  loss_cls_stage0: 0.1651  loss_box_reg_stage0: 0.2788  loss_cls_stage1: 0.1326  loss_box_reg_stage1: 0.7152  loss_cls_stage2: 0.1284  loss_box_reg_stage2: 0.9577  loss_mask: 0.0922  loss_rpn_cls: 0.01136  loss_rpn_loc: 0.02507  time: 0.4295  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:08:24] d2.utils.events INFO:  eta: 14:19:56  iter: 2139  total_loss: 2.656  loss_cls_stage0: 0.1548  loss_box_reg_stage0: 0.3007  loss_cls_stage1: 0.123  loss_box_reg_stage1: 0.7715  loss_cls_stage2: 0.1218  loss_box_reg_stage2: 1.034  loss_mask: 0.0875  loss_rpn_cls: 0.007721  loss_rpn_loc: 0.02325  time: 0.4293  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:08:33] d2.utils.events INFO:  eta: 14:19:43  iter: 2159  total_loss: 2.543  loss_cls_stage0: 0.1584  loss_box_reg_stage0: 0.2835  loss_cls_stage1: 0.1078  loss_box_reg_stage1: 0.7035  loss_cls_stage2: 0.1106  loss_box_reg_stage2: 0.9785  loss_mask: 0.09052  loss_rpn_cls: 0.007736  loss_rpn_loc: 0.02367  time: 0.4292  data_time: 0.0039  lr: 0.00016  max_mem: 7560M
[09/18 12:08:41] d2.utils.events INFO:  eta: 14:19:36  iter: 2179  total_loss: 2.573  loss_cls_stage0: 0.141  loss_box_reg_stage0: 0.2816  loss_cls_stage1: 0.1104  loss_box_reg_stage1: 0.7243  loss_cls_stage2: 0.0946  loss_box_reg_stage2: 1.016  loss_mask: 0.09266  loss_rpn_cls: 0.009575  loss_rpn_loc: 0.02429  time: 0.4292  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:08:50] d2.utils.events INFO:  eta: 14:19:59  iter: 2199  total_loss: 2.638  loss_cls_stage0: 0.1516  loss_box_reg_stage0: 0.2989  loss_cls_stage1: 0.1522  loss_box_reg_stage1: 0.741  loss_cls_stage2: 0.1162  loss_box_reg_stage2: 1.054  loss_mask: 0.0868  loss_rpn_cls: 0.009468  loss_rpn_loc: 0.02449  time: 0.4292  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:08:58] d2.utils.events INFO:  eta: 14:19:20  iter: 2219  total_loss: 2.878  loss_cls_stage0: 0.1772  loss_box_reg_stage0: 0.3169  loss_cls_stage1: 0.1558  loss_box_reg_stage1: 0.801  loss_cls_stage2: 0.1424  loss_box_reg_stage2: 1.059  loss_mask: 0.09087  loss_rpn_cls: 0.009748  loss_rpn_loc: 0.02477  time: 0.4291  data_time: 0.0038  lr: 0.00016  max_mem: 7560M
[09/18 12:09:07] d2.utils.events INFO:  eta: 14:19:23  iter: 2239  total_loss: 2.314  loss_cls_stage0: 0.1485  loss_box_reg_stage0: 0.2511  loss_cls_stage1: 0.1259  loss_box_reg_stage1: 0.6732  loss_cls_stage2: 0.09475  loss_box_reg_stage2: 0.947  loss_mask: 0.08722  loss_rpn_cls: 0.006248  loss_rpn_loc: 0.02156  time: 0.4291  data_time: 0.0038  lr: 0.00016  max_mem: 7560M
[09/18 12:09:15] d2.utils.events INFO:  eta: 14:19:38  iter: 2259  total_loss: 2.577  loss_cls_stage0: 0.1649  loss_box_reg_stage0: 0.2768  loss_cls_stage1: 0.1446  loss_box_reg_stage1: 0.7294  loss_cls_stage2: 0.1244  loss_box_reg_stage2: 1.004  loss_mask: 0.08843  loss_rpn_cls: 0.005686  loss_rpn_loc: 0.0226  time: 0.4291  data_time: 0.0039  lr: 0.00016  max_mem: 7560M
[09/18 12:09:24] d2.utils.events INFO:  eta: 14:19:37  iter: 2279  total_loss: 2.276  loss_cls_stage0: 0.1408  loss_box_reg_stage0: 0.2567  loss_cls_stage1: 0.1019  loss_box_reg_stage1: 0.6294  loss_cls_stage2: 0.1093  loss_box_reg_stage2: 0.8849  loss_mask: 0.08279  loss_rpn_cls: 0.007428  loss_rpn_loc: 0.02125  time: 0.4290  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:09:32] d2.utils.events INFO:  eta: 14:19:17  iter: 2299  total_loss: 2.486  loss_cls_stage0: 0.178  loss_box_reg_stage0: 0.3041  loss_cls_stage1: 0.1224  loss_box_reg_stage1: 0.7064  loss_cls_stage2: 0.1081  loss_box_reg_stage2: 0.921  loss_mask: 0.0956  loss_rpn_cls: 0.01103  loss_rpn_loc: 0.02446  time: 0.4289  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:09:40] d2.utils.events INFO:  eta: 14:19:12  iter: 2319  total_loss: 2.339  loss_cls_stage0: 0.1378  loss_box_reg_stage0: 0.269  loss_cls_stage1: 0.1057  loss_box_reg_stage1: 0.6693  loss_cls_stage2: 0.1018  loss_box_reg_stage2: 0.9013  loss_mask: 0.08934  loss_rpn_cls: 0.009586  loss_rpn_loc: 0.02428  time: 0.4287  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:09:48] d2.utils.events INFO:  eta: 14:18:30  iter: 2339  total_loss: 2.491  loss_cls_stage0: 0.1681  loss_box_reg_stage0: 0.2869  loss_cls_stage1: 0.1387  loss_box_reg_stage1: 0.7226  loss_cls_stage2: 0.1211  loss_box_reg_stage2: 0.9197  loss_mask: 0.09222  loss_rpn_cls: 0.009531  loss_rpn_loc: 0.0248  time: 0.4286  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:09:57] d2.utils.events INFO:  eta: 14:18:19  iter: 2359  total_loss: 2.537  loss_cls_stage0: 0.1498  loss_box_reg_stage0: 0.302  loss_cls_stage1: 0.1451  loss_box_reg_stage1: 0.715  loss_cls_stage2: 0.1395  loss_box_reg_stage2: 0.896  loss_mask: 0.09248  loss_rpn_cls: 0.00836  loss_rpn_loc: 0.02194  time: 0.4285  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:10:06] d2.utils.events INFO:  eta: 14:17:53  iter: 2379  total_loss: 2.37  loss_cls_stage0: 0.1586  loss_box_reg_stage0: 0.2819  loss_cls_stage1: 0.1361  loss_box_reg_stage1: 0.6662  loss_cls_stage2: 0.133  loss_box_reg_stage2: 0.8674  loss_mask: 0.09384  loss_rpn_cls: 0.01254  loss_rpn_loc: 0.02321  time: 0.4285  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:10:14] d2.utils.events INFO:  eta: 14:18:05  iter: 2399  total_loss: 2.501  loss_cls_stage0: 0.1485  loss_box_reg_stage0: 0.2774  loss_cls_stage1: 0.1351  loss_box_reg_stage1: 0.7204  loss_cls_stage2: 0.1012  loss_box_reg_stage2: 0.9924  loss_mask: 0.08571  loss_rpn_cls: 0.013  loss_rpn_loc: 0.02055  time: 0.4285  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:10:22] d2.utils.events INFO:  eta: 14:18:03  iter: 2419  total_loss: 2.6  loss_cls_stage0: 0.1553  loss_box_reg_stage0: 0.2805  loss_cls_stage1: 0.1301  loss_box_reg_stage1: 0.733  loss_cls_stage2: 0.1311  loss_box_reg_stage2: 0.9738  loss_mask: 0.1042  loss_rpn_cls: 0.007164  loss_rpn_loc: 0.02297  time: 0.4284  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:10:31] d2.utils.events INFO:  eta: 14:17:12  iter: 2439  total_loss: 2.778  loss_cls_stage0: 0.1652  loss_box_reg_stage0: 0.2964  loss_cls_stage1: 0.1423  loss_box_reg_stage1: 0.7826  loss_cls_stage2: 0.1597  loss_box_reg_stage2: 1.04  loss_mask: 0.09447  loss_rpn_cls: 0.008081  loss_rpn_loc: 0.02215  time: 0.4283  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:10:39] d2.utils.events INFO:  eta: 14:16:48  iter: 2459  total_loss: 2.466  loss_cls_stage0: 0.156  loss_box_reg_stage0: 0.2963  loss_cls_stage1: 0.1423  loss_box_reg_stage1: 0.6933  loss_cls_stage2: 0.1355  loss_box_reg_stage2: 0.8642  loss_mask: 0.09353  loss_rpn_cls: 0.01154  loss_rpn_loc: 0.02274  time: 0.4281  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:10:48] d2.utils.events INFO:  eta: 14:16:57  iter: 2479  total_loss: 2.394  loss_cls_stage0: 0.1328  loss_box_reg_stage0: 0.2562  loss_cls_stage1: 0.1256  loss_box_reg_stage1: 0.6969  loss_cls_stage2: 0.1166  loss_box_reg_stage2: 0.9286  loss_mask: 0.08399  loss_rpn_cls: 0.008954  loss_rpn_loc: 0.02392  time: 0.4281  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:10:56] d2.utils.events INFO:  eta: 14:17:04  iter: 2499  total_loss: 2.625  loss_cls_stage0: 0.1571  loss_box_reg_stage0: 0.2739  loss_cls_stage1: 0.1298  loss_box_reg_stage1: 0.752  loss_cls_stage2: 0.1285  loss_box_reg_stage2: 1.052  loss_mask: 0.09014  loss_rpn_cls: 0.01046  loss_rpn_loc: 0.02358  time: 0.4281  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:11:04] d2.utils.events INFO:  eta: 14:16:57  iter: 2519  total_loss: 2.565  loss_cls_stage0: 0.1446  loss_box_reg_stage0: 0.2691  loss_cls_stage1: 0.1254  loss_box_reg_stage1: 0.7337  loss_cls_stage2: 0.1334  loss_box_reg_stage2: 1.027  loss_mask: 0.08947  loss_rpn_cls: 0.01105  loss_rpn_loc: 0.02378  time: 0.4280  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:11:13] d2.utils.events INFO:  eta: 14:17:08  iter: 2539  total_loss: 2.489  loss_cls_stage0: 0.1365  loss_box_reg_stage0: 0.2624  loss_cls_stage1: 0.1205  loss_box_reg_stage1: 0.7247  loss_cls_stage2: 0.1155  loss_box_reg_stage2: 1.044  loss_mask: 0.09621  loss_rpn_cls: 0.008764  loss_rpn_loc: 0.02027  time: 0.4279  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:11:21] d2.utils.events INFO:  eta: 14:16:57  iter: 2559  total_loss: 2.852  loss_cls_stage0: 0.1724  loss_box_reg_stage0: 0.3079  loss_cls_stage1: 0.1442  loss_box_reg_stage1: 0.8222  loss_cls_stage2: 0.1575  loss_box_reg_stage2: 1.079  loss_mask: 0.09714  loss_rpn_cls: 0.008321  loss_rpn_loc: 0.02266  time: 0.4278  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:11:30] d2.utils.events INFO:  eta: 14:17:10  iter: 2579  total_loss: 2.33  loss_cls_stage0: 0.1492  loss_box_reg_stage0: 0.2859  loss_cls_stage1: 0.1241  loss_box_reg_stage1: 0.6615  loss_cls_stage2: 0.1145  loss_box_reg_stage2: 0.9438  loss_mask: 0.08938  loss_rpn_cls: 0.009428  loss_rpn_loc: 0.02368  time: 0.4278  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:11:38] d2.utils.events INFO:  eta: 14:17:18  iter: 2599  total_loss: 2.495  loss_cls_stage0: 0.1457  loss_box_reg_stage0: 0.2648  loss_cls_stage1: 0.1309  loss_box_reg_stage1: 0.7013  loss_cls_stage2: 0.131  loss_box_reg_stage2: 0.9849  loss_mask: 0.09169  loss_rpn_cls: 0.008079  loss_rpn_loc: 0.02067  time: 0.4277  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:11:46] d2.utils.events INFO:  eta: 14:17:09  iter: 2619  total_loss: 2.455  loss_cls_stage0: 0.1717  loss_box_reg_stage0: 0.2914  loss_cls_stage1: 0.1437  loss_box_reg_stage1: 0.6639  loss_cls_stage2: 0.1223  loss_box_reg_stage2: 0.8849  loss_mask: 0.09027  loss_rpn_cls: 0.01355  loss_rpn_loc: 0.02204  time: 0.4276  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:11:55] d2.utils.events INFO:  eta: 14:17:01  iter: 2639  total_loss: 2.585  loss_cls_stage0: 0.1669  loss_box_reg_stage0: 0.2843  loss_cls_stage1: 0.1302  loss_box_reg_stage1: 0.7224  loss_cls_stage2: 0.1364  loss_box_reg_stage2: 0.9851  loss_mask: 0.1032  loss_rpn_cls: 0.0104  loss_rpn_loc: 0.02403  time: 0.4275  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:12:03] d2.utils.events INFO:  eta: 14:16:45  iter: 2659  total_loss: 2.503  loss_cls_stage0: 0.1294  loss_box_reg_stage0: 0.2614  loss_cls_stage1: 0.1154  loss_box_reg_stage1: 0.7185  loss_cls_stage2: 0.1145  loss_box_reg_stage2: 0.9291  loss_mask: 0.09167  loss_rpn_cls: 0.009767  loss_rpn_loc: 0.02125  time: 0.4274  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:12:11] d2.utils.events INFO:  eta: 14:16:44  iter: 2679  total_loss: 2.621  loss_cls_stage0: 0.1682  loss_box_reg_stage0: 0.2908  loss_cls_stage1: 0.133  loss_box_reg_stage1: 0.7393  loss_cls_stage2: 0.1281  loss_box_reg_stage2: 1.017  loss_mask: 0.09268  loss_rpn_cls: 0.006542  loss_rpn_loc: 0.02068  time: 0.4273  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:12:20] d2.utils.events INFO:  eta: 14:16:18  iter: 2699  total_loss: 2.544  loss_cls_stage0: 0.1458  loss_box_reg_stage0: 0.294  loss_cls_stage1: 0.1299  loss_box_reg_stage1: 0.7476  loss_cls_stage2: 0.1295  loss_box_reg_stage2: 0.9392  loss_mask: 0.08233  loss_rpn_cls: 0.006809  loss_rpn_loc: 0.02225  time: 0.4273  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:12:28] d2.utils.events INFO:  eta: 14:15:32  iter: 2719  total_loss: 2.535  loss_cls_stage0: 0.1762  loss_box_reg_stage0: 0.3058  loss_cls_stage1: 0.139  loss_box_reg_stage1: 0.722  loss_cls_stage2: 0.09836  loss_box_reg_stage2: 0.9671  loss_mask: 0.0938  loss_rpn_cls: 0.01012  loss_rpn_loc: 0.02261  time: 0.4272  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:12:36] d2.utils.events INFO:  eta: 14:15:25  iter: 2739  total_loss: 2.595  loss_cls_stage0: 0.1688  loss_box_reg_stage0: 0.2998  loss_cls_stage1: 0.133  loss_box_reg_stage1: 0.7312  loss_cls_stage2: 0.09877  loss_box_reg_stage2: 0.9572  loss_mask: 0.0882  loss_rpn_cls: 0.01149  loss_rpn_loc: 0.02248  time: 0.4271  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:12:45] d2.utils.events INFO:  eta: 14:15:16  iter: 2759  total_loss: 2.719  loss_cls_stage0: 0.1678  loss_box_reg_stage0: 0.3038  loss_cls_stage1: 0.1489  loss_box_reg_stage1: 0.7873  loss_cls_stage2: 0.1394  loss_box_reg_stage2: 1.026  loss_mask: 0.09777  loss_rpn_cls: 0.009854  loss_rpn_loc: 0.02227  time: 0.4270  data_time: 0.0039  lr: 0.00016  max_mem: 7560M
[09/18 12:12:53] d2.utils.events INFO:  eta: 14:15:08  iter: 2779  total_loss: 2.526  loss_cls_stage0: 0.1618  loss_box_reg_stage0: 0.2962  loss_cls_stage1: 0.141  loss_box_reg_stage1: 0.6906  loss_cls_stage2: 0.1174  loss_box_reg_stage2: 0.9885  loss_mask: 0.08692  loss_rpn_cls: 0.01023  loss_rpn_loc: 0.02063  time: 0.4270  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:13:01] d2.utils.events INFO:  eta: 14:15:02  iter: 2799  total_loss: 2.447  loss_cls_stage0: 0.1482  loss_box_reg_stage0: 0.2776  loss_cls_stage1: 0.1241  loss_box_reg_stage1: 0.6938  loss_cls_stage2: 0.1178  loss_box_reg_stage2: 0.952  loss_mask: 0.09267  loss_rpn_cls: 0.0115  loss_rpn_loc: 0.02187  time: 0.4269  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:13:10] d2.utils.events INFO:  eta: 14:14:51  iter: 2819  total_loss: 2.409  loss_cls_stage0: 0.1454  loss_box_reg_stage0: 0.2723  loss_cls_stage1: 0.1208  loss_box_reg_stage1: 0.6747  loss_cls_stage2: 0.1253  loss_box_reg_stage2: 0.9228  loss_mask: 0.08735  loss_rpn_cls: 0.008591  loss_rpn_loc: 0.02225  time: 0.4269  data_time: 0.0045  lr: 0.00016  max_mem: 7560M
[09/18 12:13:18] d2.utils.events INFO:  eta: 14:15:08  iter: 2839  total_loss: 2.536  loss_cls_stage0: 0.1588  loss_box_reg_stage0: 0.2899  loss_cls_stage1: 0.1297  loss_box_reg_stage1: 0.7053  loss_cls_stage2: 0.1194  loss_box_reg_stage2: 0.9709  loss_mask: 0.08962  loss_rpn_cls: 0.009644  loss_rpn_loc: 0.02089  time: 0.4269  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:13:27] d2.utils.events INFO:  eta: 14:14:51  iter: 2859  total_loss: 2.711  loss_cls_stage0: 0.1581  loss_box_reg_stage0: 0.286  loss_cls_stage1: 0.1215  loss_box_reg_stage1: 0.7692  loss_cls_stage2: 0.1014  loss_box_reg_stage2: 1.013  loss_mask: 0.09095  loss_rpn_cls: 0.01014  loss_rpn_loc: 0.02239  time: 0.4269  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:13:35] d2.utils.events INFO:  eta: 14:14:41  iter: 2879  total_loss: 2.546  loss_cls_stage0: 0.1593  loss_box_reg_stage0: 0.2868  loss_cls_stage1: 0.1244  loss_box_reg_stage1: 0.7543  loss_cls_stage2: 0.1169  loss_box_reg_stage2: 0.9888  loss_mask: 0.09101  loss_rpn_cls: 0.005988  loss_rpn_loc: 0.02201  time: 0.4268  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:13:44] d2.utils.events INFO:  eta: 14:14:46  iter: 2899  total_loss: 2.404  loss_cls_stage0: 0.1477  loss_box_reg_stage0: 0.2798  loss_cls_stage1: 0.1154  loss_box_reg_stage1: 0.6871  loss_cls_stage2: 0.1046  loss_box_reg_stage2: 0.9558  loss_mask: 0.09224  loss_rpn_cls: 0.009116  loss_rpn_loc: 0.02133  time: 0.4268  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:13:52] d2.utils.events INFO:  eta: 14:14:47  iter: 2919  total_loss: 2.123  loss_cls_stage0: 0.1549  loss_box_reg_stage0: 0.2817  loss_cls_stage1: 0.1295  loss_box_reg_stage1: 0.5936  loss_cls_stage2: 0.1092  loss_box_reg_stage2: 0.8383  loss_mask: 0.08682  loss_rpn_cls: 0.01285  loss_rpn_loc: 0.02203  time: 0.4268  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:14:01] d2.utils.events INFO:  eta: 14:15:07  iter: 2939  total_loss: 2.402  loss_cls_stage0: 0.1436  loss_box_reg_stage0: 0.2684  loss_cls_stage1: 0.1161  loss_box_reg_stage1: 0.7025  loss_cls_stage2: 0.1161  loss_box_reg_stage2: 0.8958  loss_mask: 0.08534  loss_rpn_cls: 0.009128  loss_rpn_loc: 0.02249  time: 0.4268  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:14:11] d2.utils.events INFO:  eta: 14:15:35  iter: 2959  total_loss: 2.554  loss_cls_stage0: 0.1673  loss_box_reg_stage0: 0.2824  loss_cls_stage1: 0.1409  loss_box_reg_stage1: 0.7216  loss_cls_stage2: 0.1427  loss_box_reg_stage2: 0.9792  loss_mask: 0.08557  loss_rpn_cls: 0.009838  loss_rpn_loc: 0.02256  time: 0.4274  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:14:27] d2.utils.events INFO:  eta: 14:16:23  iter: 2979  total_loss: 2.686  loss_cls_stage0: 0.1506  loss_box_reg_stage0: 0.2762  loss_cls_stage1: 0.1339  loss_box_reg_stage1: 0.7802  loss_cls_stage2: 0.1471  loss_box_reg_stage2: 0.9839  loss_mask: 0.08713  loss_rpn_cls: 0.005884  loss_rpn_loc: 0.02025  time: 0.4297  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:14:42] d2.utils.events INFO:  eta: 14:16:55  iter: 2999  total_loss: 2.621  loss_cls_stage0: 0.1677  loss_box_reg_stage0: 0.2903  loss_cls_stage1: 0.1374  loss_box_reg_stage1: 0.7587  loss_cls_stage2: 0.1291  loss_box_reg_stage2: 1.046  loss_mask: 0.08637  loss_rpn_cls: 0.01045  loss_rpn_loc: 0.023  time: 0.4320  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:14:55] d2.utils.events INFO:  eta: 14:16:49  iter: 3019  total_loss: 2.383  loss_cls_stage0: 0.1557  loss_box_reg_stage0: 0.2736  loss_cls_stage1: 0.1162  loss_box_reg_stage1: 0.7253  loss_cls_stage2: 0.1  loss_box_reg_stage2: 0.9653  loss_mask: 0.08443  loss_rpn_cls: 0.007928  loss_rpn_loc: 0.02144  time: 0.4333  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:15:03] d2.utils.events INFO:  eta: 14:16:05  iter: 3039  total_loss: 2.247  loss_cls_stage0: 0.1327  loss_box_reg_stage0: 0.2467  loss_cls_stage1: 0.106  loss_box_reg_stage1: 0.6464  loss_cls_stage2: 0.1202  loss_box_reg_stage2: 0.8554  loss_mask: 0.08197  loss_rpn_cls: 0.005521  loss_rpn_loc: 0.02242  time: 0.4332  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:15:12] d2.utils.events INFO:  eta: 14:15:40  iter: 3059  total_loss: 2.215  loss_cls_stage0: 0.1327  loss_box_reg_stage0: 0.2407  loss_cls_stage1: 0.1075  loss_box_reg_stage1: 0.6662  loss_cls_stage2: 0.09244  loss_box_reg_stage2: 0.8813  loss_mask: 0.08931  loss_rpn_cls: 0.007507  loss_rpn_loc: 0.02245  time: 0.4333  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:15:20] d2.utils.events INFO:  eta: 14:14:40  iter: 3079  total_loss: 2.335  loss_cls_stage0: 0.1502  loss_box_reg_stage0: 0.2435  loss_cls_stage1: 0.1364  loss_box_reg_stage1: 0.646  loss_cls_stage2: 0.1268  loss_box_reg_stage2: 0.9163  loss_mask: 0.08927  loss_rpn_cls: 0.006395  loss_rpn_loc: 0.02228  time: 0.4332  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:15:29] d2.utils.events INFO:  eta: 14:14:36  iter: 3099  total_loss: 2.642  loss_cls_stage0: 0.1852  loss_box_reg_stage0: 0.3022  loss_cls_stage1: 0.1544  loss_box_reg_stage1: 0.7327  loss_cls_stage2: 0.134  loss_box_reg_stage2: 0.9684  loss_mask: 0.09828  loss_rpn_cls: 0.01087  loss_rpn_loc: 0.02184  time: 0.4331  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:15:37] d2.utils.events INFO:  eta: 14:14:37  iter: 3119  total_loss: 2.158  loss_cls_stage0: 0.1426  loss_box_reg_stage0: 0.2487  loss_cls_stage1: 0.1238  loss_box_reg_stage1: 0.6222  loss_cls_stage2: 0.1107  loss_box_reg_stage2: 0.8382  loss_mask: 0.08891  loss_rpn_cls: 0.008343  loss_rpn_loc: 0.02242  time: 0.4330  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:15:46] d2.utils.events INFO:  eta: 14:15:27  iter: 3139  total_loss: 2.103  loss_cls_stage0: 0.1259  loss_box_reg_stage0: 0.2359  loss_cls_stage1: 0.1058  loss_box_reg_stage1: 0.6013  loss_cls_stage2: 0.09979  loss_box_reg_stage2: 0.8556  loss_mask: 0.07737  loss_rpn_cls: 0.007205  loss_rpn_loc: 0.02164  time: 0.4330  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:15:54] d2.utils.events INFO:  eta: 14:15:12  iter: 3159  total_loss: 2.181  loss_cls_stage0: 0.1299  loss_box_reg_stage0: 0.2659  loss_cls_stage1: 0.09378  loss_box_reg_stage1: 0.6048  loss_cls_stage2: 0.09141  loss_box_reg_stage2: 0.8716  loss_mask: 0.08284  loss_rpn_cls: 0.009517  loss_rpn_loc: 0.02148  time: 0.4329  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:16:03] d2.utils.events INFO:  eta: 14:15:04  iter: 3179  total_loss: 2.56  loss_cls_stage0: 0.1607  loss_box_reg_stage0: 0.2774  loss_cls_stage1: 0.1395  loss_box_reg_stage1: 0.7091  loss_cls_stage2: 0.1375  loss_box_reg_stage2: 0.9622  loss_mask: 0.08923  loss_rpn_cls: 0.01336  loss_rpn_loc: 0.02207  time: 0.4328  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:16:11] d2.utils.events INFO:  eta: 14:13:59  iter: 3199  total_loss: 2.253  loss_cls_stage0: 0.1493  loss_box_reg_stage0: 0.2558  loss_cls_stage1: 0.1162  loss_box_reg_stage1: 0.6627  loss_cls_stage2: 0.1206  loss_box_reg_stage2: 0.9141  loss_mask: 0.08908  loss_rpn_cls: 0.006805  loss_rpn_loc: 0.02216  time: 0.4328  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:16:20] d2.utils.events INFO:  eta: 14:14:44  iter: 3219  total_loss: 2.45  loss_cls_stage0: 0.1457  loss_box_reg_stage0: 0.2774  loss_cls_stage1: 0.1373  loss_box_reg_stage1: 0.6851  loss_cls_stage2: 0.1513  loss_box_reg_stage2: 0.9721  loss_mask: 0.08201  loss_rpn_cls: 0.006055  loss_rpn_loc: 0.02153  time: 0.4327  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:16:28] d2.utils.events INFO:  eta: 14:13:51  iter: 3239  total_loss: 2.3  loss_cls_stage0: 0.1265  loss_box_reg_stage0: 0.2536  loss_cls_stage1: 0.1079  loss_box_reg_stage1: 0.6412  loss_cls_stage2: 0.08418  loss_box_reg_stage2: 0.9208  loss_mask: 0.07834  loss_rpn_cls: 0.009588  loss_rpn_loc: 0.0215  time: 0.4327  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:16:37] d2.utils.events INFO:  eta: 14:13:43  iter: 3259  total_loss: 2.587  loss_cls_stage0: 0.18  loss_box_reg_stage0: 0.2833  loss_cls_stage1: 0.1357  loss_box_reg_stage1: 0.7086  loss_cls_stage2: 0.1275  loss_box_reg_stage2: 0.9704  loss_mask: 0.08906  loss_rpn_cls: 0.01276  loss_rpn_loc: 0.02289  time: 0.4326  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:16:45] d2.utils.events INFO:  eta: 14:13:17  iter: 3279  total_loss: 2.083  loss_cls_stage0: 0.1197  loss_box_reg_stage0: 0.2164  loss_cls_stage1: 0.1007  loss_box_reg_stage1: 0.5799  loss_cls_stage2: 0.1012  loss_box_reg_stage2: 0.851  loss_mask: 0.07928  loss_rpn_cls: 0.0081  loss_rpn_loc: 0.01938  time: 0.4325  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:16:53] d2.utils.events INFO:  eta: 14:14:15  iter: 3299  total_loss: 2.408  loss_cls_stage0: 0.1363  loss_box_reg_stage0: 0.2525  loss_cls_stage1: 0.1201  loss_box_reg_stage1: 0.6854  loss_cls_stage2: 0.1151  loss_box_reg_stage2: 0.9589  loss_mask: 0.0794  loss_rpn_cls: 0.008647  loss_rpn_loc: 0.02033  time: 0.4324  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:17:02] d2.utils.events INFO:  eta: 14:14:26  iter: 3319  total_loss: 2.082  loss_cls_stage0: 0.1212  loss_box_reg_stage0: 0.227  loss_cls_stage1: 0.08926  loss_box_reg_stage1: 0.5819  loss_cls_stage2: 0.07745  loss_box_reg_stage2: 0.8442  loss_mask: 0.08238  loss_rpn_cls: 0.008194  loss_rpn_loc: 0.02003  time: 0.4323  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:17:10] d2.utils.events INFO:  eta: 14:15:19  iter: 3339  total_loss: 2.21  loss_cls_stage0: 0.1383  loss_box_reg_stage0: 0.2472  loss_cls_stage1: 0.1118  loss_box_reg_stage1: 0.6579  loss_cls_stage2: 0.1019  loss_box_reg_stage2: 0.8928  loss_mask: 0.0794  loss_rpn_cls: 0.005999  loss_rpn_loc: 0.02084  time: 0.4324  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:17:19] d2.utils.events INFO:  eta: 14:16:08  iter: 3359  total_loss: 2.48  loss_cls_stage0: 0.1476  loss_box_reg_stage0: 0.2717  loss_cls_stage1: 0.111  loss_box_reg_stage1: 0.7053  loss_cls_stage2: 0.1217  loss_box_reg_stage2: 0.9901  loss_mask: 0.08855  loss_rpn_cls: 0.006491  loss_rpn_loc: 0.02016  time: 0.4323  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:17:28] d2.utils.events INFO:  eta: 14:15:49  iter: 3379  total_loss: 2.44  loss_cls_stage0: 0.1504  loss_box_reg_stage0: 0.2777  loss_cls_stage1: 0.1109  loss_box_reg_stage1: 0.6717  loss_cls_stage2: 0.1084  loss_box_reg_stage2: 0.9413  loss_mask: 0.09145  loss_rpn_cls: 0.005659  loss_rpn_loc: 0.02122  time: 0.4323  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:17:36] d2.utils.events INFO:  eta: 14:15:31  iter: 3399  total_loss: 2.414  loss_cls_stage0: 0.1494  loss_box_reg_stage0: 0.2582  loss_cls_stage1: 0.1064  loss_box_reg_stage1: 0.6731  loss_cls_stage2: 0.09266  loss_box_reg_stage2: 0.9039  loss_mask: 0.08499  loss_rpn_cls: 0.008455  loss_rpn_loc: 0.02279  time: 0.4323  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:17:45] d2.utils.events INFO:  eta: 14:15:43  iter: 3419  total_loss: 2.153  loss_cls_stage0: 0.1468  loss_box_reg_stage0: 0.2591  loss_cls_stage1: 0.1249  loss_box_reg_stage1: 0.6132  loss_cls_stage2: 0.1092  loss_box_reg_stage2: 0.8477  loss_mask: 0.08291  loss_rpn_cls: 0.009354  loss_rpn_loc: 0.02131  time: 0.4322  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:17:53] d2.utils.events INFO:  eta: 14:16:10  iter: 3439  total_loss: 2.272  loss_cls_stage0: 0.1405  loss_box_reg_stage0: 0.2481  loss_cls_stage1: 0.1255  loss_box_reg_stage1: 0.608  loss_cls_stage2: 0.1187  loss_box_reg_stage2: 0.8016  loss_mask: 0.07895  loss_rpn_cls: 0.01127  loss_rpn_loc: 0.02426  time: 0.4321  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:18:01] d2.utils.events INFO:  eta: 14:16:07  iter: 3459  total_loss: 2.474  loss_cls_stage0: 0.1549  loss_box_reg_stage0: 0.276  loss_cls_stage1: 0.1165  loss_box_reg_stage1: 0.6987  loss_cls_stage2: 0.116  loss_box_reg_stage2: 0.9623  loss_mask: 0.08367  loss_rpn_cls: 0.009196  loss_rpn_loc: 0.02211  time: 0.4320  data_time: 0.0045  lr: 0.00016  max_mem: 7560M
[09/18 12:18:10] d2.utils.events INFO:  eta: 14:15:54  iter: 3479  total_loss: 2.174  loss_cls_stage0: 0.1355  loss_box_reg_stage0: 0.2668  loss_cls_stage1: 0.1002  loss_box_reg_stage1: 0.6399  loss_cls_stage2: 0.08929  loss_box_reg_stage2: 0.8845  loss_mask: 0.082  loss_rpn_cls: 0.007171  loss_rpn_loc: 0.02347  time: 0.4320  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:18:18] d2.utils.events INFO:  eta: 14:16:15  iter: 3499  total_loss: 2.056  loss_cls_stage0: 0.1302  loss_box_reg_stage0: 0.2535  loss_cls_stage1: 0.1056  loss_box_reg_stage1: 0.5868  loss_cls_stage2: 0.09615  loss_box_reg_stage2: 0.8206  loss_mask: 0.07992  loss_rpn_cls: 0.005448  loss_rpn_loc: 0.02051  time: 0.4320  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:18:27] d2.utils.events INFO:  eta: 14:16:07  iter: 3519  total_loss: 2.51  loss_cls_stage0: 0.1591  loss_box_reg_stage0: 0.2674  loss_cls_stage1: 0.1275  loss_box_reg_stage1: 0.7058  loss_cls_stage2: 0.1202  loss_box_reg_stage2: 1.038  loss_mask: 0.08662  loss_rpn_cls: 0.007228  loss_rpn_loc: 0.02161  time: 0.4319  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:18:35] d2.utils.events INFO:  eta: 14:15:59  iter: 3539  total_loss: 2.669  loss_cls_stage0: 0.1602  loss_box_reg_stage0: 0.2912  loss_cls_stage1: 0.1419  loss_box_reg_stage1: 0.7576  loss_cls_stage2: 0.1356  loss_box_reg_stage2: 0.9359  loss_mask: 0.08978  loss_rpn_cls: 0.01042  loss_rpn_loc: 0.02115  time: 0.4319  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:18:44] d2.utils.events INFO:  eta: 14:16:10  iter: 3559  total_loss: 2.388  loss_cls_stage0: 0.1362  loss_box_reg_stage0: 0.2481  loss_cls_stage1: 0.1211  loss_box_reg_stage1: 0.6444  loss_cls_stage2: 0.123  loss_box_reg_stage2: 0.8604  loss_mask: 0.08581  loss_rpn_cls: 0.00898  loss_rpn_loc: 0.02166  time: 0.4318  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:18:52] d2.utils.events INFO:  eta: 14:16:15  iter: 3579  total_loss: 2.271  loss_cls_stage0: 0.1379  loss_box_reg_stage0: 0.244  loss_cls_stage1: 0.1298  loss_box_reg_stage1: 0.6308  loss_cls_stage2: 0.1052  loss_box_reg_stage2: 0.9113  loss_mask: 0.0804  loss_rpn_cls: 0.008918  loss_rpn_loc: 0.0211  time: 0.4318  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:19:01] d2.utils.events INFO:  eta: 14:16:15  iter: 3599  total_loss: 2.566  loss_cls_stage0: 0.1629  loss_box_reg_stage0: 0.2726  loss_cls_stage1: 0.1327  loss_box_reg_stage1: 0.7317  loss_cls_stage2: 0.1226  loss_box_reg_stage2: 1.035  loss_mask: 0.08428  loss_rpn_cls: 0.00729  loss_rpn_loc: 0.02176  time: 0.4317  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:19:09] d2.utils.events INFO:  eta: 14:16:30  iter: 3619  total_loss: 2.235  loss_cls_stage0: 0.1202  loss_box_reg_stage0: 0.2576  loss_cls_stage1: 0.1  loss_box_reg_stage1: 0.626  loss_cls_stage2: 0.1097  loss_box_reg_stage2: 0.8618  loss_mask: 0.07947  loss_rpn_cls: 0.006119  loss_rpn_loc: 0.01964  time: 0.4317  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:19:18] d2.utils.events INFO:  eta: 14:16:43  iter: 3639  total_loss: 2.267  loss_cls_stage0: 0.1342  loss_box_reg_stage0: 0.2638  loss_cls_stage1: 0.1086  loss_box_reg_stage1: 0.6446  loss_cls_stage2: 0.08375  loss_box_reg_stage2: 0.9191  loss_mask: 0.08834  loss_rpn_cls: 0.009899  loss_rpn_loc: 0.02146  time: 0.4316  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:19:26] d2.utils.events INFO:  eta: 14:17:17  iter: 3659  total_loss: 2.075  loss_cls_stage0: 0.1191  loss_box_reg_stage0: 0.2573  loss_cls_stage1: 0.08961  loss_box_reg_stage1: 0.5908  loss_cls_stage2: 0.08542  loss_box_reg_stage2: 0.8283  loss_mask: 0.08486  loss_rpn_cls: 0.007424  loss_rpn_loc: 0.01975  time: 0.4316  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:19:35] d2.utils.events INFO:  eta: 14:17:14  iter: 3679  total_loss: 2.17  loss_cls_stage0: 0.1285  loss_box_reg_stage0: 0.2397  loss_cls_stage1: 0.1028  loss_box_reg_stage1: 0.5876  loss_cls_stage2: 0.1042  loss_box_reg_stage2: 0.8657  loss_mask: 0.07777  loss_rpn_cls: 0.006121  loss_rpn_loc: 0.01912  time: 0.4316  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:19:43] d2.utils.events INFO:  eta: 14:17:08  iter: 3699  total_loss: 2.18  loss_cls_stage0: 0.1283  loss_box_reg_stage0: 0.2385  loss_cls_stage1: 0.09949  loss_box_reg_stage1: 0.6182  loss_cls_stage2: 0.09695  loss_box_reg_stage2: 0.8752  loss_mask: 0.0807  loss_rpn_cls: 0.004989  loss_rpn_loc: 0.02117  time: 0.4316  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:19:52] d2.utils.events INFO:  eta: 14:17:11  iter: 3719  total_loss: 2.453  loss_cls_stage0: 0.141  loss_box_reg_stage0: 0.2653  loss_cls_stage1: 0.1209  loss_box_reg_stage1: 0.6952  loss_cls_stage2: 0.1255  loss_box_reg_stage2: 0.9564  loss_mask: 0.08687  loss_rpn_cls: 0.005578  loss_rpn_loc: 0.01985  time: 0.4316  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:20:01] d2.utils.events INFO:  eta: 14:17:13  iter: 3739  total_loss: 2.292  loss_cls_stage0: 0.1351  loss_box_reg_stage0: 0.2486  loss_cls_stage1: 0.1097  loss_box_reg_stage1: 0.6401  loss_cls_stage2: 0.1018  loss_box_reg_stage2: 0.9487  loss_mask: 0.08545  loss_rpn_cls: 0.005176  loss_rpn_loc: 0.02011  time: 0.4315  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:20:09] d2.utils.events INFO:  eta: 14:17:19  iter: 3759  total_loss: 2.017  loss_cls_stage0: 0.118  loss_box_reg_stage0: 0.224  loss_cls_stage1: 0.09178  loss_box_reg_stage1: 0.5619  loss_cls_stage2: 0.08969  loss_box_reg_stage2: 0.8333  loss_mask: 0.07307  loss_rpn_cls: 0.004905  loss_rpn_loc: 0.01875  time: 0.4315  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:20:18] d2.utils.events INFO:  eta: 14:17:26  iter: 3779  total_loss: 2.234  loss_cls_stage0: 0.1259  loss_box_reg_stage0: 0.2364  loss_cls_stage1: 0.1039  loss_box_reg_stage1: 0.6418  loss_cls_stage2: 0.09492  loss_box_reg_stage2: 0.9072  loss_mask: 0.08134  loss_rpn_cls: 0.008971  loss_rpn_loc: 0.02165  time: 0.4315  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:20:26] d2.utils.events INFO:  eta: 14:17:21  iter: 3799  total_loss: 2.3  loss_cls_stage0: 0.1242  loss_box_reg_stage0: 0.2325  loss_cls_stage1: 0.09901  loss_box_reg_stage1: 0.6667  loss_cls_stage2: 0.1009  loss_box_reg_stage2: 0.9049  loss_mask: 0.08679  loss_rpn_cls: 0.01128  loss_rpn_loc: 0.01948  time: 0.4314  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:20:35] d2.utils.events INFO:  eta: 14:17:42  iter: 3819  total_loss: 2.339  loss_cls_stage0: 0.1334  loss_box_reg_stage0: 0.2403  loss_cls_stage1: 0.103  loss_box_reg_stage1: 0.6764  loss_cls_stage2: 0.0872  loss_box_reg_stage2: 0.9387  loss_mask: 0.07732  loss_rpn_cls: 0.006942  loss_rpn_loc: 0.01991  time: 0.4314  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:20:44] d2.utils.events INFO:  eta: 14:17:44  iter: 3839  total_loss: 2.349  loss_cls_stage0: 0.1421  loss_box_reg_stage0: 0.2531  loss_cls_stage1: 0.1139  loss_box_reg_stage1: 0.6515  loss_cls_stage2: 0.09644  loss_box_reg_stage2: 0.9281  loss_mask: 0.08278  loss_rpn_cls: 0.006314  loss_rpn_loc: 0.01919  time: 0.4314  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:20:52] d2.utils.events INFO:  eta: 14:17:34  iter: 3859  total_loss: 2.44  loss_cls_stage0: 0.158  loss_box_reg_stage0: 0.2915  loss_cls_stage1: 0.1391  loss_box_reg_stage1: 0.7353  loss_cls_stage2: 0.1278  loss_box_reg_stage2: 0.8869  loss_mask: 0.08617  loss_rpn_cls: 0.006783  loss_rpn_loc: 0.02085  time: 0.4314  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:21:00] d2.utils.events INFO:  eta: 14:17:13  iter: 3879  total_loss: 2.141  loss_cls_stage0: 0.1132  loss_box_reg_stage0: 0.2481  loss_cls_stage1: 0.08737  loss_box_reg_stage1: 0.5953  loss_cls_stage2: 0.09271  loss_box_reg_stage2: 0.8692  loss_mask: 0.07587  loss_rpn_cls: 0.006744  loss_rpn_loc: 0.02012  time: 0.4313  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:21:09] d2.utils.events INFO:  eta: 14:17:22  iter: 3899  total_loss: 1.97  loss_cls_stage0: 0.1144  loss_box_reg_stage0: 0.2438  loss_cls_stage1: 0.0868  loss_box_reg_stage1: 0.5594  loss_cls_stage2: 0.07236  loss_box_reg_stage2: 0.7966  loss_mask: 0.07851  loss_rpn_cls: 0.007209  loss_rpn_loc: 0.01955  time: 0.4313  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:21:17] d2.utils.events INFO:  eta: 14:17:13  iter: 3919  total_loss: 2.38  loss_cls_stage0: 0.1606  loss_box_reg_stage0: 0.2901  loss_cls_stage1: 0.1223  loss_box_reg_stage1: 0.6565  loss_cls_stage2: 0.1236  loss_box_reg_stage2: 0.8957  loss_mask: 0.08765  loss_rpn_cls: 0.01168  loss_rpn_loc: 0.02161  time: 0.4313  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:21:26] d2.utils.events INFO:  eta: 14:16:58  iter: 3939  total_loss: 2.564  loss_cls_stage0: 0.1464  loss_box_reg_stage0: 0.2685  loss_cls_stage1: 0.1188  loss_box_reg_stage1: 0.7329  loss_cls_stage2: 0.1039  loss_box_reg_stage2: 1.068  loss_mask: 0.08711  loss_rpn_cls: 0.004361  loss_rpn_loc: 0.02093  time: 0.4313  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:21:35] d2.utils.events INFO:  eta: 14:16:32  iter: 3959  total_loss: 2.418  loss_cls_stage0: 0.16  loss_box_reg_stage0: 0.2831  loss_cls_stage1: 0.1301  loss_box_reg_stage1: 0.6555  loss_cls_stage2: 0.1073  loss_box_reg_stage2: 0.8732  loss_mask: 0.09339  loss_rpn_cls: 0.008452  loss_rpn_loc: 0.02391  time: 0.4313  data_time: 0.0046  lr: 0.00016  max_mem: 7560M
[09/18 12:21:43] d2.utils.events INFO:  eta: 14:15:59  iter: 3979  total_loss: 2.231  loss_cls_stage0: 0.1477  loss_box_reg_stage0: 0.2471  loss_cls_stage1: 0.1084  loss_box_reg_stage1: 0.6677  loss_cls_stage2: 0.1066  loss_box_reg_stage2: 0.944  loss_mask: 0.08255  loss_rpn_cls: 0.007548  loss_rpn_loc: 0.02151  time: 0.4312  data_time: 0.0045  lr: 0.00016  max_mem: 7560M
[09/18 12:21:52] d2.utils.events INFO:  eta: 14:15:30  iter: 3999  total_loss: 2.337  loss_cls_stage0: 0.1421  loss_box_reg_stage0: 0.2682  loss_cls_stage1: 0.09982  loss_box_reg_stage1: 0.6607  loss_cls_stage2: 0.09252  loss_box_reg_stage2: 0.9994  loss_mask: 0.08074  loss_rpn_cls: 0.005001  loss_rpn_loc: 0.02019  time: 0.4312  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:22:00] d2.utils.events INFO:  eta: 14:15:17  iter: 4019  total_loss: 2.204  loss_cls_stage0: 0.1181  loss_box_reg_stage0: 0.2194  loss_cls_stage1: 0.09469  loss_box_reg_stage1: 0.6181  loss_cls_stage2: 0.1026  loss_box_reg_stage2: 0.9161  loss_mask: 0.07283  loss_rpn_cls: 0.006991  loss_rpn_loc: 0.01942  time: 0.4312  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:22:09] d2.utils.events INFO:  eta: 14:15:26  iter: 4039  total_loss: 2.152  loss_cls_stage0: 0.1251  loss_box_reg_stage0: 0.2579  loss_cls_stage1: 0.1071  loss_box_reg_stage1: 0.6356  loss_cls_stage2: 0.09741  loss_box_reg_stage2: 0.9215  loss_mask: 0.07694  loss_rpn_cls: 0.009411  loss_rpn_loc: 0.01979  time: 0.4312  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:22:18] d2.utils.events INFO:  eta: 14:15:25  iter: 4059  total_loss: 2.176  loss_cls_stage0: 0.1368  loss_box_reg_stage0: 0.2295  loss_cls_stage1: 0.1139  loss_box_reg_stage1: 0.618  loss_cls_stage2: 0.1007  loss_box_reg_stage2: 0.8359  loss_mask: 0.07576  loss_rpn_cls: 0.006139  loss_rpn_loc: 0.01948  time: 0.4312  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:22:26] d2.utils.events INFO:  eta: 14:15:24  iter: 4079  total_loss: 2.272  loss_cls_stage0: 0.1378  loss_box_reg_stage0: 0.2545  loss_cls_stage1: 0.1153  loss_box_reg_stage1: 0.6404  loss_cls_stage2: 0.107  loss_box_reg_stage2: 0.9246  loss_mask: 0.07612  loss_rpn_cls: 0.00539  loss_rpn_loc: 0.02038  time: 0.4313  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:22:35] d2.utils.events INFO:  eta: 14:15:25  iter: 4099  total_loss: 2.116  loss_cls_stage0: 0.1317  loss_box_reg_stage0: 0.2528  loss_cls_stage1: 0.1184  loss_box_reg_stage1: 0.5972  loss_cls_stage2: 0.1009  loss_box_reg_stage2: 0.865  loss_mask: 0.08523  loss_rpn_cls: 0.005502  loss_rpn_loc: 0.02126  time: 0.4312  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:22:44] d2.utils.events INFO:  eta: 14:15:41  iter: 4119  total_loss: 2.296  loss_cls_stage0: 0.1401  loss_box_reg_stage0: 0.2589  loss_cls_stage1: 0.1332  loss_box_reg_stage1: 0.6382  loss_cls_stage2: 0.1189  loss_box_reg_stage2: 0.9072  loss_mask: 0.08033  loss_rpn_cls: 0.006246  loss_rpn_loc: 0.01962  time: 0.4313  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:22:52] d2.utils.events INFO:  eta: 14:15:38  iter: 4139  total_loss: 2.412  loss_cls_stage0: 0.1425  loss_box_reg_stage0: 0.2688  loss_cls_stage1: 0.1153  loss_box_reg_stage1: 0.6943  loss_cls_stage2: 0.11  loss_box_reg_stage2: 0.8995  loss_mask: 0.092  loss_rpn_cls: 0.008471  loss_rpn_loc: 0.02215  time: 0.4313  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:23:01] d2.utils.events INFO:  eta: 14:15:49  iter: 4159  total_loss: 2.51  loss_cls_stage0: 0.1492  loss_box_reg_stage0: 0.2581  loss_cls_stage1: 0.1283  loss_box_reg_stage1: 0.7016  loss_cls_stage2: 0.1153  loss_box_reg_stage2: 0.9836  loss_mask: 0.0842  loss_rpn_cls: 0.008316  loss_rpn_loc: 0.02065  time: 0.4313  data_time: 0.0046  lr: 0.00016  max_mem: 7560M
[09/18 12:23:10] d2.utils.events INFO:  eta: 14:16:03  iter: 4179  total_loss: 2.09  loss_cls_stage0: 0.1323  loss_box_reg_stage0: 0.2346  loss_cls_stage1: 0.09115  loss_box_reg_stage1: 0.5946  loss_cls_stage2: 0.08534  loss_box_reg_stage2: 0.8545  loss_mask: 0.07764  loss_rpn_cls: 0.00424  loss_rpn_loc: 0.02  time: 0.4313  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:23:18] d2.utils.events INFO:  eta: 14:16:41  iter: 4199  total_loss: 2.253  loss_cls_stage0: 0.1359  loss_box_reg_stage0: 0.237  loss_cls_stage1: 0.09712  loss_box_reg_stage1: 0.64  loss_cls_stage2: 0.1003  loss_box_reg_stage2: 0.8543  loss_mask: 0.08422  loss_rpn_cls: 0.004329  loss_rpn_loc: 0.02013  time: 0.4313  data_time: 0.0039  lr: 0.00016  max_mem: 7560M
[09/18 12:23:27] d2.utils.events INFO:  eta: 14:16:39  iter: 4219  total_loss: 2.378  loss_cls_stage0: 0.1434  loss_box_reg_stage0: 0.26  loss_cls_stage1: 0.1212  loss_box_reg_stage1: 0.6782  loss_cls_stage2: 0.1131  loss_box_reg_stage2: 0.9231  loss_mask: 0.08615  loss_rpn_cls: 0.005506  loss_rpn_loc: 0.02169  time: 0.4313  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:23:36] d2.utils.events INFO:  eta: 14:16:33  iter: 4239  total_loss: 2.389  loss_cls_stage0: 0.1327  loss_box_reg_stage0: 0.2742  loss_cls_stage1: 0.121  loss_box_reg_stage1: 0.6937  loss_cls_stage2: 0.1236  loss_box_reg_stage2: 0.915  loss_mask: 0.08017  loss_rpn_cls: 0.008898  loss_rpn_loc: 0.02223  time: 0.4313  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:23:44] d2.utils.events INFO:  eta: 14:16:51  iter: 4259  total_loss: 2.162  loss_cls_stage0: 0.1145  loss_box_reg_stage0: 0.2293  loss_cls_stage1: 0.0952  loss_box_reg_stage1: 0.6108  loss_cls_stage2: 0.1082  loss_box_reg_stage2: 0.8855  loss_mask: 0.076  loss_rpn_cls: 0.005525  loss_rpn_loc: 0.01908  time: 0.4313  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:23:53] d2.utils.events INFO:  eta: 14:17:17  iter: 4279  total_loss: 2.315  loss_cls_stage0: 0.1699  loss_box_reg_stage0: 0.2588  loss_cls_stage1: 0.1205  loss_box_reg_stage1: 0.6613  loss_cls_stage2: 0.09044  loss_box_reg_stage2: 0.9116  loss_mask: 0.07943  loss_rpn_cls: 0.007069  loss_rpn_loc: 0.02108  time: 0.4313  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:24:01] d2.utils.events INFO:  eta: 14:16:53  iter: 4299  total_loss: 2.268  loss_cls_stage0: 0.1397  loss_box_reg_stage0: 0.2607  loss_cls_stage1: 0.1133  loss_box_reg_stage1: 0.6685  loss_cls_stage2: 0.1078  loss_box_reg_stage2: 0.8223  loss_mask: 0.08154  loss_rpn_cls: 0.008682  loss_rpn_loc: 0.02094  time: 0.4312  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:24:10] d2.utils.events INFO:  eta: 14:17:13  iter: 4319  total_loss: 2.271  loss_cls_stage0: 0.1239  loss_box_reg_stage0: 0.2495  loss_cls_stage1: 0.1002  loss_box_reg_stage1: 0.6543  loss_cls_stage2: 0.09132  loss_box_reg_stage2: 0.9195  loss_mask: 0.08151  loss_rpn_cls: 0.005546  loss_rpn_loc: 0.01982  time: 0.4312  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:24:18] d2.utils.events INFO:  eta: 14:16:25  iter: 4339  total_loss: 2.491  loss_cls_stage0: 0.1503  loss_box_reg_stage0: 0.2611  loss_cls_stage1: 0.1277  loss_box_reg_stage1: 0.6908  loss_cls_stage2: 0.124  loss_box_reg_stage2: 1.01  loss_mask: 0.08806  loss_rpn_cls: 0.005301  loss_rpn_loc: 0.01977  time: 0.4311  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:24:27] d2.utils.events INFO:  eta: 14:15:52  iter: 4359  total_loss: 2.202  loss_cls_stage0: 0.1467  loss_box_reg_stage0: 0.2582  loss_cls_stage1: 0.1146  loss_box_reg_stage1: 0.6241  loss_cls_stage2: 0.1082  loss_box_reg_stage2: 0.8723  loss_mask: 0.07569  loss_rpn_cls: 0.005873  loss_rpn_loc: 0.01997  time: 0.4311  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:24:35] d2.utils.events INFO:  eta: 14:15:58  iter: 4379  total_loss: 2.053  loss_cls_stage0: 0.1276  loss_box_reg_stage0: 0.2459  loss_cls_stage1: 0.09066  loss_box_reg_stage1: 0.5713  loss_cls_stage2: 0.09552  loss_box_reg_stage2: 0.7891  loss_mask: 0.07793  loss_rpn_cls: 0.007946  loss_rpn_loc: 0.0206  time: 0.4311  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:24:44] d2.utils.events INFO:  eta: 14:16:02  iter: 4399  total_loss: 2.19  loss_cls_stage0: 0.1408  loss_box_reg_stage0: 0.2681  loss_cls_stage1: 0.1002  loss_box_reg_stage1: 0.6066  loss_cls_stage2: 0.08606  loss_box_reg_stage2: 0.8388  loss_mask: 0.08038  loss_rpn_cls: 0.005889  loss_rpn_loc: 0.02035  time: 0.4311  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:24:52] d2.utils.events INFO:  eta: 14:16:10  iter: 4419  total_loss: 1.915  loss_cls_stage0: 0.1261  loss_box_reg_stage0: 0.2441  loss_cls_stage1: 0.1078  loss_box_reg_stage1: 0.5277  loss_cls_stage2: 0.1027  loss_box_reg_stage2: 0.7438  loss_mask: 0.07761  loss_rpn_cls: 0.008112  loss_rpn_loc: 0.01986  time: 0.4310  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:25:01] d2.utils.events INFO:  eta: 14:17:03  iter: 4439  total_loss: 2.347  loss_cls_stage0: 0.1482  loss_box_reg_stage0: 0.2522  loss_cls_stage1: 0.115  loss_box_reg_stage1: 0.6392  loss_cls_stage2: 0.1219  loss_box_reg_stage2: 0.913  loss_mask: 0.07982  loss_rpn_cls: 0.004255  loss_rpn_loc: 0.02204  time: 0.4310  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:25:09] d2.utils.events INFO:  eta: 14:17:10  iter: 4459  total_loss: 2.331  loss_cls_stage0: 0.1327  loss_box_reg_stage0: 0.2606  loss_cls_stage1: 0.1113  loss_box_reg_stage1: 0.6645  loss_cls_stage2: 0.1296  loss_box_reg_stage2: 0.8833  loss_mask: 0.08205  loss_rpn_cls: 0.005525  loss_rpn_loc: 0.02128  time: 0.4310  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:25:18] d2.utils.events INFO:  eta: 14:17:31  iter: 4479  total_loss: 2.19  loss_cls_stage0: 0.1373  loss_box_reg_stage0: 0.236  loss_cls_stage1: 0.112  loss_box_reg_stage1: 0.6567  loss_cls_stage2: 0.1006  loss_box_reg_stage2: 0.855  loss_mask: 0.07806  loss_rpn_cls: 0.007662  loss_rpn_loc: 0.02065  time: 0.4310  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:25:27] d2.utils.events INFO:  eta: 14:17:17  iter: 4499  total_loss: 2.332  loss_cls_stage0: 0.1548  loss_box_reg_stage0: 0.2654  loss_cls_stage1: 0.1357  loss_box_reg_stage1: 0.6575  loss_cls_stage2: 0.1413  loss_box_reg_stage2: 0.8439  loss_mask: 0.07698  loss_rpn_cls: 0.00697  loss_rpn_loc: 0.02109  time: 0.4310  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:25:35] d2.utils.events INFO:  eta: 14:16:55  iter: 4519  total_loss: 1.765  loss_cls_stage0: 0.1389  loss_box_reg_stage0: 0.2084  loss_cls_stage1: 0.09404  loss_box_reg_stage1: 0.4807  loss_cls_stage2: 0.07976  loss_box_reg_stage2: 0.6775  loss_mask: 0.07487  loss_rpn_cls: 0.007169  loss_rpn_loc: 0.01903  time: 0.4309  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:25:44] d2.utils.events INFO:  eta: 14:16:36  iter: 4539  total_loss: 2.169  loss_cls_stage0: 0.143  loss_box_reg_stage0: 0.2534  loss_cls_stage1: 0.1152  loss_box_reg_stage1: 0.6062  loss_cls_stage2: 0.106  loss_box_reg_stage2: 0.8322  loss_mask: 0.08301  loss_rpn_cls: 0.006308  loss_rpn_loc: 0.01961  time: 0.4309  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:25:52] d2.utils.events INFO:  eta: 14:16:19  iter: 4559  total_loss: 2.454  loss_cls_stage0: 0.1445  loss_box_reg_stage0: 0.257  loss_cls_stage1: 0.1223  loss_box_reg_stage1: 0.6951  loss_cls_stage2: 0.1084  loss_box_reg_stage2: 1.014  loss_mask: 0.09011  loss_rpn_cls: 0.007712  loss_rpn_loc: 0.01944  time: 0.4309  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:26:01] d2.utils.events INFO:  eta: 14:16:16  iter: 4579  total_loss: 2.155  loss_cls_stage0: 0.1247  loss_box_reg_stage0: 0.2458  loss_cls_stage1: 0.1146  loss_box_reg_stage1: 0.5815  loss_cls_stage2: 0.1039  loss_box_reg_stage2: 0.84  loss_mask: 0.08588  loss_rpn_cls: 0.008056  loss_rpn_loc: 0.02171  time: 0.4309  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:26:09] d2.utils.events INFO:  eta: 14:16:09  iter: 4599  total_loss: 2.017  loss_cls_stage0: 0.1145  loss_box_reg_stage0: 0.2286  loss_cls_stage1: 0.08456  loss_box_reg_stage1: 0.5582  loss_cls_stage2: 0.09124  loss_box_reg_stage2: 0.7794  loss_mask: 0.07993  loss_rpn_cls: 0.007463  loss_rpn_loc: 0.02084  time: 0.4308  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:26:18] d2.utils.events INFO:  eta: 14:16:27  iter: 4619  total_loss: 2.085  loss_cls_stage0: 0.1435  loss_box_reg_stage0: 0.2446  loss_cls_stage1: 0.1102  loss_box_reg_stage1: 0.6011  loss_cls_stage2: 0.09605  loss_box_reg_stage2: 0.7757  loss_mask: 0.07414  loss_rpn_cls: 0.006199  loss_rpn_loc: 0.02203  time: 0.4308  data_time: 0.0046  lr: 0.00016  max_mem: 7560M
[09/18 12:26:26] d2.utils.events INFO:  eta: 14:15:50  iter: 4639  total_loss: 2.05  loss_cls_stage0: 0.1324  loss_box_reg_stage0: 0.2635  loss_cls_stage1: 0.09567  loss_box_reg_stage1: 0.5791  loss_cls_stage2: 0.08761  loss_box_reg_stage2: 0.8414  loss_mask: 0.07876  loss_rpn_cls: 0.009892  loss_rpn_loc: 0.01957  time: 0.4308  data_time: 0.0045  lr: 0.00016  max_mem: 7560M
[09/18 12:26:35] d2.utils.events INFO:  eta: 14:15:42  iter: 4659  total_loss: 2.063  loss_cls_stage0: 0.1131  loss_box_reg_stage0: 0.2324  loss_cls_stage1: 0.08379  loss_box_reg_stage1: 0.5946  loss_cls_stage2: 0.09523  loss_box_reg_stage2: 0.8486  loss_mask: 0.07699  loss_rpn_cls: 0.005383  loss_rpn_loc: 0.0187  time: 0.4308  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:26:43] d2.utils.events INFO:  eta: 14:15:46  iter: 4679  total_loss: 1.95  loss_cls_stage0: 0.1147  loss_box_reg_stage0: 0.2282  loss_cls_stage1: 0.07605  loss_box_reg_stage1: 0.5604  loss_cls_stage2: 0.06197  loss_box_reg_stage2: 0.7834  loss_mask: 0.08336  loss_rpn_cls: 0.008336  loss_rpn_loc: 0.02086  time: 0.4307  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:26:52] d2.utils.events INFO:  eta: 14:15:58  iter: 4699  total_loss: 2.106  loss_cls_stage0: 0.1254  loss_box_reg_stage0: 0.2511  loss_cls_stage1: 0.09821  loss_box_reg_stage1: 0.5921  loss_cls_stage2: 0.09443  loss_box_reg_stage2: 0.7761  loss_mask: 0.08285  loss_rpn_cls: 0.01017  loss_rpn_loc: 0.02073  time: 0.4307  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:27:00] d2.utils.events INFO:  eta: 14:15:39  iter: 4719  total_loss: 1.918  loss_cls_stage0: 0.1059  loss_box_reg_stage0: 0.2243  loss_cls_stage1: 0.08227  loss_box_reg_stage1: 0.5339  loss_cls_stage2: 0.07276  loss_box_reg_stage2: 0.7605  loss_mask: 0.07521  loss_rpn_cls: 0.005678  loss_rpn_loc: 0.01834  time: 0.4307  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:27:09] d2.utils.events INFO:  eta: 14:15:11  iter: 4739  total_loss: 1.923  loss_cls_stage0: 0.1152  loss_box_reg_stage0: 0.2288  loss_cls_stage1: 0.1002  loss_box_reg_stage1: 0.538  loss_cls_stage2: 0.09407  loss_box_reg_stage2: 0.7875  loss_mask: 0.07979  loss_rpn_cls: 0.003148  loss_rpn_loc: 0.01938  time: 0.4307  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:27:17] d2.utils.events INFO:  eta: 14:15:32  iter: 4759  total_loss: 2.212  loss_cls_stage0: 0.1317  loss_box_reg_stage0: 0.2386  loss_cls_stage1: 0.1147  loss_box_reg_stage1: 0.6256  loss_cls_stage2: 0.1137  loss_box_reg_stage2: 0.8758  loss_mask: 0.07984  loss_rpn_cls: 0.005499  loss_rpn_loc: 0.0214  time: 0.4307  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:27:26] d2.utils.events INFO:  eta: 14:15:25  iter: 4779  total_loss: 2.031  loss_cls_stage0: 0.1023  loss_box_reg_stage0: 0.2233  loss_cls_stage1: 0.0702  loss_box_reg_stage1: 0.5661  loss_cls_stage2: 0.06996  loss_box_reg_stage2: 0.799  loss_mask: 0.0799  loss_rpn_cls: 0.004809  loss_rpn_loc: 0.02125  time: 0.4306  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:27:34] d2.utils.events INFO:  eta: 14:15:05  iter: 4799  total_loss: 2.022  loss_cls_stage0: 0.1234  loss_box_reg_stage0: 0.2322  loss_cls_stage1: 0.09401  loss_box_reg_stage1: 0.5567  loss_cls_stage2: 0.08917  loss_box_reg_stage2: 0.7875  loss_mask: 0.08069  loss_rpn_cls: 0.006908  loss_rpn_loc: 0.0188  time: 0.4306  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:27:43] d2.utils.events INFO:  eta: 14:13:47  iter: 4819  total_loss: 2.342  loss_cls_stage0: 0.1278  loss_box_reg_stage0: 0.2658  loss_cls_stage1: 0.1256  loss_box_reg_stage1: 0.6368  loss_cls_stage2: 0.1009  loss_box_reg_stage2: 0.9212  loss_mask: 0.08287  loss_rpn_cls: 0.005699  loss_rpn_loc: 0.02245  time: 0.4305  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:27:51] d2.utils.events INFO:  eta: 14:12:43  iter: 4839  total_loss: 2.053  loss_cls_stage0: 0.1225  loss_box_reg_stage0: 0.2438  loss_cls_stage1: 0.09204  loss_box_reg_stage1: 0.5829  loss_cls_stage2: 0.07798  loss_box_reg_stage2: 0.8297  loss_mask: 0.07482  loss_rpn_cls: 0.004262  loss_rpn_loc: 0.01954  time: 0.4305  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:27:59] d2.utils.events INFO:  eta: 14:12:32  iter: 4859  total_loss: 1.988  loss_cls_stage0: 0.1161  loss_box_reg_stage0: 0.2215  loss_cls_stage1: 0.08299  loss_box_reg_stage1: 0.5592  loss_cls_stage2: 0.07303  loss_box_reg_stage2: 0.8022  loss_mask: 0.07368  loss_rpn_cls: 0.003476  loss_rpn_loc: 0.01919  time: 0.4304  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:28:08] d2.utils.events INFO:  eta: 14:12:14  iter: 4879  total_loss: 2.094  loss_cls_stage0: 0.1207  loss_box_reg_stage0: 0.244  loss_cls_stage1: 0.09831  loss_box_reg_stage1: 0.5917  loss_cls_stage2: 0.08814  loss_box_reg_stage2: 0.8668  loss_mask: 0.07767  loss_rpn_cls: 0.00418  loss_rpn_loc: 0.01771  time: 0.4304  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:28:16] d2.utils.events INFO:  eta: 14:11:43  iter: 4899  total_loss: 1.905  loss_cls_stage0: 0.09343  loss_box_reg_stage0: 0.2222  loss_cls_stage1: 0.07246  loss_box_reg_stage1: 0.5272  loss_cls_stage2: 0.06166  loss_box_reg_stage2: 0.7762  loss_mask: 0.07683  loss_rpn_cls: 0.004515  loss_rpn_loc: 0.0189  time: 0.4303  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:28:25] d2.utils.events INFO:  eta: 14:11:47  iter: 4919  total_loss: 1.865  loss_cls_stage0: 0.1104  loss_box_reg_stage0: 0.2416  loss_cls_stage1: 0.0799  loss_box_reg_stage1: 0.519  loss_cls_stage2: 0.07024  loss_box_reg_stage2: 0.7245  loss_mask: 0.07689  loss_rpn_cls: 0.005235  loss_rpn_loc: 0.02063  time: 0.4303  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:28:33] d2.utils.events INFO:  eta: 14:11:31  iter: 4939  total_loss: 2.087  loss_cls_stage0: 0.1441  loss_box_reg_stage0: 0.2306  loss_cls_stage1: 0.1127  loss_box_reg_stage1: 0.5572  loss_cls_stage2: 0.09884  loss_box_reg_stage2: 0.8068  loss_mask: 0.07482  loss_rpn_cls: 0.004754  loss_rpn_loc: 0.01927  time: 0.4303  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:28:42] d2.utils.events INFO:  eta: 14:11:18  iter: 4959  total_loss: 2.182  loss_cls_stage0: 0.1322  loss_box_reg_stage0: 0.2424  loss_cls_stage1: 0.1028  loss_box_reg_stage1: 0.6251  loss_cls_stage2: 0.09664  loss_box_reg_stage2: 0.8356  loss_mask: 0.07676  loss_rpn_cls: 0.003659  loss_rpn_loc: 0.01911  time: 0.4303  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:28:50] d2.utils.events INFO:  eta: 14:11:24  iter: 4979  total_loss: 2.441  loss_cls_stage0: 0.1375  loss_box_reg_stage0: 0.261  loss_cls_stage1: 0.118  loss_box_reg_stage1: 0.6619  loss_cls_stage2: 0.1149  loss_box_reg_stage2: 0.9668  loss_mask: 0.07301  loss_rpn_cls: 0.009499  loss_rpn_loc: 0.02019  time: 0.4303  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:28:59] fvcore.common.checkpoint INFO: Saving checkpoint to ./mvit2_lmo_output/model_0004999.pth
[09/18 12:29:00] d2.utils.events INFO:  eta: 14:11:28  iter: 4999  total_loss: 2.326  loss_cls_stage0: 0.1214  loss_box_reg_stage0: 0.239  loss_cls_stage1: 0.1088  loss_box_reg_stage1: 0.6484  loss_cls_stage2: 0.1067  loss_box_reg_stage2: 0.9178  loss_mask: 0.08026  loss_rpn_cls: 0.008144  loss_rpn_loc: 0.02067  time: 0.4303  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:29:09] d2.utils.events INFO:  eta: 14:11:35  iter: 5019  total_loss: 2.077  loss_cls_stage0: 0.1148  loss_box_reg_stage0: 0.2334  loss_cls_stage1: 0.09284  loss_box_reg_stage1: 0.5941  loss_cls_stage2: 0.09533  loss_box_reg_stage2: 0.8352  loss_mask: 0.07614  loss_rpn_cls: 0.004698  loss_rpn_loc: 0.01769  time: 0.4303  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:29:17] d2.utils.events INFO:  eta: 14:11:11  iter: 5039  total_loss: 2.212  loss_cls_stage0: 0.132  loss_box_reg_stage0: 0.2466  loss_cls_stage1: 0.1147  loss_box_reg_stage1: 0.6598  loss_cls_stage2: 0.1147  loss_box_reg_stage2: 0.8635  loss_mask: 0.07856  loss_rpn_cls: 0.005688  loss_rpn_loc: 0.01917  time: 0.4303  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:29:26] d2.utils.events INFO:  eta: 14:10:53  iter: 5059  total_loss: 2.205  loss_cls_stage0: 0.1388  loss_box_reg_stage0: 0.2318  loss_cls_stage1: 0.1192  loss_box_reg_stage1: 0.5865  loss_cls_stage2: 0.1204  loss_box_reg_stage2: 0.8963  loss_mask: 0.07383  loss_rpn_cls: 0.005736  loss_rpn_loc: 0.01771  time: 0.4303  data_time: 0.0046  lr: 0.00016  max_mem: 7560M
[09/18 12:29:35] d2.utils.events INFO:  eta: 14:11:14  iter: 5079  total_loss: 2.267  loss_cls_stage0: 0.1335  loss_box_reg_stage0: 0.253  loss_cls_stage1: 0.09127  loss_box_reg_stage1: 0.6598  loss_cls_stage2: 0.1087  loss_box_reg_stage2: 0.8815  loss_mask: 0.07836  loss_rpn_cls: 0.007021  loss_rpn_loc: 0.02252  time: 0.4303  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:29:43] d2.utils.events INFO:  eta: 14:11:14  iter: 5099  total_loss: 2.36  loss_cls_stage0: 0.1353  loss_box_reg_stage0: 0.2492  loss_cls_stage1: 0.1054  loss_box_reg_stage1: 0.6477  loss_cls_stage2: 0.1205  loss_box_reg_stage2: 0.8695  loss_mask: 0.08484  loss_rpn_cls: 0.004097  loss_rpn_loc: 0.0188  time: 0.4303  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:29:52] d2.utils.events INFO:  eta: 14:10:48  iter: 5119  total_loss: 1.876  loss_cls_stage0: 0.1185  loss_box_reg_stage0: 0.2071  loss_cls_stage1: 0.07223  loss_box_reg_stage1: 0.5333  loss_cls_stage2: 0.06717  loss_box_reg_stage2: 0.7323  loss_mask: 0.07887  loss_rpn_cls: 0.005418  loss_rpn_loc: 0.01937  time: 0.4303  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:30:00] d2.utils.events INFO:  eta: 14:10:00  iter: 5139  total_loss: 1.992  loss_cls_stage0: 0.1123  loss_box_reg_stage0: 0.2256  loss_cls_stage1: 0.09251  loss_box_reg_stage1: 0.5488  loss_cls_stage2: 0.07593  loss_box_reg_stage2: 0.806  loss_mask: 0.08075  loss_rpn_cls: 0.006482  loss_rpn_loc: 0.01762  time: 0.4303  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:30:09] d2.utils.events INFO:  eta: 14:09:57  iter: 5159  total_loss: 2.351  loss_cls_stage0: 0.1283  loss_box_reg_stage0: 0.2389  loss_cls_stage1: 0.0938  loss_box_reg_stage1: 0.6534  loss_cls_stage2: 0.0937  loss_box_reg_stage2: 0.8989  loss_mask: 0.07491  loss_rpn_cls: 0.008117  loss_rpn_loc: 0.01807  time: 0.4303  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:30:18] d2.utils.events INFO:  eta: 14:10:11  iter: 5179  total_loss: 1.904  loss_cls_stage0: 0.1205  loss_box_reg_stage0: 0.2311  loss_cls_stage1: 0.09161  loss_box_reg_stage1: 0.508  loss_cls_stage2: 0.1011  loss_box_reg_stage2: 0.7756  loss_mask: 0.08201  loss_rpn_cls: 0.006235  loss_rpn_loc: 0.01881  time: 0.4303  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:30:26] d2.utils.events INFO:  eta: 14:09:17  iter: 5199  total_loss: 2.13  loss_cls_stage0: 0.1291  loss_box_reg_stage0: 0.2521  loss_cls_stage1: 0.1119  loss_box_reg_stage1: 0.5889  loss_cls_stage2: 0.09845  loss_box_reg_stage2: 0.8381  loss_mask: 0.07906  loss_rpn_cls: 0.005903  loss_rpn_loc: 0.01954  time: 0.4303  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:30:35] d2.utils.events INFO:  eta: 14:08:58  iter: 5219  total_loss: 1.919  loss_cls_stage0: 0.1142  loss_box_reg_stage0: 0.2204  loss_cls_stage1: 0.09067  loss_box_reg_stage1: 0.5503  loss_cls_stage2: 0.1062  loss_box_reg_stage2: 0.7884  loss_mask: 0.07354  loss_rpn_cls: 0.007691  loss_rpn_loc: 0.01866  time: 0.4303  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:30:43] d2.utils.events INFO:  eta: 14:08:43  iter: 5239  total_loss: 1.946  loss_cls_stage0: 0.1067  loss_box_reg_stage0: 0.212  loss_cls_stage1: 0.08447  loss_box_reg_stage1: 0.5552  loss_cls_stage2: 0.07529  loss_box_reg_stage2: 0.8456  loss_mask: 0.07695  loss_rpn_cls: 0.004601  loss_rpn_loc: 0.0189  time: 0.4303  data_time: 0.0039  lr: 0.00016  max_mem: 7560M
[09/18 12:30:52] d2.utils.events INFO:  eta: 14:08:22  iter: 5259  total_loss: 2.101  loss_cls_stage0: 0.1094  loss_box_reg_stage0: 0.2221  loss_cls_stage1: 0.07293  loss_box_reg_stage1: 0.5987  loss_cls_stage2: 0.08128  loss_box_reg_stage2: 0.8214  loss_mask: 0.07407  loss_rpn_cls: 0.005943  loss_rpn_loc: 0.02154  time: 0.4302  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:31:00] d2.utils.events INFO:  eta: 14:08:18  iter: 5279  total_loss: 1.866  loss_cls_stage0: 0.1011  loss_box_reg_stage0: 0.2182  loss_cls_stage1: 0.07003  loss_box_reg_stage1: 0.5635  loss_cls_stage2: 0.06095  loss_box_reg_stage2: 0.7354  loss_mask: 0.07745  loss_rpn_cls: 0.003769  loss_rpn_loc: 0.01924  time: 0.4302  data_time: 0.0038  lr: 0.00016  max_mem: 7560M
[09/18 12:31:09] d2.utils.events INFO:  eta: 14:08:40  iter: 5299  total_loss: 1.942  loss_cls_stage0: 0.1084  loss_box_reg_stage0: 0.2186  loss_cls_stage1: 0.08442  loss_box_reg_stage1: 0.5522  loss_cls_stage2: 0.09234  loss_box_reg_stage2: 0.8056  loss_mask: 0.06812  loss_rpn_cls: 0.003481  loss_rpn_loc: 0.01736  time: 0.4302  data_time: 0.0039  lr: 0.00016  max_mem: 7560M
[09/18 12:31:18] d2.utils.events INFO:  eta: 14:08:49  iter: 5319  total_loss: 2.29  loss_cls_stage0: 0.1222  loss_box_reg_stage0: 0.2332  loss_cls_stage1: 0.1043  loss_box_reg_stage1: 0.6653  loss_cls_stage2: 0.1013  loss_box_reg_stage2: 0.9499  loss_mask: 0.07389  loss_rpn_cls: 0.004375  loss_rpn_loc: 0.01848  time: 0.4302  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:31:27] d2.utils.events INFO:  eta: 14:09:43  iter: 5339  total_loss: 1.849  loss_cls_stage0: 0.1046  loss_box_reg_stage0: 0.2026  loss_cls_stage1: 0.06825  loss_box_reg_stage1: 0.5224  loss_cls_stage2: 0.07357  loss_box_reg_stage2: 0.7981  loss_mask: 0.07471  loss_rpn_cls: 0.003212  loss_rpn_loc: 0.0178  time: 0.4303  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:31:36] d2.utils.events INFO:  eta: 14:10:14  iter: 5359  total_loss: 2.178  loss_cls_stage0: 0.1322  loss_box_reg_stage0: 0.2629  loss_cls_stage1: 0.09491  loss_box_reg_stage1: 0.6212  loss_cls_stage2: 0.09539  loss_box_reg_stage2: 0.8205  loss_mask: 0.08072  loss_rpn_cls: 0.006935  loss_rpn_loc: 0.01987  time: 0.4304  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:31:44] d2.utils.events INFO:  eta: 14:10:57  iter: 5379  total_loss: 1.746  loss_cls_stage0: 0.1104  loss_box_reg_stage0: 0.2112  loss_cls_stage1: 0.08486  loss_box_reg_stage1: 0.4936  loss_cls_stage2: 0.07893  loss_box_reg_stage2: 0.678  loss_mask: 0.07295  loss_rpn_cls: 0.002913  loss_rpn_loc: 0.01836  time: 0.4304  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:31:53] d2.utils.events INFO:  eta: 14:10:03  iter: 5399  total_loss: 1.935  loss_cls_stage0: 0.1139  loss_box_reg_stage0: 0.2261  loss_cls_stage1: 0.08654  loss_box_reg_stage1: 0.5504  loss_cls_stage2: 0.09027  loss_box_reg_stage2: 0.7945  loss_mask: 0.07126  loss_rpn_cls: 0.004148  loss_rpn_loc: 0.01932  time: 0.4303  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:32:01] d2.utils.events INFO:  eta: 14:09:43  iter: 5419  total_loss: 1.911  loss_cls_stage0: 0.102  loss_box_reg_stage0: 0.2043  loss_cls_stage1: 0.09071  loss_box_reg_stage1: 0.5162  loss_cls_stage2: 0.09975  loss_box_reg_stage2: 0.7487  loss_mask: 0.07037  loss_rpn_cls: 0.004677  loss_rpn_loc: 0.01905  time: 0.4303  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:32:10] d2.utils.events INFO:  eta: 14:08:45  iter: 5439  total_loss: 2.001  loss_cls_stage0: 0.114  loss_box_reg_stage0: 0.2304  loss_cls_stage1: 0.08404  loss_box_reg_stage1: 0.5589  loss_cls_stage2: 0.08607  loss_box_reg_stage2: 0.7852  loss_mask: 0.07467  loss_rpn_cls: 0.006899  loss_rpn_loc: 0.01951  time: 0.4303  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:32:18] d2.utils.events INFO:  eta: 14:08:33  iter: 5459  total_loss: 2.15  loss_cls_stage0: 0.1266  loss_box_reg_stage0: 0.2407  loss_cls_stage1: 0.1003  loss_box_reg_stage1: 0.5989  loss_cls_stage2: 0.0977  loss_box_reg_stage2: 0.8963  loss_mask: 0.07725  loss_rpn_cls: 0.004098  loss_rpn_loc: 0.01829  time: 0.4303  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:32:27] d2.utils.events INFO:  eta: 14:07:49  iter: 5479  total_loss: 2.018  loss_cls_stage0: 0.1197  loss_box_reg_stage0: 0.2254  loss_cls_stage1: 0.09067  loss_box_reg_stage1: 0.6069  loss_cls_stage2: 0.1082  loss_box_reg_stage2: 0.8009  loss_mask: 0.0824  loss_rpn_cls: 0.005122  loss_rpn_loc: 0.0198  time: 0.4302  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:32:35] d2.utils.events INFO:  eta: 14:07:15  iter: 5499  total_loss: 2.108  loss_cls_stage0: 0.1211  loss_box_reg_stage0: 0.2451  loss_cls_stage1: 0.1062  loss_box_reg_stage1: 0.6033  loss_cls_stage2: 0.1015  loss_box_reg_stage2: 0.8072  loss_mask: 0.07746  loss_rpn_cls: 0.004194  loss_rpn_loc: 0.01823  time: 0.4302  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:32:44] d2.utils.events INFO:  eta: 14:07:11  iter: 5519  total_loss: 1.897  loss_cls_stage0: 0.1219  loss_box_reg_stage0: 0.2137  loss_cls_stage1: 0.084  loss_box_reg_stage1: 0.5559  loss_cls_stage2: 0.09301  loss_box_reg_stage2: 0.7159  loss_mask: 0.07184  loss_rpn_cls: 0.00307  loss_rpn_loc: 0.01807  time: 0.4302  data_time: 0.0045  lr: 0.00016  max_mem: 7560M
[09/18 12:32:52] d2.utils.events INFO:  eta: 14:06:58  iter: 5539  total_loss: 1.828  loss_cls_stage0: 0.1201  loss_box_reg_stage0: 0.2045  loss_cls_stage1: 0.09786  loss_box_reg_stage1: 0.5276  loss_cls_stage2: 0.09138  loss_box_reg_stage2: 0.8017  loss_mask: 0.07413  loss_rpn_cls: 0.003816  loss_rpn_loc: 0.01543  time: 0.4302  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:33:01] d2.utils.events INFO:  eta: 14:07:26  iter: 5559  total_loss: 1.948  loss_cls_stage0: 0.1098  loss_box_reg_stage0: 0.2082  loss_cls_stage1: 0.08185  loss_box_reg_stage1: 0.5463  loss_cls_stage2: 0.0811  loss_box_reg_stage2: 0.7975  loss_mask: 0.07759  loss_rpn_cls: 0.005248  loss_rpn_loc: 0.01803  time: 0.4301  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:33:09] d2.utils.events INFO:  eta: 14:07:19  iter: 5579  total_loss: 1.829  loss_cls_stage0: 0.1079  loss_box_reg_stage0: 0.2172  loss_cls_stage1: 0.07194  loss_box_reg_stage1: 0.5084  loss_cls_stage2: 0.07346  loss_box_reg_stage2: 0.7062  loss_mask: 0.0713  loss_rpn_cls: 0.007562  loss_rpn_loc: 0.01655  time: 0.4301  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:33:18] d2.utils.events INFO:  eta: 14:07:34  iter: 5599  total_loss: 2.202  loss_cls_stage0: 0.1522  loss_box_reg_stage0: 0.2487  loss_cls_stage1: 0.1076  loss_box_reg_stage1: 0.6183  loss_cls_stage2: 0.1052  loss_box_reg_stage2: 0.7885  loss_mask: 0.0783  loss_rpn_cls: 0.006586  loss_rpn_loc: 0.02119  time: 0.4301  data_time: 0.0046  lr: 0.00016  max_mem: 7560M
[09/18 12:33:27] d2.utils.events INFO:  eta: 14:07:41  iter: 5619  total_loss: 2.117  loss_cls_stage0: 0.1239  loss_box_reg_stage0: 0.2328  loss_cls_stage1: 0.08825  loss_box_reg_stage1: 0.6365  loss_cls_stage2: 0.07786  loss_box_reg_stage2: 0.8224  loss_mask: 0.07583  loss_rpn_cls: 0.003671  loss_rpn_loc: 0.02034  time: 0.4301  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:33:35] d2.utils.events INFO:  eta: 14:07:51  iter: 5639  total_loss: 2.33  loss_cls_stage0: 0.1233  loss_box_reg_stage0: 0.2243  loss_cls_stage1: 0.1169  loss_box_reg_stage1: 0.6533  loss_cls_stage2: 0.1046  loss_box_reg_stage2: 0.9598  loss_mask: 0.07368  loss_rpn_cls: 0.006031  loss_rpn_loc: 0.01849  time: 0.4301  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:33:44] d2.utils.events INFO:  eta: 14:07:27  iter: 5659  total_loss: 2.13  loss_cls_stage0: 0.1337  loss_box_reg_stage0: 0.237  loss_cls_stage1: 0.09212  loss_box_reg_stage1: 0.6035  loss_cls_stage2: 0.08246  loss_box_reg_stage2: 0.8753  loss_mask: 0.0799  loss_rpn_cls: 0.006133  loss_rpn_loc: 0.01995  time: 0.4301  data_time: 0.0045  lr: 0.00016  max_mem: 7560M
[09/18 12:33:52] d2.utils.events INFO:  eta: 14:07:24  iter: 5679  total_loss: 2.086  loss_cls_stage0: 0.1256  loss_box_reg_stage0: 0.2352  loss_cls_stage1: 0.09206  loss_box_reg_stage1: 0.5803  loss_cls_stage2: 0.08104  loss_box_reg_stage2: 0.8128  loss_mask: 0.07271  loss_rpn_cls: 0.008604  loss_rpn_loc: 0.01878  time: 0.4301  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:34:01] d2.utils.events INFO:  eta: 14:07:07  iter: 5699  total_loss: 2.016  loss_cls_stage0: 0.1157  loss_box_reg_stage0: 0.2239  loss_cls_stage1: 0.0946  loss_box_reg_stage1: 0.5794  loss_cls_stage2: 0.08427  loss_box_reg_stage2: 0.842  loss_mask: 0.07495  loss_rpn_cls: 0.006091  loss_rpn_loc: 0.01907  time: 0.4301  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:34:09] d2.utils.events INFO:  eta: 14:07:02  iter: 5719  total_loss: 2.102  loss_cls_stage0: 0.1303  loss_box_reg_stage0: 0.2256  loss_cls_stage1: 0.09803  loss_box_reg_stage1: 0.6261  loss_cls_stage2: 0.09061  loss_box_reg_stage2: 0.8458  loss_mask: 0.07744  loss_rpn_cls: 0.006266  loss_rpn_loc: 0.01873  time: 0.4301  data_time: 0.0048  lr: 0.00016  max_mem: 7560M
[09/18 12:34:18] d2.utils.events INFO:  eta: 14:06:53  iter: 5739  total_loss: 2.29  loss_cls_stage0: 0.1343  loss_box_reg_stage0: 0.2391  loss_cls_stage1: 0.1031  loss_box_reg_stage1: 0.6504  loss_cls_stage2: 0.1078  loss_box_reg_stage2: 0.9462  loss_mask: 0.08205  loss_rpn_cls: 0.006048  loss_rpn_loc: 0.01944  time: 0.4301  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:34:26] d2.utils.events INFO:  eta: 14:06:29  iter: 5759  total_loss: 1.888  loss_cls_stage0: 0.1085  loss_box_reg_stage0: 0.2155  loss_cls_stage1: 0.08202  loss_box_reg_stage1: 0.54  loss_cls_stage2: 0.07408  loss_box_reg_stage2: 0.7371  loss_mask: 0.07692  loss_rpn_cls: 0.00404  loss_rpn_loc: 0.0205  time: 0.4300  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:34:35] d2.utils.events INFO:  eta: 14:06:03  iter: 5779  total_loss: 1.907  loss_cls_stage0: 0.1104  loss_box_reg_stage0: 0.226  loss_cls_stage1: 0.07708  loss_box_reg_stage1: 0.5313  loss_cls_stage2: 0.09441  loss_box_reg_stage2: 0.7593  loss_mask: 0.07993  loss_rpn_cls: 0.007229  loss_rpn_loc: 0.01806  time: 0.4300  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:34:43] d2.utils.events INFO:  eta: 14:06:15  iter: 5799  total_loss: 1.813  loss_cls_stage0: 0.1037  loss_box_reg_stage0: 0.2026  loss_cls_stage1: 0.07643  loss_box_reg_stage1: 0.5262  loss_cls_stage2: 0.07076  loss_box_reg_stage2: 0.7214  loss_mask: 0.07496  loss_rpn_cls: 0.005279  loss_rpn_loc: 0.01692  time: 0.4300  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:34:52] d2.utils.events INFO:  eta: 14:06:22  iter: 5819  total_loss: 2.07  loss_cls_stage0: 0.1113  loss_box_reg_stage0: 0.2349  loss_cls_stage1: 0.08308  loss_box_reg_stage1: 0.6314  loss_cls_stage2: 0.07912  loss_box_reg_stage2: 0.8405  loss_mask: 0.07379  loss_rpn_cls: 0.00605  loss_rpn_loc: 0.01916  time: 0.4300  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:35:01] d2.utils.events INFO:  eta: 14:06:52  iter: 5839  total_loss: 2.156  loss_cls_stage0: 0.1262  loss_box_reg_stage0: 0.2469  loss_cls_stage1: 0.09559  loss_box_reg_stage1: 0.6328  loss_cls_stage2: 0.09396  loss_box_reg_stage2: 0.9344  loss_mask: 0.07235  loss_rpn_cls: 0.006141  loss_rpn_loc: 0.0194  time: 0.4300  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:35:09] d2.utils.events INFO:  eta: 14:07:23  iter: 5859  total_loss: 1.865  loss_cls_stage0: 0.1158  loss_box_reg_stage0: 0.2001  loss_cls_stage1: 0.07533  loss_box_reg_stage1: 0.5327  loss_cls_stage2: 0.07589  loss_box_reg_stage2: 0.7925  loss_mask: 0.06774  loss_rpn_cls: 0.005499  loss_rpn_loc: 0.01613  time: 0.4300  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:35:18] d2.utils.events INFO:  eta: 14:07:27  iter: 5879  total_loss: 2.175  loss_cls_stage0: 0.1193  loss_box_reg_stage0: 0.233  loss_cls_stage1: 0.09911  loss_box_reg_stage1: 0.6559  loss_cls_stage2: 0.09901  loss_box_reg_stage2: 0.8986  loss_mask: 0.08227  loss_rpn_cls: 0.004094  loss_rpn_loc: 0.01967  time: 0.4300  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:35:26] d2.utils.events INFO:  eta: 14:07:50  iter: 5899  total_loss: 2.041  loss_cls_stage0: 0.1276  loss_box_reg_stage0: 0.2281  loss_cls_stage1: 0.0856  loss_box_reg_stage1: 0.5628  loss_cls_stage2: 0.08696  loss_box_reg_stage2: 0.8527  loss_mask: 0.07309  loss_rpn_cls: 0.004373  loss_rpn_loc: 0.01701  time: 0.4300  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:35:35] d2.utils.events INFO:  eta: 14:07:18  iter: 5919  total_loss: 1.856  loss_cls_stage0: 0.116  loss_box_reg_stage0: 0.2092  loss_cls_stage1: 0.09121  loss_box_reg_stage1: 0.5124  loss_cls_stage2: 0.08808  loss_box_reg_stage2: 0.7564  loss_mask: 0.07626  loss_rpn_cls: 0.007152  loss_rpn_loc: 0.01875  time: 0.4300  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:35:43] d2.utils.events INFO:  eta: 14:07:11  iter: 5939  total_loss: 2.053  loss_cls_stage0: 0.1263  loss_box_reg_stage0: 0.2382  loss_cls_stage1: 0.1043  loss_box_reg_stage1: 0.5672  loss_cls_stage2: 0.09683  loss_box_reg_stage2: 0.7694  loss_mask: 0.08198  loss_rpn_cls: 0.006352  loss_rpn_loc: 0.0202  time: 0.4299  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:35:52] d2.utils.events INFO:  eta: 14:07:16  iter: 5959  total_loss: 1.944  loss_cls_stage0: 0.1069  loss_box_reg_stage0: 0.2148  loss_cls_stage1: 0.07564  loss_box_reg_stage1: 0.5933  loss_cls_stage2: 0.07798  loss_box_reg_stage2: 0.8166  loss_mask: 0.07351  loss_rpn_cls: 0.007333  loss_rpn_loc: 0.019  time: 0.4299  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:36:00] d2.utils.events INFO:  eta: 14:06:49  iter: 5979  total_loss: 1.906  loss_cls_stage0: 0.1154  loss_box_reg_stage0: 0.1952  loss_cls_stage1: 0.09067  loss_box_reg_stage1: 0.5387  loss_cls_stage2: 0.09796  loss_box_reg_stage2: 0.8064  loss_mask: 0.07019  loss_rpn_cls: 0.006057  loss_rpn_loc: 0.02069  time: 0.4299  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:36:09] d2.utils.events INFO:  eta: 14:06:19  iter: 5999  total_loss: 1.958  loss_cls_stage0: 0.1228  loss_box_reg_stage0: 0.2258  loss_cls_stage1: 0.09169  loss_box_reg_stage1: 0.5269  loss_cls_stage2: 0.08981  loss_box_reg_stage2: 0.787  loss_mask: 0.07514  loss_rpn_cls: 0.006329  loss_rpn_loc: 0.01882  time: 0.4299  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:36:17] d2.utils.events INFO:  eta: 14:04:57  iter: 6019  total_loss: 2.011  loss_cls_stage0: 0.1276  loss_box_reg_stage0: 0.2196  loss_cls_stage1: 0.09935  loss_box_reg_stage1: 0.5735  loss_cls_stage2: 0.09637  loss_box_reg_stage2: 0.8002  loss_mask: 0.07362  loss_rpn_cls: 0.004383  loss_rpn_loc: 0.01686  time: 0.4299  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:36:26] d2.utils.events INFO:  eta: 14:04:44  iter: 6039  total_loss: 2.364  loss_cls_stage0: 0.129  loss_box_reg_stage0: 0.2251  loss_cls_stage1: 0.1077  loss_box_reg_stage1: 0.6467  loss_cls_stage2: 0.1244  loss_box_reg_stage2: 0.8973  loss_mask: 0.07756  loss_rpn_cls: 0.005573  loss_rpn_loc: 0.01924  time: 0.4299  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:36:35] d2.utils.events INFO:  eta: 14:04:34  iter: 6059  total_loss: 2.182  loss_cls_stage0: 0.1266  loss_box_reg_stage0: 0.2365  loss_cls_stage1: 0.09334  loss_box_reg_stage1: 0.6389  loss_cls_stage2: 0.09873  loss_box_reg_stage2: 0.8451  loss_mask: 0.077  loss_rpn_cls: 0.003527  loss_rpn_loc: 0.01805  time: 0.4299  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:36:43] d2.utils.events INFO:  eta: 14:03:56  iter: 6079  total_loss: 1.903  loss_cls_stage0: 0.1133  loss_box_reg_stage0: 0.2211  loss_cls_stage1: 0.08802  loss_box_reg_stage1: 0.5477  loss_cls_stage2: 0.07153  loss_box_reg_stage2: 0.7433  loss_mask: 0.08127  loss_rpn_cls: 0.006908  loss_rpn_loc: 0.0188  time: 0.4298  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:36:52] d2.utils.events INFO:  eta: 14:03:53  iter: 6099  total_loss: 2.162  loss_cls_stage0: 0.1262  loss_box_reg_stage0: 0.2452  loss_cls_stage1: 0.08614  loss_box_reg_stage1: 0.6189  loss_cls_stage2: 0.0857  loss_box_reg_stage2: 0.8714  loss_mask: 0.08236  loss_rpn_cls: 0.008892  loss_rpn_loc: 0.02127  time: 0.4298  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:37:00] d2.utils.events INFO:  eta: 14:03:45  iter: 6119  total_loss: 1.952  loss_cls_stage0: 0.1199  loss_box_reg_stage0: 0.2299  loss_cls_stage1: 0.09416  loss_box_reg_stage1: 0.5362  loss_cls_stage2: 0.08743  loss_box_reg_stage2: 0.748  loss_mask: 0.08506  loss_rpn_cls: 0.009378  loss_rpn_loc: 0.02031  time: 0.4298  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:37:09] d2.utils.events INFO:  eta: 14:03:48  iter: 6139  total_loss: 2.334  loss_cls_stage0: 0.1231  loss_box_reg_stage0: 0.2261  loss_cls_stage1: 0.09762  loss_box_reg_stage1: 0.6494  loss_cls_stage2: 0.09014  loss_box_reg_stage2: 0.9544  loss_mask: 0.07678  loss_rpn_cls: 0.005082  loss_rpn_loc: 0.01914  time: 0.4298  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:37:17] d2.utils.events INFO:  eta: 14:03:28  iter: 6159  total_loss: 1.82  loss_cls_stage0: 0.1065  loss_box_reg_stage0: 0.1856  loss_cls_stage1: 0.07544  loss_box_reg_stage1: 0.534  loss_cls_stage2: 0.0734  loss_box_reg_stage2: 0.7497  loss_mask: 0.07085  loss_rpn_cls: 0.002483  loss_rpn_loc: 0.01642  time: 0.4298  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:37:26] d2.utils.events INFO:  eta: 14:03:23  iter: 6179  total_loss: 2.055  loss_cls_stage0: 0.1042  loss_box_reg_stage0: 0.2397  loss_cls_stage1: 0.09383  loss_box_reg_stage1: 0.5869  loss_cls_stage2: 0.09249  loss_box_reg_stage2: 0.7634  loss_mask: 0.07233  loss_rpn_cls: 0.0079  loss_rpn_loc: 0.01778  time: 0.4299  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:37:35] d2.utils.events INFO:  eta: 14:03:36  iter: 6199  total_loss: 2.138  loss_cls_stage0: 0.1076  loss_box_reg_stage0: 0.2474  loss_cls_stage1: 0.08131  loss_box_reg_stage1: 0.5439  loss_cls_stage2: 0.07266  loss_box_reg_stage2: 0.7614  loss_mask: 0.07889  loss_rpn_cls: 0.005049  loss_rpn_loc: 0.01916  time: 0.4299  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:37:44] d2.utils.events INFO:  eta: 14:03:26  iter: 6219  total_loss: 2.173  loss_cls_stage0: 0.118  loss_box_reg_stage0: 0.2521  loss_cls_stage1: 0.08703  loss_box_reg_stage1: 0.6181  loss_cls_stage2: 0.08951  loss_box_reg_stage2: 0.8522  loss_mask: 0.08074  loss_rpn_cls: 0.006349  loss_rpn_loc: 0.02052  time: 0.4299  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:37:52] d2.utils.events INFO:  eta: 14:03:17  iter: 6239  total_loss: 1.964  loss_cls_stage0: 0.1172  loss_box_reg_stage0: 0.2183  loss_cls_stage1: 0.07782  loss_box_reg_stage1: 0.5501  loss_cls_stage2: 0.08683  loss_box_reg_stage2: 0.8347  loss_mask: 0.0752  loss_rpn_cls: 0.006298  loss_rpn_loc: 0.0183  time: 0.4299  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:38:01] d2.utils.events INFO:  eta: 14:03:10  iter: 6259  total_loss: 1.962  loss_cls_stage0: 0.1177  loss_box_reg_stage0: 0.2253  loss_cls_stage1: 0.09071  loss_box_reg_stage1: 0.5518  loss_cls_stage2: 0.08268  loss_box_reg_stage2: 0.8014  loss_mask: 0.07426  loss_rpn_cls: 0.005003  loss_rpn_loc: 0.01711  time: 0.4299  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:38:09] d2.utils.events INFO:  eta: 14:03:00  iter: 6279  total_loss: 2.446  loss_cls_stage0: 0.1476  loss_box_reg_stage0: 0.2684  loss_cls_stage1: 0.1303  loss_box_reg_stage1: 0.661  loss_cls_stage2: 0.1389  loss_box_reg_stage2: 0.9114  loss_mask: 0.08061  loss_rpn_cls: 0.009244  loss_rpn_loc: 0.02033  time: 0.4298  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:38:18] d2.utils.events INFO:  eta: 14:02:43  iter: 6299  total_loss: 1.897  loss_cls_stage0: 0.1068  loss_box_reg_stage0: 0.2269  loss_cls_stage1: 0.094  loss_box_reg_stage1: 0.5516  loss_cls_stage2: 0.07378  loss_box_reg_stage2: 0.7503  loss_mask: 0.06927  loss_rpn_cls: 0.005344  loss_rpn_loc: 0.01773  time: 0.4298  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:38:26] d2.utils.events INFO:  eta: 14:02:00  iter: 6319  total_loss: 2.178  loss_cls_stage0: 0.1234  loss_box_reg_stage0: 0.2433  loss_cls_stage1: 0.09615  loss_box_reg_stage1: 0.5826  loss_cls_stage2: 0.09469  loss_box_reg_stage2: 0.8319  loss_mask: 0.07918  loss_rpn_cls: 0.005509  loss_rpn_loc: 0.01982  time: 0.4298  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:38:35] d2.utils.events INFO:  eta: 14:01:32  iter: 6339  total_loss: 2.099  loss_cls_stage0: 0.1153  loss_box_reg_stage0: 0.244  loss_cls_stage1: 0.09575  loss_box_reg_stage1: 0.5986  loss_cls_stage2: 0.1023  loss_box_reg_stage2: 0.8275  loss_mask: 0.07901  loss_rpn_cls: 0.006515  loss_rpn_loc: 0.01957  time: 0.4298  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:38:43] d2.utils.events INFO:  eta: 14:00:35  iter: 6359  total_loss: 2.207  loss_cls_stage0: 0.1311  loss_box_reg_stage0: 0.2375  loss_cls_stage1: 0.09949  loss_box_reg_stage1: 0.5933  loss_cls_stage2: 0.09517  loss_box_reg_stage2: 0.8334  loss_mask: 0.07993  loss_rpn_cls: 0.00531  loss_rpn_loc: 0.01873  time: 0.4298  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:38:52] d2.utils.events INFO:  eta: 13:59:59  iter: 6379  total_loss: 2.163  loss_cls_stage0: 0.1239  loss_box_reg_stage0: 0.2513  loss_cls_stage1: 0.09346  loss_box_reg_stage1: 0.6081  loss_cls_stage2: 0.09048  loss_box_reg_stage2: 0.8024  loss_mask: 0.07183  loss_rpn_cls: 0.007544  loss_rpn_loc: 0.02015  time: 0.4297  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:39:00] d2.utils.events INFO:  eta: 14:00:06  iter: 6399  total_loss: 2.118  loss_cls_stage0: 0.13  loss_box_reg_stage0: 0.2455  loss_cls_stage1: 0.1091  loss_box_reg_stage1: 0.5802  loss_cls_stage2: 0.09968  loss_box_reg_stage2: 0.8374  loss_mask: 0.07557  loss_rpn_cls: 0.00602  loss_rpn_loc: 0.01759  time: 0.4297  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:39:09] d2.utils.events INFO:  eta: 14:00:14  iter: 6419  total_loss: 2.069  loss_cls_stage0: 0.1359  loss_box_reg_stage0: 0.2623  loss_cls_stage1: 0.1006  loss_box_reg_stage1: 0.5822  loss_cls_stage2: 0.09985  loss_box_reg_stage2: 0.7096  loss_mask: 0.0791  loss_rpn_cls: 0.006205  loss_rpn_loc: 0.01996  time: 0.4297  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:39:17] d2.utils.events INFO:  eta: 14:00:09  iter: 6439  total_loss: 2.03  loss_cls_stage0: 0.1075  loss_box_reg_stage0: 0.2296  loss_cls_stage1: 0.08006  loss_box_reg_stage1: 0.5954  loss_cls_stage2: 0.08473  loss_box_reg_stage2: 0.853  loss_mask: 0.07464  loss_rpn_cls: 0.005778  loss_rpn_loc: 0.02132  time: 0.4297  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:39:26] d2.utils.events INFO:  eta: 14:00:09  iter: 6459  total_loss: 1.986  loss_cls_stage0: 0.1209  loss_box_reg_stage0: 0.2342  loss_cls_stage1: 0.09186  loss_box_reg_stage1: 0.5869  loss_cls_stage2: 0.0771  loss_box_reg_stage2: 0.771  loss_mask: 0.07178  loss_rpn_cls: 0.003172  loss_rpn_loc: 0.01849  time: 0.4297  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:39:34] d2.utils.events INFO:  eta: 14:00:04  iter: 6479  total_loss: 1.951  loss_cls_stage0: 0.1111  loss_box_reg_stage0: 0.2412  loss_cls_stage1: 0.09125  loss_box_reg_stage1: 0.5501  loss_cls_stage2: 0.07487  loss_box_reg_stage2: 0.8019  loss_mask: 0.07592  loss_rpn_cls: 0.004346  loss_rpn_loc: 0.01838  time: 0.4296  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:39:43] d2.utils.events INFO:  eta: 13:59:57  iter: 6499  total_loss: 1.743  loss_cls_stage0: 0.1129  loss_box_reg_stage0: 0.22  loss_cls_stage1: 0.07387  loss_box_reg_stage1: 0.4961  loss_cls_stage2: 0.0647  loss_box_reg_stage2: 0.6985  loss_mask: 0.07285  loss_rpn_cls: 0.003623  loss_rpn_loc: 0.0166  time: 0.4296  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:39:51] d2.utils.events INFO:  eta: 13:59:47  iter: 6519  total_loss: 2.149  loss_cls_stage0: 0.1183  loss_box_reg_stage0: 0.2456  loss_cls_stage1: 0.1024  loss_box_reg_stage1: 0.6069  loss_cls_stage2: 0.1009  loss_box_reg_stage2: 0.8793  loss_mask: 0.08057  loss_rpn_cls: 0.004845  loss_rpn_loc: 0.01756  time: 0.4296  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:39:59] d2.utils.events INFO:  eta: 13:59:44  iter: 6539  total_loss: 2.075  loss_cls_stage0: 0.1324  loss_box_reg_stage0: 0.216  loss_cls_stage1: 0.1209  loss_box_reg_stage1: 0.5948  loss_cls_stage2: 0.1001  loss_box_reg_stage2: 0.8514  loss_mask: 0.07621  loss_rpn_cls: 0.004267  loss_rpn_loc: 0.02033  time: 0.4296  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:40:08] d2.utils.events INFO:  eta: 13:59:43  iter: 6559  total_loss: 2.122  loss_cls_stage0: 0.1238  loss_box_reg_stage0: 0.232  loss_cls_stage1: 0.09445  loss_box_reg_stage1: 0.587  loss_cls_stage2: 0.0994  loss_box_reg_stage2: 0.8908  loss_mask: 0.07814  loss_rpn_cls: 0.004373  loss_rpn_loc: 0.0181  time: 0.4296  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:40:17] d2.utils.events INFO:  eta: 13:59:39  iter: 6579  total_loss: 2.108  loss_cls_stage0: 0.1299  loss_box_reg_stage0: 0.2493  loss_cls_stage1: 0.09372  loss_box_reg_stage1: 0.592  loss_cls_stage2: 0.0946  loss_box_reg_stage2: 0.8635  loss_mask: 0.08168  loss_rpn_cls: 0.006063  loss_rpn_loc: 0.01995  time: 0.4296  data_time: 0.0045  lr: 0.00016  max_mem: 7560M
[09/18 12:40:25] d2.utils.events INFO:  eta: 13:59:23  iter: 6599  total_loss: 1.896  loss_cls_stage0: 0.1223  loss_box_reg_stage0: 0.2097  loss_cls_stage1: 0.09144  loss_box_reg_stage1: 0.5224  loss_cls_stage2: 0.07801  loss_box_reg_stage2: 0.7113  loss_mask: 0.07259  loss_rpn_cls: 0.005466  loss_rpn_loc: 0.01655  time: 0.4296  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:40:34] d2.utils.events INFO:  eta: 13:58:36  iter: 6619  total_loss: 2.037  loss_cls_stage0: 0.1268  loss_box_reg_stage0: 0.2228  loss_cls_stage1: 0.08007  loss_box_reg_stage1: 0.5601  loss_cls_stage2: 0.07784  loss_box_reg_stage2: 0.8409  loss_mask: 0.07998  loss_rpn_cls: 0.006474  loss_rpn_loc: 0.01829  time: 0.4296  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:40:42] d2.utils.events INFO:  eta: 13:58:09  iter: 6639  total_loss: 1.904  loss_cls_stage0: 0.113  loss_box_reg_stage0: 0.2188  loss_cls_stage1: 0.08155  loss_box_reg_stage1: 0.5286  loss_cls_stage2: 0.09597  loss_box_reg_stage2: 0.7643  loss_mask: 0.07363  loss_rpn_cls: 0.005718  loss_rpn_loc: 0.01805  time: 0.4295  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:40:51] d2.utils.events INFO:  eta: 13:58:03  iter: 6659  total_loss: 1.887  loss_cls_stage0: 0.1084  loss_box_reg_stage0: 0.203  loss_cls_stage1: 0.08229  loss_box_reg_stage1: 0.5278  loss_cls_stage2: 0.07685  loss_box_reg_stage2: 0.8355  loss_mask: 0.06749  loss_rpn_cls: 0.004243  loss_rpn_loc: 0.0167  time: 0.4295  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:40:59] d2.utils.events INFO:  eta: 13:57:45  iter: 6679  total_loss: 1.891  loss_cls_stage0: 0.1176  loss_box_reg_stage0: 0.2192  loss_cls_stage1: 0.09738  loss_box_reg_stage1: 0.5504  loss_cls_stage2: 0.09598  loss_box_reg_stage2: 0.7963  loss_mask: 0.07312  loss_rpn_cls: 0.007804  loss_rpn_loc: 0.01743  time: 0.4295  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:41:08] d2.utils.events INFO:  eta: 13:57:14  iter: 6699  total_loss: 2.036  loss_cls_stage0: 0.1114  loss_box_reg_stage0: 0.221  loss_cls_stage1: 0.08543  loss_box_reg_stage1: 0.5939  loss_cls_stage2: 0.07445  loss_box_reg_stage2: 0.8521  loss_mask: 0.07364  loss_rpn_cls: 0.003372  loss_rpn_loc: 0.01768  time: 0.4295  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:41:16] d2.utils.events INFO:  eta: 13:56:58  iter: 6719  total_loss: 1.961  loss_cls_stage0: 0.1187  loss_box_reg_stage0: 0.199  loss_cls_stage1: 0.09966  loss_box_reg_stage1: 0.5185  loss_cls_stage2: 0.09324  loss_box_reg_stage2: 0.7618  loss_mask: 0.07652  loss_rpn_cls: 0.005536  loss_rpn_loc: 0.01804  time: 0.4294  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:41:24] d2.utils.events INFO:  eta: 13:56:41  iter: 6739  total_loss: 1.733  loss_cls_stage0: 0.1005  loss_box_reg_stage0: 0.2044  loss_cls_stage1: 0.08523  loss_box_reg_stage1: 0.5129  loss_cls_stage2: 0.07535  loss_box_reg_stage2: 0.7409  loss_mask: 0.07085  loss_rpn_cls: 0.005338  loss_rpn_loc: 0.01679  time: 0.4294  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:41:33] d2.utils.events INFO:  eta: 13:56:31  iter: 6759  total_loss: 1.901  loss_cls_stage0: 0.1211  loss_box_reg_stage0: 0.2272  loss_cls_stage1: 0.1058  loss_box_reg_stage1: 0.5305  loss_cls_stage2: 0.09575  loss_box_reg_stage2: 0.7931  loss_mask: 0.07371  loss_rpn_cls: 0.005189  loss_rpn_loc: 0.01831  time: 0.4294  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:41:42] d2.utils.events INFO:  eta: 13:56:46  iter: 6779  total_loss: 1.688  loss_cls_stage0: 0.09994  loss_box_reg_stage0: 0.1938  loss_cls_stage1: 0.06718  loss_box_reg_stage1: 0.4634  loss_cls_stage2: 0.06476  loss_box_reg_stage2: 0.6854  loss_mask: 0.06848  loss_rpn_cls: 0.004976  loss_rpn_loc: 0.01581  time: 0.4294  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:41:50] d2.utils.events INFO:  eta: 13:56:56  iter: 6799  total_loss: 1.769  loss_cls_stage0: 0.09529  loss_box_reg_stage0: 0.2157  loss_cls_stage1: 0.06599  loss_box_reg_stage1: 0.4986  loss_cls_stage2: 0.06578  loss_box_reg_stage2: 0.7747  loss_mask: 0.07584  loss_rpn_cls: 0.002547  loss_rpn_loc: 0.01656  time: 0.4294  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:41:59] d2.utils.events INFO:  eta: 13:56:46  iter: 6819  total_loss: 1.952  loss_cls_stage0: 0.1066  loss_box_reg_stage0: 0.2215  loss_cls_stage1: 0.0761  loss_box_reg_stage1: 0.5503  loss_cls_stage2: 0.07715  loss_box_reg_stage2: 0.7626  loss_mask: 0.07529  loss_rpn_cls: 0.006021  loss_rpn_loc: 0.01756  time: 0.4294  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:42:07] d2.utils.events INFO:  eta: 13:56:22  iter: 6839  total_loss: 1.971  loss_cls_stage0: 0.1172  loss_box_reg_stage0: 0.2294  loss_cls_stage1: 0.107  loss_box_reg_stage1: 0.5304  loss_cls_stage2: 0.0946  loss_box_reg_stage2: 0.7329  loss_mask: 0.08038  loss_rpn_cls: 0.005165  loss_rpn_loc: 0.01939  time: 0.4294  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:42:16] d2.utils.events INFO:  eta: 13:56:08  iter: 6859  total_loss: 1.989  loss_cls_stage0: 0.1201  loss_box_reg_stage0: 0.2409  loss_cls_stage1: 0.09287  loss_box_reg_stage1: 0.562  loss_cls_stage2: 0.09524  loss_box_reg_stage2: 0.7641  loss_mask: 0.07544  loss_rpn_cls: 0.008041  loss_rpn_loc: 0.01807  time: 0.4294  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:42:25] d2.utils.events INFO:  eta: 13:56:07  iter: 6879  total_loss: 1.79  loss_cls_stage0: 0.1097  loss_box_reg_stage0: 0.2073  loss_cls_stage1: 0.08163  loss_box_reg_stage1: 0.4932  loss_cls_stage2: 0.0817  loss_box_reg_stage2: 0.6708  loss_mask: 0.06495  loss_rpn_cls: 0.005489  loss_rpn_loc: 0.0173  time: 0.4294  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:42:33] d2.utils.events INFO:  eta: 13:56:12  iter: 6899  total_loss: 1.94  loss_cls_stage0: 0.1147  loss_box_reg_stage0: 0.2215  loss_cls_stage1: 0.08887  loss_box_reg_stage1: 0.5288  loss_cls_stage2: 0.09278  loss_box_reg_stage2: 0.7486  loss_mask: 0.07588  loss_rpn_cls: 0.006147  loss_rpn_loc: 0.01874  time: 0.4294  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:42:42] d2.utils.events INFO:  eta: 13:56:29  iter: 6919  total_loss: 2.059  loss_cls_stage0: 0.1231  loss_box_reg_stage0: 0.2321  loss_cls_stage1: 0.09432  loss_box_reg_stage1: 0.5645  loss_cls_stage2: 0.09063  loss_box_reg_stage2: 0.8273  loss_mask: 0.07088  loss_rpn_cls: 0.004661  loss_rpn_loc: 0.01897  time: 0.4294  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:42:50] d2.utils.events INFO:  eta: 13:56:39  iter: 6939  total_loss: 1.966  loss_cls_stage0: 0.1272  loss_box_reg_stage0: 0.2297  loss_cls_stage1: 0.09774  loss_box_reg_stage1: 0.5735  loss_cls_stage2: 0.09625  loss_box_reg_stage2: 0.8045  loss_mask: 0.06561  loss_rpn_cls: 0.003508  loss_rpn_loc: 0.01971  time: 0.4294  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:42:59] d2.utils.events INFO:  eta: 13:56:19  iter: 6959  total_loss: 1.977  loss_cls_stage0: 0.1166  loss_box_reg_stage0: 0.2048  loss_cls_stage1: 0.09537  loss_box_reg_stage1: 0.5526  loss_cls_stage2: 0.08663  loss_box_reg_stage2: 0.797  loss_mask: 0.06654  loss_rpn_cls: 0.004331  loss_rpn_loc: 0.01818  time: 0.4294  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:43:07] d2.utils.events INFO:  eta: 13:56:02  iter: 6979  total_loss: 2.264  loss_cls_stage0: 0.1246  loss_box_reg_stage0: 0.225  loss_cls_stage1: 0.1065  loss_box_reg_stage1: 0.6187  loss_cls_stage2: 0.1071  loss_box_reg_stage2: 0.8799  loss_mask: 0.06907  loss_rpn_cls: 0.008954  loss_rpn_loc: 0.0174  time: 0.4294  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:43:16] d2.utils.events INFO:  eta: 13:55:38  iter: 6999  total_loss: 1.929  loss_cls_stage0: 0.1157  loss_box_reg_stage0: 0.2132  loss_cls_stage1: 0.09381  loss_box_reg_stage1: 0.538  loss_cls_stage2: 0.08613  loss_box_reg_stage2: 0.7855  loss_mask: 0.07266  loss_rpn_cls: 0.006339  loss_rpn_loc: 0.01705  time: 0.4293  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:43:24] d2.utils.events INFO:  eta: 13:55:20  iter: 7019  total_loss: 2.097  loss_cls_stage0: 0.1231  loss_box_reg_stage0: 0.2173  loss_cls_stage1: 0.1049  loss_box_reg_stage1: 0.5622  loss_cls_stage2: 0.09909  loss_box_reg_stage2: 0.8074  loss_mask: 0.07809  loss_rpn_cls: 0.007074  loss_rpn_loc: 0.02044  time: 0.4293  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:43:33] d2.utils.events INFO:  eta: 13:54:34  iter: 7039  total_loss: 2.135  loss_cls_stage0: 0.1356  loss_box_reg_stage0: 0.2421  loss_cls_stage1: 0.1199  loss_box_reg_stage1: 0.5951  loss_cls_stage2: 0.1052  loss_box_reg_stage2: 0.8108  loss_mask: 0.07526  loss_rpn_cls: 0.00563  loss_rpn_loc: 0.02004  time: 0.4293  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:43:41] d2.utils.events INFO:  eta: 13:53:47  iter: 7059  total_loss: 1.966  loss_cls_stage0: 0.1395  loss_box_reg_stage0: 0.2258  loss_cls_stage1: 0.1316  loss_box_reg_stage1: 0.5351  loss_cls_stage2: 0.131  loss_box_reg_stage2: 0.7931  loss_mask: 0.0715  loss_rpn_cls: 0.007721  loss_rpn_loc: 0.01967  time: 0.4293  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:43:49] d2.utils.events INFO:  eta: 13:53:50  iter: 7079  total_loss: 1.908  loss_cls_stage0: 0.09826  loss_box_reg_stage0: 0.2003  loss_cls_stage1: 0.0802  loss_box_reg_stage1: 0.5083  loss_cls_stage2: 0.09119  loss_box_reg_stage2: 0.7705  loss_mask: 0.06852  loss_rpn_cls: 0.003369  loss_rpn_loc: 0.01664  time: 0.4292  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:43:58] d2.utils.events INFO:  eta: 13:53:06  iter: 7099  total_loss: 1.89  loss_cls_stage0: 0.1056  loss_box_reg_stage0: 0.2026  loss_cls_stage1: 0.09259  loss_box_reg_stage1: 0.516  loss_cls_stage2: 0.0815  loss_box_reg_stage2: 0.7958  loss_mask: 0.07355  loss_rpn_cls: 0.006459  loss_rpn_loc: 0.01789  time: 0.4292  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:44:06] d2.utils.events INFO:  eta: 13:52:53  iter: 7119  total_loss: 1.986  loss_cls_stage0: 0.1043  loss_box_reg_stage0: 0.2317  loss_cls_stage1: 0.08935  loss_box_reg_stage1: 0.5501  loss_cls_stage2: 0.08223  loss_box_reg_stage2: 0.8016  loss_mask: 0.06654  loss_rpn_cls: 0.004793  loss_rpn_loc: 0.01775  time: 0.4292  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:44:15] d2.utils.events INFO:  eta: 13:52:40  iter: 7139  total_loss: 1.9  loss_cls_stage0: 0.1156  loss_box_reg_stage0: 0.2104  loss_cls_stage1: 0.08036  loss_box_reg_stage1: 0.5292  loss_cls_stage2: 0.06968  loss_box_reg_stage2: 0.795  loss_mask: 0.07046  loss_rpn_cls: 0.002974  loss_rpn_loc: 0.01775  time: 0.4292  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:44:24] d2.utils.events INFO:  eta: 13:52:14  iter: 7159  total_loss: 1.895  loss_cls_stage0: 0.1125  loss_box_reg_stage0: 0.214  loss_cls_stage1: 0.08392  loss_box_reg_stage1: 0.5197  loss_cls_stage2: 0.08146  loss_box_reg_stage2: 0.7239  loss_mask: 0.06527  loss_rpn_cls: 0.006314  loss_rpn_loc: 0.01855  time: 0.4292  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:44:32] d2.utils.events INFO:  eta: 13:51:56  iter: 7179  total_loss: 1.938  loss_cls_stage0: 0.1106  loss_box_reg_stage0: 0.2149  loss_cls_stage1: 0.1008  loss_box_reg_stage1: 0.5547  loss_cls_stage2: 0.09699  loss_box_reg_stage2: 0.7149  loss_mask: 0.07247  loss_rpn_cls: 0.006338  loss_rpn_loc: 0.01942  time: 0.4292  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:44:40] d2.utils.events INFO:  eta: 13:51:06  iter: 7199  total_loss: 1.828  loss_cls_stage0: 0.1071  loss_box_reg_stage0: 0.2213  loss_cls_stage1: 0.07566  loss_box_reg_stage1: 0.5291  loss_cls_stage2: 0.07032  loss_box_reg_stage2: 0.7664  loss_mask: 0.07628  loss_rpn_cls: 0.005044  loss_rpn_loc: 0.01916  time: 0.4291  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:44:49] d2.utils.events INFO:  eta: 13:50:44  iter: 7219  total_loss: 1.878  loss_cls_stage0: 0.0993  loss_box_reg_stage0: 0.2031  loss_cls_stage1: 0.08752  loss_box_reg_stage1: 0.5172  loss_cls_stage2: 0.07653  loss_box_reg_stage2: 0.7144  loss_mask: 0.07001  loss_rpn_cls: 0.002949  loss_rpn_loc: 0.01827  time: 0.4291  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:44:57] d2.utils.events INFO:  eta: 13:50:03  iter: 7239  total_loss: 1.786  loss_cls_stage0: 0.09705  loss_box_reg_stage0: 0.2018  loss_cls_stage1: 0.08338  loss_box_reg_stage1: 0.4978  loss_cls_stage2: 0.07667  loss_box_reg_stage2: 0.7309  loss_mask: 0.07676  loss_rpn_cls: 0.004338  loss_rpn_loc: 0.01834  time: 0.4291  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:45:05] d2.utils.events INFO:  eta: 13:49:47  iter: 7259  total_loss: 1.874  loss_cls_stage0: 0.1304  loss_box_reg_stage0: 0.2283  loss_cls_stage1: 0.1155  loss_box_reg_stage1: 0.5219  loss_cls_stage2: 0.1144  loss_box_reg_stage2: 0.7697  loss_mask: 0.07543  loss_rpn_cls: 0.003279  loss_rpn_loc: 0.01962  time: 0.4290  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:45:14] d2.utils.events INFO:  eta: 13:49:19  iter: 7279  total_loss: 2.146  loss_cls_stage0: 0.1144  loss_box_reg_stage0: 0.2386  loss_cls_stage1: 0.08506  loss_box_reg_stage1: 0.6201  loss_cls_stage2: 0.1008  loss_box_reg_stage2: 0.8488  loss_mask: 0.08486  loss_rpn_cls: 0.005002  loss_rpn_loc: 0.01939  time: 0.4290  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:45:22] d2.utils.events INFO:  eta: 13:49:30  iter: 7299  total_loss: 2.099  loss_cls_stage0: 0.1207  loss_box_reg_stage0: 0.2099  loss_cls_stage1: 0.08971  loss_box_reg_stage1: 0.5875  loss_cls_stage2: 0.07964  loss_box_reg_stage2: 0.7702  loss_mask: 0.07187  loss_rpn_cls: 0.00368  loss_rpn_loc: 0.01749  time: 0.4290  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:45:31] d2.utils.events INFO:  eta: 13:49:26  iter: 7319  total_loss: 1.884  loss_cls_stage0: 0.104  loss_box_reg_stage0: 0.2022  loss_cls_stage1: 0.07754  loss_box_reg_stage1: 0.555  loss_cls_stage2: 0.06384  loss_box_reg_stage2: 0.7679  loss_mask: 0.07035  loss_rpn_cls: 0.003009  loss_rpn_loc: 0.01781  time: 0.4290  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:45:39] d2.utils.events INFO:  eta: 13:49:15  iter: 7339  total_loss: 2.075  loss_cls_stage0: 0.1234  loss_box_reg_stage0: 0.2325  loss_cls_stage1: 0.09202  loss_box_reg_stage1: 0.5796  loss_cls_stage2: 0.0771  loss_box_reg_stage2: 0.817  loss_mask: 0.07916  loss_rpn_cls: 0.004477  loss_rpn_loc: 0.01889  time: 0.4290  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:45:48] d2.utils.events INFO:  eta: 13:49:15  iter: 7359  total_loss: 2.137  loss_cls_stage0: 0.118  loss_box_reg_stage0: 0.2264  loss_cls_stage1: 0.09705  loss_box_reg_stage1: 0.62  loss_cls_stage2: 0.09118  loss_box_reg_stage2: 0.8483  loss_mask: 0.07323  loss_rpn_cls: 0.003828  loss_rpn_loc: 0.01598  time: 0.4290  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:45:56] d2.utils.events INFO:  eta: 13:49:12  iter: 7379  total_loss: 1.869  loss_cls_stage0: 0.1266  loss_box_reg_stage0: 0.2233  loss_cls_stage1: 0.09027  loss_box_reg_stage1: 0.5353  loss_cls_stage2: 0.08396  loss_box_reg_stage2: 0.7391  loss_mask: 0.07587  loss_rpn_cls: 0.005472  loss_rpn_loc: 0.0169  time: 0.4289  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:46:05] d2.utils.events INFO:  eta: 13:48:48  iter: 7399  total_loss: 2.049  loss_cls_stage0: 0.1135  loss_box_reg_stage0: 0.2318  loss_cls_stage1: 0.0989  loss_box_reg_stage1: 0.5984  loss_cls_stage2: 0.08814  loss_box_reg_stage2: 0.8329  loss_mask: 0.07464  loss_rpn_cls: 0.004974  loss_rpn_loc: 0.01859  time: 0.4289  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:46:13] d2.utils.events INFO:  eta: 13:47:50  iter: 7419  total_loss: 1.794  loss_cls_stage0: 0.1022  loss_box_reg_stage0: 0.1921  loss_cls_stage1: 0.07576  loss_box_reg_stage1: 0.4893  loss_cls_stage2: 0.0731  loss_box_reg_stage2: 0.7232  loss_mask: 0.0702  loss_rpn_cls: 0.004385  loss_rpn_loc: 0.01663  time: 0.4289  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:46:22] d2.utils.events INFO:  eta: 13:47:46  iter: 7439  total_loss: 1.985  loss_cls_stage0: 0.127  loss_box_reg_stage0: 0.2216  loss_cls_stage1: 0.1076  loss_box_reg_stage1: 0.5718  loss_cls_stage2: 0.1001  loss_box_reg_stage2: 0.7969  loss_mask: 0.07238  loss_rpn_cls: 0.00608  loss_rpn_loc: 0.01953  time: 0.4289  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:46:30] d2.utils.events INFO:  eta: 13:47:19  iter: 7459  total_loss: 1.959  loss_cls_stage0: 0.1134  loss_box_reg_stage0: 0.2217  loss_cls_stage1: 0.08609  loss_box_reg_stage1: 0.5457  loss_cls_stage2: 0.08793  loss_box_reg_stage2: 0.8307  loss_mask: 0.06968  loss_rpn_cls: 0.005201  loss_rpn_loc: 0.01916  time: 0.4289  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:46:38] d2.utils.events INFO:  eta: 13:46:52  iter: 7479  total_loss: 1.979  loss_cls_stage0: 0.1153  loss_box_reg_stage0: 0.2164  loss_cls_stage1: 0.08315  loss_box_reg_stage1: 0.5593  loss_cls_stage2: 0.08234  loss_box_reg_stage2: 0.8232  loss_mask: 0.07488  loss_rpn_cls: 0.004319  loss_rpn_loc: 0.01714  time: 0.4288  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:46:47] d2.utils.events INFO:  eta: 13:46:27  iter: 7499  total_loss: 1.968  loss_cls_stage0: 0.1142  loss_box_reg_stage0: 0.2127  loss_cls_stage1: 0.08783  loss_box_reg_stage1: 0.5569  loss_cls_stage2: 0.08442  loss_box_reg_stage2: 0.7204  loss_mask: 0.08258  loss_rpn_cls: 0.004428  loss_rpn_loc: 0.01731  time: 0.4288  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:46:55] d2.utils.events INFO:  eta: 13:46:35  iter: 7519  total_loss: 1.989  loss_cls_stage0: 0.1162  loss_box_reg_stage0: 0.2194  loss_cls_stage1: 0.09259  loss_box_reg_stage1: 0.554  loss_cls_stage2: 0.0867  loss_box_reg_stage2: 0.843  loss_mask: 0.07502  loss_rpn_cls: 0.007049  loss_rpn_loc: 0.01703  time: 0.4288  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:47:04] d2.utils.events INFO:  eta: 13:46:27  iter: 7539  total_loss: 1.788  loss_cls_stage0: 0.1097  loss_box_reg_stage0: 0.2401  loss_cls_stage1: 0.08377  loss_box_reg_stage1: 0.4727  loss_cls_stage2: 0.07758  loss_box_reg_stage2: 0.649  loss_mask: 0.06962  loss_rpn_cls: 0.005424  loss_rpn_loc: 0.01889  time: 0.4288  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:47:13] d2.utils.events INFO:  eta: 13:46:00  iter: 7559  total_loss: 1.709  loss_cls_stage0: 0.1064  loss_box_reg_stage0: 0.1968  loss_cls_stage1: 0.07727  loss_box_reg_stage1: 0.4814  loss_cls_stage2: 0.06088  loss_box_reg_stage2: 0.66  loss_mask: 0.0721  loss_rpn_cls: 0.007388  loss_rpn_loc: 0.01674  time: 0.4288  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:47:21] d2.utils.events INFO:  eta: 13:45:14  iter: 7579  total_loss: 1.763  loss_cls_stage0: 0.09693  loss_box_reg_stage0: 0.1958  loss_cls_stage1: 0.06502  loss_box_reg_stage1: 0.4948  loss_cls_stage2: 0.07784  loss_box_reg_stage2: 0.7248  loss_mask: 0.0741  loss_rpn_cls: 0.004415  loss_rpn_loc: 0.01667  time: 0.4288  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:47:30] d2.utils.events INFO:  eta: 13:45:38  iter: 7599  total_loss: 1.625  loss_cls_stage0: 0.09709  loss_box_reg_stage0: 0.1899  loss_cls_stage1: 0.06121  loss_box_reg_stage1: 0.4691  loss_cls_stage2: 0.06721  loss_box_reg_stage2: 0.6973  loss_mask: 0.06779  loss_rpn_cls: 0.002908  loss_rpn_loc: 0.01601  time: 0.4288  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:47:38] d2.utils.events INFO:  eta: 13:45:41  iter: 7619  total_loss: 1.963  loss_cls_stage0: 0.1279  loss_box_reg_stage0: 0.2133  loss_cls_stage1: 0.101  loss_box_reg_stage1: 0.5346  loss_cls_stage2: 0.1045  loss_box_reg_stage2: 0.845  loss_mask: 0.07333  loss_rpn_cls: 0.004459  loss_rpn_loc: 0.01793  time: 0.4288  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:47:47] d2.utils.events INFO:  eta: 13:45:38  iter: 7639  total_loss: 1.951  loss_cls_stage0: 0.1144  loss_box_reg_stage0: 0.2214  loss_cls_stage1: 0.08531  loss_box_reg_stage1: 0.5526  loss_cls_stage2: 0.09205  loss_box_reg_stage2: 0.8143  loss_mask: 0.07587  loss_rpn_cls: 0.006406  loss_rpn_loc: 0.01808  time: 0.4287  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:47:55] d2.utils.events INFO:  eta: 13:45:41  iter: 7659  total_loss: 1.814  loss_cls_stage0: 0.1141  loss_box_reg_stage0: 0.2056  loss_cls_stage1: 0.08584  loss_box_reg_stage1: 0.5185  loss_cls_stage2: 0.08903  loss_box_reg_stage2: 0.7678  loss_mask: 0.06938  loss_rpn_cls: 0.003759  loss_rpn_loc: 0.01747  time: 0.4288  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:48:04] d2.utils.events INFO:  eta: 13:45:27  iter: 7679  total_loss: 1.799  loss_cls_stage0: 0.09656  loss_box_reg_stage0: 0.2063  loss_cls_stage1: 0.08602  loss_box_reg_stage1: 0.5229  loss_cls_stage2: 0.07494  loss_box_reg_stage2: 0.7475  loss_mask: 0.06804  loss_rpn_cls: 0.006889  loss_rpn_loc: 0.01745  time: 0.4287  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:48:12] d2.utils.events INFO:  eta: 13:46:12  iter: 7699  total_loss: 1.954  loss_cls_stage0: 0.1219  loss_box_reg_stage0: 0.2271  loss_cls_stage1: 0.09598  loss_box_reg_stage1: 0.5408  loss_cls_stage2: 0.08924  loss_box_reg_stage2: 0.7835  loss_mask: 0.06723  loss_rpn_cls: 0.004207  loss_rpn_loc: 0.01793  time: 0.4288  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:48:21] d2.utils.events INFO:  eta: 13:46:49  iter: 7719  total_loss: 1.766  loss_cls_stage0: 0.0937  loss_box_reg_stage0: 0.1987  loss_cls_stage1: 0.07077  loss_box_reg_stage1: 0.4946  loss_cls_stage2: 0.07139  loss_box_reg_stage2: 0.7389  loss_mask: 0.068  loss_rpn_cls: 0.00385  loss_rpn_loc: 0.01678  time: 0.4288  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:48:30] d2.utils.events INFO:  eta: 13:46:59  iter: 7739  total_loss: 1.736  loss_cls_stage0: 0.1102  loss_box_reg_stage0: 0.2119  loss_cls_stage1: 0.08287  loss_box_reg_stage1: 0.5062  loss_cls_stage2: 0.06623  loss_box_reg_stage2: 0.7097  loss_mask: 0.06953  loss_rpn_cls: 0.005128  loss_rpn_loc: 0.017  time: 0.4288  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:48:38] d2.utils.events INFO:  eta: 13:47:01  iter: 7759  total_loss: 1.885  loss_cls_stage0: 0.1073  loss_box_reg_stage0: 0.2184  loss_cls_stage1: 0.08206  loss_box_reg_stage1: 0.5231  loss_cls_stage2: 0.07155  loss_box_reg_stage2: 0.797  loss_mask: 0.06634  loss_rpn_cls: 0.005274  loss_rpn_loc: 0.01572  time: 0.4287  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:48:46] d2.utils.events INFO:  eta: 13:46:32  iter: 7779  total_loss: 1.803  loss_cls_stage0: 0.09752  loss_box_reg_stage0: 0.1948  loss_cls_stage1: 0.07059  loss_box_reg_stage1: 0.5276  loss_cls_stage2: 0.06952  loss_box_reg_stage2: 0.7327  loss_mask: 0.06919  loss_rpn_cls: 0.003913  loss_rpn_loc: 0.01631  time: 0.4287  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:48:55] d2.utils.events INFO:  eta: 13:45:24  iter: 7799  total_loss: 1.839  loss_cls_stage0: 0.1075  loss_box_reg_stage0: 0.2113  loss_cls_stage1: 0.0787  loss_box_reg_stage1: 0.502  loss_cls_stage2: 0.08117  loss_box_reg_stage2: 0.686  loss_mask: 0.07519  loss_rpn_cls: 0.01101  loss_rpn_loc: 0.0177  time: 0.4287  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:49:04] d2.utils.events INFO:  eta: 13:45:22  iter: 7819  total_loss: 2.041  loss_cls_stage0: 0.12  loss_box_reg_stage0: 0.2474  loss_cls_stage1: 0.1108  loss_box_reg_stage1: 0.594  loss_cls_stage2: 0.1194  loss_box_reg_stage2: 0.78  loss_mask: 0.07409  loss_rpn_cls: 0.005986  loss_rpn_loc: 0.01871  time: 0.4287  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:49:12] d2.utils.events INFO:  eta: 13:45:30  iter: 7839  total_loss: 1.607  loss_cls_stage0: 0.09166  loss_box_reg_stage0: 0.1998  loss_cls_stage1: 0.08011  loss_box_reg_stage1: 0.483  loss_cls_stage2: 0.06215  loss_box_reg_stage2: 0.692  loss_mask: 0.07097  loss_rpn_cls: 0.005197  loss_rpn_loc: 0.01526  time: 0.4287  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:49:21] d2.utils.events INFO:  eta: 13:45:43  iter: 7859  total_loss: 1.696  loss_cls_stage0: 0.09306  loss_box_reg_stage0: 0.1932  loss_cls_stage1: 0.07184  loss_box_reg_stage1: 0.4713  loss_cls_stage2: 0.07021  loss_box_reg_stage2: 0.6556  loss_mask: 0.07806  loss_rpn_cls: 0.003935  loss_rpn_loc: 0.01858  time: 0.4287  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:49:29] d2.utils.events INFO:  eta: 13:45:10  iter: 7879  total_loss: 2.088  loss_cls_stage0: 0.1252  loss_box_reg_stage0: 0.2261  loss_cls_stage1: 0.0926  loss_box_reg_stage1: 0.592  loss_cls_stage2: 0.08755  loss_box_reg_stage2: 0.8354  loss_mask: 0.08535  loss_rpn_cls: 0.003456  loss_rpn_loc: 0.01927  time: 0.4287  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:49:38] d2.utils.events INFO:  eta: 13:44:33  iter: 7899  total_loss: 1.984  loss_cls_stage0: 0.1157  loss_box_reg_stage0: 0.2189  loss_cls_stage1: 0.085  loss_box_reg_stage1: 0.5658  loss_cls_stage2: 0.0845  loss_box_reg_stage2: 0.7825  loss_mask: 0.07816  loss_rpn_cls: 0.005142  loss_rpn_loc: 0.01786  time: 0.4287  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:49:46] d2.utils.events INFO:  eta: 13:43:52  iter: 7919  total_loss: 1.804  loss_cls_stage0: 0.1112  loss_box_reg_stage0: 0.1948  loss_cls_stage1: 0.1003  loss_box_reg_stage1: 0.5017  loss_cls_stage2: 0.07354  loss_box_reg_stage2: 0.7682  loss_mask: 0.07733  loss_rpn_cls: 0.006421  loss_rpn_loc: 0.01771  time: 0.4287  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:49:55] d2.utils.events INFO:  eta: 13:43:39  iter: 7939  total_loss: 1.964  loss_cls_stage0: 0.1074  loss_box_reg_stage0: 0.2133  loss_cls_stage1: 0.07778  loss_box_reg_stage1: 0.551  loss_cls_stage2: 0.0797  loss_box_reg_stage2: 0.7741  loss_mask: 0.06972  loss_rpn_cls: 0.006818  loss_rpn_loc: 0.01969  time: 0.4287  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:50:03] d2.utils.events INFO:  eta: 13:43:37  iter: 7959  total_loss: 1.881  loss_cls_stage0: 0.09618  loss_box_reg_stage0: 0.2006  loss_cls_stage1: 0.08004  loss_box_reg_stage1: 0.532  loss_cls_stage2: 0.07126  loss_box_reg_stage2: 0.829  loss_mask: 0.07051  loss_rpn_cls: 0.005874  loss_rpn_loc: 0.01862  time: 0.4286  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:50:12] d2.utils.events INFO:  eta: 13:43:52  iter: 7979  total_loss: 1.77  loss_cls_stage0: 0.0969  loss_box_reg_stage0: 0.2058  loss_cls_stage1: 0.06864  loss_box_reg_stage1: 0.5103  loss_cls_stage2: 0.05907  loss_box_reg_stage2: 0.793  loss_mask: 0.06742  loss_rpn_cls: 0.002986  loss_rpn_loc: 0.01772  time: 0.4286  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:50:20] d2.utils.events INFO:  eta: 13:43:34  iter: 7999  total_loss: 1.828  loss_cls_stage0: 0.1111  loss_box_reg_stage0: 0.2202  loss_cls_stage1: 0.08667  loss_box_reg_stage1: 0.5246  loss_cls_stage2: 0.08009  loss_box_reg_stage2: 0.7523  loss_mask: 0.07331  loss_rpn_cls: 0.004159  loss_rpn_loc: 0.01937  time: 0.4286  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:50:29] d2.utils.events INFO:  eta: 13:43:15  iter: 8019  total_loss: 1.509  loss_cls_stage0: 0.08602  loss_box_reg_stage0: 0.1889  loss_cls_stage1: 0.05392  loss_box_reg_stage1: 0.4438  loss_cls_stage2: 0.05871  loss_box_reg_stage2: 0.5853  loss_mask: 0.07019  loss_rpn_cls: 0.002112  loss_rpn_loc: 0.01551  time: 0.4286  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:50:37] d2.utils.events INFO:  eta: 13:43:21  iter: 8039  total_loss: 1.778  loss_cls_stage0: 0.1108  loss_box_reg_stage0: 0.2238  loss_cls_stage1: 0.07962  loss_box_reg_stage1: 0.4948  loss_cls_stage2: 0.07027  loss_box_reg_stage2: 0.6442  loss_mask: 0.07169  loss_rpn_cls: 0.003706  loss_rpn_loc: 0.01731  time: 0.4286  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:50:45] d2.utils.events INFO:  eta: 13:43:02  iter: 8059  total_loss: 1.646  loss_cls_stage0: 0.09782  loss_box_reg_stage0: 0.1997  loss_cls_stage1: 0.07206  loss_box_reg_stage1: 0.4673  loss_cls_stage2: 0.05864  loss_box_reg_stage2: 0.6604  loss_mask: 0.07268  loss_rpn_cls: 0.002688  loss_rpn_loc: 0.0165  time: 0.4285  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:50:54] d2.utils.events INFO:  eta: 13:42:38  iter: 8079  total_loss: 1.885  loss_cls_stage0: 0.1092  loss_box_reg_stage0: 0.2224  loss_cls_stage1: 0.08268  loss_box_reg_stage1: 0.5186  loss_cls_stage2: 0.08395  loss_box_reg_stage2: 0.7481  loss_mask: 0.07213  loss_rpn_cls: 0.004227  loss_rpn_loc: 0.01792  time: 0.4285  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:51:02] d2.utils.events INFO:  eta: 13:42:10  iter: 8099  total_loss: 2.098  loss_cls_stage0: 0.1162  loss_box_reg_stage0: 0.2327  loss_cls_stage1: 0.08782  loss_box_reg_stage1: 0.6028  loss_cls_stage2: 0.08492  loss_box_reg_stage2: 0.8623  loss_mask: 0.0759  loss_rpn_cls: 0.003368  loss_rpn_loc: 0.01636  time: 0.4285  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:51:11] d2.utils.events INFO:  eta: 13:42:29  iter: 8119  total_loss: 1.836  loss_cls_stage0: 0.09875  loss_box_reg_stage0: 0.2105  loss_cls_stage1: 0.07249  loss_box_reg_stage1: 0.5245  loss_cls_stage2: 0.07334  loss_box_reg_stage2: 0.7512  loss_mask: 0.07292  loss_rpn_cls: 0.004638  loss_rpn_loc: 0.01931  time: 0.4285  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:51:19] d2.utils.events INFO:  eta: 13:42:26  iter: 8139  total_loss: 1.669  loss_cls_stage0: 0.08658  loss_box_reg_stage0: 0.1961  loss_cls_stage1: 0.05619  loss_box_reg_stage1: 0.4791  loss_cls_stage2: 0.06326  loss_box_reg_stage2: 0.664  loss_mask: 0.06524  loss_rpn_cls: 0.002335  loss_rpn_loc: 0.01657  time: 0.4285  data_time: 0.0038  lr: 0.00016  max_mem: 7560M
[09/18 12:51:28] d2.utils.events INFO:  eta: 13:41:49  iter: 8159  total_loss: 1.853  loss_cls_stage0: 0.09061  loss_box_reg_stage0: 0.2039  loss_cls_stage1: 0.06697  loss_box_reg_stage1: 0.5399  loss_cls_stage2: 0.06879  loss_box_reg_stage2: 0.7712  loss_mask: 0.06978  loss_rpn_cls: 0.003024  loss_rpn_loc: 0.0169  time: 0.4285  data_time: 0.0039  lr: 0.00016  max_mem: 7560M
[09/18 12:51:36] d2.utils.events INFO:  eta: 13:41:08  iter: 8179  total_loss: 1.826  loss_cls_stage0: 0.1086  loss_box_reg_stage0: 0.2142  loss_cls_stage1: 0.08429  loss_box_reg_stage1: 0.5383  loss_cls_stage2: 0.08735  loss_box_reg_stage2: 0.7246  loss_mask: 0.06858  loss_rpn_cls: 0.004197  loss_rpn_loc: 0.01858  time: 0.4284  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:51:44] d2.utils.events INFO:  eta: 13:41:08  iter: 8199  total_loss: 1.794  loss_cls_stage0: 0.09698  loss_box_reg_stage0: 0.1952  loss_cls_stage1: 0.07736  loss_box_reg_stage1: 0.5081  loss_cls_stage2: 0.07007  loss_box_reg_stage2: 0.7046  loss_mask: 0.07029  loss_rpn_cls: 0.003564  loss_rpn_loc: 0.0178  time: 0.4284  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:51:53] d2.utils.events INFO:  eta: 13:41:20  iter: 8219  total_loss: 2.185  loss_cls_stage0: 0.1477  loss_box_reg_stage0: 0.2741  loss_cls_stage1: 0.1123  loss_box_reg_stage1: 0.6173  loss_cls_stage2: 0.1112  loss_box_reg_stage2: 0.88  loss_mask: 0.0704  loss_rpn_cls: 0.006972  loss_rpn_loc: 0.01846  time: 0.4284  data_time: 0.0045  lr: 0.00016  max_mem: 7560M
[09/18 12:52:01] d2.utils.events INFO:  eta: 13:41:44  iter: 8239  total_loss: 2.08  loss_cls_stage0: 0.1104  loss_box_reg_stage0: 0.23  loss_cls_stage1: 0.09305  loss_box_reg_stage1: 0.5957  loss_cls_stage2: 0.08738  loss_box_reg_stage2: 0.8533  loss_mask: 0.0757  loss_rpn_cls: 0.003187  loss_rpn_loc: 0.01769  time: 0.4284  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:52:10] d2.utils.events INFO:  eta: 13:41:33  iter: 8259  total_loss: 1.897  loss_cls_stage0: 0.1026  loss_box_reg_stage0: 0.2247  loss_cls_stage1: 0.07887  loss_box_reg_stage1: 0.5685  loss_cls_stage2: 0.07774  loss_box_reg_stage2: 0.83  loss_mask: 0.07425  loss_rpn_cls: 0.006295  loss_rpn_loc: 0.01897  time: 0.4284  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:52:18] d2.utils.events INFO:  eta: 13:41:24  iter: 8279  total_loss: 2.032  loss_cls_stage0: 0.1147  loss_box_reg_stage0: 0.2125  loss_cls_stage1: 0.1038  loss_box_reg_stage1: 0.5609  loss_cls_stage2: 0.09864  loss_box_reg_stage2: 0.7643  loss_mask: 0.07524  loss_rpn_cls: 0.007525  loss_rpn_loc: 0.0186  time: 0.4283  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:52:27] d2.utils.events INFO:  eta: 13:40:30  iter: 8299  total_loss: 2.018  loss_cls_stage0: 0.1102  loss_box_reg_stage0: 0.2205  loss_cls_stage1: 0.09353  loss_box_reg_stage1: 0.5353  loss_cls_stage2: 0.09687  loss_box_reg_stage2: 0.8017  loss_mask: 0.07589  loss_rpn_cls: 0.004947  loss_rpn_loc: 0.01887  time: 0.4283  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:52:35] d2.utils.events INFO:  eta: 13:40:28  iter: 8319  total_loss: 1.921  loss_cls_stage0: 0.09731  loss_box_reg_stage0: 0.2042  loss_cls_stage1: 0.07089  loss_box_reg_stage1: 0.5464  loss_cls_stage2: 0.08057  loss_box_reg_stage2: 0.7837  loss_mask: 0.0797  loss_rpn_cls: 0.002861  loss_rpn_loc: 0.01787  time: 0.4283  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:52:44] d2.utils.events INFO:  eta: 13:40:38  iter: 8339  total_loss: 1.831  loss_cls_stage0: 0.1085  loss_box_reg_stage0: 0.2243  loss_cls_stage1: 0.0819  loss_box_reg_stage1: 0.5052  loss_cls_stage2: 0.09273  loss_box_reg_stage2: 0.6852  loss_mask: 0.07341  loss_rpn_cls: 0.005834  loss_rpn_loc: 0.01733  time: 0.4283  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:52:52] d2.utils.events INFO:  eta: 13:40:05  iter: 8359  total_loss: 1.739  loss_cls_stage0: 0.0964  loss_box_reg_stage0: 0.1901  loss_cls_stage1: 0.07676  loss_box_reg_stage1: 0.4837  loss_cls_stage2: 0.07962  loss_box_reg_stage2: 0.7246  loss_mask: 0.07259  loss_rpn_cls: 0.004644  loss_rpn_loc: 0.01776  time: 0.4283  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:53:01] d2.utils.events INFO:  eta: 13:39:34  iter: 8379  total_loss: 1.829  loss_cls_stage0: 0.1054  loss_box_reg_stage0: 0.2196  loss_cls_stage1: 0.07488  loss_box_reg_stage1: 0.5213  loss_cls_stage2: 0.07396  loss_box_reg_stage2: 0.7536  loss_mask: 0.07661  loss_rpn_cls: 0.004786  loss_rpn_loc: 0.01595  time: 0.4283  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:53:09] d2.utils.events INFO:  eta: 13:39:16  iter: 8399  total_loss: 2.055  loss_cls_stage0: 0.1022  loss_box_reg_stage0: 0.2253  loss_cls_stage1: 0.07803  loss_box_reg_stage1: 0.5753  loss_cls_stage2: 0.08056  loss_box_reg_stage2: 0.7977  loss_mask: 0.07325  loss_rpn_cls: 0.003949  loss_rpn_loc: 0.01765  time: 0.4283  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:53:17] d2.utils.events INFO:  eta: 13:38:54  iter: 8419  total_loss: 1.676  loss_cls_stage0: 0.0884  loss_box_reg_stage0: 0.1851  loss_cls_stage1: 0.07409  loss_box_reg_stage1: 0.4945  loss_cls_stage2: 0.06596  loss_box_reg_stage2: 0.7127  loss_mask: 0.07281  loss_rpn_cls: 0.003787  loss_rpn_loc: 0.01789  time: 0.4282  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:53:26] d2.utils.events INFO:  eta: 13:38:39  iter: 8439  total_loss: 1.898  loss_cls_stage0: 0.09046  loss_box_reg_stage0: 0.2106  loss_cls_stage1: 0.06587  loss_box_reg_stage1: 0.5486  loss_cls_stage2: 0.06963  loss_box_reg_stage2: 0.7769  loss_mask: 0.06827  loss_rpn_cls: 0.003716  loss_rpn_loc: 0.01781  time: 0.4282  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:53:34] d2.utils.events INFO:  eta: 13:38:30  iter: 8459  total_loss: 1.821  loss_cls_stage0: 0.0863  loss_box_reg_stage0: 0.2009  loss_cls_stage1: 0.07163  loss_box_reg_stage1: 0.5066  loss_cls_stage2: 0.07567  loss_box_reg_stage2: 0.6694  loss_mask: 0.06633  loss_rpn_cls: 0.00404  loss_rpn_loc: 0.01568  time: 0.4282  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:53:43] d2.utils.events INFO:  eta: 13:38:22  iter: 8479  total_loss: 2.042  loss_cls_stage0: 0.1342  loss_box_reg_stage0: 0.2231  loss_cls_stage1: 0.1115  loss_box_reg_stage1: 0.5898  loss_cls_stage2: 0.0881  loss_box_reg_stage2: 0.7731  loss_mask: 0.0749  loss_rpn_cls: 0.005517  loss_rpn_loc: 0.01983  time: 0.4282  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:53:51] d2.utils.events INFO:  eta: 13:38:01  iter: 8499  total_loss: 2.042  loss_cls_stage0: 0.1207  loss_box_reg_stage0: 0.2293  loss_cls_stage1: 0.09549  loss_box_reg_stage1: 0.5878  loss_cls_stage2: 0.09259  loss_box_reg_stage2: 0.8258  loss_mask: 0.07234  loss_rpn_cls: 0.008098  loss_rpn_loc: 0.01852  time: 0.4281  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:53:59] d2.utils.events INFO:  eta: 13:37:13  iter: 8519  total_loss: 2.098  loss_cls_stage0: 0.1323  loss_box_reg_stage0: 0.2413  loss_cls_stage1: 0.1036  loss_box_reg_stage1: 0.61  loss_cls_stage2: 0.104  loss_box_reg_stage2: 0.8135  loss_mask: 0.07238  loss_rpn_cls: 0.007079  loss_rpn_loc: 0.01916  time: 0.4281  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:54:07] d2.utils.events INFO:  eta: 13:36:56  iter: 8539  total_loss: 2.222  loss_cls_stage0: 0.1182  loss_box_reg_stage0: 0.2487  loss_cls_stage1: 0.09591  loss_box_reg_stage1: 0.6213  loss_cls_stage2: 0.09228  loss_box_reg_stage2: 0.8452  loss_mask: 0.07784  loss_rpn_cls: 0.00733  loss_rpn_loc: 0.01936  time: 0.4281  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:54:16] d2.utils.events INFO:  eta: 13:36:44  iter: 8559  total_loss: 1.829  loss_cls_stage0: 0.1125  loss_box_reg_stage0: 0.2176  loss_cls_stage1: 0.08953  loss_box_reg_stage1: 0.5295  loss_cls_stage2: 0.08267  loss_box_reg_stage2: 0.7508  loss_mask: 0.06841  loss_rpn_cls: 0.004985  loss_rpn_loc: 0.01628  time: 0.4281  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:54:25] d2.utils.events INFO:  eta: 13:36:46  iter: 8579  total_loss: 1.963  loss_cls_stage0: 0.116  loss_box_reg_stage0: 0.2141  loss_cls_stage1: 0.08614  loss_box_reg_stage1: 0.5351  loss_cls_stage2: 0.09246  loss_box_reg_stage2: 0.7294  loss_mask: 0.07151  loss_rpn_cls: 0.003629  loss_rpn_loc: 0.01782  time: 0.4281  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:54:33] d2.utils.events INFO:  eta: 13:36:04  iter: 8599  total_loss: 1.711  loss_cls_stage0: 0.09681  loss_box_reg_stage0: 0.1943  loss_cls_stage1: 0.06796  loss_box_reg_stage1: 0.4604  loss_cls_stage2: 0.06317  loss_box_reg_stage2: 0.6879  loss_mask: 0.07442  loss_rpn_cls: 0.007178  loss_rpn_loc: 0.01716  time: 0.4280  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:54:41] d2.utils.events INFO:  eta: 13:35:28  iter: 8619  total_loss: 1.737  loss_cls_stage0: 0.09016  loss_box_reg_stage0: 0.1843  loss_cls_stage1: 0.07849  loss_box_reg_stage1: 0.4759  loss_cls_stage2: 0.08273  loss_box_reg_stage2: 0.7648  loss_mask: 0.07968  loss_rpn_cls: 0.002607  loss_rpn_loc: 0.01765  time: 0.4280  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:54:50] d2.utils.events INFO:  eta: 13:35:29  iter: 8639  total_loss: 1.86  loss_cls_stage0: 0.1134  loss_box_reg_stage0: 0.2103  loss_cls_stage1: 0.07783  loss_box_reg_stage1: 0.5282  loss_cls_stage2: 0.07961  loss_box_reg_stage2: 0.7522  loss_mask: 0.07407  loss_rpn_cls: 0.006804  loss_rpn_loc: 0.01854  time: 0.4280  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:54:58] d2.utils.events INFO:  eta: 13:35:00  iter: 8659  total_loss: 1.75  loss_cls_stage0: 0.1014  loss_box_reg_stage0: 0.2067  loss_cls_stage1: 0.06089  loss_box_reg_stage1: 0.476  loss_cls_stage2: 0.05955  loss_box_reg_stage2: 0.699  loss_mask: 0.06921  loss_rpn_cls: 0.005006  loss_rpn_loc: 0.01812  time: 0.4280  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:55:06] d2.utils.events INFO:  eta: 13:34:39  iter: 8679  total_loss: 1.562  loss_cls_stage0: 0.09831  loss_box_reg_stage0: 0.1836  loss_cls_stage1: 0.07628  loss_box_reg_stage1: 0.4268  loss_cls_stage2: 0.06162  loss_box_reg_stage2: 0.656  loss_mask: 0.07276  loss_rpn_cls: 0.003995  loss_rpn_loc: 0.01679  time: 0.4279  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:55:15] d2.utils.events INFO:  eta: 13:34:03  iter: 8699  total_loss: 1.85  loss_cls_stage0: 0.09894  loss_box_reg_stage0: 0.2082  loss_cls_stage1: 0.07454  loss_box_reg_stage1: 0.5387  loss_cls_stage2: 0.07738  loss_box_reg_stage2: 0.7691  loss_mask: 0.0731  loss_rpn_cls: 0.005377  loss_rpn_loc: 0.01892  time: 0.4279  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:55:23] d2.utils.events INFO:  eta: 13:33:35  iter: 8719  total_loss: 2.062  loss_cls_stage0: 0.1232  loss_box_reg_stage0: 0.2175  loss_cls_stage1: 0.1046  loss_box_reg_stage1: 0.5988  loss_cls_stage2: 0.1021  loss_box_reg_stage2: 0.8266  loss_mask: 0.0727  loss_rpn_cls: 0.007525  loss_rpn_loc: 0.0199  time: 0.4279  data_time: 0.0045  lr: 0.00016  max_mem: 7560M
[09/18 12:55:32] d2.utils.events INFO:  eta: 13:33:29  iter: 8739  total_loss: 1.764  loss_cls_stage0: 0.08994  loss_box_reg_stage0: 0.207  loss_cls_stage1: 0.0811  loss_box_reg_stage1: 0.474  loss_cls_stage2: 0.08158  loss_box_reg_stage2: 0.6924  loss_mask: 0.06958  loss_rpn_cls: 0.004705  loss_rpn_loc: 0.01797  time: 0.4279  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:55:40] d2.utils.events INFO:  eta: 13:33:12  iter: 8759  total_loss: 1.679  loss_cls_stage0: 0.09577  loss_box_reg_stage0: 0.1981  loss_cls_stage1: 0.07772  loss_box_reg_stage1: 0.4611  loss_cls_stage2: 0.07519  loss_box_reg_stage2: 0.7277  loss_mask: 0.0696  loss_rpn_cls: 0.005804  loss_rpn_loc: 0.01768  time: 0.4279  data_time: 0.0045  lr: 0.00016  max_mem: 7560M
[09/18 12:55:49] d2.utils.events INFO:  eta: 13:32:59  iter: 8779  total_loss: 1.784  loss_cls_stage0: 0.09476  loss_box_reg_stage0: 0.2028  loss_cls_stage1: 0.08098  loss_box_reg_stage1: 0.5224  loss_cls_stage2: 0.06772  loss_box_reg_stage2: 0.7037  loss_mask: 0.07296  loss_rpn_cls: 0.004429  loss_rpn_loc: 0.01607  time: 0.4279  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:55:57] d2.utils.events INFO:  eta: 13:32:54  iter: 8799  total_loss: 1.881  loss_cls_stage0: 0.12  loss_box_reg_stage0: 0.2238  loss_cls_stage1: 0.09717  loss_box_reg_stage1: 0.5285  loss_cls_stage2: 0.09345  loss_box_reg_stage2: 0.7189  loss_mask: 0.0728  loss_rpn_cls: 0.006393  loss_rpn_loc: 0.01627  time: 0.4278  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:56:05] d2.utils.events INFO:  eta: 13:32:25  iter: 8819  total_loss: 2.043  loss_cls_stage0: 0.1287  loss_box_reg_stage0: 0.2417  loss_cls_stage1: 0.09695  loss_box_reg_stage1: 0.5768  loss_cls_stage2: 0.08363  loss_box_reg_stage2: 0.7996  loss_mask: 0.07182  loss_rpn_cls: 0.006714  loss_rpn_loc: 0.01961  time: 0.4278  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:56:14] d2.utils.events INFO:  eta: 13:32:09  iter: 8839  total_loss: 1.738  loss_cls_stage0: 0.1004  loss_box_reg_stage0: 0.1981  loss_cls_stage1: 0.07414  loss_box_reg_stage1: 0.4943  loss_cls_stage2: 0.06864  loss_box_reg_stage2: 0.7405  loss_mask: 0.07708  loss_rpn_cls: 0.005609  loss_rpn_loc: 0.01677  time: 0.4278  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:56:22] d2.utils.events INFO:  eta: 13:31:52  iter: 8859  total_loss: 1.852  loss_cls_stage0: 0.105  loss_box_reg_stage0: 0.2062  loss_cls_stage1: 0.08854  loss_box_reg_stage1: 0.516  loss_cls_stage2: 0.0795  loss_box_reg_stage2: 0.7593  loss_mask: 0.06816  loss_rpn_cls: 0.003106  loss_rpn_loc: 0.01878  time: 0.4278  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:56:31] d2.utils.events INFO:  eta: 13:31:44  iter: 8879  total_loss: 1.859  loss_cls_stage0: 0.0951  loss_box_reg_stage0: 0.2026  loss_cls_stage1: 0.06856  loss_box_reg_stage1: 0.515  loss_cls_stage2: 0.05701  loss_box_reg_stage2: 0.8332  loss_mask: 0.07175  loss_rpn_cls: 0.003599  loss_rpn_loc: 0.01812  time: 0.4278  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:56:39] d2.utils.events INFO:  eta: 13:31:36  iter: 8899  total_loss: 1.919  loss_cls_stage0: 0.1019  loss_box_reg_stage0: 0.2112  loss_cls_stage1: 0.07108  loss_box_reg_stage1: 0.557  loss_cls_stage2: 0.06983  loss_box_reg_stage2: 0.8062  loss_mask: 0.07209  loss_rpn_cls: 0.003176  loss_rpn_loc: 0.0184  time: 0.4278  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:56:48] d2.utils.events INFO:  eta: 13:31:32  iter: 8919  total_loss: 2.001  loss_cls_stage0: 0.1186  loss_box_reg_stage0: 0.2304  loss_cls_stage1: 0.08668  loss_box_reg_stage1: 0.5579  loss_cls_stage2: 0.09474  loss_box_reg_stage2: 0.7917  loss_mask: 0.07351  loss_rpn_cls: 0.007382  loss_rpn_loc: 0.01905  time: 0.4277  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:56:56] d2.utils.events INFO:  eta: 13:30:51  iter: 8939  total_loss: 1.744  loss_cls_stage0: 0.08668  loss_box_reg_stage0: 0.2107  loss_cls_stage1: 0.06984  loss_box_reg_stage1: 0.5061  loss_cls_stage2: 0.07475  loss_box_reg_stage2: 0.7062  loss_mask: 0.0771  loss_rpn_cls: 0.004968  loss_rpn_loc: 0.01786  time: 0.4277  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:57:04] d2.utils.events INFO:  eta: 13:29:23  iter: 8959  total_loss: 1.611  loss_cls_stage0: 0.09697  loss_box_reg_stage0: 0.1949  loss_cls_stage1: 0.06984  loss_box_reg_stage1: 0.4408  loss_cls_stage2: 0.06287  loss_box_reg_stage2: 0.651  loss_mask: 0.07038  loss_rpn_cls: 0.007082  loss_rpn_loc: 0.01606  time: 0.4277  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:57:12] d2.utils.events INFO:  eta: 13:28:38  iter: 8979  total_loss: 1.973  loss_cls_stage0: 0.1181  loss_box_reg_stage0: 0.2364  loss_cls_stage1: 0.1221  loss_box_reg_stage1: 0.5027  loss_cls_stage2: 0.1159  loss_box_reg_stage2: 0.7591  loss_mask: 0.08714  loss_rpn_cls: 0.007165  loss_rpn_loc: 0.02069  time: 0.4276  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:57:21] d2.utils.events INFO:  eta: 13:28:41  iter: 8999  total_loss: 1.847  loss_cls_stage0: 0.09894  loss_box_reg_stage0: 0.2153  loss_cls_stage1: 0.0788  loss_box_reg_stage1: 0.5385  loss_cls_stage2: 0.08181  loss_box_reg_stage2: 0.7848  loss_mask: 0.06883  loss_rpn_cls: 0.003174  loss_rpn_loc: 0.01834  time: 0.4276  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:57:29] d2.utils.events INFO:  eta: 13:28:33  iter: 9019  total_loss: 2.144  loss_cls_stage0: 0.1128  loss_box_reg_stage0: 0.2283  loss_cls_stage1: 0.08628  loss_box_reg_stage1: 0.6353  loss_cls_stage2: 0.09081  loss_box_reg_stage2: 0.8158  loss_mask: 0.07555  loss_rpn_cls: 0.004608  loss_rpn_loc: 0.01783  time: 0.4276  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:57:38] d2.utils.events INFO:  eta: 13:28:21  iter: 9039  total_loss: 2.017  loss_cls_stage0: 0.1162  loss_box_reg_stage0: 0.2158  loss_cls_stage1: 0.1012  loss_box_reg_stage1: 0.5635  loss_cls_stage2: 0.09302  loss_box_reg_stage2: 0.7537  loss_mask: 0.06905  loss_rpn_cls: 0.004807  loss_rpn_loc: 0.0177  time: 0.4276  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:57:46] d2.utils.events INFO:  eta: 13:28:24  iter: 9059  total_loss: 1.853  loss_cls_stage0: 0.1063  loss_box_reg_stage0: 0.2086  loss_cls_stage1: 0.0811  loss_box_reg_stage1: 0.5397  loss_cls_stage2: 0.08383  loss_box_reg_stage2: 0.7978  loss_mask: 0.07363  loss_rpn_cls: 0.002386  loss_rpn_loc: 0.01662  time: 0.4276  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:57:54] d2.utils.events INFO:  eta: 13:28:32  iter: 9079  total_loss: 1.969  loss_cls_stage0: 0.09912  loss_box_reg_stage0: 0.2025  loss_cls_stage1: 0.07295  loss_box_reg_stage1: 0.5635  loss_cls_stage2: 0.07116  loss_box_reg_stage2: 0.8115  loss_mask: 0.06786  loss_rpn_cls: 0.002952  loss_rpn_loc: 0.0184  time: 0.4275  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:58:03] d2.utils.events INFO:  eta: 13:28:45  iter: 9099  total_loss: 2.056  loss_cls_stage0: 0.1067  loss_box_reg_stage0: 0.2294  loss_cls_stage1: 0.09011  loss_box_reg_stage1: 0.5965  loss_cls_stage2: 0.09518  loss_box_reg_stage2: 0.8526  loss_mask: 0.07256  loss_rpn_cls: 0.005362  loss_rpn_loc: 0.01764  time: 0.4275  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 12:58:11] d2.utils.events INFO:  eta: 13:28:22  iter: 9119  total_loss: 1.729  loss_cls_stage0: 0.09696  loss_box_reg_stage0: 0.1976  loss_cls_stage1: 0.06608  loss_box_reg_stage1: 0.4855  loss_cls_stage2: 0.0537  loss_box_reg_stage2: 0.7593  loss_mask: 0.06988  loss_rpn_cls: 0.002781  loss_rpn_loc: 0.01566  time: 0.4275  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 12:58:20] d2.utils.events INFO:  eta: 13:27:58  iter: 9139  total_loss: 1.994  loss_cls_stage0: 0.119  loss_box_reg_stage0: 0.2219  loss_cls_stage1: 0.08393  loss_box_reg_stage1: 0.5629  loss_cls_stage2: 0.07701  loss_box_reg_stage2: 0.8284  loss_mask: 0.07063  loss_rpn_cls: 0.005002  loss_rpn_loc: 0.02011  time: 0.4275  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:58:28] d2.utils.events INFO:  eta: 13:27:50  iter: 9159  total_loss: 1.684  loss_cls_stage0: 0.09856  loss_box_reg_stage0: 0.1932  loss_cls_stage1: 0.08222  loss_box_reg_stage1: 0.4782  loss_cls_stage2: 0.06226  loss_box_reg_stage2: 0.6639  loss_mask: 0.06842  loss_rpn_cls: 0.004639  loss_rpn_loc: 0.0165  time: 0.4275  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:58:37] d2.utils.events INFO:  eta: 13:27:31  iter: 9179  total_loss: 1.909  loss_cls_stage0: 0.1059  loss_box_reg_stage0: 0.202  loss_cls_stage1: 0.08383  loss_box_reg_stage1: 0.5121  loss_cls_stage2: 0.08598  loss_box_reg_stage2: 0.7081  loss_mask: 0.07043  loss_rpn_cls: 0.004186  loss_rpn_loc: 0.01771  time: 0.4275  data_time: 0.0038  lr: 0.00016  max_mem: 7560M
[09/18 12:58:45] d2.utils.events INFO:  eta: 13:27:46  iter: 9199  total_loss: 2.189  loss_cls_stage0: 0.1272  loss_box_reg_stage0: 0.2263  loss_cls_stage1: 0.1134  loss_box_reg_stage1: 0.6063  loss_cls_stage2: 0.1098  loss_box_reg_stage2: 0.8155  loss_mask: 0.07018  loss_rpn_cls: 0.005756  loss_rpn_loc: 0.01851  time: 0.4275  data_time: 0.0038  lr: 0.00016  max_mem: 7560M
[09/18 12:58:54] d2.utils.events INFO:  eta: 13:27:38  iter: 9219  total_loss: 1.937  loss_cls_stage0: 0.1043  loss_box_reg_stage0: 0.2301  loss_cls_stage1: 0.07559  loss_box_reg_stage1: 0.5336  loss_cls_stage2: 0.08296  loss_box_reg_stage2: 0.7479  loss_mask: 0.07195  loss_rpn_cls: 0.00367  loss_rpn_loc: 0.01747  time: 0.4275  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:59:02] d2.utils.events INFO:  eta: 13:27:46  iter: 9239  total_loss: 1.743  loss_cls_stage0: 0.1013  loss_box_reg_stage0: 0.2011  loss_cls_stage1: 0.07432  loss_box_reg_stage1: 0.5091  loss_cls_stage2: 0.07331  loss_box_reg_stage2: 0.7549  loss_mask: 0.07093  loss_rpn_cls: 0.003399  loss_rpn_loc: 0.01684  time: 0.4275  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 12:59:11] d2.utils.events INFO:  eta: 13:27:22  iter: 9259  total_loss: 1.976  loss_cls_stage0: 0.1066  loss_box_reg_stage0: 0.2133  loss_cls_stage1: 0.07706  loss_box_reg_stage1: 0.5423  loss_cls_stage2: 0.08738  loss_box_reg_stage2: 0.7822  loss_mask: 0.06812  loss_rpn_cls: 0.004246  loss_rpn_loc: 0.01839  time: 0.4275  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:59:19] d2.utils.events INFO:  eta: 13:27:33  iter: 9279  total_loss: 1.746  loss_cls_stage0: 0.09273  loss_box_reg_stage0: 0.1894  loss_cls_stage1: 0.06767  loss_box_reg_stage1: 0.4863  loss_cls_stage2: 0.07544  loss_box_reg_stage2: 0.738  loss_mask: 0.06682  loss_rpn_cls: 0.002799  loss_rpn_loc: 0.01711  time: 0.4275  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:59:28] d2.utils.events INFO:  eta: 13:27:37  iter: 9299  total_loss: 1.761  loss_cls_stage0: 0.1013  loss_box_reg_stage0: 0.196  loss_cls_stage1: 0.08013  loss_box_reg_stage1: 0.4858  loss_cls_stage2: 0.08555  loss_box_reg_stage2: 0.7241  loss_mask: 0.06596  loss_rpn_cls: 0.002655  loss_rpn_loc: 0.01631  time: 0.4274  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 12:59:36] d2.utils.events INFO:  eta: 13:27:29  iter: 9319  total_loss: 2.053  loss_cls_stage0: 0.1042  loss_box_reg_stage0: 0.2208  loss_cls_stage1: 0.07746  loss_box_reg_stage1: 0.5788  loss_cls_stage2: 0.07931  loss_box_reg_stage2: 0.8031  loss_mask: 0.06369  loss_rpn_cls: 0.005245  loss_rpn_loc: 0.01573  time: 0.4274  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:59:45] d2.utils.events INFO:  eta: 13:27:11  iter: 9339  total_loss: 1.988  loss_cls_stage0: 0.1137  loss_box_reg_stage0: 0.2315  loss_cls_stage1: 0.09515  loss_box_reg_stage1: 0.5722  loss_cls_stage2: 0.08316  loss_box_reg_stage2: 0.8144  loss_mask: 0.07052  loss_rpn_cls: 0.00408  loss_rpn_loc: 0.01805  time: 0.4274  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 12:59:53] d2.utils.events INFO:  eta: 13:27:25  iter: 9359  total_loss: 1.873  loss_cls_stage0: 0.1018  loss_box_reg_stage0: 0.19  loss_cls_stage1: 0.07245  loss_box_reg_stage1: 0.5362  loss_cls_stage2: 0.07596  loss_box_reg_stage2: 0.8269  loss_mask: 0.06545  loss_rpn_cls: 0.004762  loss_rpn_loc: 0.01701  time: 0.4274  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:00:02] d2.utils.events INFO:  eta: 13:27:38  iter: 9379  total_loss: 1.796  loss_cls_stage0: 0.09223  loss_box_reg_stage0: 0.1815  loss_cls_stage1: 0.0763  loss_box_reg_stage1: 0.5046  loss_cls_stage2: 0.06688  loss_box_reg_stage2: 0.7432  loss_mask: 0.06357  loss_rpn_cls: 0.003204  loss_rpn_loc: 0.01609  time: 0.4274  data_time: 0.0039  lr: 0.00016  max_mem: 7560M
[09/18 13:00:10] d2.utils.events INFO:  eta: 13:27:59  iter: 9399  total_loss: 2.292  loss_cls_stage0: 0.1207  loss_box_reg_stage0: 0.2401  loss_cls_stage1: 0.08048  loss_box_reg_stage1: 0.6562  loss_cls_stage2: 0.08583  loss_box_reg_stage2: 0.9341  loss_mask: 0.06871  loss_rpn_cls: 0.004488  loss_rpn_loc: 0.01899  time: 0.4274  data_time: 0.0038  lr: 0.00016  max_mem: 7560M
[09/18 13:00:19] d2.utils.events INFO:  eta: 13:28:06  iter: 9419  total_loss: 1.975  loss_cls_stage0: 0.1079  loss_box_reg_stage0: 0.2197  loss_cls_stage1: 0.07876  loss_box_reg_stage1: 0.5555  loss_cls_stage2: 0.07998  loss_box_reg_stage2: 0.8176  loss_mask: 0.0797  loss_rpn_cls: 0.004143  loss_rpn_loc: 0.01587  time: 0.4274  data_time: 0.0037  lr: 0.00016  max_mem: 7560M
[09/18 13:00:27] d2.utils.events INFO:  eta: 13:28:01  iter: 9439  total_loss: 1.791  loss_cls_stage0: 0.1175  loss_box_reg_stage0: 0.205  loss_cls_stage1: 0.08693  loss_box_reg_stage1: 0.5034  loss_cls_stage2: 0.08425  loss_box_reg_stage2: 0.7328  loss_mask: 0.06506  loss_rpn_cls: 0.003411  loss_rpn_loc: 0.01721  time: 0.4274  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 13:00:36] d2.utils.events INFO:  eta: 13:27:53  iter: 9459  total_loss: 1.657  loss_cls_stage0: 0.0888  loss_box_reg_stage0: 0.1781  loss_cls_stage1: 0.06443  loss_box_reg_stage1: 0.4662  loss_cls_stage2: 0.05798  loss_box_reg_stage2: 0.691  loss_mask: 0.06554  loss_rpn_cls: 0.00334  loss_rpn_loc: 0.01621  time: 0.4274  data_time: 0.0039  lr: 0.00016  max_mem: 7560M
[09/18 13:00:44] d2.utils.events INFO:  eta: 13:27:38  iter: 9479  total_loss: 1.764  loss_cls_stage0: 0.09739  loss_box_reg_stage0: 0.2298  loss_cls_stage1: 0.06573  loss_box_reg_stage1: 0.5149  loss_cls_stage2: 0.06756  loss_box_reg_stage2: 0.7037  loss_mask: 0.06813  loss_rpn_cls: 0.00288  loss_rpn_loc: 0.01786  time: 0.4274  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:00:52] d2.utils.events INFO:  eta: 13:27:26  iter: 9499  total_loss: 1.843  loss_cls_stage0: 0.1018  loss_box_reg_stage0: 0.2054  loss_cls_stage1: 0.0814  loss_box_reg_stage1: 0.5277  loss_cls_stage2: 0.07911  loss_box_reg_stage2: 0.7568  loss_mask: 0.06998  loss_rpn_cls: 0.003982  loss_rpn_loc: 0.01761  time: 0.4273  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:01:01] d2.utils.events INFO:  eta: 13:27:28  iter: 9519  total_loss: 2.02  loss_cls_stage0: 0.1072  loss_box_reg_stage0: 0.2186  loss_cls_stage1: 0.09359  loss_box_reg_stage1: 0.575  loss_cls_stage2: 0.08474  loss_box_reg_stage2: 0.775  loss_mask: 0.06937  loss_rpn_cls: 0.002963  loss_rpn_loc: 0.01637  time: 0.4273  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 13:01:09] d2.utils.events INFO:  eta: 13:27:09  iter: 9539  total_loss: 2.167  loss_cls_stage0: 0.1185  loss_box_reg_stage0: 0.2238  loss_cls_stage1: 0.08951  loss_box_reg_stage1: 0.6222  loss_cls_stage2: 0.09647  loss_box_reg_stage2: 0.8618  loss_mask: 0.07075  loss_rpn_cls: 0.004268  loss_rpn_loc: 0.01699  time: 0.4273  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 13:01:17] d2.utils.events INFO:  eta: 13:26:31  iter: 9559  total_loss: 1.858  loss_cls_stage0: 0.09648  loss_box_reg_stage0: 0.1908  loss_cls_stage1: 0.06754  loss_box_reg_stage1: 0.5247  loss_cls_stage2: 0.06599  loss_box_reg_stage2: 0.7787  loss_mask: 0.07285  loss_rpn_cls: 0.002824  loss_rpn_loc: 0.01729  time: 0.4272  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:01:26] d2.utils.events INFO:  eta: 13:25:47  iter: 9579  total_loss: 1.951  loss_cls_stage0: 0.1067  loss_box_reg_stage0: 0.2027  loss_cls_stage1: 0.08076  loss_box_reg_stage1: 0.5391  loss_cls_stage2: 0.08548  loss_box_reg_stage2: 0.8203  loss_mask: 0.06865  loss_rpn_cls: 0.003166  loss_rpn_loc: 0.01628  time: 0.4272  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:01:34] d2.utils.events INFO:  eta: 13:25:23  iter: 9599  total_loss: 1.928  loss_cls_stage0: 0.1086  loss_box_reg_stage0: 0.212  loss_cls_stage1: 0.09037  loss_box_reg_stage1: 0.5309  loss_cls_stage2: 0.09033  loss_box_reg_stage2: 0.8216  loss_mask: 0.07107  loss_rpn_cls: 0.002694  loss_rpn_loc: 0.0171  time: 0.4272  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 13:01:42] d2.utils.events INFO:  eta: 13:25:13  iter: 9619  total_loss: 1.854  loss_cls_stage0: 0.1046  loss_box_reg_stage0: 0.1977  loss_cls_stage1: 0.0866  loss_box_reg_stage1: 0.4999  loss_cls_stage2: 0.08626  loss_box_reg_stage2: 0.7459  loss_mask: 0.06579  loss_rpn_cls: 0.00471  loss_rpn_loc: 0.01795  time: 0.4271  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 13:01:50] d2.utils.events INFO:  eta: 13:24:46  iter: 9639  total_loss: 1.882  loss_cls_stage0: 0.1058  loss_box_reg_stage0: 0.2012  loss_cls_stage1: 0.08564  loss_box_reg_stage1: 0.5129  loss_cls_stage2: 0.07526  loss_box_reg_stage2: 0.7442  loss_mask: 0.07431  loss_rpn_cls: 0.005137  loss_rpn_loc: 0.01639  time: 0.4271  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:01:59] d2.utils.events INFO:  eta: 13:24:30  iter: 9659  total_loss: 1.925  loss_cls_stage0: 0.09986  loss_box_reg_stage0: 0.2171  loss_cls_stage1: 0.08059  loss_box_reg_stage1: 0.557  loss_cls_stage2: 0.08439  loss_box_reg_stage2: 0.7826  loss_mask: 0.06872  loss_rpn_cls: 0.004776  loss_rpn_loc: 0.01775  time: 0.4271  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 13:02:07] d2.utils.events INFO:  eta: 13:24:29  iter: 9679  total_loss: 1.722  loss_cls_stage0: 0.106  loss_box_reg_stage0: 0.2115  loss_cls_stage1: 0.07209  loss_box_reg_stage1: 0.4876  loss_cls_stage2: 0.07939  loss_box_reg_stage2: 0.6781  loss_mask: 0.07251  loss_rpn_cls: 0.00265  loss_rpn_loc: 0.01695  time: 0.4271  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 13:02:15] d2.utils.events INFO:  eta: 13:24:34  iter: 9699  total_loss: 1.783  loss_cls_stage0: 0.09591  loss_box_reg_stage0: 0.2067  loss_cls_stage1: 0.07487  loss_box_reg_stage1: 0.5104  loss_cls_stage2: 0.07657  loss_box_reg_stage2: 0.7089  loss_mask: 0.06738  loss_rpn_cls: 0.003991  loss_rpn_loc: 0.01622  time: 0.4271  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 13:02:24] d2.utils.events INFO:  eta: 13:24:37  iter: 9719  total_loss: 1.818  loss_cls_stage0: 0.09841  loss_box_reg_stage0: 0.1955  loss_cls_stage1: 0.0685  loss_box_reg_stage1: 0.5276  loss_cls_stage2: 0.07772  loss_box_reg_stage2: 0.7636  loss_mask: 0.07072  loss_rpn_cls: 0.003508  loss_rpn_loc: 0.01677  time: 0.4271  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 13:02:32] d2.utils.events INFO:  eta: 13:24:17  iter: 9739  total_loss: 1.709  loss_cls_stage0: 0.09752  loss_box_reg_stage0: 0.1861  loss_cls_stage1: 0.0705  loss_box_reg_stage1: 0.4898  loss_cls_stage2: 0.06195  loss_box_reg_stage2: 0.6998  loss_mask: 0.07219  loss_rpn_cls: 0.006729  loss_rpn_loc: 0.01577  time: 0.4270  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 13:02:41] d2.utils.events INFO:  eta: 13:24:20  iter: 9759  total_loss: 2.149  loss_cls_stage0: 0.1124  loss_box_reg_stage0: 0.2312  loss_cls_stage1: 0.09582  loss_box_reg_stage1: 0.6056  loss_cls_stage2: 0.1104  loss_box_reg_stage2: 0.8197  loss_mask: 0.06704  loss_rpn_cls: 0.004847  loss_rpn_loc: 0.01915  time: 0.4270  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 13:02:49] d2.utils.events INFO:  eta: 13:24:08  iter: 9779  total_loss: 1.882  loss_cls_stage0: 0.1045  loss_box_reg_stage0: 0.2005  loss_cls_stage1: 0.07939  loss_box_reg_stage1: 0.5306  loss_cls_stage2: 0.07677  loss_box_reg_stage2: 0.7696  loss_mask: 0.07097  loss_rpn_cls: 0.005969  loss_rpn_loc: 0.01684  time: 0.4270  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 13:02:58] d2.utils.events INFO:  eta: 13:23:48  iter: 9799  total_loss: 1.677  loss_cls_stage0: 0.09402  loss_box_reg_stage0: 0.2033  loss_cls_stage1: 0.07626  loss_box_reg_stage1: 0.4582  loss_cls_stage2: 0.07892  loss_box_reg_stage2: 0.683  loss_mask: 0.06728  loss_rpn_cls: 0.003247  loss_rpn_loc: 0.01641  time: 0.4270  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 13:03:06] d2.utils.events INFO:  eta: 13:23:37  iter: 9819  total_loss: 1.743  loss_cls_stage0: 0.09138  loss_box_reg_stage0: 0.1858  loss_cls_stage1: 0.07807  loss_box_reg_stage1: 0.4892  loss_cls_stage2: 0.06511  loss_box_reg_stage2: 0.6573  loss_mask: 0.06573  loss_rpn_cls: 0.006463  loss_rpn_loc: 0.01471  time: 0.4270  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 13:03:14] d2.utils.events INFO:  eta: 13:23:31  iter: 9839  total_loss: 1.796  loss_cls_stage0: 0.09152  loss_box_reg_stage0: 0.1954  loss_cls_stage1: 0.07712  loss_box_reg_stage1: 0.5057  loss_cls_stage2: 0.08209  loss_box_reg_stage2: 0.7835  loss_mask: 0.0675  loss_rpn_cls: 0.00291  loss_rpn_loc: 0.01718  time: 0.4270  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 13:03:23] d2.utils.events INFO:  eta: 13:23:17  iter: 9859  total_loss: 1.734  loss_cls_stage0: 0.09216  loss_box_reg_stage0: 0.2084  loss_cls_stage1: 0.06615  loss_box_reg_stage1: 0.5025  loss_cls_stage2: 0.06303  loss_box_reg_stage2: 0.689  loss_mask: 0.0663  loss_rpn_cls: 0.003842  loss_rpn_loc: 0.01874  time: 0.4270  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 13:03:31] d2.utils.events INFO:  eta: 13:23:13  iter: 9879  total_loss: 1.792  loss_cls_stage0: 0.09975  loss_box_reg_stage0: 0.2036  loss_cls_stage1: 0.0789  loss_box_reg_stage1: 0.5124  loss_cls_stage2: 0.07464  loss_box_reg_stage2: 0.6988  loss_mask: 0.06785  loss_rpn_cls: 0.004886  loss_rpn_loc: 0.01747  time: 0.4269  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:03:40] d2.utils.events INFO:  eta: 13:22:57  iter: 9899  total_loss: 1.905  loss_cls_stage0: 0.1173  loss_box_reg_stage0: 0.2274  loss_cls_stage1: 0.0857  loss_box_reg_stage1: 0.5376  loss_cls_stage2: 0.1034  loss_box_reg_stage2: 0.8279  loss_mask: 0.07385  loss_rpn_cls: 0.003647  loss_rpn_loc: 0.01937  time: 0.4269  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 13:03:48] d2.utils.events INFO:  eta: 13:22:44  iter: 9919  total_loss: 1.891  loss_cls_stage0: 0.1051  loss_box_reg_stage0: 0.21  loss_cls_stage1: 0.07695  loss_box_reg_stage1: 0.5392  loss_cls_stage2: 0.07175  loss_box_reg_stage2: 0.8154  loss_mask: 0.07309  loss_rpn_cls: 0.003646  loss_rpn_loc: 0.01694  time: 0.4269  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:03:57] d2.utils.events INFO:  eta: 13:23:01  iter: 9939  total_loss: 1.557  loss_cls_stage0: 0.08973  loss_box_reg_stage0: 0.1855  loss_cls_stage1: 0.06983  loss_box_reg_stage1: 0.4558  loss_cls_stage2: 0.07237  loss_box_reg_stage2: 0.6541  loss_mask: 0.07139  loss_rpn_cls: 0.00388  loss_rpn_loc: 0.01433  time: 0.4269  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:04:05] d2.utils.events INFO:  eta: 13:23:17  iter: 9959  total_loss: 1.93  loss_cls_stage0: 0.09736  loss_box_reg_stage0: 0.2243  loss_cls_stage1: 0.06698  loss_box_reg_stage1: 0.5551  loss_cls_stage2: 0.07247  loss_box_reg_stage2: 0.7998  loss_mask: 0.07148  loss_rpn_cls: 0.002088  loss_rpn_loc: 0.01866  time: 0.4269  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 13:04:13] d2.utils.events INFO:  eta: 13:23:14  iter: 9979  total_loss: 1.663  loss_cls_stage0: 0.08596  loss_box_reg_stage0: 0.1846  loss_cls_stage1: 0.06094  loss_box_reg_stage1: 0.4679  loss_cls_stage2: 0.05691  loss_box_reg_stage2: 0.7139  loss_mask: 0.06602  loss_rpn_cls: 0.003687  loss_rpn_loc: 0.01619  time: 0.4269  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:04:22] fvcore.common.checkpoint INFO: Saving checkpoint to ./mvit2_lmo_output/model_0009999.pth
[09/18 13:04:23] d2.utils.events INFO:  eta: 13:23:10  iter: 9999  total_loss: 1.88  loss_cls_stage0: 0.1121  loss_box_reg_stage0: 0.2038  loss_cls_stage1: 0.07506  loss_box_reg_stage1: 0.5301  loss_cls_stage2: 0.07181  loss_box_reg_stage2: 0.7794  loss_mask: 0.06712  loss_rpn_cls: 0.002586  loss_rpn_loc: 0.01725  time: 0.4269  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:04:32] d2.utils.events INFO:  eta: 13:23:40  iter: 10019  total_loss: 2.007  loss_cls_stage0: 0.09978  loss_box_reg_stage0: 0.2134  loss_cls_stage1: 0.07547  loss_box_reg_stage1: 0.6013  loss_cls_stage2: 0.08681  loss_box_reg_stage2: 0.8126  loss_mask: 0.06845  loss_rpn_cls: 0.003818  loss_rpn_loc: 0.01675  time: 0.4269  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:04:40] d2.utils.events INFO:  eta: 13:23:15  iter: 10039  total_loss: 1.747  loss_cls_stage0: 0.09502  loss_box_reg_stage0: 0.2059  loss_cls_stage1: 0.07287  loss_box_reg_stage1: 0.4977  loss_cls_stage2: 0.07558  loss_box_reg_stage2: 0.7143  loss_mask: 0.06908  loss_rpn_cls: 0.003885  loss_rpn_loc: 0.01683  time: 0.4268  data_time: 0.0039  lr: 0.00016  max_mem: 7560M
[09/18 13:04:48] d2.utils.events INFO:  eta: 13:22:41  iter: 10059  total_loss: 1.865  loss_cls_stage0: 0.1025  loss_box_reg_stage0: 0.2135  loss_cls_stage1: 0.08227  loss_box_reg_stage1: 0.533  loss_cls_stage2: 0.07662  loss_box_reg_stage2: 0.7231  loss_mask: 0.06563  loss_rpn_cls: 0.002999  loss_rpn_loc: 0.01578  time: 0.4268  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:04:57] d2.utils.events INFO:  eta: 13:22:29  iter: 10079  total_loss: 1.691  loss_cls_stage0: 0.09531  loss_box_reg_stage0: 0.1962  loss_cls_stage1: 0.07247  loss_box_reg_stage1: 0.4965  loss_cls_stage2: 0.06449  loss_box_reg_stage2: 0.7102  loss_mask: 0.0671  loss_rpn_cls: 0.003207  loss_rpn_loc: 0.01604  time: 0.4268  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 13:05:05] d2.utils.events INFO:  eta: 13:22:21  iter: 10099  total_loss: 1.871  loss_cls_stage0: 0.0991  loss_box_reg_stage0: 0.1957  loss_cls_stage1: 0.06923  loss_box_reg_stage1: 0.5325  loss_cls_stage2: 0.06784  loss_box_reg_stage2: 0.7398  loss_mask: 0.06967  loss_rpn_cls: 0.002695  loss_rpn_loc: 0.01637  time: 0.4268  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 13:05:13] d2.utils.events INFO:  eta: 13:22:12  iter: 10119  total_loss: 1.922  loss_cls_stage0: 0.117  loss_box_reg_stage0: 0.2293  loss_cls_stage1: 0.086  loss_box_reg_stage1: 0.5513  loss_cls_stage2: 0.07865  loss_box_reg_stage2: 0.7311  loss_mask: 0.06899  loss_rpn_cls: 0.003084  loss_rpn_loc: 0.01706  time: 0.4268  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 13:05:22] d2.utils.events INFO:  eta: 13:22:16  iter: 10139  total_loss: 1.644  loss_cls_stage0: 0.08389  loss_box_reg_stage0: 0.2034  loss_cls_stage1: 0.05523  loss_box_reg_stage1: 0.46  loss_cls_stage2: 0.06066  loss_box_reg_stage2: 0.6461  loss_mask: 0.06587  loss_rpn_cls: 0.001629  loss_rpn_loc: 0.01656  time: 0.4268  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:05:30] d2.utils.events INFO:  eta: 13:22:02  iter: 10159  total_loss: 1.808  loss_cls_stage0: 0.0812  loss_box_reg_stage0: 0.2071  loss_cls_stage1: 0.06218  loss_box_reg_stage1: 0.5079  loss_cls_stage2: 0.07062  loss_box_reg_stage2: 0.7372  loss_mask: 0.06415  loss_rpn_cls: 0.003336  loss_rpn_loc: 0.01674  time: 0.4267  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:05:38] d2.utils.events INFO:  eta: 13:21:31  iter: 10179  total_loss: 1.872  loss_cls_stage0: 0.09806  loss_box_reg_stage0: 0.2149  loss_cls_stage1: 0.07787  loss_box_reg_stage1: 0.5414  loss_cls_stage2: 0.07835  loss_box_reg_stage2: 0.7297  loss_mask: 0.06573  loss_rpn_cls: 0.004812  loss_rpn_loc: 0.01779  time: 0.4267  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 13:05:47] d2.utils.events INFO:  eta: 13:20:55  iter: 10199  total_loss: 1.9  loss_cls_stage0: 0.1075  loss_box_reg_stage0: 0.2178  loss_cls_stage1: 0.09319  loss_box_reg_stage1: 0.539  loss_cls_stage2: 0.0856  loss_box_reg_stage2: 0.7979  loss_mask: 0.07086  loss_rpn_cls: 0.00486  loss_rpn_loc: 0.01854  time: 0.4267  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 13:05:55] d2.utils.events INFO:  eta: 13:20:29  iter: 10219  total_loss: 1.945  loss_cls_stage0: 0.1025  loss_box_reg_stage0: 0.1986  loss_cls_stage1: 0.07827  loss_box_reg_stage1: 0.5674  loss_cls_stage2: 0.08117  loss_box_reg_stage2: 0.8622  loss_mask: 0.06652  loss_rpn_cls: 0.00394  loss_rpn_loc: 0.01661  time: 0.4267  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 13:06:03] d2.utils.events INFO:  eta: 13:20:01  iter: 10239  total_loss: 1.932  loss_cls_stage0: 0.1117  loss_box_reg_stage0: 0.2125  loss_cls_stage1: 0.07514  loss_box_reg_stage1: 0.5579  loss_cls_stage2: 0.06966  loss_box_reg_stage2: 0.7624  loss_mask: 0.06518  loss_rpn_cls: 0.00344  loss_rpn_loc: 0.0151  time: 0.4266  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 13:06:12] d2.utils.events INFO:  eta: 13:20:05  iter: 10259  total_loss: 1.772  loss_cls_stage0: 0.09525  loss_box_reg_stage0: 0.2039  loss_cls_stage1: 0.07055  loss_box_reg_stage1: 0.5048  loss_cls_stage2: 0.05533  loss_box_reg_stage2: 0.7233  loss_mask: 0.07098  loss_rpn_cls: 0.003066  loss_rpn_loc: 0.01678  time: 0.4266  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 13:06:20] d2.utils.events INFO:  eta: 13:18:39  iter: 10279  total_loss: 1.664  loss_cls_stage0: 0.08426  loss_box_reg_stage0: 0.1882  loss_cls_stage1: 0.05389  loss_box_reg_stage1: 0.4563  loss_cls_stage2: 0.04825  loss_box_reg_stage2: 0.7062  loss_mask: 0.06705  loss_rpn_cls: 0.002511  loss_rpn_loc: 0.01679  time: 0.4266  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:06:28] d2.utils.events INFO:  eta: 13:18:13  iter: 10299  total_loss: 1.865  loss_cls_stage0: 0.09636  loss_box_reg_stage0: 0.2042  loss_cls_stage1: 0.06629  loss_box_reg_stage1: 0.5379  loss_cls_stage2: 0.07504  loss_box_reg_stage2: 0.7869  loss_mask: 0.07529  loss_rpn_cls: 0.004235  loss_rpn_loc: 0.01881  time: 0.4266  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 13:06:37] d2.utils.events INFO:  eta: 13:17:07  iter: 10319  total_loss: 1.845  loss_cls_stage0: 0.09772  loss_box_reg_stage0: 0.2082  loss_cls_stage1: 0.07096  loss_box_reg_stage1: 0.5232  loss_cls_stage2: 0.08397  loss_box_reg_stage2: 0.7347  loss_mask: 0.07563  loss_rpn_cls: 0.004969  loss_rpn_loc: 0.01727  time: 0.4265  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 13:06:45] d2.utils.events INFO:  eta: 13:16:06  iter: 10339  total_loss: 1.677  loss_cls_stage0: 0.09159  loss_box_reg_stage0: 0.2114  loss_cls_stage1: 0.06238  loss_box_reg_stage1: 0.4842  loss_cls_stage2: 0.05178  loss_box_reg_stage2: 0.6944  loss_mask: 0.07009  loss_rpn_cls: 0.002927  loss_rpn_loc: 0.01712  time: 0.4265  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 13:06:53] d2.utils.events INFO:  eta: 13:15:11  iter: 10359  total_loss: 1.744  loss_cls_stage0: 0.1036  loss_box_reg_stage0: 0.225  loss_cls_stage1: 0.0801  loss_box_reg_stage1: 0.4719  loss_cls_stage2: 0.0705  loss_box_reg_stage2: 0.7132  loss_mask: 0.07019  loss_rpn_cls: 0.003586  loss_rpn_loc: 0.01718  time: 0.4265  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 13:07:01] d2.utils.events INFO:  eta: 13:14:30  iter: 10379  total_loss: 1.672  loss_cls_stage0: 0.08319  loss_box_reg_stage0: 0.1829  loss_cls_stage1: 0.05351  loss_box_reg_stage1: 0.4703  loss_cls_stage2: 0.05292  loss_box_reg_stage2: 0.6721  loss_mask: 0.06483  loss_rpn_cls: 0.001931  loss_rpn_loc: 0.01668  time: 0.4265  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 13:07:09] d2.utils.events INFO:  eta: 13:13:54  iter: 10399  total_loss: 1.753  loss_cls_stage0: 0.08873  loss_box_reg_stage0: 0.202  loss_cls_stage1: 0.0713  loss_box_reg_stage1: 0.4764  loss_cls_stage2: 0.0745  loss_box_reg_stage2: 0.7166  loss_mask: 0.06978  loss_rpn_cls: 0.003469  loss_rpn_loc: 0.01664  time: 0.4264  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 13:07:18] d2.utils.events INFO:  eta: 13:13:31  iter: 10419  total_loss: 1.772  loss_cls_stage0: 0.0945  loss_box_reg_stage0: 0.2  loss_cls_stage1: 0.07401  loss_box_reg_stage1: 0.5209  loss_cls_stage2: 0.0698  loss_box_reg_stage2: 0.7746  loss_mask: 0.06255  loss_rpn_cls: 0.002937  loss_rpn_loc: 0.01766  time: 0.4264  data_time: 0.0038  lr: 0.00016  max_mem: 7560M
[09/18 13:07:26] d2.utils.events INFO:  eta: 13:13:20  iter: 10439  total_loss: 1.982  loss_cls_stage0: 0.1035  loss_box_reg_stage0: 0.2126  loss_cls_stage1: 0.07721  loss_box_reg_stage1: 0.5671  loss_cls_stage2: 0.07846  loss_box_reg_stage2: 0.7724  loss_mask: 0.06895  loss_rpn_cls: 0.005908  loss_rpn_loc: 0.01731  time: 0.4264  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:07:34] d2.utils.events INFO:  eta: 13:13:09  iter: 10459  total_loss: 1.668  loss_cls_stage0: 0.09618  loss_box_reg_stage0: 0.1927  loss_cls_stage1: 0.06553  loss_box_reg_stage1: 0.461  loss_cls_stage2: 0.0676  loss_box_reg_stage2: 0.6918  loss_mask: 0.06437  loss_rpn_cls: 0.003383  loss_rpn_loc: 0.01624  time: 0.4264  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 13:07:43] d2.utils.events INFO:  eta: 13:13:04  iter: 10479  total_loss: 2.046  loss_cls_stage0: 0.1108  loss_box_reg_stage0: 0.2205  loss_cls_stage1: 0.0838  loss_box_reg_stage1: 0.5775  loss_cls_stage2: 0.07441  loss_box_reg_stage2: 0.8386  loss_mask: 0.07081  loss_rpn_cls: 0.004178  loss_rpn_loc: 0.01705  time: 0.4263  data_time: 0.0038  lr: 0.00016  max_mem: 7560M
[09/18 13:07:51] d2.utils.events INFO:  eta: 13:13:25  iter: 10499  total_loss: 1.786  loss_cls_stage0: 0.1043  loss_box_reg_stage0: 0.205  loss_cls_stage1: 0.06524  loss_box_reg_stage1: 0.5074  loss_cls_stage2: 0.06422  loss_box_reg_stage2: 0.7795  loss_mask: 0.07023  loss_rpn_cls: 0.00406  loss_rpn_loc: 0.01726  time: 0.4263  data_time: 0.0039  lr: 0.00016  max_mem: 7560M
[09/18 13:07:59] d2.utils.events INFO:  eta: 13:13:08  iter: 10519  total_loss: 1.761  loss_cls_stage0: 0.09175  loss_box_reg_stage0: 0.1793  loss_cls_stage1: 0.08607  loss_box_reg_stage1: 0.5146  loss_cls_stage2: 0.09258  loss_box_reg_stage2: 0.7138  loss_mask: 0.0683  loss_rpn_cls: 0.00334  loss_rpn_loc: 0.01626  time: 0.4263  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:08:08] d2.utils.events INFO:  eta: 13:13:11  iter: 10539  total_loss: 1.942  loss_cls_stage0: 0.09672  loss_box_reg_stage0: 0.1963  loss_cls_stage1: 0.07224  loss_box_reg_stage1: 0.562  loss_cls_stage2: 0.0659  loss_box_reg_stage2: 0.6943  loss_mask: 0.07163  loss_rpn_cls: 0.003881  loss_rpn_loc: 0.01652  time: 0.4263  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:08:16] d2.utils.events INFO:  eta: 13:13:21  iter: 10559  total_loss: 1.688  loss_cls_stage0: 0.09519  loss_box_reg_stage0: 0.1987  loss_cls_stage1: 0.06679  loss_box_reg_stage1: 0.48  loss_cls_stage2: 0.05783  loss_box_reg_stage2: 0.746  loss_mask: 0.06596  loss_rpn_cls: 0.004859  loss_rpn_loc: 0.01744  time: 0.4263  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 13:08:25] d2.utils.events INFO:  eta: 13:13:03  iter: 10579  total_loss: 1.696  loss_cls_stage0: 0.09725  loss_box_reg_stage0: 0.2031  loss_cls_stage1: 0.0648  loss_box_reg_stage1: 0.481  loss_cls_stage2: 0.05457  loss_box_reg_stage2: 0.651  loss_mask: 0.06547  loss_rpn_cls: 0.004595  loss_rpn_loc: 0.01839  time: 0.4263  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:08:33] d2.utils.events INFO:  eta: 13:13:08  iter: 10599  total_loss: 1.869  loss_cls_stage0: 0.09455  loss_box_reg_stage0: 0.2031  loss_cls_stage1: 0.06921  loss_box_reg_stage1: 0.5454  loss_cls_stage2: 0.0657  loss_box_reg_stage2: 0.7076  loss_mask: 0.06655  loss_rpn_cls: 0.003236  loss_rpn_loc: 0.01689  time: 0.4262  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 13:08:41] d2.utils.events INFO:  eta: 13:13:23  iter: 10619  total_loss: 1.701  loss_cls_stage0: 0.1024  loss_box_reg_stage0: 0.199  loss_cls_stage1: 0.07347  loss_box_reg_stage1: 0.4731  loss_cls_stage2: 0.07575  loss_box_reg_stage2: 0.6838  loss_mask: 0.06847  loss_rpn_cls: 0.007674  loss_rpn_loc: 0.01869  time: 0.4262  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:08:50] d2.utils.events INFO:  eta: 13:14:01  iter: 10639  total_loss: 1.865  loss_cls_stage0: 0.1092  loss_box_reg_stage0: 0.2259  loss_cls_stage1: 0.09204  loss_box_reg_stage1: 0.58  loss_cls_stage2: 0.08779  loss_box_reg_stage2: 0.762  loss_mask: 0.0696  loss_rpn_cls: 0.005177  loss_rpn_loc: 0.01837  time: 0.4262  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:08:59] d2.utils.events INFO:  eta: 13:14:27  iter: 10659  total_loss: 1.803  loss_cls_stage0: 0.09543  loss_box_reg_stage0: 0.2049  loss_cls_stage1: 0.05866  loss_box_reg_stage1: 0.4968  loss_cls_stage2: 0.06891  loss_box_reg_stage2: 0.6654  loss_mask: 0.06734  loss_rpn_cls: 0.003605  loss_rpn_loc: 0.01704  time: 0.4262  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 13:09:07] d2.utils.events INFO:  eta: 13:14:24  iter: 10679  total_loss: 1.962  loss_cls_stage0: 0.09776  loss_box_reg_stage0: 0.218  loss_cls_stage1: 0.07774  loss_box_reg_stage1: 0.5774  loss_cls_stage2: 0.07195  loss_box_reg_stage2: 0.81  loss_mask: 0.07277  loss_rpn_cls: 0.004395  loss_rpn_loc: 0.01886  time: 0.4262  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 13:09:16] d2.utils.events INFO:  eta: 13:14:25  iter: 10699  total_loss: 1.65  loss_cls_stage0: 0.1071  loss_box_reg_stage0: 0.1935  loss_cls_stage1: 0.08041  loss_box_reg_stage1: 0.4702  loss_cls_stage2: 0.0784  loss_box_reg_stage2: 0.653  loss_mask: 0.07403  loss_rpn_cls: 0.003456  loss_rpn_loc: 0.01816  time: 0.4262  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 13:09:24] d2.utils.events INFO:  eta: 13:14:09  iter: 10719  total_loss: 1.686  loss_cls_stage0: 0.08826  loss_box_reg_stage0: 0.1889  loss_cls_stage1: 0.07002  loss_box_reg_stage1: 0.4862  loss_cls_stage2: 0.06376  loss_box_reg_stage2: 0.6642  loss_mask: 0.07106  loss_rpn_cls: 0.003734  loss_rpn_loc: 0.01815  time: 0.4262  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:09:32] d2.utils.events INFO:  eta: 13:13:47  iter: 10739  total_loss: 1.765  loss_cls_stage0: 0.09272  loss_box_reg_stage0: 0.1846  loss_cls_stage1: 0.06925  loss_box_reg_stage1: 0.5062  loss_cls_stage2: 0.05902  loss_box_reg_stage2: 0.7274  loss_mask: 0.06976  loss_rpn_cls: 0.003118  loss_rpn_loc: 0.01743  time: 0.4262  data_time: 0.0038  lr: 0.00016  max_mem: 7560M
[09/18 13:09:41] d2.utils.events INFO:  eta: 13:13:13  iter: 10759  total_loss: 1.56  loss_cls_stage0: 0.08058  loss_box_reg_stage0: 0.1823  loss_cls_stage1: 0.05167  loss_box_reg_stage1: 0.4667  loss_cls_stage2: 0.05034  loss_box_reg_stage2: 0.6621  loss_mask: 0.06619  loss_rpn_cls: 0.003862  loss_rpn_loc: 0.01778  time: 0.4262  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 13:09:49] d2.utils.events INFO:  eta: 13:13:44  iter: 10779  total_loss: 1.841  loss_cls_stage0: 0.1187  loss_box_reg_stage0: 0.2125  loss_cls_stage1: 0.07522  loss_box_reg_stage1: 0.5075  loss_cls_stage2: 0.06921  loss_box_reg_stage2: 0.7461  loss_mask: 0.07094  loss_rpn_cls: 0.002174  loss_rpn_loc: 0.01624  time: 0.4262  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:09:58] d2.utils.events INFO:  eta: 13:13:57  iter: 10799  total_loss: 1.97  loss_cls_stage0: 0.1023  loss_box_reg_stage0: 0.2266  loss_cls_stage1: 0.07924  loss_box_reg_stage1: 0.5788  loss_cls_stage2: 0.08732  loss_box_reg_stage2: 0.7851  loss_mask: 0.06439  loss_rpn_cls: 0.003943  loss_rpn_loc: 0.018  time: 0.4262  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 13:10:06] d2.utils.events INFO:  eta: 13:13:59  iter: 10819  total_loss: 1.746  loss_cls_stage0: 0.0953  loss_box_reg_stage0: 0.1988  loss_cls_stage1: 0.07105  loss_box_reg_stage1: 0.4928  loss_cls_stage2: 0.06336  loss_box_reg_stage2: 0.6951  loss_mask: 0.07022  loss_rpn_cls: 0.002624  loss_rpn_loc: 0.01701  time: 0.4262  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 13:10:15] d2.utils.events INFO:  eta: 13:13:36  iter: 10839  total_loss: 1.93  loss_cls_stage0: 0.1182  loss_box_reg_stage0: 0.214  loss_cls_stage1: 0.08922  loss_box_reg_stage1: 0.5396  loss_cls_stage2: 0.0949  loss_box_reg_stage2: 0.7816  loss_mask: 0.07434  loss_rpn_cls: 0.007893  loss_rpn_loc: 0.01788  time: 0.4261  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:10:23] d2.utils.events INFO:  eta: 13:13:30  iter: 10859  total_loss: 1.821  loss_cls_stage0: 0.1111  loss_box_reg_stage0: 0.2271  loss_cls_stage1: 0.08211  loss_box_reg_stage1: 0.5156  loss_cls_stage2: 0.0809  loss_box_reg_stage2: 0.687  loss_mask: 0.06941  loss_rpn_cls: 0.006719  loss_rpn_loc: 0.01842  time: 0.4261  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 13:10:32] d2.utils.events INFO:  eta: 13:13:20  iter: 10879  total_loss: 1.502  loss_cls_stage0: 0.08526  loss_box_reg_stage0: 0.187  loss_cls_stage1: 0.05621  loss_box_reg_stage1: 0.4177  loss_cls_stage2: 0.04701  loss_box_reg_stage2: 0.6082  loss_mask: 0.06653  loss_rpn_cls: 0.003544  loss_rpn_loc: 0.01576  time: 0.4261  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 13:10:40] d2.utils.events INFO:  eta: 13:13:34  iter: 10899  total_loss: 1.684  loss_cls_stage0: 0.08587  loss_box_reg_stage0: 0.2101  loss_cls_stage1: 0.0499  loss_box_reg_stage1: 0.4729  loss_cls_stage2: 0.043  loss_box_reg_stage2: 0.6523  loss_mask: 0.06395  loss_rpn_cls: 0.00322  loss_rpn_loc: 0.01603  time: 0.4261  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 13:10:48] d2.utils.events INFO:  eta: 13:13:36  iter: 10919  total_loss: 1.75  loss_cls_stage0: 0.09447  loss_box_reg_stage0: 0.2063  loss_cls_stage1: 0.0673  loss_box_reg_stage1: 0.5046  loss_cls_stage2: 0.06219  loss_box_reg_stage2: 0.7055  loss_mask: 0.06767  loss_rpn_cls: 0.0035  loss_rpn_loc: 0.01753  time: 0.4261  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:10:57] d2.utils.events INFO:  eta: 13:12:55  iter: 10939  total_loss: 1.67  loss_cls_stage0: 0.08331  loss_box_reg_stage0: 0.1929  loss_cls_stage1: 0.06295  loss_box_reg_stage1: 0.4914  loss_cls_stage2: 0.06  loss_box_reg_stage2: 0.7366  loss_mask: 0.07073  loss_rpn_cls: 0.003976  loss_rpn_loc: 0.01793  time: 0.4261  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 13:11:05] d2.utils.events INFO:  eta: 13:12:43  iter: 10959  total_loss: 2.055  loss_cls_stage0: 0.1185  loss_box_reg_stage0: 0.2421  loss_cls_stage1: 0.09658  loss_box_reg_stage1: 0.5936  loss_cls_stage2: 0.1026  loss_box_reg_stage2: 0.8202  loss_mask: 0.07326  loss_rpn_cls: 0.00328  loss_rpn_loc: 0.01745  time: 0.4261  data_time: 0.0038  lr: 0.00016  max_mem: 7560M
[09/18 13:11:13] d2.utils.events INFO:  eta: 13:12:18  iter: 10979  total_loss: 1.841  loss_cls_stage0: 0.09336  loss_box_reg_stage0: 0.2129  loss_cls_stage1: 0.07125  loss_box_reg_stage1: 0.5166  loss_cls_stage2: 0.06569  loss_box_reg_stage2: 0.7582  loss_mask: 0.07439  loss_rpn_cls: 0.003372  loss_rpn_loc: 0.01678  time: 0.4261  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 13:11:22] d2.utils.events INFO:  eta: 13:11:30  iter: 10999  total_loss: 1.818  loss_cls_stage0: 0.1081  loss_box_reg_stage0: 0.2128  loss_cls_stage1: 0.07741  loss_box_reg_stage1: 0.5099  loss_cls_stage2: 0.06325  loss_box_reg_stage2: 0.7344  loss_mask: 0.07008  loss_rpn_cls: 0.003493  loss_rpn_loc: 0.01601  time: 0.4260  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 13:11:30] d2.utils.events INFO:  eta: 13:11:04  iter: 11019  total_loss: 1.647  loss_cls_stage0: 0.1018  loss_box_reg_stage0: 0.1909  loss_cls_stage1: 0.07199  loss_box_reg_stage1: 0.4942  loss_cls_stage2: 0.06711  loss_box_reg_stage2: 0.6922  loss_mask: 0.06851  loss_rpn_cls: 0.004082  loss_rpn_loc: 0.01747  time: 0.4260  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:11:38] d2.utils.events INFO:  eta: 13:11:47  iter: 11039  total_loss: 1.754  loss_cls_stage0: 0.09576  loss_box_reg_stage0: 0.2082  loss_cls_stage1: 0.06814  loss_box_reg_stage1: 0.5025  loss_cls_stage2: 0.05962  loss_box_reg_stage2: 0.7301  loss_mask: 0.07039  loss_rpn_cls: 0.002833  loss_rpn_loc: 0.01765  time: 0.4260  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:11:47] d2.utils.events INFO:  eta: 13:12:05  iter: 11059  total_loss: 1.773  loss_cls_stage0: 0.08954  loss_box_reg_stage0: 0.1904  loss_cls_stage1: 0.07347  loss_box_reg_stage1: 0.5214  loss_cls_stage2: 0.08088  loss_box_reg_stage2: 0.7246  loss_mask: 0.06526  loss_rpn_cls: 0.006078  loss_rpn_loc: 0.01714  time: 0.4260  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 13:11:55] d2.utils.events INFO:  eta: 13:11:30  iter: 11079  total_loss: 1.592  loss_cls_stage0: 0.09256  loss_box_reg_stage0: 0.1813  loss_cls_stage1: 0.07881  loss_box_reg_stage1: 0.4485  loss_cls_stage2: 0.07691  loss_box_reg_stage2: 0.6218  loss_mask: 0.06842  loss_rpn_cls: 0.004299  loss_rpn_loc: 0.01449  time: 0.4260  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 13:12:03] d2.utils.events INFO:  eta: 13:11:00  iter: 11099  total_loss: 1.673  loss_cls_stage0: 0.09149  loss_box_reg_stage0: 0.1885  loss_cls_stage1: 0.07682  loss_box_reg_stage1: 0.4941  loss_cls_stage2: 0.0863  loss_box_reg_stage2: 0.6826  loss_mask: 0.07292  loss_rpn_cls: 0.003062  loss_rpn_loc: 0.01774  time: 0.4259  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 13:12:12] d2.utils.events INFO:  eta: 13:11:08  iter: 11119  total_loss: 1.809  loss_cls_stage0: 0.08903  loss_box_reg_stage0: 0.2  loss_cls_stage1: 0.06493  loss_box_reg_stage1: 0.518  loss_cls_stage2: 0.06726  loss_box_reg_stage2: 0.7519  loss_mask: 0.06797  loss_rpn_cls: 0.004294  loss_rpn_loc: 0.01677  time: 0.4259  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 13:12:20] d2.utils.events INFO:  eta: 13:10:18  iter: 11139  total_loss: 1.746  loss_cls_stage0: 0.1026  loss_box_reg_stage0: 0.1939  loss_cls_stage1: 0.07534  loss_box_reg_stage1: 0.4991  loss_cls_stage2: 0.06561  loss_box_reg_stage2: 0.7281  loss_mask: 0.06777  loss_rpn_cls: 0.00281  loss_rpn_loc: 0.01499  time: 0.4259  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:12:29] d2.utils.events INFO:  eta: 13:10:06  iter: 11159  total_loss: 1.905  loss_cls_stage0: 0.09917  loss_box_reg_stage0: 0.2084  loss_cls_stage1: 0.07716  loss_box_reg_stage1: 0.514  loss_cls_stage2: 0.09478  loss_box_reg_stage2: 0.7833  loss_mask: 0.06381  loss_rpn_cls: 0.004948  loss_rpn_loc: 0.01605  time: 0.4259  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 13:12:37] d2.utils.events INFO:  eta: 13:10:43  iter: 11179  total_loss: 1.87  loss_cls_stage0: 0.1025  loss_box_reg_stage0: 0.2266  loss_cls_stage1: 0.0818  loss_box_reg_stage1: 0.5416  loss_cls_stage2: 0.07291  loss_box_reg_stage2: 0.7237  loss_mask: 0.0674  loss_rpn_cls: 0.003051  loss_rpn_loc: 0.01806  time: 0.4259  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 13:12:46] d2.utils.events INFO:  eta: 13:11:07  iter: 11199  total_loss: 1.703  loss_cls_stage0: 0.09046  loss_box_reg_stage0: 0.1786  loss_cls_stage1: 0.07467  loss_box_reg_stage1: 0.4677  loss_cls_stage2: 0.06769  loss_box_reg_stage2: 0.7458  loss_mask: 0.06213  loss_rpn_cls: 0.002501  loss_rpn_loc: 0.01561  time: 0.4259  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 13:12:54] d2.utils.events INFO:  eta: 13:11:15  iter: 11219  total_loss: 1.632  loss_cls_stage0: 0.08669  loss_box_reg_stage0: 0.1742  loss_cls_stage1: 0.06239  loss_box_reg_stage1: 0.4525  loss_cls_stage2: 0.0561  loss_box_reg_stage2: 0.6993  loss_mask: 0.06231  loss_rpn_cls: 0.001521  loss_rpn_loc: 0.0147  time: 0.4259  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 13:13:02] d2.utils.events INFO:  eta: 13:10:51  iter: 11239  total_loss: 1.699  loss_cls_stage0: 0.08976  loss_box_reg_stage0: 0.2074  loss_cls_stage1: 0.05475  loss_box_reg_stage1: 0.4856  loss_cls_stage2: 0.07284  loss_box_reg_stage2: 0.6843  loss_mask: 0.06836  loss_rpn_cls: 0.00388  loss_rpn_loc: 0.01751  time: 0.4259  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 13:13:11] d2.utils.events INFO:  eta: 13:10:26  iter: 11259  total_loss: 1.634  loss_cls_stage0: 0.09271  loss_box_reg_stage0: 0.1907  loss_cls_stage1: 0.06478  loss_box_reg_stage1: 0.4817  loss_cls_stage2: 0.0558  loss_box_reg_stage2: 0.6103  loss_mask: 0.06553  loss_rpn_cls: 0.005129  loss_rpn_loc: 0.01565  time: 0.4258  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:13:19] d2.utils.events INFO:  eta: 13:10:17  iter: 11279  total_loss: 2.006  loss_cls_stage0: 0.1241  loss_box_reg_stage0: 0.2225  loss_cls_stage1: 0.08453  loss_box_reg_stage1: 0.5795  loss_cls_stage2: 0.0865  loss_box_reg_stage2: 0.8205  loss_mask: 0.07149  loss_rpn_cls: 0.003605  loss_rpn_loc: 0.01644  time: 0.4258  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:13:27] d2.utils.events INFO:  eta: 13:10:02  iter: 11299  total_loss: 1.783  loss_cls_stage0: 0.1074  loss_box_reg_stage0: 0.1973  loss_cls_stage1: 0.1047  loss_box_reg_stage1: 0.4956  loss_cls_stage2: 0.09059  loss_box_reg_stage2: 0.6816  loss_mask: 0.065  loss_rpn_cls: 0.007052  loss_rpn_loc: 0.01509  time: 0.4258  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:13:35] d2.utils.events INFO:  eta: 13:09:58  iter: 11319  total_loss: 1.906  loss_cls_stage0: 0.1158  loss_box_reg_stage0: 0.2138  loss_cls_stage1: 0.09204  loss_box_reg_stage1: 0.5432  loss_cls_stage2: 0.07844  loss_box_reg_stage2: 0.7921  loss_mask: 0.07267  loss_rpn_cls: 0.005995  loss_rpn_loc: 0.01672  time: 0.4258  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:13:44] d2.utils.events INFO:  eta: 13:10:08  iter: 11339  total_loss: 1.927  loss_cls_stage0: 0.1025  loss_box_reg_stage0: 0.2088  loss_cls_stage1: 0.07731  loss_box_reg_stage1: 0.549  loss_cls_stage2: 0.08694  loss_box_reg_stage2: 0.7102  loss_mask: 0.06622  loss_rpn_cls: 0.005491  loss_rpn_loc: 0.01743  time: 0.4258  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 13:13:52] d2.utils.events INFO:  eta: 13:10:20  iter: 11359  total_loss: 2.004  loss_cls_stage0: 0.109  loss_box_reg_stage0: 0.2207  loss_cls_stage1: 0.08765  loss_box_reg_stage1: 0.5743  loss_cls_stage2: 0.08343  loss_box_reg_stage2: 0.8542  loss_mask: 0.06811  loss_rpn_cls: 0.003198  loss_rpn_loc: 0.0172  time: 0.4258  data_time: 0.0044  lr: 0.00016  max_mem: 7560M
[09/18 13:14:00] d2.utils.events INFO:  eta: 13:10:11  iter: 11379  total_loss: 2.016  loss_cls_stage0: 0.1086  loss_box_reg_stage0: 0.2192  loss_cls_stage1: 0.07593  loss_box_reg_stage1: 0.5976  loss_cls_stage2: 0.07349  loss_box_reg_stage2: 0.8845  loss_mask: 0.0679  loss_rpn_cls: 0.001646  loss_rpn_loc: 0.01724  time: 0.4257  data_time: 0.0039  lr: 0.00016  max_mem: 7560M
[09/18 13:14:09] d2.utils.events INFO:  eta: 13:10:17  iter: 11399  total_loss: 1.896  loss_cls_stage0: 0.1041  loss_box_reg_stage0: 0.2136  loss_cls_stage1: 0.09439  loss_box_reg_stage1: 0.5521  loss_cls_stage2: 0.07546  loss_box_reg_stage2: 0.827  loss_mask: 0.07711  loss_rpn_cls: 0.00511  loss_rpn_loc: 0.01717  time: 0.4257  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 13:14:17] d2.utils.events INFO:  eta: 13:09:59  iter: 11419  total_loss: 1.971  loss_cls_stage0: 0.1055  loss_box_reg_stage0: 0.2041  loss_cls_stage1: 0.09303  loss_box_reg_stage1: 0.5469  loss_cls_stage2: 0.08726  loss_box_reg_stage2: 0.789  loss_mask: 0.07165  loss_rpn_cls: 0.004939  loss_rpn_loc: 0.0169  time: 0.4257  data_time: 0.0040  lr: 0.00016  max_mem: 7560M
[09/18 13:14:26] d2.utils.events INFO:  eta: 13:09:39  iter: 11439  total_loss: 1.767  loss_cls_stage0: 0.09329  loss_box_reg_stage0: 0.193  loss_cls_stage1: 0.05695  loss_box_reg_stage1: 0.4883  loss_cls_stage2: 0.06957  loss_box_reg_stage2: 0.7438  loss_mask: 0.06836  loss_rpn_cls: 0.00229  loss_rpn_loc: 0.01531  time: 0.4257  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 13:14:34] d2.utils.events INFO:  eta: 13:09:17  iter: 11459  total_loss: 1.853  loss_cls_stage0: 0.102  loss_box_reg_stage0: 0.1891  loss_cls_stage1: 0.06848  loss_box_reg_stage1: 0.5178  loss_cls_stage2: 0.07497  loss_box_reg_stage2: 0.7796  loss_mask: 0.07188  loss_rpn_cls: 0.005124  loss_rpn_loc: 0.01694  time: 0.4257  data_time: 0.0042  lr: 0.00016  max_mem: 7560M
[09/18 13:14:42] d2.utils.events INFO:  eta: 13:09:11  iter: 11479  total_loss: 1.954  loss_cls_stage0: 0.1116  loss_box_reg_stage0: 0.2099  loss_cls_stage1: 0.08362  loss_box_reg_stage1: 0.5244  loss_cls_stage2: 0.0804  loss_box_reg_stage2: 0.7627  loss_mask: 0.06586  loss_rpn_cls: 0.004924  loss_rpn_loc: 0.01712  time: 0.4256  data_time: 0.0043  lr: 0.00016  max_mem: 7560M
[09/18 13:14:51] d2.utils.events INFO:  eta: 13:09:00  iter: 11499  total_loss: 1.964  loss_cls_stage0: 0.1021  loss_box_reg_stage0: 0.2019  loss_cls_stage1: 0.08156  loss_box_reg_stage1: 0.5417  loss_cls_stage2: 0.07099  loss_box_reg_stage2: 0.7257  loss_mask: 0.06773  loss_rpn_cls: 0.00281  loss_rpn_loc: 0.01703  time: 0.4256  data_time: 0.0039  lr: 0.00016  max_mem: 7560M
[09/18 13:14:59] d2.utils.events INFO:  eta: 13:09:20  iter: 11519  total_loss: 1.847  loss_cls_stage0: 0.1031  loss_box_reg_stage0: 0.2047  loss_cls_stage1: 0.08906  loss_box_reg_stage1: 0.5332  loss_cls_stage2: 0.08864  loss_box_reg_stage2: 0.7748  loss_mask: 0.07178  loss_rpn_cls: 0.003499  loss_rpn_loc: 0.01877  time: 0.4256  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:15:08] d2.utils.events INFO:  eta: 13:09:30  iter: 11539  total_loss: 1.685  loss_cls_stage0: 0.09224  loss_box_reg_stage0: 0.189  loss_cls_stage1: 0.06738  loss_box_reg_stage1: 0.485  loss_cls_stage2: 0.06148  loss_box_reg_stage2: 0.7515  loss_mask: 0.06354  loss_rpn_cls: 0.003152  loss_rpn_loc: 0.01643  time: 0.4256  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:15:16] d2.utils.events INFO:  eta: 13:09:22  iter: 11559  total_loss: 1.572  loss_cls_stage0: 0.08273  loss_box_reg_stage0: 0.1811  loss_cls_stage1: 0.06778  loss_box_reg_stage1: 0.4714  loss_cls_stage2: 0.06701  loss_box_reg_stage2: 0.6716  loss_mask: 0.06383  loss_rpn_cls: 0.002072  loss_rpn_loc: 0.01532  time: 0.4256  data_time: 0.0041  lr: 0.00016  max_mem: 7560M
[09/18 13:15:25] d2.utils.events INFO:  eta: 13:09:55  iter: 11579  total_loss: 1.779  loss_cls_stage0: 0.1002  loss_box_reg_stage0: 0.1905  loss_cls_stage1: 0.07675  loss_box_reg_stage1: 0.5125  loss_cls_stage2: 0.06569  loss_box_reg_stage2: 0.7596  loss_mask: 0.06796  loss_rpn_cls: 0.003865  loss_rpn_loc: 0.01834  time: 0.4256  data_time: 0.0039  lr: 0.00016  max_mem: 7560M
